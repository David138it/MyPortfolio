Mail for feedback: David138it@gmail.com

Clouds
    Task:
    Практическая работа. Создание аккаунта
    Decision:
    В Профессии "Инженер облачных сервисов" вас ожидают не только теоретические уроки на платформе Яндекс Практикум, но и практические занятия в Yandex Cloud. Чтобы их выполнить, вам понадобится своё облако. Давайте его создадим!
    Для того, чтобы работать в облаке, нужен платёжный аккаунт. Если это ваш первый аккаунт в Yandex Cloud, то после его создания вы сможете активировать 60-дневный пробный период и получить стартовый грант. Это позволит вам выполнять практические работы, не тратя собственных средств.
    Размер гранта для резидентов России составляет 4 000 ₽. Этого должно хватить на прохождение всей программы. А в случае её успешного завершения обучения и соблюдения некоторых условий вы получите еще один грант, который можно будет использовать, чтобы применить полученные знания и навыки для решения ваших задач в облаке.
    Если у вас нет личного платёжного аккаунта:
    Откройте в браузере сайт cloud.yandex.ru. Если вы не авторизованы, в правом верхнем углу или на баннере нажмите кнопку Подключиться. - Войдите в свой аккаунт на Яндексе. - Если у вас его нет, то вам будет предложено создать новый Яндекс ID. При регистрации в Яндекс ID заполните все поля, в том числе номер телефона: он потребуется, чтобы создать платёжный аккаунт в Yandex Cloud. - Примите условия использования Yandex Cloud. - После аутентификации при первом входе в консоль управления вам будет предложено создать облако. Укажите название облака и нажмите кнопку Создать. - Вам будет назначена роль владельца — owner. - На этом этапе у вас есть облако, но нет платёжного аккаунта. Если вы перейдете в раздел Биллинг (в левом верхнем углу выберите меню Все сервисы -> Биллинг), то увидите, что в списке аккаунтов пока пусто. - Чтобы создать платёжный аккаунт, нажмите кнопку Создать аккаунт в разделе Биллинг или на стартовой странице. При создании аккаунта заполните все данные (выберите тип плательщика — Физическое лицо) и добавьте банковскую карту. Для проверки система спишет с неё небольшую сумму денег, а затем сразу вернёт на счёт. - Важно! Обязательно выберите опцию Включить пробный период. Если этого не сделать, то ваш платежный аккаунт будет сразу переведен в режим платного потребления. - Пробный период позволяет использовать ресурсы Yandex Cloud в ограниченном режиме в течение 60 дней. Потреблённые ресурсы оплачиваются из стартового гранта. После завершения пробного периода ваши ресурсы в облаке будут остановлены, а чтобы возобновить работу в полном объёме потребуется перейти на платную версию. - Важно! Не отвязывайте банковскую карту во время пробного периода: в этом случае облако заблокируется. Если вы не перейдёте на платную версию, средства с карты списываться не будут.
    Task:
    Практическая работа. Создание виртуальной машины и подключение к ней
    Decision:
    Создание ВМ
    На стартовой странице консоли управления выберите каталог для ВМ. По умолчанию при регистрации в Yandex Cloud создаётся каталог с именем default. 
    В списке сервисов выберите Compute Cloud.
    Нажмите кнопку Создать ВМ.
    В открывшемся окне нужно указать параметры ВМ. Подробно мы разберем их на следующем уроке, а пока предлагаем использовать наши рекомендации и значения по умолчанию. В блоке Базовые параметры укажите имя ВМ из строчных латинских букв и цифр. Поле Описание необязательное — его обычно заполняют, чтобы не запутаться, если ВМ несколько. Выберите из списка зону доступности ru-central1-a.
    В блоке Выбор образа/загрузочного диска на вкладке Операционные системы выберите систему, которая будет установлена в ВМ — Ubuntu 20.04.
    В блоке Диски оставьте значения по умолчанию: тип — HDD, размер — 15 ГБ.
    В блоке Вычислительные ресурсы оставьте значения по умолчанию: платформу Intel Ice Lake, гарантированную долю vCPU 100% и объём оперативной памяти (RAM) 2 ГБ.
    В блоке Сетевые настройки оставьте используемую по умолчанию подсеть. Публичный адрес и Внутренний адрес — выберите Автоматически.
    В блоке Доступ заполните поле Логин. Не указывайте идентификатор root или другие имена, зарезервированные операционной системой. Для операций, требующих прав суперпользователя, нужно будет использовать команду sudo.
    Чтобы иметь возможность подключиться к создаваемой ВМ по протоколу SSH понадобится пара SSH-ключей. Открытый ключ хранится на ВМ, а закрытый — у пользователя. В публичных образах Linux, предоставляемых Yandex Cloud, возможность подключения по SSH с использованием логина и пароля по умолчанию отключена.
    Как создать SSH-ключи? (В Linux/macOS или Windows 10/11)
    Откройте терминал в Linux/macOS или утилиту cmd.exe в Windows 10/11 и создайте пару ключей с помощью команды ssh-keygen:
     ssh-keygen -t rsa -b 2048
    После выполнения команды укажите имена файлов, куда сохранятся ключи, и введите пароль для закрытого ключа. По умолчанию используется имя id_rsa, ключи создаются в папке ~/.ssh текущего пользователя.
    Открытый ключ сохранится в файле <имя_ключа>.pub. Содержимое этого файла вставляется в поле SSH-ключ виртуальной машины.
    Как создать SSH-ключи? (В Windows 7/8)
    Если на вашем компьютере установлена Windows 7 или 8, то создать SSH-ключи можно с помощью приложения PuTTY. Как это сделать, рассказывается в документации.
    В поле SSH-ключ вставьте содержимое созданного открытого ключа. Убедитесь, что вставляете ключ без переносов строки.
    Например:
    ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAoIT+oFLFEHwNlGO71wZiamqHkzduK7V/B8ITxgLnddm725QZJbaO1JAUfaOryGckWqEHr0NQxZ+CfozLjtYwcYhnPfNs1vw7Ii5gnL4ne+Vu5Kl4f8rb+tOXAv6GAZIO1+05kB8K3nINfBkKFD1J0VmOr5P2MWy7aqdbyIqVJCH+YeU4SW5RGFPJbl5zGhlwSavVU0bgTYQmqWAOnR95bQVx1vRf4SyB003C8MYl8ccZ+emixM12eQPJ74fJyy1kKLRmU/IAlxyEiYESQglAaNQKN2ivnbfMaSVBnxMlYipxyeMDyCs8RD7zVUndTOJQg8PV7QVWqfAQjlY4uYlk8Q== rsa-key-20210429 
    Изменяя параметры ВМ, в блоке Тарифы и цены (справа) вы увидите, как меняется её месячная стоимость.
    Нажмите кнопку Создать ВМ.
    Развёртывание ВМ занимает несколько минут. Вы можете отслеживать этот процесс по смене статуса.
    Статус ВМ влияет на то, какие операции вы можете с ней выполнять. Например, статус Stopped означает, что машина остановлена и к ней невозможно подключиться. При запуске статус меняется на Provisioning (Yandex Cloud выделяет ВМ ресурсы), а после загрузки операционной системы — на Running.
    Теперь, когда ВМ создана и запущена, к ней можно подключиться. Для этого понадобятся логин пользователя и публичный IP-адрес ВМ, который можно скопировать из строки с информацией о ней на странице Виртуальные машины.
    Удалённое подключение к ВМ
    Чтобы подключиться к запущенной ВМ (статус RUNNING) по протоколу SSH, используют утилиту ssh в Linux/macOS или Windows 10/11, программу PuTTY в Windows 7/8 или любой другой клиент SSH.
    Подключение по SSH в Linux/macOS или Windows 10/11
    Откройте терминал в Linux/macOS или запустите PowerShell в Windows 10/11 и выполните команду:
    ssh <имя_пользователя>@<публичный_IP-адрес_ВМ> 
    Если у вас несколько закрытых ключей, укажите нужный:
    ssh -i <путь_к_ключу/имя_файла_ключа> <имя_пользователя>@<публичный_IP-адрес_ВМ> 
    При первом подключении может появиться предупреждение о неизвестном хосте:
    The authenticity of host '130.193.40.101 (130.193.40.101)' can't be established.
    ECDSA key fingerprint is SHA256:PoaSwqxRc8g6iOXtiH7ayGHpSN0MXwUfWHkGgpLELJ8.
    Are you sure you want to continue connecting (yes/no)? 
    Введите в терминале yes, нажмите Enter и вы окажетесь в консоли созданной ВМ.
    Установите обновления. Для этого запустите в консоли команды:
    sudo apt-get update
    sudo apt-get upgrade 
    ВМ готова к работе. Если вы не собираетесь делать большой перерыв в обучении, то не удаляйте её — она понадобится нам на следующем практическом занятии.
    Decision:
    $ ssh-keygen -t rsa -b 2048
    $ cat ubcloud.pub
    $ ssh -i ubcloud test138@178.154.223.142
    test138@linux-test:~$ sudo apt-get update
    test138@linux-test:~$ sudo apt-get upgrade
    Task:
    Практическая работа. Получаем доступ к серийной консоли
    Decision:
    Работа серийной консоли зависит от настроек операционной системы. Yandex Compute Cloud обеспечивает канал связи между пользователем и COM-портом ВМ, но не гарантирует стабильность работы консоли со стороны операционной системы ВМ.
    Доступ к серийной консоли ВМ с ОС на базе Linux возможен следующими способами:
        по протоколу SSH с другого компьютера;
        через консоль управления Yandex Cloud;
        с помощью интерфейса командной строки Yandex Cloud CLI. Подробнее работу с CLI мы рассмотрим в одной из следующих практических работ.
    Вход по протоколу SSH
        Подключитесь к ВМ по протоколу SSH. Установите пароль текущему пользователю с помощью утилиты passwd в привилегированном режиме:
    sudo passwd <имя_пользователя> 
    После ввода команды дважды наберите одинаковый пароль.
        Для доступа к серийной консоли ВМ необходимо знать её идентификатор (ID). В консоли управления перейдите в раздел Compute Cloud. По умолчанию откроется страница со списком ВМ. В столбце справа указан идентификатор каждой ВМ.
        Используйте для входа идентификатор ВМ и имя (логин) созданного в ней пользователя. Вот шаблон команды подключения для Linux:
    ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/<имя закрытого ключа> <ID виртуальной машины>.<имя пользователя>@serialssh.cloud.yandex.net 
        Вот так вы подключитесь к консоли, если в ВМ с ID fhm0b28lgfp4tkoa3jl6 есть пользователь yc-user:
    ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/id_rsa fhm0b28lgfp4tkoa3jl6.yc-user@serialssh.cloud.yandex.net 
        Введите установленный ранее пароль.
        Чтобы отключиться от серийной консоли, нажмите клавишу Enter, а затем введите символы ~. (тильда и точка). В терминалах Linux для отключения также можно использовать комбинацию клавиш Ctrl + D.
    Вход через консоль управления
    В консоли управления откройте страницу ВМ и через меню слева перейдите на страницу Серийная консоль. При авторизации используйте логин, указанный при создании ВМ, и пароль, который вы установили после подключения к ней.
    Decision:
    $ ssh -i ubcloud test138@51.250.93.58
    test138@linux-test:~$ sudo passwd test138
    test138@linux-test:~$ exit
    $ ssh -t -p 9600 -o IdentitiesOnly=yes -i ubcloud fhmppfei6vr0fgrqnd53.test138@serialssh.cloud.yandex.net
    Task:
    Практическая работа. Создаем ВМ с 5% vCPU и учимся использовать мониторинг
    Decision:
    Давайте создадим ВМ с гарантированной долей vCPU, равной 5%, и проверим её производительность.
    В консоли управления перейдите в раздел Compute Cloud и нажмите кнопку Создать ВМ. Заполните имя и описание, выберите операционную систему CentOS 7.
    В блоке Вычислительные ресурсы выберите платформу Intel Cascade Lake и укажите гарантированную долю vCPU 5%. Другие параметры оставьте по умолчанию.
    После создания и запуска ВМ в списке машин нажмите её название. Вы перейдёте на страницу ВМ. Затем на левой боковой панели выберите Мониторинг. Откроется страница, где в динамике показывается информация о загрузке процессора, операциях с диском и сетевой активности. По умолчанию видны данные за одни сутки (1d — 1 day).
    Переключитесь на один час: вверху слева нажмите 1h (1 hour).
    На графике видно, что при запуске использование процессорных ресурсов было высоким, а позже снизилось до приемлемого. Чтобы посмотреть точные значения в определённый момент, поместите указатель над линией графика. Вы увидите всплывающее окно с показателями для этой точки времени.
    Теперь удалите ВМ. Для этого сначала остановите её — вернитесь в список ВМ, отметьте нужную ВМ и на появившейся внизу контекстной панели нажмите Остановить.
    Во всплывающем окне подтвердите действие и нажмите кнопку Остановить. Дождитесь смены статуса на Stopped.
    Чтобы удалить ВМ, в списке ВМ справа напротив машины нажмите ... и в раскрывшемся меню выберите Удалить. Подтвердите действие. Через некоторое время ВМ будет удалена.
    Task:
    Практическая работа. Создаем снимок диска ВМ
    Decision:
    Допустим, вы планируете обновить ПО на виртуальной машине ВМ. Вы знаете, что взаимодействие приложений и системных сервисов после обновления может нарушиться, а данные могут быть повреждены. Поэтому хотите иметь резервную копию полностью работоспособной системы на случай неудачи.
    Давайте на практике разберём, как сделать снимок и восстановить из него ВМ при повреждении системы. Для этого используем ВМ, на которой развернута система на основе Ubuntu или CentOS.
    Целостность данных
    В первую очередь обеспечьте целостность данных. Для этого подключитесь к ВМ по SSH. Чтобы записать кеш операционной системы на диск, выполните команду sync (иначе изменения файлов, хранящиеся в оперативной памяти, будут потеряны). Диски в Linux монтируются в ОС в виде файлов. Чтобы узнать нужный файл устройства диска, выполните команду df -h для вывода полного списка устройств и соответствующих точек монтирования. Затем, чтобы заморозить файловую систему, выполните команду fsfreeze -f <точка монтирования>.
    Создание снимка
        В консоли управления откройте раздел Compute Cloud и перейдите на вкладку Диски. Справа от диска нажмите ... и выберите Создать снимок.
        В открывшемся окне вы увидите автоматически сформированное имя снимка диска. Оно состоит из идентификатора диска и идентификатора снимка. Вы можете переименовать снимок и заполнить его описание. После этого нажмите кнопку Создать.
        Откройте Снимки дисков. Как только снимок будет создан, статус операции сменится с Creating на Ready.
        Разморозьте файловую систему. Для этого в командной строке с интерфейсом подключения к ВМ по SSH выполните команду fsfreeze --unfreeze <точка монтирования>.
    Намеренное повреждение системы
        Теперь протестируйте обновление системы: в командной строке с интерфейсом подключения к ВМ по SSH последовательно выполните команды apt-get update и apt-get dist-upgrade. Дождитесь, пока обновление завершится. Важно! Перед выполнением следующей команды убедитесь в том, что находитесь в консоли именно тестовой ВМ.
        Сымитируйте повреждение системы: выполните команду sudo rm -rf --no-preserve-root /. Вы увидите предупреждение, что все данные на диске будут удалены. Подтвердите своё намерение.
    Восстановление из снимка
        Поскольку снимок создан с загрузочного диска, который всегда подключён к ВМ, для восстановления создайте новую ВМ вместо старой. При создании загрузочного диска машины выберите готовый снимок диска. Для этого в блоке Выбор образа/загрузочного диска перейдите на вкладку Пользовательские и нажмите кнопку Выбрать. В открывшемся окне перейдите на вкладку Снимок, выберите нужный снимок и нажмите кнопку Применить.
        Дождитесь завершения создания и запуска новой ВМ. Теперь старую ВМ можно остановить и удалить.
    Будьте внимательны при создании новых виртуальных машин: в облаке действуют квоты и лимиты на используемые ресурсы.
    Decision:
    $ ssh -i ubcloud test138@51.250.93.58
    test138@linux-test:~$ sync
    test138@linux-test:~$ df -h
    Filesystem      Size  Used Avail Use% Mounted on
    tmpfs           198M  1.1M  197M   1% /run
    /dev/vda2        15G  4.2G  9.9G  30% /
    tmpfs           988M     0  988M   0% /dev/shm
    tmpfs           5.0M     0  5.0M   0% /run/lock
    tmpfs           198M  4.0K  198M   1% /run/user/1000
    test138@linux-test:~$ sudo fsfreeze -f /
    test138@linux-test:~$ sudo fsfreeze --unfreeze /
    test138@linux-test:~$ sudo apt-get update
    test138@linux-test:~$ sudo apt-get dist-upgrade
    test138@linux-test:~$ sudo rm -rf --no-preserve-root /
    Task:
    С CLI вы познакомитесь в одной из следующих практических работ этого курса. Но если вам интересно, как с его помощью создать публичный образ, — загляните под кат.
    Decision:
        Установите и настройте Yandex Cloud CLI.
        В командной строке выполните команду:
    yc resource-manager folders list 
    Результат:
    +----------------------+---------+--------+--------+
    |          ID          |  NAME   | LABELS | STATUS |
    +----------------------+---------+--------+--------+
    | b1gdf1scqef5bpqrpo5j | default |        | ACTIVE |
    +----------------------+---------+--------+-------- 
    Скопируйте название каталога, где хранится образ. В примере это default.
        Назначьте системной группе allAuthenticatedUsers роль compute.images.user:
    yc resource-manager folder add-access-binding default \
      --role compute.images.user \
      --subject system:allAuthenticatedUsers 
    Готово. Вы открыли всем аутентифицированным пользователям Yandex Cloud доступ к папке с образом.
    Чтобы просмотреть список публичных образов, введите команду:
    yc compute image list --folder-id standard-images 
    Decision:
    $ ssh -i ubcloud test@51.250.88.153
    test@ubuntu-test:~$ curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
    Task:
    Практическая работа. Создание новой сети с подсетями и ВМ
    Decision:
    Облачные сети (Virtual Private Cloud, VPC) являются частью публичного облака, которая связывает пользовательские, инфраструктурные, платформенные и прочие ресурсы воедино, где бы они ни находились — в нашем облаке или за его пределами. При этом VPC позволяет не публиковать без необходимости эти ресурсы в интернете, они остаются в пределах вашей изолированной сети.
    Когда вы создаёте облако, в нём автоматически появляется сеть и подсети в каждой зоне доступности. Но иногда их бывает недостаточно. На этой практической работе вы научитесь вручную создавать сеть и добавлять подсети.
    Рассмотрим пример, как настроить облачную сеть, чтобы организовать работу сервера с доступом из публичной сети. Сначала создадим единую для всех ресурсов облака изолированную сеть с ВМ и другими объектами инфраструктуры.
        В консоли управления откройте раздел Virtual Private Cloud и нажмите кнопку Создать сеть. Заполните имя (пусть сеть называется yc) и описание. Оставьте выбранной опцию Создать подсети и нажмите кнопку Создать сеть. В результате появятся три подсети: yc-ru-central1-a, yc-ru-central1-b, yc-ru-central1-c.
        Для сервера создадим ещё одну подсеть с маской /28.
        В разделе Virtual Private Cloud перейдите на страницу сети yc и нажмите кнопку Добавить подсеть. Введите параметры: имя — yc-public, зона — ru-central-1a, CIDR — 192.168.0.0/28. Нажмите кнопку Создать подсеть. Доступом пользователей облака к сетевым ресурсам управляют с помощью назначения ролей.
        Теперь создайте ВМ с именем server. Убедитесь, что в блоке Базовые параметры выбрана зона доступности ru-central-1a. В качестве образа выберите Ubuntu 20.04, в блоке Сетевые настройки выберите подсеть yc-public. В блоке Доступ введите логин user и вставьте открытый SSH-ключ в соответствующее поле .
    Чтобы ВМ полноценно заработала, организуйте доступ в интернет. Есть три способа:
        Назначить машине публичный IP-адрес.
        Включить NAT для подсети.
        Установить NAT-сервер и создать соответствующий маршрут. С точки зрения безопасности лучше выбрать способ 2 или 3, чтобы на сервер не было прямого доступа из интернета. Но в таком случае придётся установить бастион-хост: отдельный сервер, задача которого — противостоять атакам извне. В нашем примере для простоты присвоим серверу публичный адрес. Для этого при создании ВМ выберите автоматический способ назначения IP-адреса.
    После создания ВМ, проверьте доступность сервера, чтобы убедиться, корректно ли настроена сетевая конфигурация. Для этого на странице с информацией о ВМ в блоке Сеть найдите публичный IP-адрес сервера:
    Откройте интерфейс командной строки и введите команду:
        ping 51.250.78.143 
    Если конфигурация корректна, в результаты выполнения команды ping вы увидите:
        Ping statistics for 51.250.78.143:
        Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
        Approximate round trip times in milli-seconds:
        Minimum = 8ms, Maximum = 9ms, Average = 8ms 
    Такой пример конфигурации подходит для небольшого веб-сервера. Если вы собираетесь строить озеро данных или обрабатывать математические вычисления, не рекомендуется давать к ресурсам прямой доступ из интернета — разместите их за NAT.
    Task:
    Практическая работа. Создаем группу безопасности и открываем доступ к серверу
    Decision:
    Давайте попробуем создать группу безопасности и сделать доступными страницы, предоставляемые веб-сервером NGINX.
        Перейдите по ссылке https://console.cloud.yandex.ru/link/vpc/security-groups и нажмите кнопку Создать группу.
        Введите имя группы yc-security и выберите сеть yc (вы создали её на одном из предыдущих практических занятий).
        В блоке Правила добавьте правила для исходящего трафика. Опишите правило, укажите диапазон портов 80 (HTTP) и протокол TCP, выберите назначение "CIDR" 0.0.0.0/0 . Создайте аналогичные исходящие правила для портов 443 (HTTPS) и 22 (SSH). Для входящего трафика добавьте правила для 80 и 22 портов, чтобы подключаться к веб-серверу и управлять ВМ извне. Вы увидите созданную группу в списке групп безопасности. Если инициировано соединение по определенному порту и протоколу с ВМ и есть исходящее правило, то значит и на входящий трафик будет разрешена передача данных в эту же сеть, на этот же протокол и порт. Если назначить сетевому интерфейсу ВМ группу безопасности без правил, ВМ не сможет передавать и принимать трафик.
        Создайте виртуальную машину на базе Ubuntu 20.04, выберите сеть yc и вновь созданную группу безопасности yc-security в сетевых настройках создаваемой ВМ, дождитесь запуска машины, подключитесь к ВМ по SSH и установите веб-сервер NGINX (по умолчанию он отсутствует). Для этого выполните команду:
    sudo apt-get install nginx 
        Установка возможна, поскольку вы открыли порт 80: команда apt-get использует его для получения пакетов с ПО.
        После установки сервер автоматически запустится и будет доступен извне благодаря открытому порту 80. Проверьте это, зайдя через браузер на публичный IP-адрес ВМ, который найдёте на странице параметров машины в консоли управления.
    Decision:
    $ ssh -i ubcloud testuser@51.250.66.47
    testuser@ubuntu-test2:~$ sudo apt-get install nginx
        51.250.66.47
    Task:
    Практическая работа. Знакомство с Yandex Cloud CLI
    Decision:
    На этом практическом занятии вы научитесь быстрее создавать веб-серверы, чтобы затем помещать их за сетевой балансировщик.
    В качестве веб-серверов выступят две ВМ, на которых доступна информационная страница запущенного веб-сервера NGINX. Для реальных проектов с высокой нагрузкой вам, скорее всего, придётся создавать гораздо больше ВМ, поэтому вам пригодится умение делать это автоматически: с помощью консольного интерфейса Yandex Cloud CLI. Подробнее о работе с CLI поговорим в рамках другого курса. Здесь же остановимся на простом примере применения этого инструмента.
        Установите и настройте Yandex Cloud CLI, следуя инструкциям в документации.
        Создайте файл startup.sh , который будет запускаться на ВМ после её создания, со следующим содержимым.
    #!/bin/bash
    apt-get update
    apt-get install -y nginx
    service nginx start
    sed -i -- "s/nginx/Yandex Cloud - ${HOSTNAME}/" /var/www/html/index.nginx-debian.html
    EOF 
    Скрипт получает список актуальных пакетов с софтом, устанавливает и запускает NGINX, а затем меняет информационную страницу работающего сервера.
        Создайте первую ВМ с помощью команды yc compute instance create. Чтобы указать, какую именно ВМ мы хотим создать, в команде используются параметры. В данном случае — имя ВМ (--name demo-1), откуда взять метаданные (из файла скрипта startup.sh, созданного ранее), из какого образа создать загрузочный диск (ubuntu-2004-lts), в какой зоне создать машину (--zone ru-central1-a) и в какую подсеть подключить сетевой интерфейс с IPv4-адресом (--network-interface ...).
    yc compute instance create \
    --name demo-1 \
    --metadata-from-file user-data=startup.sh \
    --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
    --zone ru-central1-a \
    --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 
        Самостоятельно измените и запустите еще раз эту команду, чтобы создать такую же ВМ с именем demo-2.
        Теперь проверим, что обе ВМ успешно созданы. Это можно сделать, заглянув в консоль управления:
        А можно и из командной строки с помощью команды yc compute instance list. Попробуйте этот способ. У вас должен получиться примерно такой результат:
    +----------------------+--------+---------------+---------+----------------+-------------+
    |          ID          |  NAME  |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
    +----------------------+--------+---------------+---------+----------------+-------------+
    | fhmnc7nireh12e25ctk0 | demo-1 | ru-central1-a | RUNNING | 84.201.174.97  | 10.130.0.18 |
    | fhmrbmchmu3ganplskp2 | demo-2 | ru-central1-a | RUNNING | 84.252.129.231 | 10.130.0.31 |
    +----------------------+--------+---------------+---------+----------------+-------------+ 
        Убедитесь, что статус машин сменился на Running, а скрипт выполнился (иногда нужно подождать до одной минуты, пока обновится список пакетов и установится NGINX).
        Введите публичные IP-адреса ВМ в браузере и проверьте, что главные страницы веб-серверов доступны:
    Decision:
    $ curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
    $ yc init
    ...
    Please enter OAuth token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
    You have one cloud available: 'cloud-test138it' (id = b1gg01f1vt0rkid9qsuk). It is going to be used by default.
    Please choose folder to use:
     [1] default (id = b1geto6411pvmr7j3pd2)
     [2] Create a new folder
    Please enter your numeric choice: 1
    Your current folder has been set to 'default' (id = b1geto6411pvmr7j3pd2).
    Do you want to configure a default Compute zone? [Y/n] y
    Which zone do you want to use as a profile default?
     [1] ru-central1-a
     [2] ru-central1-b
     [3] ru-central1-c
     [4] Don't set default zone
    Please enter your numeric choice: 1
    Your profile default Compute zone has been set to 'ru-central1-a'.
    $ yc config list
    token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
    cloud-id: b1gg01f1vt0rkid9qsuk
    folder-id: b1geto6411pvmr7j3pd2
    compute-default-zone: ru-central1-a
    $ vim startup.sh
    $ cat startup.sh
    #!/bin/bash
    apt-get update
    apt-get install -y nginx
    service nginx start
    sed -i -- "s/nginx/Yandex Cloud - ${HOSTNAME}/" /var/www/html/index.nginx-debian.html
    EOF
    $ yc compute instance create \
    > --name demo-1 \
    > --metadata-from-file user-data=startup.sh \
    > --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
    > --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
    $ yc compute instance create \
    > --name demo-2 \
    > --metadata-from-file user-data=startup.sh \
    > --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
    > --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
    $ yc compute instance list
    Conclusion:
    62.84.114.54
    http://51.250.79.17/
    Task:
    Практическая работа. Создание балансировщика
    Decision:
    Итак, у вас есть виртуальные машины. Можно сразу создать и балансировщик, и целевую группу, но мы поступим иначе: сначала создадим целевую группу, затем подключим её к балансировщику.
    В консоли управления откройте раздел Network Load Balancer, на вкладке Целевые группы нажмите кнопку Создать целевую группу. На открывшейся странице введите имя целевой группы (например demo-web), выберите обе ВМ, созданные на предыдущем уроке, и нажмите кнопку Создать.
    Остаётся создать балансировщик. Для этого сначала создайте обработчик и настройте проверку состояния ресурсов в целевой группе:
        На вкладке Балансировщики нажмите кнопку Создать сетевой балансировщик.
        Заполните имя балансировщика (например lb-demo-web) и нажмите кнопку Добавить обработчик.
        В открывшемся окне введите имя обработчика (например demo-web-listener). В качестве портов укажите 80и нажмите кнопку Добавить.
        После создания обработчика нажмите кнопку Добавить целевую группу. Укажите имя проверки состояния (например hc-demo-web), тип проверки (HTTP), порт (80), интервал отправки проверок состояния в секундах, порог работоспособности и порог неработоспособности. Оставьте указанный по умолчанию путь для проверок, используйте значения по умолчанию и для других параметров. Нажмите кнопку Применить, а затем кнопку Создать.
        После создания балансировщика проверьте состояние ресурсов: в консоли управления откройте страницу балансировщика и убедитесь, что его статус — Active. Значит, балансировщик готов передавать трафик целевым ресурсам.
        Перейдите на страницу балансировщика и посмотрите на блок Целевые группы. У запущенных ВМ, готовых принимать трафик, будет статус Healthy.
        Введите внешний IP-адрес балансировщика в адресную строку браузера — и балансировщик перенаправит вас на одну из машин целевой группы. Обратите внимание на имя ВМ, указанное во второй строке веб-страницы.
        Чтобы протестировать отказоустойчивость, в консоли управления перейдите в раздел Compute Cloud и остановите одну из ВМ целевой группы.
        Вернитесь на страницу балансировщика и убедитесь, что статус остановленной ВМ изменился на Unhealthy. Это означает, что целевой ресурс группы не прошёл проверку состояния и не готов принимать трафик.
        Обновите страницу с IP-адресом балансировщика, и вы увидите, что трафик перенаправлен на другую ВМ (изменилось имя ВМ, указанное во второй строке веб-страницы).
    После завершения работы не забудьте удалить использованные ресурсы: две ВМ и балансировщик.
    Task:
    Практическая работа. Создание группы виртуальных машин
    Decision:
    Иногда вам требуется не автоматическое масштабирование, а автоматическое восстановление ВМ. Например, если вы отлаживаете работу веб-сервиса, который периодически падает. Для этого подойдут группы ВМ фиксированного размера. Давайте создадим и настроим такую группу.
    В консоли управления откройте раздел Compute Cloud, перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу.
    Откроется страница Создание группы виртуальных машин.
    В блоке Базовые параметры введите имя и описание группы ВМ. Создайте новый сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
    ВМ группы могут находиться в разных зонах и регионах. В блоке Распределение выберите две зоны доступности, чтобы обеспечить доступность сервиса, если в одной из них случится сбой.
    В блоке Шаблон виртуальной машины нажмите кнопку Задать.
    Шаблон создается так же, как и сама ВМ. В блоке Базовые параметры введите описание шаблона конфигурации, затем в блоке Выбор образа/загрузочного диска на вкладке Операционные системы выберите Ubuntu.
    В блоках Диски и Вычислительные ресурсы для загрузочного диска оставьте значения по умолчанию. В блоке Сетевые настройки выберите существующую сеть и подсеть или создайте новые. В блоке Доступ выберите существующий или создайте новый сервисный аккаунт, укажите логин, вставьте в поле SSH-ключ содержимое файла с публичным ключом, доступ к серийной консоли не разрешайте.
    Сохраните параметры и вы вернётесь на страницу Создание группы виртуальных машин.
    В блоке В процессе создания и обновления разрешено установите политику развертывания:
        Добавлять выше целевого значения (на сколько ВМ можно превышать размер группы) — 2.
        Уменьшать относительно целевого значения (на сколько ВМ можно уменьшать размер группы) — 1.
        Одновременно создавать (сколько ВМ можно сразу создавать в группе) — 2.
        Время запуска (сколько времени должно пройти, прежде чем будут пройдены все проверки состояния и ВМ начнет получать нагрузку) — 2 минуты.
        Одновременно останавливать (сколько ВМ можно сразу удалять) — 1.
        Останавливать машины по стратегии — Принудительная. При принудительной стратегии Instance Groups самостоятельно выбирает, какие ВМ остановить.
    В блоке Масштабирование выберите фиксированный тип, Размер (количество ВМ) — 3.
    В блоке Интеграция с Load Balancer оставьте опцию Создать целевую группу выключенной. Не включайте пока проверку состояний, которая позволяет Instance Groups получать сведения о состоянии ВМ.
    Нажмите кнопку Создать и вернитесь на страницу Группы виртуальных машин. В правом нижнем углу появится сообщение «Группа виртуальных машин создаётся». Одновременно можно создавать не более двух ВМ. Поэтому сначала будут созданы две ВМ, потом — третья.
    После того как вы создали группу, протестируйте включение и выключение всех машин сразу. Обратите внимание: в соответствии с настройками сервис инициирует запуск не более двух машин одновременно. Третья ВМ будет оставаться остановленной. Как только первая будет запущена, один слот на запуск освободится, поэтому сразу будет инициирован запуск третьей и последней ВМ.
    Task:
    Практическая работа. Автоматическое масштабирование под нагрузкой
    Decision:
    Давайте разберёмся, как обеспечить доступность сервиса под высокой нагрузкой. Вы уже научились создавать группы ВМ. Теперь создадим автоматически масштабируемую группу ВМ.
    В консоли управления откройте раздел Compute Cloud. Перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу. Задайте имя группе ВМ.
    Создайте сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
    В блоке Распределение выберите только одну зону доступности.
    В блоке Шаблон виртуальной машины нажмите кнопку Задать. В открывшемся окне выберите:
        ОС: Ubuntu 20.04.
        Размер загрузочного диска: 50 ГБ.
        Тип загрузочного диска: SSD.
        Остальные параметры — по умолчанию. Не забудьте добавить публичный SSH-ключ. Он понадобится нам на следующем практическом занятии.
    В блоке В процессе создания и обновления разрешено оставьте параметры по умолчанию.
    Перейдите к блоку Масштабирование и выберите тип Автоматический.
    Задайте параметры масштабирования:
        Тип автомасштабирования — зональное. При зональном автомасштабировании количество ВМ регулируется отдельно в каждой зоне доступности, указанной в настройках группы.
        Минимальное количество ВМ в зоне — 2. Сервис Instance Groups не будет удалять ВМ в зоне доступности, если их там всего две.
        Максимальный размер группы — 4. Instance Groups не будет создавать ВМ, если их уже четыре. В этот раз размер загрузочного диска ВМ — 50 ГБ, поэтому с учётом квот на суммарный объём SSD-дисков в одном облаке смогут запуститься четыре ВМ.
        Промежуток измерения загрузки (это период усреднения: время, за которое следует усреднять замеры нагрузки для каждой ВМ в группе) — 60 секунд.
        Время на разогрев ВМ — 3 минуты. В течение этого времени ВМ не учитывается в измерении средней нагрузки на группу. Фактически данное время мы можем определить, измерив, как быстро запускается ВМ.
        Период стабилизации — 5 минут. Отсчитывается с момента, когда Compute Cloud принял последнее решение о том, что количество ВМ в группе нужно увеличить.
        Начальный размер группы — 4. Это количество ВМ, которое следует создать вместе с группой.
    В блоке Метрики укажите:
        Тип — CPU.
        Целевой уровень загрузки CPU, % — 80. Instance Groups будет управлять размером группы так, чтобы поддерживать указанную нагрузку CPU.
    Нажмите кнопку Создать. Сервис начнет создавать ВМ. После создания статус группы изменится на Active. Обратите внимание, как меняются Состояния ВМ.
        Creating instance — ВМ создаётся и запускается.
        Awaiting warmup duration — ВМ начинает принимать сетевой трафик. В этом статусе ВМ находится в течение периода прогрева, указанного в настройках автоматического масштабирования. Значения метрик ВМ в этом статусе заменяются средними значениями ВМ из той же зоны доступности.
        Running actual — ВМ запущена, на неё подается сетевой трафик, пользовательские приложения работают.
    Группа ВМ готова принимать рабочую нагрузку.
    Task:
    Практическая работа. Воссоздание виртуальных машин в группе
    Decision:
    Давайте сымитируем рост нагрузки на ВМ и посмотрим, как сервис на это отреагирует.
        В консоли управления откройте раздел Compute Cloud и перейдите на страницу группы ВМ, которую вы создали на прошлом практическом занятии. Откройте в двух вкладках браузера страницы Группы виртуальных машин и Мониторинг (открывается из раздела Группы виртуальных машин).
        После запуска группы зайдите по SSH на каждую из двух ВМ и установите приложение для стресс-тестирования Linux-систем. Для этого выполните команду:
    sudo apt-get install stress 
        После этого для каждой ВМ запустите установленное приложение:
    stress -c 2 
    Аргумент -c означает, что при тестировании будет нагружаться процессор, а число после аргумента задаёт количество ядер процессора, которые будут нагружаться. Чтобы эксперимент удался — укажите количество ядер, которое вы выбрали в шаблоне ВМ.
        На вкладке со страницей мониторинга на графике Average CPU utilization in ru-central1-a следите за тем, как усреднённое значение нагрузки будет постепенно расти.
    Как только усреднённое значение нагрузки превысит порог, сервис Instance Groups начнёт прогревать две дополнительные ВМ и вводить их в строй. Это будет видно на странице Группы виртуальных машин.
    Поскольку стресс-тест не остановлен, сервис завершает запуск двух ВМ.
    Через некоторое время усреднённое значение нагрузки процессоров в группе упадёт до 50%, поскольку первая половина ВМ загружена полностью, а вторая не загружена вовсе.
        Остановите работу стресс-теста на первой ВМ. В командной строке используйте сочетание клавиш Ctrl + C.
    Через некоторое время усреднённое значение достигнет 25%, тогда Instance Groups удалит лишнюю ВМ:
        Остановите второй стресс-тест. Через некоторое время после того, как усредненное значение достигнет нуля, Instance Groups удалит вторую дополнительную ВМ.
        При минимальной нагрузке остаются работать две машины:
    Вот так при растущей нагрузке группа ВМ автоматически масштабируется, чтобы обеспечить доступность ресурса.
    Decision:
    $ ssh -i ubcloud newu@178.154.207.23
    $ sudo apt-get install stress 
    $ ssh -i ubcloud newu@178.154.200.224
    $ sudo apt-get install stress 
    Task:
    Практическая работа. Создание бакетов и загрузка объектов
    Decision:
    Потренируемся работать с объектным хранилищем на практике. Представьте, что вы создаёте облачную систему хранения рентгеновских снимков для крупной клиники.
    Рентгеновские снимки — это неструктурированные данные, которые нельзя изменять, нужно надежно хранить и легко находить. Загруженные файлы будут скачивать нечасто. Также важно предоставлять доступ к файлам другим клиникам (это пригодится, если пациента переводят или врачу надо посоветоваться с коллегами). Объектное хранилище — подходящее решение задачи.
    Выберите на стартовой странице консоли управления сервис Object Storage.
    Давайте создадим бакет для рентгеновских снимков.
    Нажмите кнопку Создать бакет. Откроется окно с основными параметрами: 
    Имя. Придумайте его с учетом правил. Обратите внимание, что дать бакету имя hospital не получится. Имена бакетов во всем Yandex Object Storage уникальны — назвать два бакета одинаково нельзя даже в разных облаках. Помните об этом, если будете создавать бакеты автоматически.
    Макс. размер. У вас есть два варианта:
        Выбрать опцию Без ограничения. Размер бакета будет увеличиваться, сколько бы объектов в него ни помещали.
        Указать максимальный размер. Это убережёт вас от финансовых потерь, если что-то пойдёт не так и в бакет загрузится слишком много объектов.
    Другие опции. Далее для всех типов операций оставьте ограниченный доступ (публичный позволяет выполнять операции всем пользователям интернета), выберите стандартный класс хранилища и нажмите кнопку Создать бакет.
    На странице объектного хранилища появился пустой бакет. Мы приготовили два рентгеновских снимка: image01.dat и image02.dat. Файлы можно загрузить в бакет с помощью:
        консоли управления;
        приложений;
        S3-совместимого HTTP API;
        HTML-форм на сайте.
    Разберём два способа: ручную загрузку через консоль управления и автоматическую с помощью утилиты S3cmd.
    Для загрузки файла через консоль управления выберите созданный бакет и в открывшемся окне нажмите кнопку Загрузить объекты.
    Выберем файл image01.dat. В появившейся форме нажмите кнопку Загрузить — и вы увидите, что файл оказался в хранилище.
    Загрузите второй файл с помощью утилиты S3cmd — консольного клиента для Linux и MacOS, предназначенного для работы с S3-совместимым HTTP API. Для работы в Windows используйте один из вариантов:
        установите другой консольный клиент для объектных хранилищ, например AWS CLI;
        установите подсистему Linux на Windows с помощью утилиты WSL (Windows Subsystem for Linux) и работайте с S3cmd в ней;
        создайте в облаке виртуальную машину с Ubuntu и работайте с S3cmd в ней. Для загрузки файла-примера в виртуальную машину воспользуйтесь командой:
    $ wget "https://disk.yandex.ru/i/2UlugGkurhcxWw" -O image02.dat 
    Установите S3cmd (в Ubuntu, например, это делается с помощью команды sudo apt-get install s3cmd). Теперь настройте S3cmd для работы с Yandex Object Storage:
    $ s3cmd --configure
    Инструкции о настройке клиента вы найдете в документации.
    После ввода параметров утилита попытается установить соединение с объектным хранилищем и в случае успеха покажет такое сообщение: Success. Your access key and secret key worked fine :-)
    Загрузим в бакет второй файл (image02.dat) и затем получим список хранящихся в бакете объектов:
    s3cmd put <путь к второму файлу>/image02.dat s3://<имя бакета>
    s3cmd ls s3://<имя бакета>
    Вернемся в консоль управления.
    Мы видим, что класс хранилища у обоих объектов — стандартное. Напомним: стандартное хранилище подходит для данных, к которым обращаются часто, а тариф за размещение данных в нем примерно в два раза выше, чем в холодном хранилище.
    Спустя несколько недель после того, как рентгеновский снимок сделан, к нему будут редко обращаться (если вообще будут), потому что пациент, скорее всего, выздоровеет.
    Чтобы оптимизировать затраты на хранение данных, настроим жизненный цикл объектов в бакете. Создадим правило, согласно которому через 30 дней после загрузки объектов в бакет класс их хранилища будет автоматически меняться со стандартного на холодное.
    Перейдите на вкладку Жизненный цикл и нажмите кнопку Настроить. Задайте произвольное описание. В поле Префикс укажите Все объекты. Выберите тип операции Transition. В качестве Условия срабатывания правила задайте Точную дату или Количество дней. В первом случае правило сработает в 00:00 установленной даты. Во втором — через указанное количество дней после загрузки объекта в бакет.
    Если понадобится настроить автоматическое удаление объектов, выберите тип операции Expiration. Нажмите кнопку Сохранить.
    Представим теперь, что объекты в хранилище — это оцифрованные рентгеновские снимки пациента Петрова. Первый из них (image01.dat) сделали несколько месяцев назад в ходе профосмотра, а второй (image02.dat) — вчера, после того как Петров обратился к врачу с жалобой на недомогание. В обоих случаях на снимках не увидели патологий.
    Опишите с помощью пользовательских метаданных эти снимки, и позже вы быстро найдете их среди множества объектов в бакете.
    С помощью утилиты S3cmd задайте для загруженных объектов метаданные с фамилией пациента (x-amz-meta-patient:petrov) и с результатами обследования (x-amz-meta-status:ok):
     s3cmd modify --add-header=x-amz-meta-patient:petrov --add-header=x-amz-meta-status:ok s3://hospital/image01.dat s3://hospital/image02.dat
    Выведите на экран информацию об этих объектах, чтобы проверить, что получилось:
     s3cmd info s3://hospital/image01.dat s3://hospital/image02.dat
    В результате вы должны увидеть информацию об объектах в бакете.
    s3://hospital/image01.dat (object):
       File size: 33
       Last mod:  Thu, 04 Mar 2021 22:05:31 GMT
       MIME type: application/x-www-form-urlencoded
       Storage:   STANDARD
       MD5 sum:   6f6d5a1cb79839e523582ed8810a42fd
       SSE:       none
       Policy:    none
       CORS:      none
       x-amz-meta-patient: petrov
       x-amz-meta-status: ok
    s3://hospital/image02.dat (object):
       File size: 16
       Last mod:  Thu, 04 Mar 2021 23:11:27 GMT
       MIME type: text/plain
       Storage:   STANDARD
       MD5 sum:   0366a1d19e584ce79d5c05ddedc69310
       SSE:       none
       Policy:    none
       CORS:      none
       x-amz-meta-patient: petrov
       x-amz-meta-status: ok 
    Предположим, Петров чувствует себя хуже. Судя по анализам, он действительно болен. Лечащий врач решает проконсультироваться с более опытной коллегой Ивановой из профильной клиники. Объекты в бакете недоступны для внешних пользователей, поскольку при его создании мы ограничили доступ. Чтобы Иванова увидела рентгеновский снимок Петрова, отправим ей временную ссылку на объект image02.dat.
    Для этого в консоли управления кликните на объект и в открывшемся окне информации об объекте нажмите кнопку Получить ссылку. Укажите время жизни ссылки в часах или днях.
    Можно поделиться ссылкой или использовать ее в любом сервисе для доступа к файлу.
    После консультации Иванова поставила Петрову правильный диагноз: вирусная пневмония (код J12 по Международной классификации болезней). Вам осталось исправить метаданные объекта image02.dat. Замените значение метаданных с результатами обследования с ok на J12 самостоятельно.
    Decision:
    $ sudo apt-get install s3cmd
    $ s3cmd --configure
    Decision:
    %(bucket)s.storage.yandexcloud.net
    Идентификатор ключа:
    YCAJEuBFLYg8HHAHiZTgHJJv5
    Ваш секретный ключ:
    YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
    Decision:
    $ s3cmd --configure
    Access Key: YCAJEuBFLYg8HHAHiZTgHJJv5
    Secret Key: YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
    Default Region [US]: ru-central1
    S3 Endpoint [s3.amazonaws.com]: storage.yandexcloud.net
    DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.storage.yandexcloud.net
    Encryption password: 1992
    Path to GPG program [/usr/bin/gpg]:
    Use HTTPS protocol [Yes]:
    HTTP Proxy server name:
    New settings:
      Access Key: YCAJEuBFLYg8HHAHiZTgHJJv5
      Secret Key: YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
      Default Region: ru-central1
      S3 Endpoint: storage.yandexcloud.net
      DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.storage.yandexcloud.net
      Encryption password: 1992
      Path to GPG program: /usr/bin/gpg
      Use HTTPS protocol: True
      HTTP Proxy server name:
      HTTP Proxy server port: 0
    Test access with supplied credentials? [Y/n] Y
    Please wait, attempting to list all buckets...
    Success. Your access key and secret key worked fine :-)
    Now verifying that encryption works...
    Success. Encryption and decryption worked fine :-)
    Save settings? [y/N] Y
    $ s3cmd put /home/administrator/image02.dat s3://bucket-testt
    $ s3cmd ls s3://bucket-testt
    $ s3cmd modify --add-header=x-amz-meta-patient:petrov --add-header=x-amz-meta-status:ok s3://hospital/image01.dat s3://hospital/image02.dat
    $ s3cmd info s3://hospital/image01.dat s3://hospital/image02.dat
    Task:
    Практическая работа. Хранение статических веб-сайтов в Object Storage
    Decision:
    Представьте, что вам нужно выбрать оптимальный хостинг для сайта клиники. Главные критерии: отказоустойчивый, недорогой и простой в обслуживании. Один из вариантов решения такой задачи — использовать объектное хранилище. Вы можете, не настраивая никаких серверов, просто загрузить HTML-файлы, скрипты, стили и другие файлы в хранилище. Пользователи будут открывать в браузере ваш сайт, а по сути — скачивать файлы прямо из бакета.
    Важно понимать, что этот вариант подойдет только для полностью статических сайтов. Иными словами, сайт должен быть сделан с помощью клиентских технологий (HTML, CSS и JavaScript) и не требовать запуска чего-либо на стороне веб-сервера.
    Предположим, что сайт нашей клиники как раз такой — полностью статический. Опубликуем его с помощью объектного хранилища. Прежде всего создадим бакет:
    Обратите внимание на несколько особенностей:
        Если вы планируете использовать собственный домен (например www.example.com), то присвойте бакету точно такое же имя.
        Откройте публичный доступ на чтение объектов. Это позволит пользователям интернета скачивать объекты из бакета и просматривать сайт в браузере.
    Если публичный доступ открыт, то рядом с именем бакета в списке появится соответствующая надпись
    Теперь загрузите в бакет файлы сайта (например, этот и этот) любым удобным способом.
    Чтобы настроить хостинг, войдите в бакет в консоли управления. На левой панели выберите вкладку Веб-сайт и на открывшейся странице включите Хостинг.
    Укажите файл с главной страницей сайта (как правило, это index.html), а поле со страницей ошибки можно не заполнять.
    Сохраните настройки, и сайт станет доступен по адресам:
        http(s)://<имя_бакета>.website.yandexcloud.net
        http(s)://website.yandexcloud.net/<имя_бакета>
    По умолчанию сайт будет доступен только по протоколу HTTP. Для поддержки HTTPS нужно загрузить в объектное хранилище собственный сертификат безопасности. Как это сделать, рассказано в документации.
    Также о доступе к статическому сайту по протоколу HTTPS вы узнает из курса Безопасность.
    Теперь пациенты точно вас найдут!
    Если у вас есть собственный домен и вы хотите опубликовать сайт на нём, то настройте CNAME-запись у DNS-провайдера или на своем DNS-сервере. Например, для домена www.example.com CNAME-запись выглядела бы так:
    www.example.com CNAME www.example.com.website.yandexcloud.net 
    В этом случае можно использовать домены не ниже третьего уровня (то есть использовать домен example.com не получится, только www.example.com). Это связано с особенностями обработки CNAME-записей на DNS-хостингах.
    Decision:
    $ s3cmd put /home/administrator/index.html s3://www.aibolit38.ru
    Conclusion:
    http://www.aibolit38.ru.website.yandexcloud.net
    Task:
    Практическая работа. Создание кластера базы данных MySQL
    Decision:
    Object Storage — удобный и полезный инструмент для хранения данных в облаке. Но для решения практических задач важно не просто хранить данные, но и иметь возможность их изменять и выполнять с ними различные операции (сортировать, группировать, делать выборки и так далее). Для этого используются базы данных. В этой и следующих темах вы научитесь работать с несколькими управляемыми БД. И начнем мы с одной из самых популярных — MySQL.
    На этом уроке вы создадите и настроите кластер управляемой БД MySQL, подключитесь к нему, перенесёте данные в облако, познакомитесь с возможностями резервного копирования и мониторинга. Эти навыки пригодятся вам и в других сервисах управляемых БД, поскольку принципы работы в них очень похожи.
    Предположим, вы решили добавить в разрабатываемый вами мессенджер новую функциональность. Вы написали микросервис, который позволяет оценивать сообщения в групповых чатах и хранит оценки в БД MySQL. Давайте поместим эту БД в Yandex Cloud.
    Прежде всего понадобится создать кластер: набор виртуальных машин (ВМ, или хостов), на которых будет развёрнута БД. Это обязательный первый шаг при использовании любого сервиса управляемых БД.
    Войдите в консоль управления Yandex Cloud и выберите каталог для кластера. Вверху справа нажмите кнопку Создать ресурс и выберите из выпадающего списка Кластер MySQL.
    Откроется страница с основными настройками кластера. Рассмотрим их подробнее.
    Базовые параметры
    Имя кластера может включать только цифры, прописные и строчные латинские буквы, дефисы.
    Поле Описание заполнять необязательно. Оно полезно, если вам нужно создать несколько кластеров для разных целей, чтобы в них было проще ориентироваться.
    О том, какое бывает Окружение кластера и чем различаются PRESTABLE и PRODUCTION, мы говорили на одном из предыдущих уроков. Поскольку микросервис только разрабатывается, выберите окружение PRESTABLE.
    Версия. В качестве сервера MySQL в Yandex Cloud используется Percona Server версии 5.7 или 8.0. У этих реализаций сервера улучшенная производительность на многоядерных машинах. Если для вас критична стабильность работы микросервиса, выбирайте проверенную временем 5.7. Для нашей задачи подойдёт 8.0: в ней много новых функций, но она ещё не полностью обкатана.
    Класс хостов
    Следующий шаг — выбор класса хостов, или шаблона ВМ. Хосты кластера будут развёрнуты на базе ВМ Compute Cloud с использованием этого шаблона.
    Платформа определяет тип физического процессора (Intel Broadwell или Intel Cascade Lake), а также конфигурации числа ядер виртуального процессора (vCPU) и размера оперативной памяти.
    Если тип процессора для вас неважен, выбирайте более современную платформу Intel Cascade Lake. Она предоставляет широкий выбор конфигураций вычислительных ресурсов.
    Также на конфигурации влияет тип ВМ, на которой будет развёрнута БД.
    Standard — это обычные ВМ с 4 ГБ RAM на ядро vCPU. Это оптимальный баланс между количеством запущенных процессов, быстродействием и потребляемой оперативной памятью.
    Memory-optimized — машины с вдвое увеличенным объёмом RAM на каждое ядро. Выбирайте их для высоконагруженных сервисов с повышенными требованиями к кешу.
    Burstable — машины, для которых гарантируется использование лишь доли ядра vCPU (5, 20 или 50%) с вероятностью временного повышения вплоть до 100%. Они стоят дешевле и подходят для задач, где не нужен постоянный уровень производительности, т. е. для тестирования или разработки.
    Выберем для микросервиса следующий класс хоста: платформа — Intel Cascade Lake; тип — standard; конфигурация вычислительных ресурсов — s2.micro (два ядра vCPU, 8 ГБ RAM).
    Хранилище данных
    Хранилище БД может быть сетевым или локальным. В первом случае данные находятся на виртуальных дисках в инфраструктуре Yandex Cloud. Локальное хранилище — это диски, которые физически размещаются в серверах хостов БД.
    При создании кластера можно выбирать между следующими типами хранилища:
        Стандартное сетевое (network-hdd) — это наиболее экономичный вариант. Выбирайте его, если к скорости записи и чтения нет особых требований.
        Быстрое сетевое (network-ssd) стоит примерно в четыре раза дороже, но при размере хранилища от 100 ГБ работает быстрее стандартного в десять и более раз (чем больше размер, тем заметнее разница в скорости).
        Сетевое на нереплицируемых SSD-дисках (network-ssd-nonreplicated) — использует сетевые SSD-диски с повышенной производительностью, реализованной за счет устранения избыточности. Объём такого хранилища можно увеличивать только с шагом 93 ГБ.
        Быстрое локальное (local-ssd) — самое быстрое и дорогое. Если локальный диск откажет, все сохранённые на нём данные будут потеряны. Чтобы этого избежать, при выборе локального хранилища сервис автоматически создаст отказоустойчивый кластер минимум из трёх хостов.
    При создании кластера внимательно выбирайте тип хранилища. Размер хранилища можно будет позже изменить, а тип — нет.
    Выберите для кластера стандартное сетевое хранилище network-hdd размером 50 ГБ.
    База данных
    В этом разделе настроек задаются атрибуты базы: Имя БД, уникальное в рамках кластера, Имя пользователя (владельца БД) и Пароль пользователя.
    Сеть
    Здесь можно выбрать облачную сеть для кластера и группы безопасности для его сетевого трафика.
    Оставьте сеть по умолчанию (default) или выберите сеть, которую создали на предыдущем курсе. Кластер будет доступен для всех ВМ, которые подключены к вашей облачной сети.
    Параметры хостов
    В этом блоке можно добавить количество хостов, которые будут созданы вместе с кластером, и изменить их параметры. Дополнительные хосты могут понадобиться, например, для репликации БД или снижения нагрузки на хост-мастер.
    Для наших целей достаточно кластера из одного хоста. Нажмите значок редактирования параметров хоста и в открывшемся окне выберите опцию Публичный доступ. Это означает, что к хосту можно будет подключиться из интернета, а не только из облачной сети. Остальные параметры оставьте без изменений.
    Дополнительные настройки
    Здесь можно:
        указать время Начала резервного копирования и Окна обслуживания. Это пригодится, если вы хотите, чтобы резервное копирование и техобслуживание хостов кластера не совпадали с периодами пиковых нагрузок на БД;
        разрешить Доступ из DataLens, если вы планируете анализировать в DataLens данные из базы. Подробнее о DataLens вы узнаете на одном из следующих занятий;
        разрешить Доступ из консоли управления, чтобы выполнять SQL-запросы к БД из консоли управления Yandex Cloud. Отметьте этот пункт: доступ из консоли понадобится нам на следующих практических работах;
        разрешить Доступ из Data Transfer, чтобы разрешить доступ к кластеру из сервиса Yandex Data Transfer в Serverless-режиме;
        разрешить Сбор статистики, чтобы воспользоваться инструментом Диагностика производительности в кластере;
        установить Защиту от удаления, чтобы защитить кластер от непреднамеренного удаления пользователем.
    В этом блоке также можно задать настройки БД (например используемую сервером MySQL кодировку при работе с данными и обмене информацией с клиентами). По умолчанию при создании кластера сервис выбирает оптимальные настройки. Изменяйте их, если уверены, что это необходимо.
    Настройка завершена. Осталось только нажать кнопку Создать кластер.
    Создание кластера займёт несколько минут. Когда он будет готов к работе, его статус на панели Managed Service for MySQL сменится с Creating на Running, а состояние — на Alive.
    Статус показывает, что происходит с кластером:
        Creating — создаётся;
        Running — работает;
        Error — не отвечает, возникла проблема;
        Updating — обновляется;
        Stopped — остановлен;
        Unknown — статус неизвестен (так может быть, например, когда кластер не виден из интернета).
    Состояние — это показатель доступности кластера:
        Alive — все хосты кластера работают;
        Degraded — часть хостов (один или больше) не работает;
        Dead — все хосты не работают.
    Task:
    Практическая работа. Подключение к БД и добавление данных
    Decision:
    Доступ из консоли управления
    Продолжим практическую работу. В кластере, который вы создали, уже есть БД. Она пока пустая. Поскольку при создании кластера вы выбрали в настройках пункт Доступ из консоли управления, в консоли управления Yandex Cloud появилась вкладка с интерфейсом для выполнения SQL-запросов к БД.
    Давайте зайдём туда и создадим в БД таблицу для нашего микросервиса.
    На странице Managed Service for MySQL выберите строку с созданным вами кластером. В панели консоли управления перейдите на вкладку SQL. Вам будет предложено выбрать БД для SQL-запросов и имя пользователя, а также ввести пароль. Все эти атрибуты вы задавали при создании  кластера.
    Нажмите кнопку Подключиться. Откроется структура БД (сейчас там написано, что данных нет) и окно ввода для SQL-запросов.
    Теперь создадим таблицу. Введите в окне ввода следующий запрос и нажмите кнопку Выполнить.
    CREATE TABLE IF NOT EXISTS ratings (
        rating_id INT AUTO_INCREMENT PRIMARY KEY,
        user_id INT NOT NULL,
        message_id INT NOT NULL,
        rating INT NOT NULL
    ) ENGINE=INNODB; 
    Обратите внимание, что в качестве движка в сервисе управляемых БД MySQL используется только InnoDB.
    В таблицу можно добавить данные с помощью команды INSERT.
    INSERT INTO ratings (user_id,message_id,rating) VALUES (44,368,4); 
    Чтобы отобразить обновлённую структуру БД, нажмите на имя БД и выберите таблицу ratings.
    Наведите указатель на заголовок столбца, чтобы увидеть тип данных в нём.
    SQL-запросы через консоль управления Yandex Cloud — нетипичный способ работы с БД. Используйте его для небольших, разовых задач, когда быстрее и проще открыть подключение в браузере. Этот способ не очень удобен: текст запроса и результат его выполнения доступны, только пока вы не закрыли или не перезагрузили страницу в браузере. Конечно, если запрос успешно запущен, то сервис обработает его независимо от состояния консоли управления.
    В консоли выводятся только первые 1000 строк результата запроса, даже если данных больше. Чтобы увидеть строку, введите её номер в поле Номер первой строки.
    Подключение к кластеру
    В основном вы будете работать с БД из приложений или из командной строки. Однако для этого нужно подключиться к хосту, на котором развёрнута БД.
    Есть два варианта подключения. Если публичный доступ к хосту открыт, подключитесь к нему через интернет с помощью защищённого SSL-соединения. Если публичного доступа нет, подключитесь к хосту с виртуальной машины, созданной в той же виртуальной сети. SSL-соединение можно не использовать, но тогда трафик между виртуальной машиной и БД шифроваться не будет.
    Давайте подключимся к БД через интернет и создадим в ней ещё одну таблицу. Для выполнения этого задания вы можете использовать виртуальную машину с Ubuntu.
    Для создания таблицы сделаем в текстовом редакторе файл createTables.sql с командами. Например, такой:
    CREATE TABLE IF NOT EXISTS users (
        user_id INT AUTO_INCREMENT,
        nickname VARCHAR(128) NOT NULL,
        avatar VARCHAR(255),
        mail VARCHAR(255),
            PRIMARY KEY (user_id)
    ) ENGINE=INNODB; 
    Чтобы выполнить этот запрос в БД, подключимся к хосту. Для этого понадобится SSL-сертификат. Команды для его получения в Ubuntu:
    mkdir ~/.mysql
    wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
    chmod 0600 ~/.mysql/root.crt 
    Чтобы получить команды для подключения к БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться. В результате их выполнения в директории /home/<домашняя_директория>/.mysql/ сохранится SSL-сертификат root.crt.
    Установите утилиту mysql-client, если на вашем компьютере или виртуальной машине её нет.
    sudo apt update
    sudo apt install -y mysql-client 
    Чтобы подключиться к БД, введите команду mysql. Для запуска нашего скрипта она выглядит следующим образом:
    mysql --host=<адрес хоста> \
            --port=3306 \
            --ssl-ca=~/.mysql/root.crt \
            --ssl-mode=VERIFY_IDENTITY \
            --user=<имя пользователя> \
            --password \
        <имя_базы_данных> < createTables.sql 
    Сервис помогает заполнить параметры в команде. Чтобы посмотреть пример команды с адресом хоста, именами пользователя и БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться.
    После запуска команды введите пароль к БД, после чего в ней будет создана таблица users.
    Если при создании кластера вы не включили публичный доступ, то к БД можно подключиться с виртуальной машины из той же облачной сети без использования шифрования. \\Следовательно, в этом случае в команде для подключения опускается параметр --ssl-ca, а --ssl-mode передаётся со значением DISABLED:
    mysql --host=адрес_хоста \
          --port=3306 \
          --ssl-mode=DISABLED \
          --user=<имя пользователя> \
          --password \
          <имя_базы_данных> < createTables.sql 
    Естественно, подключаться к БД можно не только из командной оболочки, но и из приложений. Нажмите уже знакомую вам кнопку Подключиться и посмотрите примеры кода для Python, PHP, Java, Node.js, Go, Ruby или настроек для драйвера ODBC.
    Если вы хотите перенести БД в облако, то понадобится создать дамп и восстановить его в нужном кластере. Дамп — это копия БД или её части, представляющая собой текстовый файл с командами SQL (например, CREATE TABLE или INSERT). Его создают с помощью утилиты mysqldump.
    Давайте попробуем перенести данные в кластер с помощью дампа. Для этого воспользуемся тестовой БД с данными о сотрудниках компании (имя, дата рождения, дата найма, место работы, зарплата и т. д.). Размер БД — около 167 Мб.
    Скачайте из репозитория и сохраните на компьютере файлы с расширениями .sql и .dump. В файле employees.sql содержатся SQL команды, необходимые для создания таблиц и добавления в них данных из dump-файлов. Для переноса тестовой БД в облако понадобится запустить этот файл. Но, прежде чем приступить к переносу БД, откройте этот файл и удалите или закомментируйте (допишите в начало строки --) в нём строку 110. В этой строке расположена команда FLUSH LOGS, которая закрывает и снова открывает файлы журналов, а они в этой тестовой БД отсутствуют.
    Создайте базу данных employees через консоль управления. Для этого на странице кластера перейдите на вкладку Базы данных и нажмите кнопку Добавить.
    Добавьте пользователю, например user1, разрешение на доступ к БД employees. Для этого на странице кластера перейдите на вкладку Пользователи, напротив пользователя user1 нажмите кнопку ··· и выберите Настроить. Во всплывающем окне нажмите Добавить базу данных, выберите employees, добавьте роль ALL_PRIVILEGES и нажмите Сохранить.
    Затем в командной строке перейдите в папку сохраненными файлами .sql и .dump и восстановите данные из дампа с помощью команды:
    mysql --host=<адрес хоста> \
            --port=3306 \
            --ssl-ca=~/.mysql/root.crt \
            --ssl-mode=VERIFY_IDENTITY \
            --user=<имя_пользователя> \
            --password \
        employees < ~/employees.sql 
    После того как данные скопируются, ваш кластер и БД будут готовы к работе. Подключитесь к БД в консоли управления и убедитесь, что данные перенесены.
    Decision:
    $ ssh -i ubcloud test@178.154.220.177
    $ vim createTables.sql
    $ cat createTables.sql
    CREATE TABLE IF NOT EXISTS users (
            user_id INT AUTO_INCREMENT,
            nickname VARCHAR(128) NOT NULL,
            avatar VARCHAR(255),
            mail VARCHAR(255),
            PRIMARY KEY (user_id)
    ) ENGINE=INNODB;
    $ mkdir ~/.mysql
    $ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
    $ chmod 0600 ~/.mysql/root.crt
    $ sudo apt update
    $ sudo apt install -y mysql-client 
    $ mysql --host=rc1a-lv4y1m7nmsaa620f.mdb.yandexcloud.net \
    >       --port=3306 \
    >       --ssl-ca=~/.mysql/root.crt \
    >       --ssl-mode=VERIFY_IDENTITY \
    >       --user=user1 \
    >       --password \
    >       db1
    mysql> Show tables;
    mysql> select * from ratings;
    Task:
    Практическая работа. Создание кластера базы данных PostgreSQL
    Decision:
    В этой практической работе вы создадите кластер еще одной управляемой БД, на этот раз PostgreSQL, подключитесь к ней и загрузите в нее данные. Вы сможете убедиться, что принципы организации сервисов управляемых БД в Yandex Cloud практически одинаковы. Хорошо изучив один из них, вы будете легко ориентироваться и в остальных.
    Создание кластера
    Создание кластера управляемой базы данных PostgreSQL аналогично созданию кластера базы данных MySQL.
    Перейдите в сервис управляемых баз данных PostgreSQL и нажмите кнопку Создать кластер.
    В появившемся окне настроек задайте необходимые параметры.
        Имя кластера и его описание. Выберите уникальное в облаке имя кластера. Описание опционально, поэтому можно оставить это поле пустым.
        В поле Окружение выберите PRODUCTION.
        Выберите версию PostgreSQL и класс хоста.
        Выберите размер и тип сетевого хранилища.
        Задайте атрибуты базы данных.
        Выберите из списка сеть, в которой будут находиться хосты кластера (для подключения потребуются публичные хосты).
        В блоке Хосты добавьте ещё два хоста в других зонах доступности для обеспечения отказоустойчивости кластера. База автоматически реплицируется.
        В блоке Дополнительные настройки задайте время начала резервного копирования и включите доступ из консоли управления.
        Нажмите кнопку Создать кластер.
    Подключение
    Как и в случае с MySQL, к хостам кластера Managed Service for PostgreSQL можно подключиться двумя способами.
    Через интернет
    Если вы настроили публичный доступ для нужного хоста, то подключиться к нему можно с помощью SSL-соединения.
    С виртуальных машин Yandex Cloud
    Они должны быть расположены в той же облачной сети. Если к хосту нет публичного доступа, для подключения с таких виртуальных машин SSL-соединение использовать необязательно. Обратите внимание, что если публичный доступ в вашем кластере настроен только для некоторых хостов, автоматическая смена мастера может привести к тому, что вы не сможете подключиться к мастеру из интернета.
    Установите клиент для подключения к БД PostgreSQL. Команда установки в Ubuntu:
    sudo apt update && sudo apt install -y postgresql-client 
    Скачайте сертификат для подключения к БД PostgreSQL:
    mkdir -p ~/.postgresql
    wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt 
    chmod 0600 ~/.postgresql/root.crt 
    Пример команды для подключения можно посмотреть в консоли управления, нажав на кнопку Подключиться на странице кластера. Подключение с SSL происходит при помощи следующей команды:
    psql "host=<FQDN_хоста> \ 
          port=6432 \
          sslmode=verify-full \
          dbname=<имя базы данных> \
          user=<имя пользователя базы данных> \
          target_session_attrs=read-write"
    Загрузка данных в базу данных из CSV
    Одним из способов добавления данных в базу является их загрузка из csv-файла.
    Предположим, вы используете БД для организации работы транспортной службы интернет-магазина. Вам нужно добавить в базу таблицу, содержащую данные о расстояниях между складом и пунктами самовывоза, а также о стандартном времени доставки товаров со склада в эти пункты. Создадим csv-файл, например DTM.csv, который содержит такие данные (100 - код склада, 101-109 - коды пунктов, Time - стандартное время доставки в минутах, Distance - расстояние в километрах):
    "depot","store","time","distance"
    "100","101",31,12
    "100","102",38,17
    "100","103",56,33
    "100","104",70,60
    "100","105",41,25
    "100","106",21,8
    "100","107",33,14
    "100","108",62,42
    "100","109",45,29 
    Важные моменты при миграции из CSV:
        Названия колонок в файле и в таблице необязательно совпадают.
        Файл содержит заголовок, который не нужно импортировать.
        Первые 2 колонки конвертируем из строк (string) в целые числа (int).
    PostgreSQL позволяет импортировать данные из файла несколькими способами:
        Командой copy.
        Через функции pl/pgsql.
        Средствами другого языка, например Python.
    Воспользуемся первым способом.
    Сначала нам понадобится создать таблицу, в которую будет осуществлена миграция данных. Подключитесь к БД согласно инструкциям выше. Выполните следующую команду:
    CREATE TABLE dtm (
        id serial PRIMARY KEY,
        depot int NOT NULL,
        store int  NOT NULL,
        time int NOT NULL,
           distance int  NOT NULL
    );
    Загрузите данные:
    \copy dtm(depot,store,time,distance) from '/<путь к файлу>/DTM.csv' DELIMITERS ',' CSV HEADER;
    В этой команде мы учли те моменты, о которых говорили вначале:
        dtm (depot, store, time, distance) маппинг колонок связывает колонки в файле с колонками в таблице, их имена могут не совпадать
        CSV HEADER показывает, что заголовок импортировать не нужно
        Колонки в таблице уже имеют правильные типы данных, конвертация будет выполнена автоматически.
    В консоли управления на странице кластера перейдите на вкладку SQL. Введите пароль пользователя БД и нажмите кнопку Подключиться. Выберите таблицу dtm, чтобы убедиться, что добавление данных выполнено правильно.
    Decision:
    $ sudo apt install -y postgresql-client 
    $ mkdir -p ~/.postgresql
    $ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt
    $ chmod 0600 ~/.postgresql/root.crt
    $ vim DTM.csv
    $ cat DTM.csv
    "deport","store","time","distance"
    "100","101",31,12
    "100","102",38,17
    "100","103",56,33
    "100","104",70,60
    "100","105",41,25
    "100","106",21,8
    "100","107",33,14
    "100","108",62,42
    "100","109",45,29
    $ psql "host=rc1a-4ze6pmpwek1evn80.mdb.yandexcloud.net,rc1b-kg04sg8hlh6fcgvc.mdb.yandexcloud.net \
    >       port=6432 \
    >       sslmode=verify-full \
    >       dbname=db2 \
    >       user=user2 \
    >       target_session_attrs=read-write"
    db2=> CREATE TABLE dtm (
    db2(>     id serial PRIMARY KEY,
    db2(>     depot int NOT NULL,
    db2(>     store int  NOT NULL,
    db2(>     time int NOT NULL,
    db2(>        distance int  NOT NULL
    db2(> );
    db2=> \copy dtm(depot,store,time,distance) from '/home/test/DTM.csv' DELIMITERS ',' CSV HEADER;
    db2=> exit
    Task:
    Практическая работа. Создание кластера MongoDB
    Decision:
    На этом уроке вы создадите кластер MongoDB, подключитесь к нему и загрузите в него данные. Раньше вы работали только с реляционными БД, но использование кластера MongoDB принципиально не отличается от работы с кластером MySQL или PostgreSQL, так что многое будет вам знакомо.
    Создание кластера базы данных
    Выберите в консоли управления Yandex Cloud каталог для кластера БД. На дашборде каталога откройте раздел Managed Service for MongoDB. В открывшемся окне нажмите кнопку Создать кластер.
    Установите основные настройки кластера. Для этого урока создайте кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano, стандартное сетевое хранилище размером 10 ГБ. Откройте публичный доступ к хосту и задайте пароль пользователя БД. Остальные значения оставьте по умолчанию.
    Подключение к базе данных
    В сервисе управляемых БД MongoDB к хостам можно подключаться через интернет или с виртуальных машин в той же сети. Порт для подключения — 27018.
    Для подключения через интернет хосты кластера должны находиться в публичном доступе. Подключаться можно только через зашифрованное соединение.
    Обратите внимание: если публичный доступ настроен только для некоторых хостов в кластере, то при автоматической смене основной реплики она может оказаться недоступной из интернета.
    Если к хосту нет публичного доступа и вы подключаетесь к нему с виртуальных машин Yandex Cloud, то зашифрованное соединение необязательно.
    Подключитесь к созданной БД из интернета. Используйте SSL-сертификат, который вы подготовили на одной из предыдущих практических работ, или команду (для Ubuntu):
    sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
    sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt 
    Если всё пройдет успешно — вы получите сообщение операционной системы о том, что сертификат сохранён.
    Установите утилиту Mongo Shell:
    sudo apt install mongodb-clients 
    Подключитесь к БД с помощью команды mongo. Чтобы получить строку подключения, на основной странице сервиса в консоли управления выберите кластер, на вкладке Обзор нажмите кнопку Подключиться.
    Сервис сформирует пример строки подключения для кластера. Там же вы можете посмотреть примеры кода на Python, PHP, Java, Node.js, Go для подключения из приложений.
    Подключитесь к кластеру из командной строки.
    mongo --norc \
            --ssl \
            --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
            --host '<FQDN хоста MongoDB>:27018' \
            -u <имя пользователя БД> \
            -p <пароль пользователя БД> \
            <имя БД> 
    При успешном подключении вы получите сообщение:
    Создадим в БД коллекцию users. Предположим, в ней содержится информация о пользователях вашего приложения.
    db.createCollection("users") 
    Загрузим в коллекцию тестовые данные с помощью методов добавления одного документа db.insertOne(...) и сразу нескольких db.insertMany(...).
    Сначала добавим один документ (данные одного пользователя).
    db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"}); 
    Ответ должен выглядеть примерно так:
    Дополним коллекцию данными еще двух пользователей.
    db.users.insertMany( [
          {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
          {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
       ] ); 
    Обратите внимание, что документы в коллекции users содержат разный набор данных. С помощью MongoDB мы можем работать с данными, структура которых частично не совпадает.
    Теперь посмотрим на содержимое коллекции с помощью команды db.users.find(). Результат показывает, что все данные успешно добавлены:
    Проверим, есть ли среди пользователей те, кому больше 37 лет. Сделаем запрос к БД с помощью метода find.
    db.users.find({age: {$gt: 37}}); 
    Decision:
    $ sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
    > sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt
    $ wget https://downloads.mongodb.com/linux/mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
    $ tar -zxvf mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
    $ sudo ln -s  /path/to/the/mongodb-directory/bin/* /usr/local/bin/
    $ sudo apt install mongodb-clients
    $ mongo --norc \
            --ssl \
            --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
            --host 'rs01/rc1b-b7xwau9lvu3hdt0w.mdb.yandexcloud.net:27018' \
            -u user1 \
            -p 2019Atdhfkz \
            db1
    rs01:PRIMARY> db.createCollection("users")
    {
            "ok" : 1,
            "$clusterTime" : {
                    "clusterTime" : Timestamp(1665235984, 1),
                    "signature" : {
                            "hash" : BinData(0,"JH1IX4da7oRrpZn9wNQuuIVg7Zw="),
                            "keyId" : NumberLong("7151968284189917188")
                    }
            },
            "operationTime" : Timestamp(1665235984, 1)
    }
    rs01:PRIMARY> db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"});
    {
            "acknowledged" : true,
            "insertedId" : ObjectId("63417c1dbd4e36e89a4d2c78")
    }
    rs01:PRIMARY> db.users.insertMany( [
    ...       {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
    ...       {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
    ...    ] );
    {
            "acknowledged" : true,
            "insertedIds" : [
                    ObjectId("63417c29bd4e36e89a4d2c79"),
                    ObjectId("63417c29bd4e36e89a4d2c7a")
            ]
    }
    rs01:PRIMARY> db.users.find({age: {$gt: 37}});
    { "_id" : ObjectId("63417c29bd4e36e89a4d2c79"), "firstName" : "Viktoria", "lastName" : "Holmes", "age" : 73, "email" : "viktoria.holmes@test.com", "phone" : "737772727" }
    Task:
    Практическая работа. Создание кластера ClickHouse и подключение к нему
    Decision:
    Создание кластера
    В этой практической работе вы создадите кластер ClickHouse. Вы уже знаете, как создавать кластеры и выставлять их основные настройки в сервисах платформы данных. Но у БД ClickHouse есть свои особенности.
    Когда вы создадите кластер из двух или более хостов, сервис дополнительно создаст ещё один кластер из трёх хостов, где развернёт Apache ZooKeeper. Это служба для распределенных систем, которая управляет конфигурацией, репликацией и распределением запросов по хостам БД. Без неё кластер ClickHouse работать не будет. К ZooKeeper у пользователей доступа нет, однако его хосты учитываются при расчёте квоты ресурсов облака и стоимости сервиса.
    ZooKeeper синхронизирует шарды (т. е. хосты) ClickHouse. В отличие от классических реляционных БД, у ClickHouse нет главного узла (мастера), через который добавляются данные. В ClickHouse данные можно и записывать, и читать с любого узла.
    Давайте приступим к практике. Перейдите в каталог, где нужно создать кластер БД, выберите Managed Service for ClickHouse и нажмите кнопку Создать кластер.
    Для практической работы нам понадобится кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano и стандартное сетевое хранилище размером 10 ГБ.
    Задайте настройки: введите имена для кластера и БД, а также имя и пароль пользователя. Откройте публичный доступ к хосту.
    Обратите внимание: в отличие от сервисов, которые мы уже рассматривали, здесь в разделе База данных можно включить опции управления пользователями и БД с помощью SQL-запросов.
    Кроме того, в дополнительных настройках можно включить доступ к БД из консоли управления, сервисов DataLens, Яндекс Метрики и AppMetrica, а также возможность использовать бессерверные вычисления (подробно о них мы расскажем на курсе «Serverless»). С помощью DataLens, например, вы визуализируете результаты поисковых запросов в виде графиков, диаграмм и дашбордов, а подключение AppMetrica позволит импортировать данные из этого сервиса в кластер.
    Отметьте пункт Доступ из DataLens: он понадобится вам на одном из следующих уроков.
    Нажмите кнопку Создать кластер.
    Подключение к базе данных
    К хостам кластера ClickHouse можно подключаться через интернет или с виртуальных машин в той же виртуальной сети. Если к хостам БД открыт публичный доступ, то для подключения к ним используется шифрованное соединение.
    Подключайтесь к кластеру с помощью HTTP-протокола или более низкоуровневого Native TCP-протокола. В большинстве случаев рекомендуется взаимодействовать с ClickHouse не напрямую, а с помощью инструмента или библиотеки. Официально поддерживаются консольный клиент, драйверы JDBC и ODBC, клиентская библиотека для C++. Также можно использовать библиотеки сторонних разработчиков для Python, PHP, Go, Ruby и т. д.
    Примеры строк подключения приводятся в документации и консоли управления на вкладке Обзор страницы кластера.
    С БД удобно работать в приложении с графическим интерфейсом. Один из вариантов — универсальный клиент DBeaver. Другие варианты вы найдёте в полном списке клиентов.
    Подробная информация о настройке подключения приведена в документации. Чтобы создать подключение к ClickHouse в DBeaver, помимо обычных параметров (адреса хоста, порта, имени БД, логина и пароля) задайте на вкладке Свойства драйвера настройки свойств драйвера JDBC. Укажите следующие параметры: ssl = true; sslmode = strict; sslrootcert = <путь к SSL-сертификату>. Как получить SSL-сертификат, вы уже узнали на предыдущих уроках.
    При подключении DBeaver покажет номер версии ClickHouse и пинг до хоста.
    В двух следующих практических работах мы используем кластер для аналитической работы с датасетами и для создания БД ClickHouse.
    Task:
    Практическая работа. Работа с данными из объектного хранилища
    Decision:
    В интернете выложено множество датасетов —  структурированных наборов данных, связанных общей темой. Например в репозитории проекта Our World in Data находится около тысячи разнообразных датасетов: от численности населения государств до сведений об употреблении алкоголя в США с 1850 года.
    Датасеты часто выкладывают в виде CSV- или TSV-файлов. В них значения разделены запятой (comma separated values, CSV) или табуляцией (tab separated values, TSV).
    Сохраняйте датасеты в объектное хранилище и анализируйте данные с помощью ClickHouse. При этом не требуется создавать БД и копировать в неё данные из датасета. Отправляйте запросы к ClickHouse — а ClickHouse сходит за данными напрямую в объектное хранилище.
    В качестве примера возьмем датасет с историей метеонаблюдений за 10 лет и попробуем развеять мифы о разнице погоды в Москве и Санкт-Петербурге. Датасет содержит примерно 50 тысяч записей, он выложен в объектном хранилище Yandex Cloud и доступен всем.
    Воспользуемся кластером БД, который мы создали на предыдущем уроке. Откройте его в консоли управления. Запросы к датасету будем делать через SQL-консоль. На панели слева выберите вкладку SQL и введите пароль пользователя. В правом поле открывшейся консоли мы и станем вводить SQL-запросы.
    Как вы думаете, где зарегистрирована самая низкая температура? Наверняка в Санкт-Петербурге! Давайте проверим.
    Выполните запрос:
    SELECT
        City,
        LocalDate,
        TempC
    FROM s3(
            'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
            'TSV',
            'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
    ORDER BY TempC ASC
    LIMIT 1 
    Всё-таки наши интуитивные представления не всегда верны и могут опровергаться данными.
    А что насчет самой высокой температуры, скорости ветра и влажности? Проверьте сами, изменив поля в запросе (средняя скорость ветра за 10 минут — WindSpeed10MinAvg, относительная влажность — RelHumidity; сортировка по возрастанию — ASC, по убыванию — DESC). Увеличив количество выводимых данных, вы получите более точное представление (измените параметр LIMIT c 1 до 10).
    Но это были крайние значения. Давайте проверим, насколько в этих городах отличается климат в целом. Узнаем, например, разницу среднегодовых температур.
    SELECT
        Year,
        msk.t - spb.t
    FROM
    (
        SELECT
            toYear(LocalDate) AS Year,
            avg(TempC) AS t
        FROM s3(
            'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
            'TSV',
            'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
        WHERE City = 'Moscow'
        GROUP BY Year
        ORDER BY Year ASC
    ) AS msk
    INNER JOIN
    (
        SELECT
            toYear(LocalDate) AS Year,
            avg(TempC) AS t
        FROM s3(
            'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
            'TSV',
            'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
        WHERE City = 'Saint-Petersburg'
        GROUP BY Year
        ORDER BY Year ASC
    ) AS spb ON msk.Year = spb.Year 
    Task:
    Измените поля в запросе, чтобы проверить разницу относительной влажности.
    Давайте теперь рассчитаем, где раньше начинается лето. Будем считать началом лета день, начиная с которого температура поднималась выше +15 °С хотя бы пять раз в течение 10-дневного периода (864 тысячи секунд).
    Decision:
    SELECT
        City,
        toYear(LocalDate) AS year,
        MIN(LocalDate)
    FROM
    (
        SELECT
            City,
            LocalDate,
            windowFunnel(864000)(LocalDateTime, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15) AS warmdays
        FROM s3(
            'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
            'TSV',
            'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
        GROUP BY
            City,
            LocalDate
    )
    WHERE warmdays = 5
    GROUP BY
        year,
        City
    ORDER BY
        year ASC,
        City ASC 
    Task:
    Практическая работа. Добавление данных
    Decision:
    Предположим, вы работаете в метеорологической службе и постоянно изучаете датасеты с погодными данными. Сбор данных о погоде автоматизирован: на территории области расположены несколько десятков пунктов наблюдения с датчиками. Информация о температуре, давлении, влажности и скорости ветра раз в полчаса передаётся с датчиков на центральный сервер. Приложение на сервере обрабатывает данные, переводит их в нужный формат и записывает в файл. Каждый файл содержит данные за три часа наблюдений. Для прогноза нужно учитывать всю историю наблюдений за последние несколько лет, то есть все файлы потребуется собрать в одну БД.
    Давайте потренируемся добавлять данные из файлов в БД ClickHouse.
    На предыдущих уроках мы создали кластер, на котором развёрнута БД, и научились подключаться к нему. Продолжим работать с этой БД, а в качестве добавляемого файла возьмем уже известный вам датасет с данными о погоде в Москве и Санкт-Петербурге.
    Сохраните файл на компьютере.
    Прежде чем добавлять файл в БД, создадим в ней таблицу, куда будут вставляться данные. Перейдите в SQL-консоль кластера и выполните команду:
    CREATE TABLE <имя вашей БД>.Weather
    (  LocalDateTime DateTime,
       LocalDate Date,
       Month Int8,
       Day Int8,
       TempC Float32,
       Pressure Float32,
       RelHumidity Int32,
       WindSpeed10MinAvg Int32,
       VisibilityKm Float32,
       City String
    ) ENGINE=MergeTree
    ORDER BY LocalDateTime; 
    В результате будет создана пустая таблица с полями и типами данных, соответствующими полям и типам данных в нашем файле (датасете).
    Вставим данные в таблицу с помощью клиента командной строки clickhouse-client. Команды для его установки (для Ubuntu):
    sudo apt update && sudo apt install --yes apt-transport-https ca-certificates dirmngr && \
    sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 && \
    echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee \
    /etc/apt/sources.list.d/clickhouse.list
    sudo apt update && sudo apt install --yes clickhouse-client
    mkdir --parents ~/.clickhouse-client && \
    wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" \
    --output-document ~/.clickhouse-client/config.xml 
    Подробности о том, как установить клиент и работать с ним, вы найдёте в документации ClickHouse.
    Подключитесь к кластеру. Пример строки подключения посмотрите в консоли управления.
    Добавим файл с данными в БД с помощью команды:
    cat weather_data.tsv | clickhouse-client \
    --host <адрес вашей БД> \
    --secure \
    --user user1 \
    --database db1 \
    --port 9440 \
    -q "INSERT INTO db1.Weather FORMAT TabSeparated" \
    --ask-password 
    Переключившись в SQL-консоль, вы увидите, что данные появились в таблице.
    Данные в БД можно загружать и другими способами: из приложений или клиентов с графическим интерфейсом (например DBeaver). В этом случае подключение к БД и передача данных будут идти по HTTP-протоколу через порт 8443.
    Теперь вы можете анализировать 10-летний срез данных о погоде в Москве и Санкт-Петербурге непосредственно в ClickHouse, без обращений к внешним источникам. Попробуйте, например, выяснить, какой день был самым ветреным в этих городах.
    После практической работы остановите кластер, но не удаляйте его. Кластер ещё понадобится, когда мы будем рассматривать сервис визуализации и анализа данных Yandex DataLens.
    Decision:
    $ sudo apt update && sudo apt install --yes apt-transport-https ca-certificates dirmngr && \
    > sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 && \
    > echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee \
    > /etc/apt/sources.list.d/clickhouse.list
    $ sudo apt update && sudo apt install --yes clickhouse-client
    $ mkdir --parents ~/.clickhouse-client && \
    > wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" \
    > --output-document ~/.clickhouse-client/config.xml
    $ wget https://storage.yandexcloud.net/arhipov/weather_data.tsv
    $ cat weather_data.tsv | clickhouse-client \
    > --host rc1a-mg8yquor7pspcwkc.mdb.yandexcloud.net \
    > --secure \
    > --user user1 \
    > --database db1 \
    > --port 9440 \
    > -q "INSERT INTO db1.Weather FORMAT TabSeparated" \
    > --ask-password
    Task:
    Практическая работа. Создание базы данных
    Decision:
    В этой практической работе вы создадите БД YDB в dedicated режиме, научитесь подключаться к ней и добавлять данные из тестового приложения.
    Создание базы данных
        На стартовой странице консоли управления перейдите в каталог, в котором будете создавать БД, выберите в списке сервисов База данных YDB и нажмите кнопку Создать ресурс.
        В открывшемся окне выберите тип БД dedicated. Появившийся интерфейс создания новой БД практически идентичен уже знакомым вам интерфейсам создания кластеров управляемых БД.
        Выберите для вашей БД имя, назначьте необходимые вычислительные ресурсы (для этой и следующих практических работ достаточно одного хоста конфигурации medium), тип и количество групп хранения (достаточно одной группы).
        Группа хранения – это массив независимых дисковых накопителей, объединённых по сети в единый логический элемент. В YDB такой массив состоит из 9 дисков, расположенных по три в каждой из трёх зон доступности. Такая конфигурация обеспечивает устойчивость при одновременном отказе одной из зон и отказе диска в другой зоне. Стандартный размер группы хранения — 100 ГБ.
        Выберите облачную сеть и подсети для работы с БД. Вы можете оставить сеть по умолчанию или выбрать ту, которую создали на предыдущем курсе. БД будет доступна для всех виртуальных машин, которые подключены к той же облачной сети.
    Также выберите опцию присвоения публичного IP-адреса, чтобы иметь возможность подключаться к БД из интернета.
        Нажмите кнопку Создать базу данных.
        Создание БД занимает несколько минут. Когда статус БД изменится с Provisioning на Running, она готова к работе.
        Кликнув на созданную БД в консоли управления, вы перейдёте на вкладку Обзор.
    В разделе Соединение на этой странице приведена информация, которая вам понадобится для подключения к БД:
        Эндпоинт — точка подключения с указанием протокола, представляющая собой в данном случае адрес, на который посылаются сообщения;
        Размещение базы данных — полный путь к БД.
        Примеры подключений из командной строки и приложений вы можете посмотреть, нажав на кнопку Подключиться.
    Подключение к базе данных и запуск тестового приложения
    В этой части практической работы вы подключитесь к БД и запустите тестовое приложение, чтобы создать в ней несколько таблиц с данными о популярных сериалах.
        Для того, чтобы выполнить эту задачу, вам понадобится сервисный аккаунт с ролями viewer и editor. Перейдите в дашборд каталога и выберите вкладку Сервисные аккаунты. Создайте сервисный аккаунт, назначив для него указанные роли. Сохраните идентификатор этого аккаунта.
        Вы можете запускать тестовое приложение со своего компьютера или с виртуальной машины в Yandex Cloud. В данном примере используется OC Ubuntu и приложение на Python.
    Если при создании БД вы не присвоили ей публичный IP-адрес, то подключиться к ней вы сможете только с виртуальной машины, расположенной в той же облачной сети.
    Для запуска приложения нужно склонировать на свою машину репозиторий YDB Python SDK, из которого оно будет вызываться, а также установить библиотеки ydb, iso8601 и yandexcloud. Воспользуйтесь для этого следующими командами:
    git clone https://github.com/yandex-cloud/ydb-python-sdk.git
    sudo pip3 install iso8601 ydb yandexcloud 
        Создайте авторизованный ключ для вашего сервисного аккаунта и сохраните его в файл с помощью интерфейса командной строки Yandex Cloud.
    mkdir ~/.ydb
    yc iam key create \
      --folder-id <идентификатор каталога> \
      --service-account-name <имя сервисного аккаунта> \
      --output ~/.ydb/sa_name.json 
        Получите SSL-сертификат:
    wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" \
      -O ~/.ydb/CA.pem 
    Установите переменную окружения YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS и переменную окружения с SSL-сертификатом.
    export YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS=~/.ydb/sa_name.json
    export YDB_SSL_ROOT_CERTIFICATES_FILE=~/.ydb/CA.pem 
        Запустите тестовое приложение basic_example_v1 из репозитория ydb-python-sdk, указав в качестве параметров подключения значения протокола, эндпоинта и полного пути к БД.
    cd ./ydb-python-sdk/examples/basic_example_v1
    python3 __main__.py \
    -e <Эндпоинт> \
    -d <Размещение базы данных> 
    Результат выполнения приложения должен выглядеть так:
    > describe table: series
    column, name: series_id , Uint64
    column, name: title , Utf8
    column, name: series_info , Utf8
    column, name: release_date , Uint64
    > select_simple_transaction:
    series, id:  1 , title:  IT Crowd , release date:  b'2006-02-03'
    > bulk upsert: episodes
    > select_prepared_transaction:
    episode title: To Build a Better Beta , air date: b'2016-06-05'
    > select_prepared_transaction:
    episode title: Bachman's Earnings Over-Ride , air date: b'2016-06-12'
    > explicit TCL call
    > select_prepared_transaction:
    episode title: TBD , air date: b'2022-08-24' 
        Вернитесь в консоль управления Yandex Cloud, чтобы посмотреть на результаты работы приложения. Переключитесь на вкладку Навигация.
    В вашей БД созданы три таблицы: episodes, seasons и series с информацией о двух популярных сериалах IT Crowd и Silicon Valley. Кликнув по названию таблицы, вы увидите содержащиеся в ней данные. А если подвести к названию таблицы курсор и кликнуть на значок «информация» справа, то внизу появится дополнительное окно с вкладками Обзор, Схема и Партиции.
    Кнопка Создать на панели Навигация служит для создания директорий и таблиц. С её помощью можно создать новую таблицу, не прибегая к командам YQL.
    Task:
    Практическая работа. YQL и работа с данными
    Decision:
    В этом уроке вы освоите базовый набор операций для работы с данными с использованием YQL и консоли управления Yandex.Cloud. Подробная информация о YQL приведена в разделе Справочник YQL в документации.
    Чтобы начать, войдите в раздел Навигация консоли управления и откройте редактор SQL, нажав на кнопку SQL-запрос.
    На прошлом уроке мы уже создали в нашей БД три таблицы, содержащие информацию о сериалах IT Crowd и Silicon Valley.
        Добавим в БД еще одну таблицу с рейтингами эпизодов сериала IT Crowd на IMDb.com.
    YQL является диалектом SQL, поэтому многие инструкции в этих языках идентичны.
    Для создания таблицы вам понадобится сделать запрос к БД, содержащий инструкцию CREATE TABLE. Например, если бы мы хотели создать таблицу seasons (она уже есть в вашей БД), то SQL запрос выглядел бы следующим образом:
    CREATE TABLE seasons
    (
        series_id Uint64, 
        season_id Uint64, 
        first_aired Date, 
        last_aired Date, 
        title Utf8, 
            PRIMARY KEY (series_id, season_id)
    ); 
    Обратите внимание, что в пределах директории YDB имена таблиц должны быть уникальны. Первичный ключ (PRIMARY KEY) — это столбец или комбинация столбцов, однозначно идентифицирующих каждую строку в таблице. Он может содержать только неповторяющиеся значения. Для таблицы YDB указание первичного ключа обязательно, при этом он может быть только один.
    Первичный ключ по сути является первичным индексом, который помогает СУБД быстрее обнаруживать отдельные записи в таблице и сокращает время выполнения запросов. Также в таблицу можно добавить один или несколько вторичных индексов. Они служат той же цели, но в отличие от первичного индекса могут содержать повторяющиеся значения. Добавить вторичные индексы можно в любой момент, когда возникнет необходимость, и это не вызовет деградацию производительности БД. Чтобы при создании таблицы добавить в нее вторичный индекс, используется такая конструкция:
    INDEX <имя индекса> GLOBAL ON (<имя столбца1>, <имя столбца2>, ...) 
    Вторичный индекс можно добавить и в уже существующую таблицу. Работа БД при этом не прерывается. В отличие от предыдущего случая в существующую таблицу можно добавлять только один вторичный индекс за раз. Делается это с помощью следующей команды:
    ALTER TABLE <имя таблицы> ADD INDEX <имя индекса> GLOBAL ON (<имя столбца>); 
    Задание 1: создайте таблицу ratings, в которой будут содержаться рейтинги всех эпизодов сериала IT Crowd, со столбцами season_id (Uint64), episodes_id (Uint64), title (Utf8), air_date (Date) и imdb_rating (Uint64) и вторичным индексом rating_index по полю imdb_rating.
    Ваш запрос должен быть написан так:
    CREATE TABLE ratings (
        season_id Uint64, 
        episodes_id Uint64, 
        title Utf8, 
        air_date Date, 
        imdb_rating Uint64, 
            PRIMARY KEY (season_id, episodes_id), 
            INDEX rating_index GLOBAL ON (imdb_rating)
    ); 
        Добавим в эту таблицу данные. Для вставки данных в YDB помимо обычной SQL инструкции INSERT также используются инструкции REPLACE и UPSERT.
    При выполнении INSERT перед операцией записи выполняется операция чтения данных. Это позволяет убедиться, что уникальность первичного ключа будет соблюдена. При выполнении инструкций REPLACE и UPSERT осуществляется слепая запись.
    Инструкции REPLACE и UPSERT используются для добавления новой или изменения существующей строки по заданному значению первичного ключа. При операциях записи и изменения данных использование этих инструкций эффективнее.
    Если при выполнении этих инструкций строка с указанным значением первичного ключа не существует, то она будет создана. Если же такая строка существует, то значения ее столбцов будут заменены на новые. Отличие между REPLACE и UPSERT заключается в том, что первая из этих инструкций устанавливает значения столбцов, не участвующих в операции, в значения по умолчанию, а вторая такие значения не меняет.
    Одним запросом REPLACE, UPSERT или INSERT можно вставить в таблицу несколько строк.
    Например, если бы мы хотели добавить в таблицу series те данные, которые в ней сейчас содержатся, то SQL запрос выглядел бы так:
    REPLACE INTO series (series_id, title, release_date, series_info) 
    VALUES 
        ( 
            1, 
            "IT Crowd", 
            Date("2006-02-03"), 
            "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."), 
        ( 
            2, 
            "Silicon Valley", 
            Date("2014-04-06"), 
            "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley." 
        ); 
    Задание 2: добавьте в таблицу ratings данные из этого файла.
    Ваш запрос должен выглядеть таким образом:
    REPLACE INTO ratings (season_id, episodes_id, title, air_date, imdb_rating) VALUES 
        (1, 1, "Yesterday's Jam", Date("2006-02-03"), 76),
        (1, 2, "Calamity Jen", Date("2006-02-03"), 82),
        (1, 3, "Fifty-Fifty", Date("2006-02-10"), 79),
        (1, 4, "The Red Door", Date("2006-02-17"), 80),
        (1, 5, "The Haunting of Bill Crouse", Date("2006-02-24"), 85),
        (1, 6, "Aunt Irma Visits", Date("2006-03-03"), 81),
        (2, 1, "The Work Outing", Date("2006-08-24"), 95),
        (2, 2, "Return of the Golden Child", Date("2007-08-31"), 82),
        (2, 3, "Moss and the German", Date("2007-09-07"), 82),
        (2, 4, "The Dinner Party", Date("2007-09-14"), 87),
        (2, 5, "Smoke and Mirrors", Date("2007-09-21"), 78),
        (2, 6, "Men Without Women", Date("2007-09-28"), 76),
        (3, 1, "From Hell", Date("2008-11-21"), 78),
        (3, 2, "Are We Not Men?", Date("2008-11-28"), 85),
        (3, 3, "Tramps Like Us", Date("2008-12-05"), 82),
        (3, 4, "The Speech", Date("2008-12-12"), 90),
        (3, 5, "Friendface", Date("2008-12-19"), 85),
        (3, 6, "Calendar Geeks", Date("2008-12-26"), 78),
        (4, 1, "Jen The Fredo", Date("2010-06-25"), 80),
        (4, 2, "The Final Countdown", Date("2010-07-02"), 84),
        (4, 3, "Something Happened", Date("2010-07-09"), 75),
        (4, 4, "Italian For Beginners", Date("2010-07-16"), 82),
        (4, 5, "Bad Boys", Date("2010-07-23"), 84),
        (4, 6, "Reynholm vs Reynholm", Date("2010-07-30"), 76); 
        C помощью SQL запросов можно добавлять и удалять не только строки таблицы, но и столбцы. Для этого используется команда ALTER TABLE и фразы ADD COLUMN и DROP COLUMN.
    Например, если вы хотите добавить в таблицу ratings столбец viewed с данными о том, какие эпизоды сериала вы уже посмотрели, то это можно сделать с помощью следующей команды.
    ALTER TABLE ratings ADD COLUMN viewed Bool;  
    Задание 3: Вы решили, что столбец с датой выхода эпизодов в таблице ratings не нужен, поскольку эта информация уже содержится в другой таблице. Удалите столбец air_date из таблицы ratings.
    Для этого понадобится выполнить такую команду:
    ALTER TABLE ratings DROP COLUMN air_date;  
        Теперь потренируемся извлекать данные из БД. Для этого используется команда SELECT. В простейшем случае ее синтаксис выглядит так:
    SELECT <имя столбца1>, <имя столбца2>, ...
    FROM <имя таблицы>; 
    Например, чтобы выбрать всю информацию из таблицы seasons, нужно сделать следующий запрос к БД.
    SELECT * FROM seasons; 
    Если нужно выбрать из таблицы только те строки, которые удовлетворяют определенному условию, в запросе используют секцию WHERE. В этой секции должно находиться выражение, возвращающее логический результат. Обычно оно состоит из логических операций and, or, not и операций сравнения.
    Например, выбрать из таблицы episodes только первые эпизоды всех сезонов можно так:
    SELECT * FROM episodes
    WHERE episode_id = 1
    ;  
    Запрос SELECT извлекает строки без определенного порядка. Чтобы отсортировать полученные данные нужным образом, в этот запрос включают секцию ORDER BY. В ней указывается список столбцов, которые будут определять порядок сортировки результатов запроса.
    Задание 4: получите список самых популярных (с рейтингом не менее 85) эпизодов сериала IT Crowd. При поиске используйте созданный ранее вторичный индекс rating_index. Чтобы упорядочить результаты по убыванию рейтинга используйте конструкцию ORDER BY … DESC.
    Используемый для этого запрос:
    SELECT 
        season_id, 
        episodes_id, 
        title, 
        imdb_rating
    FROM ratings VIEW rating_index 
    WHERE 
        imdb_rating >= 85 
    ORDER BY 
        imdb_rating DESC
    ;  
        Для получения обобщённых сведений о содержащихся в таблице данных — например, о числе строк в таблице или среднем значении какого-либо выражения — в запрос SELECT включают агрегатные функции и секцию GROUP BY. Эта секция используется для агрегации внутри каждого ключа. Ключом является значение одной или более колонок, указанных в GROUP BY.
    Примеры агрегатных функций:
    COUNT(*) — вычисляет число строк в таблице.
    MAX(expr) — находит максимум выражения expr по всем строкам.
    SUM(expr) — суммирует выражение expr по всем строкам. Тип выражения должен быть числовым.
    AVG(expr) — находит среднее значение выражения expr по всем строкам. Тип выражения должен быть числовым или интервалом.
    SOME(expr) — возвращает одно произвольное значение выражения по всем строкам.
    Результаты выполнения агрегатной функции выводятся в отдельном столбце. Чтобы задать этому столбцу имя, используют оператор AS. Конструкция может выглядеть, например, так:
    SELECT 
        <имя столбца1>, 
        MAX(<имя столбца2>) AS max_value
    ...
    ; 
    Задание 5: Напишите SQL запрос к таблице episodes, который выводит данные о числе эпизодов каждого сериала.
    Подсказка:
    Вам понадобится вычислить число строк для каждого значения столбца series_id и сгруппировать результаты по series_id.
    Ответ:
    SELECT 
        series_id, 
        COUNT(*) AS total_episodes 
    FROM episodes 
    GROUP BY 
        series_id 
    ORDER BY 
        series_id 
    ; 
    Задание 6: Напишите SQL запрос, с помощью которого можно сравнить популярность сезонов сериала IT Crowd. 
    Подсказка:
    Вам понадобится вычислить средний рейтинг эпизодов для каждого сезона и сгруппировать результаты по столбцу season_id.
    Ответ:
    SELECT 
        season_id, 
        AVG (imdb_rating) AS avg_rating
    FROM ratings 
    GROUP BY season_id
    ORDER BY avg_rating DESC; 
        В реляционной БД таблицы логически связаны друг с другом. С помощью объединений (JOIN) можно получить данные из нескольких связанных друг с другом таблиц и представить их в виде одной результирующей таблицы.
    Столбцы, по которым выполняется объединение, можно указать одним из двух способов.
        После ключевого слова USING, например table1 AS a JOIN table2 AS b USING (foo). Это более короткий способ записи, удобный для простых случаев. Имена столбцов, по которым происходит объединение таблиц, должны быть одинаковы.
        После ключевого слова ON (например, a JOIN b ON a.foo = b.bar). Этот способ позволяет использовать разные имена столбцов и указывать дополнительные условия по аналогии с WHERE.
    Поскольку такие запросы затрагивают столбцы разных таблиц, имена столбцов должны содержать и имя таблицы (то есть, например, не просто series_id, а seasons.series_id).
    В YDB доступны следующие логические типы объединений:
    INNER (используется по умолчанию) — строки попадают в результат, только если значение ключевых колонок присутствует в обеих таблицах;
    FULL, LEFT и RIGHT — при отсутствии значения в обеих или в одной из таблиц включает строку в результат, но оставляет пустыми (NULL) колонки, соответствующие противоположной таблице.
    LEFT/RIGHT SEMI — одна сторона выступает как белый список (whitelist) ключей, её значения недоступны. В результат включаются столбцы только из одной таблицы, декартового произведения не возникает;
    LEFT/RIGHT ONLY — вычитание множеств по ключам (blacklist). Практически эквивалентно добавлению условия IS NULL на ключ противоположной стороны в обычном LEFT/RIGHT, но, как и в SEMI, нет доступа к значениям;
    CROSS — декартово произведение двух таблиц целиком без указания ключевых колонок, секция с ON/USING явно не пишется;
    EXCLUSION — обе стороны минус пересечение.
    Простой пример запроса с объединением таблиц приведен ниже.
    SELECT
        sa.title AS season_title,
        sr.title AS series_title,
        sr.series_id, sa.season_id 
    FROM seasons AS sa
    INNER JOIN series AS sr ON sa.series_id = sr.series_id 
    WHERE sa.season_id = 1
    ORDER BY sr.series_id; 
    Этот запрос извлекает из таблиц series и seasons сведения о первых сезонах всех сериалов и выводит объединённые данные в результирующей таблице.
    Задание 7: напишите запрос, который выводит таблицу, содержащую название сериала IT Crowd и названия всех его эпизодов (то есть, каждая строка итоговой таблице должна содержать название сериала и название отдельного эпизода).
    Это запрос может выглядеть следующим образом:
    SELECT 
        sr.title AS series_title, 
        ep.title AS episode_title, 
        ep.season_id,     
        ep.episode_id 
    FROM 
        series AS sr 
    INNER JOIN 
        episodes AS ep 
    ON sr.series_id = ep.series_id 
    WHERE sr.series_id = 1 
    ORDER BY 
        ep.season_id,     
        ep.episode_id 
    ;  
    План запроса
    Decision:
    Когда вы обращаетесь к БД, оптимизатор запросов YDB пытается составить наилучший, по его мнению, план выполнения запроса.
    Чтобы оптимизировать свои запросы к БД с точки зрения скорости их выполнения (и/или стоимости, что актуально для бессерверного режима YDB), нужно получить и проанализировать этот план. Вы можете это сделать через консоль управления или с помощью YDB CLI.
    Давайте разберём план запроса, который мы использовали на прошлом уроке в качестве примера объединения таблиц.
    SELECT 
        sa.title AS season_title, 
        sr.title AS series_title, 
        sr.series_id, sa.season_id 
    FROM seasons AS sa 
    INNER JOIN series AS sr ON sa.series_id = sr.series_id 
    WHERE sa.season_id = 1  
    Войдите в редактор SQL вашей БД и вставьте в поле ввода текст запроса. Нажмите на стрелку справа от кнопки Выполнить и в выпадающем меню выберите опцию Explain.
    В результате внизу отобразится поле, содержащее план запроса.
    {
      "plan": {
        "meta": {
          "type": "script",
          "version": "0.2"
        },
        "queries": [
          {
            "tables": [
              {
                "name": "/ru-central1/b1glk1805em030s2ir60/etn9c1c7v3s2bc06lfm8/seasons",
                "reads": [
                  {
                    "columns": [
                      "season_id",
                      "series_id",
                      "title"
                    ],
                    "type": "FullScan",
                    "scan_by": [
                      "series_id",
                      "season_id"
                    ]
                  }
                ]
              },
              {
                "name": "/ru-central1/b1glk1805em030s2ir60/etn9c1c7v3s2bc06lfm8/series",
                "reads": [
                  {
                    "columns": [
                      "series_id",
                      "title"
                    ],
                    "lookup_by": [
                      "series_id (expr)"
                    ],
                    "type": "MultiLookup"
                  }
                ]
              }
            ]
          }
        ]
      }
    } 
    Основная секция (tables) плана запроса содержит информацию об обращениях к таблицам. Операция чтения описываются в разделе reads, а операции записи — в разделе writes (в этом плане запроса данный раздел отсутствует).
    Ключевой характеристикой любого обращения к таблице является его тип.
    Типы чтения:
        FullScan — полное сканирование таблицы, читаются все записи на всех шардах;
        Scan — читается определённый диапазон записей;
        Lookup — чтение по ключу или префиксу ключа;
        MultiLookup — множественные чтения по ключу или префиксу ключа (такой тип обращения возможен, например при выполнении инструкций JOIN).
    Типы записи:
        Upsert — добавление одной записи;
        MultiUpsert — добавление нескольких записей;
        Erase — единичное удаление по ключу;
        MultiErase — множественные удаления.
    Рассмотрим план запроса из нашего примера.
    Параметр lookup_by показывает, по каким колонкам (ключу или префиксу ключа) выполняется чтение. Параметр scan_by показывает, по каким колонкам выполняется scan, то есть чтение всех записей в определённом диапазоне значений. В columns перечислены колонки, значения которых будут считываться из таблицы.
    Из плана запроса следует, что для таблицы seasons будет выполнен FullScan, а для таблицы series — множественные чтения (тип обращения MultiLookup) по ключу series_id (lookup_by). Это говорит нам  о том, что данный запрос составлен не лучшим образом. Тип чтения FullScan означает, что для выполнения запроса потребуется полностью прочитать всю таблицу. Если таблица большая, то такой запрос приведет к избыточному росту нагрузки на БД и задержкам, а в режиме serverless — еще и к повышенным расходам.
    Task:
    Практическая работа. Создание кластера Hadoop
    Decision:
    На этом уроке вы создадите и настроите кластер Hadoop с помощью сервиса Yandex Data Proc. Hadoop предназначается для работы с большими данными, поэтому создание кластера потребует от вас больше усилий, чем на предыдущих практических работах (но гораздо меньше, чем если бы вы делали это самостоятельно).
    Создание кластера
    Для хранения зависимостей заданий нашего кластера и результатов их выполнения нужно предварительно создать бакет в объектном хранилище. О том, как это сделать, мы рассказывали на одном из предыдущих занятий.
    Также создайте сервисный аккаунт для доступа к кластеру. Обратите внимание: можно использовать только аккаунт с ролью mdb.dataproc.agent. Для автоматического масштабирования кластера сервисному аккаунту также понадобятся роли editor и dataproc.agent.
    Откройте каталог, где будете создавать кластер, и выберите сервис Data Proc.
    В открывшемся окне нажмите кнопку Создать кластер.
    Задайте для кластера имя и выберите версию образа — 1.4. В образ включена одна из версий Hadoop и дополнительные компоненты. Некоторые вы можете устанавливать по выбору. Кроме того, в каждую версию образа входит Conda (менеджер окружений для Python) и набор инструментов машинного обучения (scikit-learn, TensorFlow, CatBoost, LightGBM и XGBoost).
    Обратите внимание на то, что некоторые из сервисов обязательны, чтобы использовать другие. На следующем уроке нам понадобится сервис HIVE. Выберите его, и рядом с MAPREDUCE и YARN вы увидите напоминания о том, что они нужны для HIVE.
    Вставьте в поле публичный ключ публичную часть SSH-ключа. Как сгенерировать и использовать SSH-ключи, мы рассказывали в одной из практических работ о виртуальных машинах.
    Выберите созданный сервисный аккаунт для доступа к кластеру.
    Выберите зону доступности для кластера. Все подкластеры будут находиться в этой 
    Если нужно, задайте свойства Hadoop и его компонентов. Доступные свойства перечислены в документации.
    Выберите бакет в объектном хранилище, где будут храниться зависимости заданий и результаты их выполнения.
    Выберите или создайте сеть для кластера. Включите опцию NAT в интернет для подсетей, в которых размещается кластер.
    Если нужно, создайте группу безопасности. Правила для неё вы добавите позже в сервисе Virtual Private Cloud.
    Включите опцию UI Proxy, чтобы получить доступ к веб-интерфейсам компонентов Data Proc. У некоторых компонентов (например Hadoop, Spark, YARN и Zeppelin) есть пользовательские веб-интерфейсы, доступные на мастер-узле кластера. С помощью этих интерфейсов вы можете:
        отслеживать ресурсы кластера и управлять ими (YARN Resource Manager, HDFS NameNode);
        просматривать статус и отлаживать задания (Spark History, JobHistory);
        проводить эксперименты, совместно работать или выполнять отдельные операции (Zeppelin).
    Подробности об интерфейсах вы найдёте в документации.
    Настройка подкластеров
    В состав кластера входит один главный подкластер (Мастер) с управляющим хостом, а также подкластеры для хранения данных (Data) или вычислений (Compute).
    В подкластерах Data можно разворачивать компоненты для хранения данных, а в подкластерах Compute — для обработки данных. Хранилище в подкластере Compute предназначено только для временного хранения обрабатываемых файлов.
    Для каждого подкластера можно задать число и класс хостов, размер и тип хранилища, а также подсеть той сети, в которой расположен кластер. Кроме того, для подкластеров Compute можно настроить автоматическое масштабирование. Это позволит выполнять задания на обработку данных быстрее без дополнительных усилий с вашей стороны.
    Создадим подкластер Compute с одним хостом.
    В блоке Добавить подкластер нажмите кнопку Добавить.
    В поле Роли выберите COMPUTENODE. В блоке Масштабирование включите опцию Автоматическое масштабирование.
    Все открывшиеся настройки знакомы вам из практических работ по созданию виртуальных машин.
    Автоматическое масштабирование подкластеров обработки данных поддерживается в кластерах Yandex Data Proc версии 1.2 и выше. Чтобы оно работало, в кластере с установленным Spark или Hive должен быть также установлен сервис YARN.
    Yandex Data Proc автоматически масштабирует кластер, используя для этого системные метрики нагрузки на кластер. Когда их значение выходит из установленного диапазона, запускается масштабирование. Если значение метрики превысит порог, в подкластер добавятся хосты. Если опустится ниже порога, начнётся декомиссия (высвобождение ненужных ресурсов), а избыточные хосты удалятся.
    По умолчанию для масштабирования используется внутренняя метрика YARN (yarn.cluster.containersPending). Она показывает, сколько единиц ресурсов нужно заданиям в очереди. Выбирайте эту опцию Масштабирование по умолчанию, если в кластере выполняется много относительно небольших заданий.
    Другой вариант — масштабирование на основе метрики загрузки процессора (vCPU). Чтобы использовать его, отключите опцию Масштабирование по умолчанию и укажите целевой уровень загрузки vCPU.
    Настроив подкластеры, нажмите кнопку Создать кластер.
    Сервис запустит создание кластера. После того как статус кластера изменится на Running, вы сможете подключиться к любому активному подкластеру с помощью указанного в настройках SSH-ключа.
    Завершив практическую работу, не удаляйте кластер: он понадобится вам на следующем уроке.
    Task:
    Практическая работа. Подключение к кластеру и работа с Hive
    Decision:
    На этом уроке вы научитесь подключаться к кластеру Hadoop и работать с ним на примере выполнения запросов с помощью Hive.
    Подключение к кластеру
    Подключимся к управляющему хосту главного подкластера. Поскольку хостам кластера Hadoop не назначается публичный IP-адрес, для подключения к ним нужна виртуальная машина, расположенная в той же сети Yandex Cloud.
    Выберите машину, которую создавали раньше, или создайте новую. Подключитесь к ней по SSH. Вы уже делали это, когда изучали виртуальные машины.
    Подключитесь с этой машины к хосту главного подкластера также с помощью SSH. Для этого на машине должна быть закрытая часть SSH-ключа, открытую часть которого вы указали при создании кластера Data Proc. Вы можете скопировать ключ на виртуальную машину или подключаться к ней с запущенным SSH-агентом.
    Скопировать ключ можно с помощью утилиты nano. На виртуальной машине выполните команду:
    sudo nano ~/.ssh/<имя ключа> 
    В открывшийся редактор скопируйте содержимое закрытой части SSH-ключа с вашей локальной машины.
    Запустите SSH-агент:
    eval `ssh-agent -s` 
    Добавьте ключ в список доступных агенту:
    ssh-add ~/.ssh/<имя ключа> 
    Узнайте внутренний FQDN хоста главного подкластера. Для этого в консоли управления на странице кластера перейдите на вкладку Хосты и выберите хост с ролью MASTERNODE.
    Откройте SSH-соединение с хостом Data Proc для пользователя root, например:
    ssh root@<FQDN хоста> 
    Пошаговые инструкции по различным способам подключения к кластеру Data Proc приведены в документации.
    Проверим, что команды Hadoop выполняются, например:
    hadoop version 
    Результат выполнения этой команды выглядит так:
    Запуск заданий Apache Hive
    Как мы уже говорили ранее, Hive — это платформа для хранения данных и управления ими в экосистеме Hadoop. Она используется для доступа к большим датасетам, сохранённым в распределённом хранилище.
    Hive позволяет работать с данными различного формата (csv, tsv, Parquet, ORC, Avro и другими), подключаться к БД и взаимодействовать с ней с помощью SQL-подобного языка запросов. Hive используется преимущественно для работы с данными в HDFS, HBase, S3-совместимых хранилищах и реляционных СУБД.
    Запрос на действия с данными в Hive называется заданием. Задания можно запускать на управляющем хосте с помощью командной оболочки CLI Hive, а также с помощью CLI Yandex Cloud.
    Для запуска Hive CLI выполните команду hive на управляющем хосте.
    Проверьте, всё ли работает: выполните, например, команду select 1; — корректный результат выглядит так:
    Теперь создайте внешнюю таблицу (external table) в формате Parquet, содержащую открытые данные о списке перелётов между городами США в 2018 году. Для этого с помощью Hive CLI выполните запрос:
    hive> CREATE EXTERNAL TABLE flights (Year bigint, Month bigint, FlightDate string, Flight_Number_Reporting_Airline bigint, OriginAirportID bigint, DestAirportID bigint) STORED AS PARQUET LOCATION 's3a://yc-mdb-examples/dataproc/example01/set01'; 
    Проверим список таблиц, выполнив команду show tables. Результат должен выглядеть так:
    Запросим число перелётов с разбивкой по месяцам:
    hive> SELECT Month, COUNT(*) FROM flights GROUP BY Month; 
    Пример результата такого запроса:
    Безусловно, на одном примере сложно показать возможности сервиса Data Proc. Если вас интересует работа с большими данными в облаке, посмотрите доклады сотрудников Yandex Cloud об управлении кластерами Hadoop и заданиями в Data Proc на YouTube-канале Yandex Cloud.
    Task:
    Практическая работа. Создание датасетов, чартов и дашбордов
    Decision:
    Приступим к практике. На этом уроке вы научитесь создавать чарты и дашборды. Мы пройдём по всей цепочке сущностей DataLens начиная с источника данных.
    Изучая ClickHouse, мы анализировали данные о погоде с помощью SQL-запросов. Давайте посмотрим на примере того же самого набора данных, как с помощью DataLens быстро и наглядно показать отличия климата в Москве и Санкт-Петербурге.
    Источник данных
    ClickHouse и DataLens интегрированы друг с другом, поэтому подключение DataLens к ClickHouse можно настроить всего за пару кликов.
    В консоли управления запустите кластер ClickHouse, в котором развёрнута БД с таблицей Weather, созданной вами ранее. Перейдите на страницу кластера, на панели слева выберите DataLens.
    Подключение
    Нажмите кнопку Создать подключение. В открывшемся диалоговом окне вы увидите, что кластер ClickHouse, из которого мы возьмём данные для анализа, имя хоста и имя пользователя БД уже указаны.
    Вам осталось только дать имя подключению в пустом поле вверху, ввести пароль к БД, нажать кнопку Проверить подключение и убедиться, что всё в порядке, а потом — кнопку Создать.
    Датасет
    После того как подключение будет создано, DataLens выведет на панели слева таблицы из БД и предложит создать датасет. Наш датасет будет состоять из одной таблицы: db1.Weather. Перетащите её на центральную панель, и внизу откроется предпросмотр данных.
    Нажмите кнопку Сохранить и задайте имя датасета.
    Подготовим данные. Это важная часть аналитической работы, и её не стоит пропускать. Прежде всего укажем имена полей на русском языке. Перейдите на вкладку Поля и переименуйте их:
        LocalDateTime → Дата и время
        LocalDate → Дата
        Month → Месяц
        Day → День
        TempC → Температура
        Pressure → Давление
        RelHumidity → Влажность
        Тип WindSpeed10MinAvg → Скорость ветра
        VisibilityKm → Видимость
        City → Город
    Поля Дата и время, Дата, Месяц, День, Город будут полями-измерениями, а Температура, Давление, Влажность, Скорость ветра, Видимость — полями-показателями. Зададим для показателей тип агрегации Среднее.
    Чарты
    Приступим к созданию первого чарта. Нажмите кнопку Создать чарт. Выберите тип чарта Линейная диаграмма и перетащите Дата в раздел X , а Температура — в раздел Y.
    На этом примере видно, что средства визуализации иногда помогают быстро проверить качество датасета: есть ли в нём пропущенные или странные, выбивающиеся из общей тенденции данные.
    В нашем случае можно сделать вывод о том, что в датасете не хватает данных примерно с середины 2015-го по середину 2016-го.
    Разделим показатели температуры для двух городов. Для этого перетащим Город в раздел Цвета. Кроме того, округлим значения поля Дата до месяцев, чтобы лучше увидеть, как различаются данные для Москвы и Санкт-Петербурга. Для этого слева от поля Дата нажмите зелёный значок календаря и в разделе Группировка выберите округление по месяцам.
    Из этого графика уже можно делать выводы. В целом температура в Москве выше, чем в Санкт-Петербурге. Летом примерно на 5 градусов, зимой — на 1−2 градуса.
    Сохраните чарт, чтобы затем использовать его для дашборда.
    Чтобы окончательно разобраться с температурой, построим ещё один чарт — Столбчатую диаграмму — и сравним среднегодовую температуру. Выберите тип диаграммы. Добавьте поле Город в раздел X, чтобы разделить отображение значений температуры. Также для поля Дата выберите группировку по годам.
    Кроме того, для чарта понадобится задать фильтр по датам. Поскольку мы сравниваем среднегодовые значения, неполные данные за 2009 и 2019 годы отбросим. В разделе Фильтры нажмите + и выберите поле Дата.
    Для этого чарта мы возьмём только данные из диапазона с начала 2010-го по конец 2018-го. Нажмите кнопку Применить фильтр и сохраните чарт.
    Сделайте сами два таких же чарта с данными о скорости ветра: линейную диаграмму со среднемесячными значениями скорости ветра в городах и столбчатую диаграмму со среднегодовыми значениями.
    Теперь у нас достаточно чартов для информативного дашборда.
    Дашборд
    На панели слева выберите Дашборды и нажмите кнопку Создать дашборд. Введите название дашборда и нажмите Создать.
    Если в каталоге это первый дашборд — он откроется сразу после создания. Если в каталоге есть другие дашборды, вы увидите список. В этом случае выберите из списка только что созданный дашборд.
    Теперь добавим созданные нами чарты на дашборд. Нажмите Добавить и в выпадающем списке выберите Чарт. Поочерёдно выбирайте из списка и добавляйте чарты.
    В результате на дашборде появятся четыре виджета с чартами. Меняйте размеры и положение виджетов для лучшей визуализации.
    Осталось лишь несколько последних штрихов. В том же пункте меню Добавить создадим пару заголовков и селектор по датам. В правом верхнем углу каждого виджета нажмите значок шестерёнки, чтобы изменить названия.
    Сохраним дашборд. У вас могло получиться примерно так:
    То, какие чарты сделать и как их разместить на дашборде, бывает понятно не сразу. Рассмотрите несколько вариантов, когда строите дашборд, чтобы разобраться, какая именно визуализация лучше помогает ответить на вопросы.
    В маркетплейсе DataLens вы найдёте ещё один дашборд с погодой. Он хорошо демонстрирует возможности визуализации данных этого сервиса.
    Чтобы открыть публичный доступ к дашборду, справа от его названия нажмите ··· и выберите Публичный доступ. Скопируйте ссылку. По ней дашборд будет доступен всем, с любых устройств и без аутентификации.
    Task:
    Практическая работа. Начало работы в CLI
    Decision:
    На этой практической работе мы установим утилиту yc, познакомимся с режимом подсказок --help и выполним несколько простых команд.
    Установка CLI
    Первым делом скачайте и установите CLI (если вы ещё не сделали этого в предыдущих курсах).
    Теперь настройте программу для работы с вашим аккаунтом и облаком. Для этого запустите командную строку и введите команду:
    yc init 
    В консоли появится предложение перейти по ссылке, чтобы программа получила доступ к аккаунту и облаку. Перейдите по ссылке, согласитесь с условиями, затем скопируйте токен и вставьте его в окно командной строки.
    Срок жизни токена — один год. Через год необходимо получить новый токен и повторить аутентификацию.
    Если кто-то узнал ваш токен, отзовите его и запросите новый.
    Продолжайте установку: выберите облако, каталог и зону доступности по умолчанию. Следуйте указаниям системы и завершите настройку.
    Готово!
    При установке автоматически создается профиль default, в него записываются выбранные настройки.
    Список профилей можно посмотреть командой:
    yc config profile list 
    Флаг --help
    На прошлом уроке вы узнали о флагах, которые используются при запуске команд. Пожалуй, самый важный и частый флаг — --help (сокращенно -h), поэтому поговорим о нём подробнее.
    Команд для управления ресурсами облака много. Вы запомните некоторые, но помнить всё невозможно. Пользуйтесь флагом --help, когда пишете команду. В этом случае она не выполняется, а в консоль выводится информация о ресурсах, которыми управляет команда, и параметрах запуска.
    Помните, на предыдущем уроке мы говорили о типичной структуре большинства команд CLI (сервис — ресурс — действие — флаги)?
    Важная особенность флага --help заключается в том, что его можно применять для каждого уровня этой структуры и писать команду постепенно.
    В этой практической работе нам понадобится ВМ в облаке. Если у вас нет ВМ — создайте её в консоли управления, как мы это делали в предыдущих курсах.
    Допустим, вы хотите перезапустить эту ВМ, но не помните полный синтаксис команды.
        Сначала вызовите описание сервиса Yandex Compute Cloud:
    yc compute --help 
    Вы увидите синтаксис команды (раздел Usage), список ресурсов, которыми можно управлять (Groups), а также действий (Commands) и флагов:
    Usage:
      yc compute <group|command>
    Groups:
      instance               Manage virtual machine instances
      disk                   Manage disks
      disk-type              Show available disk types
      image                  Manage images
      snapshot               Manage snapshots
      zone                   Show availability zones
      instance-group         Manage instance groups
      placement-group        Manage placement groups
      host-type              Show available host types
      host-group             Manage host groups
      disk-placement-group   Manage disk placement groups
      filesystem             Manage filesystems
    Commands:
      connect-to-serial-port Connect to serial port
    Global Flags:
          --profile string       Set the custom configuration file.
          --debug                Debug logging.
          --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
          --no-user-output       Disable printing user intended output to stderr.
          --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                                 Pass 0 to disable retries. Pass any negative value for infinite retries.
                                 Even infinite retries are capped with 2 minutes timeout.
          --cloud-id string      Set the ID of the cloud to use.
          --folder-id string     Set the ID of the folder to use.
          --folder-name string   Set the name of the folder to use (will be resolved to id).
          --endpoint string      Set the Cloud API endpoint (host:port).
          --token string         Set the OAuth token to use.
          --format string        Set the output format: text (default), yaml, json, json-rest.
      -h, --help                 Display help for the command. 
        Вам нужна ВМ — ресурс instance. Теперь узнайте, как получить список активных ВМ и какая команда отвечает за перезапуск:
    yc compute instance --help 
    Вы увидите список возможных действий (команд) над ВМ:
    Manage virtual machine instances
    Usage:
      yc compute instance <command>
    Aliases:
      instances
    Commands:
      get                      Show information about the specified virtual machine instance
      list                     List virtual machine instances
      create                   Create a virtual machine instance
      create-with-container    Create a virtual machine instance running Docker container
      update                   Update the specified virtual machine instance
      update-container         Update the specified virtual machine instance running Docker container
      add-metadata             Add or update metadata for the specified virtual machine instance
      remove-metadata          Remove keys from metadata for the specified virtual machine instance
      add-labels               Add labels to specified virtual machine instance
      remove-labels            Remove labels from specified virtual machine instance
      delete                   Delete the specified virtual machine instance
      get-serial-port-output   Return the serial port output of the specified virtual machine instance
      stop                     Stop the specified virtual machine instance
      start                    Start the specified virtual machine instance
      restart                  Restart the specified virtual machine instance
      attach-disk              Attach existing disk to the the specified virtual machine instance
      attach-new-disk          Attach new disk to the the specified virtual machine instance
      detach-disk              Detach disk from the the specified virtual machine instance
      attach-filesystem        Attach existing filesystem to the specified virtual machine instance
      detach-filesystem        Detach filesystem from the specified virtual machine instance
      update-network-interface Update the specified network interface of virtual machine instance
      add-one-to-one-nat       Add one-to-one NAT to the the specified network interface of virtual machine instance
      remove-one-to-one-nat    Remove one-to-one NAT to the the specified network interface of virtual machine instance
      list-operations          List operations for the specified instance
    Global Flags:
          --profile string       Set the custom configuration file.
          --debug                Debug logging.
          --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
          --no-user-output       Disable printing user intended output to stderr.
          --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                                 Pass 0 to disable retries. Pass any negative value for infinite retries.
                                 Even infinite retries are capped with 2 minutes timeout.
          --cloud-id string      Set the ID of the cloud to use.
          --folder-id string     Set the ID of the folder to use.
          --folder-name string   Set the name of the folder to use (will be resolved to id).
          --endpoint string      Set the Cloud API endpoint (host:port).
          --token string         Set the OAuth token to use.
          --format string        Set the output format: text (default), yaml, json, json-rest.
      -h, --help                 Display help for the command. 
        Сначала получите список ВМ, это команда list:
    yc compute instance list 
    Результатом выполнения команды будет список машин в том каталоге, где вы работаете, с именами и идентификаторами:
    +----------------------+-------------+---------------+---------+----------------+-------------+
    |          ID          |    NAME     |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
    +----------------------+-------------+---------------+---------+----------------+-------------+
    | fhm2p20bifmg3k3voda7 | my-instance | ru-central1-a | RUNNING | ХХХ.ХХХ.ХХХ.ХХ | ХХ.ХХХ.Х.ХХ |
    +----------------------+-------------+---------------+---------+----------------+-------------+ 
        На шаге 2 вы нашли команду перезапуска — restart, теперь можно посмотреть её синтаксис:
    yc compute instance restart --help 
    Результат выполнения будет таким:
    Restart the specified virtual machine instance
    Usage:
      yc compute instance restart <INSTANCE-NAME>|<INSTANCE-ID> [Global Flags...]
    Flags:
          --id string     Instance id.
          --name string   Instance name.
          --async         Display information about the operation in progress, without waiting for the operation to complete.
    Global Flags:
          --profile string       Set the custom configuration file.
          --debug                Debug logging.
          --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
          --no-user-output       Disable printing user intended output to stderr.
          --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                                 Pass 0 to disable retries. Pass any negative value for infinite retries.
                                 Even infinite retries are capped with 2 minutes timeout.
          --cloud-id string      Set the ID of the cloud to use.
          --folder-id string     Set the ID of the folder to use.
          --folder-name string   Set the name of the folder to use (will be resolved to id).
          --token string         Set the OAuth token to use.
          --format string        Set the output format: text (default), yaml, json, json-rest.
      -h, --help                 Display help for the command. 
        Наконец, вы получили полный синтаксис команды перезапуска. Примените её к ВМ:
    yc compute instance restart --name <имя_ВМ> 
    По такому принципу вы можете сформировать в консоли любую команду.
    Итак, подведём итоги. Чтобы уточнить синтаксис или порядок действий при работе с CLI, выберите один из трёх путей:
        найдите нужное действие на странице сервиса Yandex Cloud в документации и там переключитесь на вкладку CLI;
        найти команду в справочнике о yc;
        воспользуйтесь флагом --help, шаг за шагом уточняя возможности и синтаксис команды.
    Синхронный и асинхронный режимы работы
    Некоторые команды выполняются очень быстро. Например, создание каталога или просмотр настроек профиля. Такие команды можно выполнять подряд, одну за другой — без задержек.
    Если при выполнении команды ресурс изменяет состояние, создается операция. Примеры операций — перезапуск ВМ после обновления или резервное копирование базы данных.
    Если каждая следующая команда ждёт завершения предыдущей операции, такой режим выполнения называется синхронным. Когда какая-то операция в синхронном режиме выполняется долго, CLI выводит в консоли точки и время с начала операции, чтобы показать, что процесс не завис:
    ...1s...6s...12s...17s 
    Операция может выполняться довольно долго, а ожидание — затормозить процесс. В таких случаях важно оценить, нужен ли результат операции для выполнения следующих команд. Если нет — можно не ждать её завершения и сразу же переходить к следующей команде. Этот режим называется асинхронным.
    Чтобы выполнить команду асинхронно, используйте флаг --async.
    Перезапустите одну из ранее созданных ВМ в асинхронном режиме (в команде укажите имя этой ВМ):
    yc compute instance restart --name <имя_ВМ> --async 
    В ответ на асинхронный вызов CLI выводит идентификатор операции (в поле id) и информацию о ней:
    id: fhm5k7iq03rm2s7enhdk
    description: Restart instance
    created_at: "2021-03-27T08:32:47.562595036Z"
    created_by: aje9cb7k03512mrugcee
    modified_at: "2021-03-27T08:32:47.562595036Z"
    metadata:
      '@type': type.googleapis.com/yandex.cloud.compute.v1.RestartInstanceMetadata
      instance_id: fhm2p20bifmg3k3voda7 
    С помощью идентификатора операции вы можете проверить результаты её выполнения:
    yc operation get <идентификатор_операции> 
    Когда операция завершится, вы увидите результат:
    id: fhm5k7iq03rm2s7enhdk
    ...
    done: true
    ... 
    Decision:
    $ yc init
    $ yc config profile list
    $ yc compute --help
    $ yc compute instance --help
    $ yc compute instance list
    $ yc compute instance restart --help
    $ yc compute instance restart --name ubuntu-test
    $ yc compute instance restart --name ubuntu-test --async
    $ yc operation get fhmq5a4aren4lmigbt03
    Task:
    Практическая работа. Создание виртуальных машин с помощью CLI
    Decision:
    Создание ВМ — одна из самых сложных команд CLI, потому что в ней очень много параметров. Давайте потренируемся работать с ней.  Но сначала подготовим окружение.
    Мы будем работать в каталоге по умолчанию. Создадим в нём сеть, в ней — три подсети, а затем по ВМ в каждой подсети.
        Создайте сеть my-network в Virtual Private Cloud. Эта команда относится к группе vpc.
    Проверьте синтаксис команды
    Выполнение операции займёт какое-то время. В итоге сеть появится в каталоге, который вы выбрали в предыдущей практической работе.
        Теперь создадим три подсети в разных зонах доступности (ru-central1-a, ru-central1-b, ru-central1-c). Чтобы проще было выполнять задания дальше, назовите их my-subnet-1, my-subnet-2 и my-subnet-3. А пространства IP-адресов для подсетей укажите, соответственно, как 192.168.1.0/24, 192.168.2.0/24 и 192.168.3.0/24.
    Не забудьте указать, что вы создаёте подсети в новой сети, созданной на предыдущем шаге. Иначе они появятся в сети по умолчанию, которая есть в каждом каталоге.
    Вот так выглядит команда создания подсети в зоне доступности ru-central1-a:
    yc vpc subnet create \
      --name my-subnet-1 \
      --zone ru-central1-a \
      --range 192.168.1.0/24 \
      --network-name my-network 
    Где:
        name — имя подсети.
        zone — зона доступности.
        range — адресное пространство подсети.
        network-name — имя сети, в которой создаётся подсеть.
    Создайте две другие подсети сами.
    Проверьте синтаксис команд
    yc vpc subnet create \
      --name my-subnet-2 \
      --zone ru-central1-b \
      --range 192.168.2.0/24 \
      --network-name my-network
    yc vpc subnet create \
      --name my-subnet-3 \
      --zone ru-central1-c \
      --range 192.168.3.0/24 \
      --network-name my-network 
          Осталось создать три ВМ в нужных зонах доступности и привязать к ним подсети.
    Пусть машины работают под управлением ОС Ubuntu 20.04 LTS, имеют диски объёмом 30 Гб, 4 Гб оперативной памяти и два виртуальных процессорных ядра.
    ВМ могут создаваться долго, поэтому запускайте команды в асинхронном режиме.
    Пример команды для первой ВМ:
    yc compute instance create \
      --name my-instance-1 \
      --hostname my-instance-1 \
      --zone ru-central1-a \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
      --async 
    Посмотрите внимательно на все параметры. Подумайте, какой из них за что отвечает.
    Создайте две другие машины сами.
    Проверьте синтаксис команд
    yc compute instance create \
      --name my-instance-2 \
      --hostname my-instance-2 \
      --zone ru-central1-b \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
      --async
     yc compute instance create \
      --name my-instance-3 \
      --hostname my-instance-3 \
      --zone ru-central1-c \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
      --async 
    После выполнения каждой команды благодаря флагу --async вы получите идентификатор операции и ее описание в виде:
    id: c9q9v4bsn1hs9api4b13
    description: Create instance
    created_at: "2021-03-01T03:23:00.079888Z"
    created_by: aje8s4vd4pp7cduq2o4k
    modified_at: "2022-07-29T09:14:53.567744154Z"
    metadata:
      '@type': type.googleapis.com/yandex.cloud.compute.v1.CreateInstanceMetadata
      instance_id: fhmepiaciq5l9slqid3k 
        Проследите за статусом одной из операций, используя ее идентификатор.
    Проверьте синтаксис команды
    yc operation get <идентификатор_операции> 
        Дождитесь, пока операция завершится. Используйте для этого команду wait.
    Проверьте синтаксис команды
    yc operation wait <идентификатор_операции> 
        Убедитесь, что ВМ созданы. Для этого выведите их список.
    yc compute instance list 
    По умолчанию список выдается в виде таблицы:
    +----------------------+---------------+---------------+---------+---------------+--------------+
    |          ID          |     NAME      |    ZONE ID    | STATUS  |  EXTERNAL IP  | INTERNAL IP  |
    +----------------------+---------------+---------------+---------+---------------+--------------+
    | ef34r4fs8dsva3qtsivs | my-instance-3 | ru-central1-c | RUNNING | 51.250.44.130 | 192.168.3.34 |
    | epdj7u79isrolup3vfo8 | my-instance-2 | ru-central1-b | RUNNING | 158.160.6.249 | 192.168.2.28 |
    | fhmepiaciq5l9slqid3k | my-instance-1 | ru-central1-a | RUNNING | 62.84.126.39  | 192.168.1.30 |
    +----------------------+---------------+---------------+---------+---------------+--------------+ 
    Список можно вывести в формате YAML или JSON (эта возможность пригодится вам на следующих уроках):
    yc compute instance list --format json 
    Список в формате JSON содержит больше информации, чем таблица.
    Посмотрите результат
    [
      {
        "id": "ef3rutmaas72bsujcja7",
        "folder_id": "b1gfdbij3ijgopgqv9m9",
        "created_at": "2021-06-21T12:41:10Z",
        "name": "my-instance-3",
        "zone_id": "ru-central1-c",
        "platform_id": "standard-v2",
        "resources": {
          "memory": "4294967296",
          "cores": "2",
          "core_fraction": "100"
        },
        "status": "RUNNING",
        "metadata_options": {
          "gce_http_endpoint": "ENABLED",
          "aws_v1_http_endpoint": "ENABLED",
          "gce_http_token": "ENABLED",
          "aws_v1_http_token": "ENABLED"
        },
        "boot_disk": {
          "mode": "READ_WRITE",
          "device_name": "ef3v2lor1u4pfn3ce1al",
          "auto_delete": true,
          "disk_id": "ef3v2lor1u4pfn3ce1al"
        },
        "network_interfaces": [
          {
            "index": "0",
            "mac_address": "d0:0d:1b:f7:6c:a5",
            "subnet_id": "b0c4h992tbuodl5hudpu",
            "primary_v4_address": {
              "address": "10.128.0.32",
              "one_to_one_nat": {
                "address": "178.154.212.5",
                "ip_version": "IPV4"
              }
            }
          }
        ],
        "fqdn": "my-instance-3.ru-central1.internal",
        "scheduling_policy": {},
        "network_settings": {
          "type": "STANDARD"
        },
        "placement_policy": {}
      },
      {
        "id": "epd928ffks7m8ssc4i3k",
        "folder_id": "b1gfdbij3ijgopgqv9m9",
        "created_at": "2021-06-21T12:40:35Z",
        "name": "my-instance-2",
        "zone_id": "ru-central1-b",
        "platform_id": "standard-v2",
        "resources": {
          "memory": "4294967296",
          "cores": "2",
          "core_fraction": "100"
        },
        "status": "RUNNING",
        "metadata_options": {
          "gce_http_endpoint": "ENABLED",
          "aws_v1_http_endpoint": "ENABLED",
          "gce_http_token": "ENABLED",
          "aws_v1_http_token": "ENABLED"
        },
        "boot_disk": {
          "mode": "READ_WRITE",
          "device_name": "epddf7t0ljn9i1jp2pbs",
          "auto_delete": true,
          "disk_id": "epddf7t0ljn9i1jp2pbs"
        },
        "network_interfaces": [
          {
            "index": "0",
            "mac_address": "d0:0d:91:21:ef:a7",
            "subnet_id": "e2l1fgq2fbhnp6b929t7",
            "primary_v4_address": {
              "address": "10.129.0.9",
              "one_to_one_nat": {
                "address": "84.201.176.134",
                "ip_version": "IPV4"
              }
            }
          }
        ],
        "fqdn": "my-instance-2.ru-central1.internal",
        "scheduling_policy": {},
        "network_settings": {
          "type": "STANDARD"
        },
        "placement_policy": {}
      },
      {
        "id": "fhm1op9id0dc6bubfags",
        "folder_id": "b1gfdbij3ijgopgqv9m9",
        "created_at": "2021-06-21T12:39:43Z",
        "name": "my-instance-1",
        "zone_id": "ru-central1-a",
        "platform_id": "standard-v2",
        "resources": {
          "memory": "4294967296",
          "cores": "2",
          "core_fraction": "100"
        },
        "status": "RUNNING",
        "metadata_options": {
          "gce_http_endpoint": "ENABLED",
          "aws_v1_http_endpoint": "ENABLED",
          "gce_http_token": "ENABLED",
          "aws_v1_http_token": "ENABLED"
        },
        "boot_disk": {
          "mode": "READ_WRITE",
          "device_name": "fhmms7r7ia4uteikv1to",
          "auto_delete": true,
          "disk_id": "fhmms7r7ia4uteikv1to"
        },
        "network_interfaces": [
          {
            "index": "0",
            "mac_address": "d0:0d:1c:65:32:68",
            "subnet_id": "e9bcvlanhbum9ggdvkh2",
            "primary_v4_address": {
              "address": "10.130.0.6",
              "one_to_one_nat": {
                "address": "178.154.225.167",
                "ip_version": "IPV4"
              }
            }
          }
        ],
        "fqdn": "my-instance-1.ru-central1.internal",
        "scheduling_policy": {},
        "network_settings": {
          "type": "STANDARD"
        },
        "placement_policy": {}
      }
    ] 
        Чтобы избежать ненужных расходов, удалите три созданные ВМ (в следующих практических работах они не понадобятся).
    yc compute instance delete my-instance-1 my-instance-2 my-instance-3 
    Decision:
    $ yc vpc network create --name my-network
    $ yc vpc subnet create \
      --name my-subnet-1 \
      --zone ru-central1-a \
      --range 192.168.1.0/24 \
      --network-name my-network
    $ yc vpc subnet create \
      --name my-subnet-2 \
      --zone ru-central1-b \
      --range 192.168.2.0/24 \
      --network-name my-network
    $ yc vpc subnet create \
      --name my-subnet-3 \
      --zone ru-central1-c \
      --range 192.168.3.0/24 \
      --network-name my-network
    $ yc compute instance create \
      --name my-instance-1 \
      --hostname my-instance-1 \
      --zone ru-central1-a \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
      --async
    $ yc compute instance create \
      --name my-instance-2 \
      --hostname my-instance-2 \
      --zone ru-central1-b \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
      --async
    $ yc compute instance create \
      --name my-instance-3 \
      --hostname my-instance-3 \
      --zone ru-central1-c \
      --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
      --image-folder-id standard-images \
      --memory 4 --cores 2 --core-fraction 100 \
      --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
      --async 
    $ yc operation get ef3vcn9383adr1anudcg
    $ yc operation get epdk80vb9jjjc3190s6a
    $ yc operation get fhm0u5fo2bq3cl2gagpr
    $ yc operation wait fhm0u5fo2bq3cl2gagpr
    $ yc compute instance list
    $ yc compute instance list --format json
    $ yc compute instance delete my-instance-1 my-instance-2 my-instance-3
    Task:
    Практическая работа. Использование файлов спецификаций
    Decision:
    В этой практической работе вы создадите, обновите и удалите группу ВМ.
    Вы уже убедились, что создать даже одну ВМ через yc непросто: нужно установить много разных параметров. Создание группы ВМ требует ещё больше параметров. Чтобы не указывать их все в командной строке, конфигурацию описывают в файле, который используют при создании группы. Такой файл называется спецификацией. Использование спецификаций — это первый шаг в освоении подхода Infrastructure as Code (IaC), который мы будем применять на следующих уроках.
    Спецификации пишутся в разных форматах. Для группы ВМ используется язык YAML. Если вы не знакомы с ним — ничего страшного. В документации есть шаблоны спецификаций, и на первых порах вам будет достаточно лишь немного их изменять. Ниже мы разберём, как составлять спецификации.
    Часть 1. Создание Instance Group
        Для разворачивания группы ВМ потребуется сеть. Если сети ещё нет, создайте её.
        Посмотрите информацию об имеющихся сетях.
     yc vpc network list
    Сохраните идентификатор сети, он понадобиться нам в дальнейшем.
    По умолчанию все операции в Instance Groups выполняются от имени сервисного аккаунта c ролью editor на каталог. Если сервисного аккаунта нет, то тоже создайте его и назначьте эту роль.
    Посмотрите информацию об имеющихся сервисных аккаунтах.
     yc iam service-account list
        Сохраните идентификатор сервисного аккаунта, он понадобится нам в дальнейшем.
        Для создания группы необходимо подготовить её спецификацию. Создайте в любом текстовом редакторе файл с расширением yaml, например specification.yaml.
    Обратите внимание: в формате YAML важны отступы слева. Даже если текст правильный, но отступы не соблюдены, при выполнении спецификации возникнут ошибки.
        Сначала внесите информацию о группе. Пусть группа называется my-group. Укажите идентификатор сервисного аккаунта, от имени которого будете работать (см. шаг 2).
    Идентификаторы ресурсов уникальны. Копируя команды из текста урока, не забывайте подставлять свои идентификаторы.
        name: my-group
        service_account_id: <идентификатор_сервисного_аккаунта>
    Наша группа будет содержать три одинаковые ВМ. Машины создадим из публичного образа Ubuntu 18.04 LTS (возьмём не последнюю версию, чтобы потренироваться обновлять ВМ). Узнайте идентификатор образа с помощью команды:
     yc compute image list --folder-id standard-images
    В столбце FAMILY найдите ubuntu-1804-lts, в столбце ID будет указан нужный идентификатор.
    Опишите в спецификации ВМ. Это раздел instance_template.
    Пусть каждая машина использует платформу Intel Broadwell (посмотрите поддерживаемые платформы в документации Yandex Compute Cloud),  имеет 2 Гб оперативной памяти и два процессорных ядра.
     instance_template:
         platform_id: standard-v1
         resources_spec:
             memory: 2g
             cores: 2
    Добавьте описание загрузочного диска. Он будет использоваться на чтение и запись (режим READ_WRITE). Укажите идентификатор образа, который получили на шаге 5. Выделите сетевой HDD объёмом 32 Гб.
         boot_disk_spec:
            mode: READ_WRITE
            disk_spec:
                image_id: <идентификатор_образа>
                type_id: network-hdd
                size: 32g
    Теперь опишите сеть: идентификатор сети из каталога по умолчанию (см. шаг 1). Задайте публичный IP-адрес, чтобы к ВМ можно было обращаться извне.
         network_interface_specs:
             - network_id: <идентификатор_сети>
               primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
    В политике планирования укажите, что машина не прерываемая.
         scheduling_policy:
             preemptible: false
    В политике развертывания (раздел deploy policy) укажите, что в каждый момент времени может быть неработоспособной только одна машина, не больше. Запретите увеличивать число ВМ, т. е. создавать больше трех машин одновременно. Мы чуть подробнее разберём эти настройки, когда будем обновлять ВМ в группе.
         deploy_policy:
             max_unavailable: 1
             max_expansion: 0
    Мы создаем группу фиксированного размера из трёх ВМ. Укажите это в политике масштабирования (раздел scale_policy):
        scale_policy:
            fixed_scale:
            size: 3 
    Наконец, в политике распределения машин по зонам (раздел allocation_policy) укажите, что будет использоваться зона ru-central1-a. Мы делаем это для простоты. Лучше распределять ВМ группы по разным зонам доступности — это позволит пережить краткие сбои или выход зоны из строя.
        allocation_policy:
            zones:
                - zone_id: ru-central1-a 
    Для балансировщика нагрузки (раздел load_balancer_spec) укажите целевую группу, к которой он будет привязан (это мы рассмотрим чуть ниже).
        load_balancer_spec:
            target_group_spec:
            name: my-target-group 
    Нашей спецификации уже достаточно, чтобы создать группу ВМ. Но на эти машины не будет установлено никакого ПО, только операционная система из публичного образа. Если не менять конфигурацию, то после создания ВМ вам придётся устанавливать программы вручную.
    Чтобы сэкономить время и сократить число ошибок, давайте максимально автоматизируем создание ВМ, включая установку ПО. Для этого добавим в конфигурацию машины секцию, где будут вызываться команды установки программ. В этой же секции можно описать создание пользователей, но мы этого делать не будем, так как заходить на ВМ не планируем.
    Установим на машины веб-сервер NGINX и на веб-странице index.nginx-debian.html, которая создается по умолчанию и выводит приветственное сообщение «Welcome to nginx», заменим слово nginx идентификатором активной ВМ и версией ОС. Поскольку мы подключим балансировщик нагрузки, идентификатор активной ВМ будет различаться для разных пользователей. Это и позволит нам убедиться в том, что балансировщик работает.
    Для установки ПО используйте cloud-init — пакет, выполняющий команды на ВМ при первом запуске. Вы узнали о нём из курса о ВМ. Команды опишите в блоке конфигурации #cloud-config. Примеры команд смотрите в документации cloud-init.
    Содержимое #cloud-config описывается в разделе instance_template в секции metadata:
        metadata:
          user-data: |-
            #cloud-config
              package_update: true
              runcmd:
                - [apt-get, install, -y, nginx ]
                - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html'] 
        Спецификация готова. Вот ее полный текст. Помните, что в формате YAML важно соблюдать отступы слева.
    name: my-group
    service_account_id: ajeu495h1s9tn1rorulb
    instance_template:
        platform_id: standard-v1
        resources_spec:
            memory: 2g
            cores: 2
        boot_disk_spec:
            mode: READ_WRITE
            disk_spec:
                image_id: fd8fosbegvnhj5haiuoq 
                type_id: network-hdd
                size: 32g
        network_interface_specs:
            - network_id: enpnr4onfs6ihtoao32u
              primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
        scheduling_policy:
            preemptible: false
        metadata:
          user-data: |-
            #cloud-config
              package_update: true
              runcmd:
                - [ apt-get, install, -y, nginx ]
                - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
    deploy_policy:
        max_unavailable: 1
        max_expansion: 0
    scale_policy:
        fixed_scale:
            size: 3
    allocation_policy:
        zones:
            - zone_id: ru-central1-a
    load_balancer_spec:
        target_group_spec:
            name: my-target-group 
    Теперь создайте группу ВМ по подготовленной спецификации. Уточните синтаксис команды сами:
    yc compute instance-group --help 
    Проверить синтаксис команды
    yc compute instance-group create --file <путь_к_файлу_specification.yaml>  
    Для тренировки можете вызвать эту команду в асинхронном режиме, а затем проверить её статус и дождаться завершения.
        Убедитесь, что группа создана, в веб-консоли или выведя список групп с помощью yc.
    yc compute instance-group list 
    В списке вы должны увидеть свою группу машин my-group:
    +----------------------+------------+------+
    |          ID          |    NAME    | SIZE |
    +----------------------+------------+------+
    | amc65sbgfqeqf00m02sc | my-group   |    3 |
    +----------------------+------------+------+ 
    Часть 2. Балансировщик
        Создайте балансировщик my-load-balancer. Посмотрите, какие параметры должны быть у соответствующей команды:
    yc load-balancer network-load-balancer create --help 
    В выводе справки обратите внимание, что при создании балансировщика можно сразу создать и обработчик входящего трафика (параметр --listener).
    Формат параметра --listener достаточно хитрый: в нём можно указать сразу несколько подпараметров через запятую:
    ...
    --listener name=my-listener,external-ip-version=ipv4,port=80
    ... 
    Помимо имени обработчика, здесь указывается версия IP-протокола и порт, на котором балансировщик будет принимать трафик.
    Проверить синтаксис команды
    yc load-balancer network-load-balancer create \
      --region-id ru-central1 \
      --name my-load-balancer \
      --listener name=my-listener,external-ip-version=ipv4,port=80 
    Затем подключите к балансировщику целевую группу (команда attach-target-group). Вам понадобится идентификатор целевой группы. Чтобы узнать его, запросите с помощью yc список доступных целевых групп и выберите ту, которую вы указали в спецификации specification.yaml. 
    Проверить синтаксис команды
    yc load-balancer target-group list 
    Целевая группа также подключается с помощью нескольких подпараметров, которые соответствуют настройкам в консоли управления (их вы изучали на первом курсе). Для целевой группы укажите такие параметры:
        target-group-id — идентификатор группы;
        healthcheck-name, healthcheck-interval, healthcheck-timeout, healthcheck-unhealthythreshold, healthcheck-healthythreshold, healthcheck-http-port — параметры проверки состояния (см. документацию). Эти параметры аналогичны тем, что задаются в консоли управления при создании балансировщика. Вы изучали их в первом курсе.
    Укажите 80-й порт, на котором запущен NGINX.
    Проверить синтаксис команды
    yc load-balancer network-load-balancer attach-target-group my-load-balancer \ 
      --target-group target-group-id=<идентификатор целевой группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
    Можно не выполнять две команды (создание балансировщика и подключение целевой группы) по очереди, а одной командой create создать балансировщик с привязанной целевой группой.
        Убедитесь, что балансировщик создан, а целевая группа подключена через консоль управления или с помощью yc.
    Часть 3. Доступ к машинам группы
        Проверьте состояние машин группы. Для этого запросите список машин и дождитесь статуса HEALTHY.
    yc load-balancer network-load-balancer target-states my-load-balancer \
        --target-group-id <идентификатор_целевой_группы> 
    Теперь откройте в браузере страницу балансировщика. IP-адрес балансировщика вы можете узнать с помощью консоли управления или yc.
    На странице вы увидите приветственное сообщение и в нём идентификатор одной из машин.
    Часть 4. Обновление Instance Group
        При создании на ВМ группы была установлена ОС Ubuntu 18.04 LTS. Теперь обновите её до Ubuntu 20.04 LTS (ubuntu-2004-lts в столбце FAMILY). Ещё раз посмотрите список доступных образов (см. часть 1) и в файле спецификации specification.yaml измените параметр image_id.
    ...
    boot_disk_spec:
       mode: READ_WRITE
       disk_spec:
           image_id: <идентификатор_образа>
           type_id: network-hdd
           size: 32g
    ... 
        Теперь запустите обновление группы с изменённым файлом спецификации.
    Проверить синтаксис команды
    yc compute instance-group update \
      --name my-group \
      --file <путь_к_файлу_specification.yaml> 
    Группа будет обновляться постепенно: когда одна машина из группы удаляется, ей на замену создаётся новая. Общее число машин в группе не увеличится. Именно такую политику обновления мы задали в файле спецификации (см. часть 1):
    ...
    deploy_policy:
        max_unavailable: 1
        max_expansion: 0
    ... 
    Есть и другой режим обновления: сначала в группу добавляется ВМ с новой конфигурацией, а затем отключается старая машина. Это повторяется, пока не обновятся все машины. Такому режиму соответствовала бы другая конфигурация:
    ...
    deploy_policy:
        max_unavailable: 0
        max_expansion: 1
    ... 
    Убедитесь, что машины обновились. На приветственной странице должна выводиться новая версия ОС.
    Часть 5. Удаление машины из группы
    На приветственной странице балансировщика посмотрите имя активной машины и попробуйте удалить ее. Убедитесь, что приветственная страница остаётся доступна всё время: балансировщик переключит трафик на другую машину группы. А Yandex Cloud тем временем пересоздаст удалённую машину.
    Проверить синтаксис команды
    yc compute instance delete <имя_ВМ> 
    Часть 6. Удаление Instance Group
    Теперь удалите группу и балансировщик командами yc.
    Проверить синтаксис команд
    yc compute instance-group delete --name my-group
    yc load-balancer network-load-balancer delete --name my-load-balancer 
    Кстати, ключевой параметр --name можно и не писать. Достаточно указать имя группы или балансировщика.
    Убедитесь, что группы и балансировщика больше нет, через консоль управления или с помощью yc. 
    Decision:
    $ yc vpc network list
    $ yc iam service-account list
    $ vim specification.yaml
    $ cat specification.yaml
    name: my-group
    service_account_id: ajeq7kga9ms7bhup4gbe
    $ yc compute image list --folder-id standard-images
    $ vim specification.yaml
    $ cat specification.yaml
    name: my-group
    service_account_id: ajeq7kga9ms7bhup4gbe
    instance_template:
        platform_id: standard-v1
        resources_spec:
            memory: 2g
            cores: 2
        boot_disk_spec:
            mode: READ_WRITE
            disk_spec:
                image_id: fd8k6joqhuk8ts8eb1ao 
                type_id: network-hdd
                size: 32g
        network_interface_specs:
            - network_id: enpboucd6803lg6jspnh
              primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
        scheduling_policy:
            preemptible: false
        metadata:
          user-data: |-
            #cloud-config
              package_update: true
              runcmd:
                - [ apt-get, install, -y, nginx ]
                - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
    deploy_policy:
        max_unavailable: 1
        max_expansion: 0
    scale_policy:
        fixed_scale:
            size: 3
    allocation_policy:
        zones:
            - zone_id: ru-central1-a
    load_balancer_spec:
        target_group_spec:
            name: my-target-group
    $ yc compute instance-group --help 
    $ yc compute instance-group create --file /home/administrator/specification.yaml
    $ yc compute instance-group list
    $ yc load-balancer network-load-balancer create --help
    $ yc load-balancer network-load-balancer create \
      --region-id ru-central1 \
      --name my-load-balancer \
      --listener name=my-listener,external-ip-version=ipv4,port=80 
    $ yc load-balancer target-group list
    $ yc load-balancer network-load-balancer attach-target-group my-load-balancer \ 
    --target-group target-group-id=enp3edjdaoot0v64qth0,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
    $ yc load-balancer network-load-balancer target-states my-load-balancer \
    --target-group-id enp3edjdaoot0v64qth0
    $ yc compute instance-group update \
      --name my-group \
      --file /home/administrator/specification.yaml
    $ yc compute instance-group delete --name my-group
    $ yc load-balancer network-load-balancer delete --name my-load-balancer
    Task:
    Практическая работа. Создаём образ виртуальной машины
    Decision:
    В этой практической работе вы установите Packer, подготовите с его помощью образ, а затем создадите из образа виртуальную машину.
        Установите Packer, если ещё не сделали это на предыдущем уроке. Он поддерживает все популярные операционные системы — Windows, macOS, Linux и FreeBSD.
        Скачать дистрибутив Packer для вашей ОС также можно с зеркала Yandex Cloud.
        Подготовьте файл в формате HCL со спецификацией образа, например my-ubuntu-nginx.pkr.hcl.
        При создании файла опирайтесь на документацию Packer.
        В качестве примера можете взять спецификацию из предыдущего урока:
     source "yandex" "ubuntu-nginx" {
       token               = "<OAuth-токен>"
       folder_id           = "<идентификатор_каталога>"
       source_image_family = "ubuntu-2004-lts"
       ssh_username        = "ubuntu"
       use_ipv4_nat        = "true"
       image_description   = "my custom ubuntu with nginx"
       image_family        = "ubuntu-2004-lts"
       image_name          = "my-ubuntu-nginx"
       subnet_id           = "<идентификатор_подсети>"
       disk_type           = "network-ssd"
       zone                = "ru-central1-a"
     }
     build {
       sources = ["source.yandex.ubuntu-nginx"]
       provisioner "shell" {
         inline = ["sudo apt-get update -y",
               "sudo apt-get install -y nginx",
               "sudo systemctl enable nginx.service"]
       }
     }
    Не забудьте подставить в спецификацию идентификаторы своего каталога и подсети (подсеть должна быть в той же зоне доступности, которая указана в параметре zone). Также укажите свой OAuth-токен (или воспользуйтесь переменной окружения YC_TOKEN при сборке образа).
    Теперь создайте образ ВМ на основе файла спецификации:
     packer build <путь_к_файлу_my-ubuntu-nginx.pkr.hcl>
        После того как команда отработает, убедитесь, что образ появился в каталоге. Для этого в консоли управления перейдите в сервис Compute Cloud. Найдите образ на вкладке Образы.
        Перейдите на вкладку Виртуальные машины и начните создавать ВМ.
        Раньше для создания загрузочного диска вы выбирали один из публичных образов, например Ubuntu 20.04. Теперь вместо этого переключитесь на вкладку Пользовательские. Нажмите кнопку Выбрать и в открывшемся окне переключитесь на вкладку Образ.
        Выберите созданный образ и нажмите Применить.
        Из образа создастся загрузочный диск.
        Завершите создание ВМ.
        Проверьте ВМ: введите её IP-адрес в адресную строку браузера. Убедитесь, что веб-сервер работает.
        Удалите ВМ: на следующих уроках она не понадобится. А вот образ удалять не стоит.
    Task:
    Практическая работа. Создаём виртуальную машину из образа и базу данных
    Decision:
    В этой практической работе вы установите Terraform и подготовите спецификацию, с помощью которой создадите виртуальную машину, а затем управляемую базу данных.
    Подсказки для создания спецификации смотрите в документации Yandex Cloud и в справочнике ресурсов (раздел Resources).
    Установите Terraform. Дистрибутив для вашей платформы можно скачать из зеркала. После загрузки добавьте путь к папке, в которой находится исполняемый файл, в переменную PATH.
    Настройте провайдер.
    Если раньше у вас был настроен провайдер из реестра Hashicorp, сохраните его настройки:
    mv ~/.terraformrc ~/.terraformrc.old
    Укажите источник, из которого будет устанавливаться провайдер.
    Откройте файл конфигурации Terraform CLI:
    nano ~/.terraformrc
    Добавьте в него следующий блок:
    provider_installation {
      network_mirror {
        url = "https://terraform-mirror.yandexcloud.net/"
        include = ["registry.terraform.io/*/*"]
      }
      direct {
        exclude = ["registry.terraform.io/*/*"]
      }
    }
    Подробнее о настройках зеркал см. в документации.
    В начале конфигурационного файла .tf добавьте следующие блоки:
    terraform {
      required_providers {
        yandex = {
          source = "yandex-cloud/yandex"
        }
      }
      required_version = ">= 0.13"
    }
    provider "yandex" {
      zone = "<зона доступности по умолчанию>"
    }
    Где:
        source — глобальный адрес источника провайдера.
        required_version — минимальная версия Terraform, с которой совместим провайдер.
        provider — название провайдера.
        zone — зона доступности, в которой по умолчанию будут создаваться все облачные ресурсы.
    Выполните команду terraform init в папке с конфигурационным файлом .tf. Эта команда инициализирует провайдеров, указанных в конфигурационных файлах, и позволяет работать с ресурсами и источниками данных провайдера.
    Если провайдер не установился, создайте обращение в поддержку с именем и версией провайдера.
    Если вы использовали файл .terraform.lock.hcl, то перед инициализацией выполните команду terraform providers lock, указав адрес зеркала, откуда будет загружаться провайдер, и платформы, на которых будет использоваться конфигурация:
    terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 -platform=darwin_arm64 yandex-cloud/yandex
    Если вы использовали модули, то сначала выполните terraform init, затем удалите lock-файл, а затем выполните команду terraform providers lock.
        Создайте файл спецификации my-config.tf и укажите в нём Yandex Cloud в качестве провайдера.
     terraform {
       required_providers {
         yandex = {
           source = "yandex-cloud/yandex"
         }
       }
     }
     provider "yandex" {
       token  =  "<OAuth-токен>"
       cloud_id  = "<идентификатор_облака>"
       folder_id = "<идентификатор_каталога>"
       zone      = "<зона_доступности_по_умолчанию>"
     }
    Далее мы будем считать, что в качестве зоны доступности по умолчанию выбрана ru-central1-a.
    Добавьте в файл блок, описывающий создание ВМ. Его сложно написать с нуля, поэтому опирайтесь на пример из документации. Чтобы вам было проще опознать в консоли управления объекты, созданные по этой спецификации, указывайте уникальные имена для ВМ, сети и подсети, а не оставляйте имена по умолчанию (default).
    Для создания ВМ используйте образ, созданный с помощью Packer в предыдущей практической работе.
    Можно использовать переменные в спецификации Terraform и передавать в них разные значения при запуске команд. Например, если сделать переменную для идентификатора образа image-id, тогда с помощью одного и того же файла спецификации вы сможете создавать ВМ с разным наполнением.
    Переменные Terraform хранятся в файлах с расширением .tfvars. Создайте файл my-variables.tfvars и укажите в нём идентификатор своего образа Packer (узнайте идентификатор с помощью команды yc compute image list):
     image-id = "<идентификатор_образа>"
    В файле спецификации my-config.tf объявите эту переменную (ключевое слово variable). Тогда в секции, где описываются настройки ВМ, вы сможете обратиться к переменной как var.image-id:
     ...
     variable "image-id" {
         type = string
     }
     resource "yandex_compute_instance" "vm-1" {
     ...   
         boot_disk {
             initialize_params {
                 image_id = var.image-id
             }
         }
     ...
    Скорректируйте описание для сети и подсети.
    Для сети достаточно указать имя:
      resource "yandex_vpc_network" "network-1" {
          name = "from-terraform-network"
      }
    Для подсети укажите зону доступности и сеть, а также внутренние IP-адреса, уникальные в рамках сети. Используйте адреса из адресного пространства 10.0.0.0/16.
      resource "yandex_vpc_subnet" "subnet-1" {
          name           = "from-terraform-subnet"
          zone           = "ru-central1-a"
          network_id     = "${yandex_vpc_network.network-1.id}"
          v4_cidr_blocks = ["10.2.0.0/16"]
      }
    Проверьте синтаксис спецификации:
    variable "image-id" {
      type = string
    }
    resource "yandex_compute_instance" "vm-1" {
      name = "from-terraform-vm"
      platform_id = "standard-v1"
      zone = "ru-central1-a"
      resources {
        cores  = 2
        memory = 2
      }
      boot_disk {
        initialize_params {
          image_id = var.image-id
        }
      }
      network_interface {
        subnet_id = yandex_vpc_subnet.subnet-1.id
        nat       = true
      }
      metadata = {
        ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
      }
    }
    resource "yandex_vpc_network" "network-1" {
      name = "from-terraform-network"
    }
    resource "yandex_vpc_subnet" "subnet-1" {
      name           = "from-terraform-subnet"
      zone           = "ru-central1-a"
      network_id     = "${yandex_vpc_network.network-1.id}"
      v4_cidr_blocks = ["10.2.0.0/16"]
    }
    output "internal_ip_address_vm_1" {
      value = yandex_compute_instance.vm-1.network_interface.0.ip_address
    }
    output "external_ip_address_vm_1" {
      value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
    }
        Теперь попробуйте применить спецификацию. Перейдите в папку с файлом спецификации и выполните инициализацию.
     terraform init
    Если всё сделано верно, Terraform покажет сообщение:
     ...
     Terraform has been successfully initialized!
     ...
    Важно: выполняйте команды Terraform в папке, где находится файл спецификации.
        Проверьте спецификацию с помощью команды terraform plan.
        Terraform использует все файлы .tf из папки, в которой запущена команда. Поэтому название файла спецификации my-config.tf указывать не нужно: его Terraform подхватит и так.
        Если файл с переменными называется стандартно (terraform.tfvars), его тоже можно не указывать при запуске команды. А если название файла нестандартное, то его нужно указывать:
     terraform plan -var-file=my-variables.tfvars
    Terraform выведет план: объекты, которые будут созданы, и т. п.:
     ...
     Terraform will perform the following actions:
     ...
    На самом деле необязательно помещать переменные в файл, их можно просто указывать при запуске команды. Поскольку у вас только одна переменная, это было бы несложно:
     terraform plan -var="image-id=<идентификатор_образа>"
    Создайте в облаке инфраструктуру по описанной вами спецификации. Выполните команду:
     terraform apply -var-file=my-variables.tfvars
    Terraform запросит подтверждение:
     ...
     Do you want to perform these actions?
          Terraform will perform the actions described above.
          Only 'yes' will be accepted to approve.
          Enter a value:
    В ответ введите yes.
    Когда команда будет выполнена, вы увидите сообщение:
      Apply complete! Resources: ... added, 0 changed, 0 destroyed.
      Outputs:
      external_ip_address_vm_1 = "84.201.133.49"
      internal_ip_address_vm_1 = "10.2.0.24"
    В консоли управления убедитесь, что ВМ создана. Откройте в браузере страницу с указанным IP-адресом и проверьте, доступна ли ВМ.
    Как мы говорили на предыдущем уроке, Terraform хранит описание инфраструктуры в стейт-файлах. Посмотрите, как выглядит стейт-файл сейчас:
     terraform state list
    Вы увидите список объектов:
     yandex_compute_instance.vm-1
     yandex_vpc_network.network-1
     yandex_vpc_subnet.subnet-1
        Теперь добавьте в файл спецификации блок, описывающий создание кластера БД PostgreSQL. Подсказки ищите в справочнике ресурсов. Не забудьте заменить в спецификации имя подсети.
    Проверьте синтаксис спецификации:
    resource "yandex_mdb_postgresql_cluster" "postgres-1" {
      name        = "postgres-1"
      environment = "PRESTABLE"
      network_id  = yandex_vpc_network.network-1.id
      config {
        version = 12
        resources {
          resource_preset_id = "s2.micro"
          disk_type_id       = "network-ssd"
          disk_size          = 16
        }
        postgresql_config = {
          max_connections                   = 395
          enable_parallel_hash              = true
          vacuum_cleanup_index_scale_factor = 0.2
          autovacuum_vacuum_scale_factor    = 0.34
          default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
          shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
        }
      }
       database {
        name  = "postgres-1"
        owner = "my-name"
      }
      user {
        name       = "my-name"
        password   = "Test1234"
        conn_limit = 50
        permission {
          database_name = "postgres-1"
        }
        settings = {
          default_transaction_isolation = "read committed"
          log_min_duration_statement    = 5000
        }
      }
      host {
        zone      = "ru-central1-a"
        subnet_id = yandex_vpc_subnet.subnet-1.id
      }
    }
    Сохраните файл спецификации.
    Проверьте синтаксис спецификации:
    terraform {
      required_providers {
        yandex = {
          source = "yandex-cloud/yandex"
        }
      }
    }
    provider "yandex" {
      token  =  "<OAuth-токен>"
      cloud_id  = "<идентификатор_облака>"
      folder_id = "<идентификатор_каталога>"
      zone      = "ru-central1-a"
    }
    variable "image-id" {
      type = string
    }
    resource "yandex_compute_instance" "vm-1" {
      name = "from-terraform-vm"
      platform_id = "standard-v1"
      zone = "ru-central1-a"
      resources {
        cores  = 2
        memory = 2
      }
      boot_disk {
        initialize_params {
          image_id = var.image-id
        }
      }
      network_interface {
        subnet_id = yandex_vpc_subnet.subnet-1.id
        nat       = true
      }
      metadata = {
        ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
      }
    }
    resource "yandex_vpc_network" "network-1" {
      name = "from-terraform-network"
    }
    resource "yandex_vpc_subnet" "subnet-1" {
      name           = "from-terraform-subnet"
      zone           = "ru-central1-a"
      network_id     = yandex_vpc_network.network-1.id
      v4_cidr_blocks = ["10.2.0.0/16"]
    }
    resource "yandex_mdb_postgresql_cluster" "postgres-1" {
      name        = "postgres-1"
      environment = "PRESTABLE"
      network_id  = yandex_vpc_network.network-1.id
      config {
        version = 12
        resources {
          resource_preset_id = "s2.micro"
          disk_type_id       = "network-ssd"
          disk_size          = 16
        }
        postgresql_config = {
          max_connections                   = 395
          enable_parallel_hash              = true
          vacuum_cleanup_index_scale_factor = 0.2
          autovacuum_vacuum_scale_factor    = 0.34
          default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
          shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
        }
      }
      database {
        name  = "postgres-1"
        owner = "my-name"
      }
      user {
        name       = "my-name"
        password   = "Test1234"
        conn_limit = 50
        permission {
          database_name = "postgres-1"
        }
        settings = {
          default_transaction_isolation = "read committed"
          log_min_duration_statement    = 5000
        }
      }
      host {
        zone      = "ru-central1-a"
        subnet_id = yandex_vpc_subnet.subnet-1.id
      }
    }
    output "internal_ip_address_vm_1" {
      value = yandex_compute_instance.vm-1.network_interface.0.ip_address
    }
    output "external_ip_address_vm_1" {
      value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
    }
    Теперь примените обновлённую спецификацию. В папке с файлом спецификации выполните команду terraform plan:
    terraform plan -var-file=my-variables.tfvars 
    Если появляются сообщения об ошибках — исправьте ошибки и снова выполните команду.
    Обновите инфраструктуру в соответствии с дополненной спецификацией командой terraform apply:
    terraform apply -var-file=my-variables.tfvars 
    Поскольку спецификация теперь включает создание БД, команда может выполняться довольно долго (около 10 минут).
    В консоли управления откройте раздел Managed Service for PostgreSQL и убедитесь, что кластер postgres-1 создан и имеет статус Alive.
    Проверьте, как изменился стейт-файл:
    terraform state list 
    В списке появился новый объект:
    yandex_compute_instance.vm-1
    yandex_mdb_postgresql_cluster.postgres-1
    yandex_vpc_network.network-1
    yandex_vpc_subnet.subnet-1 
    Удалите инфраструктуру:
    terraform destroy -var-file=my-variables.tfvars 
    В конце вы увидите сообщение о выполнении команды:
    ...
    Destroy complete! Resources: 4 destroyed. 
        В консоли управления убедитесь, что объекты удалены.
    Task:
    Практическая работа. Создание докер-образа и загрузка его в Container Registry
    Decision:
    В этой практической работе вы создадите реестр в Yandex Container Registry, подготовите Docker-образ виртуальной машины и поместите его в реестр, а затем создадите машину из этого образа.
        Установите Docker.
        Создайте реестр в Yandex Container Registry:
     yc container registry create --name my-registry
    Обратите внимание, что в выводе есть уникальный идентификатор (id) реестра. Он пригодится вам для следующих команд.
     id: crpfpd8jhhldiqah91rc
     folder_id: b1gfdbij3ijgopgqv9m9
     name: my-registry
     status: ACTIVE
     created_at: "2021-04-06T00:46:48.150Z"
    Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper. Это нужно для того, чтобы внешняя платформа Docker могла от вашего имени отправить образ в ваш приватный реестр в Yandex Cloud.
     yc container registry configure-docker
    Подготовьте Dockerfile. Можно использовать файл из урока о Docker:
     FROM ubuntu:latest
     RUN apt-get update -y
     RUN apt-get install -y nginx
     ENTRYPOINT ["nginx", "-g", "daemon off;"]
    По умолчанию Docker использует файл с именем Dockerfile и без расширения.
    Перейдите в папку с Dockerfile и соберите образ (не забудьте подставить идентификатор своего реестра):
     docker build . -t cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
    Ключ -t позволяет задать образу имя.
    Напоминаем, что в Yandex Container Registry можно загрузить только образы, названные по такому шаблону:
     cr.yandex/<ID реестра>/<имя Docker-образа>:<тег>
    Загрузите Docker-образ в реестр:
     docker push cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
    В консоли управления перейдите в реестр и предоставьте всем пользователям право использовать хранящиеся образы. Для этого перейдите на вкладку Права доступа, в правом верхнем углу нажмите кнопку Назначить роли. В открывшемся окне нажмите кнопку Выбрать пользователя, на вкладке Группы выберите All users. Нажмите кнопку Добавить роль и последовательно введите viewer и container-registry.images.puller. Нажмите кнопку Сохранить.
    В консоли управления создайте ВМ с помощью Container Optimized Image.
    При создании машины в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный образ, остальные настройки оставьте по умолчанию и нажмите Применить.
        Другие настройки ВМ мы уже разбирали.
        Когда новая ВМ получит статус Running, найдите её внешний IP адрес в консоли управления и убедитесь, что по этому адресу отображается приветственная страница NGINX.
    Обратите внимание! C помощью Docker-образа вы создали и запустили виртуальную машину с предустановленным, нужным вам ПО. При этом вам даже не потребовалось заходить внутрь ВМ и выполнять установку или настройку ПО вручную.
    Decision:
    $ sudo apt update
    $ sudo apt install apt-transport-https ca-certificates curl software-properties-common
    $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
    $ sudo apt update
    $ apt-cache policy docker-ce
    $ sudo apt install docker-ce
    $ sudo /etc/init.d/docker start
    $ yc container registry create --name my-registry
    done (1s)
    id: crppr4k34p7u5pr94lvf
    folder_id: b1geto6411pvmr7j3pd2
    name: my-registry
    status: ACTIVE
    created_at: "2022-10-16T03:16:59.989Z"
    $ yc container registry configure-docker
    $ cd docker/
    $ vim Dockerfile
    $ cat Dockerfile
    FROM ubuntu:latest
    RUN apt-get update -y
    RUN apt-get install -y nginx
    ENTRYPOINT ["nginx", "-g", "daemon off;"]
    $ sudo docker build . -t cr.yandex/crppr4k34p7u5pr94lvf/ubuntu-nginx:latest
    $ sudo docker push cr.yandex/crppr4k34p7u5pr94lvf/ubuntu-nginx:latest
    Task:
    Практическая работа. Создание кластера
    Decision:
    В этой практической работе вы создадите кластер Kubernetes и группу узлов в нём.
        Выберите каталог для кластера.
        Выберите сервис Managed Service for Kubernetes. Нажмите кнопку Создать кластер. Дальше заполним настройки кластера:
        Для Kubernetes необходим сервисный аккаунт для ресурсов и узлов.
        Сервисный аккаунт для ресурсов — это аккаунт, под которым сервису Kubernetes будут выделяться ресурсы в нашем облаке.
        Сервисный аккаунт для узлов необходим уже созданным узлам самого кластера Kubernetes для доступа к другим ресурсам. Например, чтобы получить Docker-образы из Container Registry.
        Этим аккаунтам нужны разные права, и поэтому у них бывают разные роли. В общем случае вы можете использовать один и тот же сервисный аккаунт. Выберите аккаунт, который создали на первом курсе, или заведите новый.
        Ключ шифрования Yandex Key Management Service позволяет защитить конфиденциальную информацию (пароли, OAuth-токены и SSH-ключи) и повысить безопасность. Это необязательно — кластер запустится и без ключа. Для этой практической работы не создавайте его.
        Релизные каналы RAPID, REGULAR и STABLE отличаются процессом обновления и доступными вам версиями Kubernetes.
        RAPID и REGULAR содержат все версии, включая минорные. STABLE — только стабильные версии. RAPID обновляется автоматически, а в REGULAR и STABLE обновление можно отключить. Когда появляется обновление, информация о нём отображается в консоли управления.
        Выберите REGULAR.
    Внимательно выбирайте релизный канал! Изменить его после создания кластера Kubernetes нельзя.
    Конфигурация мастера
    Мастер — ведущая нода группы узлов кластера — следит за состоянием Kubernetes и запускает управляющие процессы. Сконфигурируем мастер:
        Выберите версию Kubernetes. Их набор зависит от релизного канала. Версии мастера и других нод могут не совпадать, но это достаточно тонкая настройка, могут возникнуть проблемы совместимости, которые повлияют на работу всего кластера.
        Кластеру может назначаться публичный IP-адрес. Выберите вариант Автоматически. В этом случае IP выбирается из пула свободных IP-адресов. Если вы не используете Cloud Interconnect или VPN для подключения к облаку, то без автоматического назначения IP-адресов вы не сможете подключиться к кластеру: он будет доступен только во внутренней сети вашего облака.
        Тип мастера влияет на отказоустойчивость. Зональный работает только в одной зоне доступности, а региональный — в трёх подсетях в каждой зоне доступности.
        Выберите зональный тип. В будущем для рабочей среды используйте региональные кластеры, а для разработки и тестирования — более дешёвые зональные.
        Выбор типа мастера также влияет на подсети, в которых будет развёрнут кластер. У вас уже есть подсети, созданные по умолчанию для функционирования облака. Выберите их.
    Настройки окна обновлений
        Режимов обновления четыре: Отключено, В любое время, Ежедневно и В выбранные дни. Региональный мастер во время обновления остаётся доступен, зональный — нет.
        Группа узлов кластера обновляется с выделением дополнительных ресурсов, так как при обновлении создаются узлы с обновлённой конфигурацией. При обновлении поды с контейнерами будут переезжать с одного узла на другой.
        По умолчанию выставлен пункт В любое время. Оставьте его.
    Сетевые настройки кластера
        Сетевые политики для кластера Kubernetes необязательны. Эта опция включает сетевой контроллер Calico, который позволяет применять тонкие настройки политик доступа для кластера.
        Не выбирайте эту опцию.
        Во время работы кластера подам с контейнерами и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы IP-адреса подов и сервисов Kubernetes не пересеклись с другими адресами в вашем облаке, задайте CIDR (Classless Inter-Domain Routing — бесклассовая междоменная маршрутизация). Оставьте адреса пустыми: они будут назначены автоматически.
        Маска подсети узлов влияет на количество подов, которые могут запускаться. Если адресов не хватит, под не запустится.
        Вы заполнили все настройки, теперь нажмите Создать кластер. Дождитесь, пока статус кластера станет RUNNING, а состояние — HEALTHY. Это может занять около 10 минут.
    Создание группы узлов
        Зайдите в созданный кластер, перейдите на вкладку Управление узлами и нажмите Создать группу узлов. Группы узлов — это группы виртуальных машин.
        Введите имя и описание группы, выберите версию Kubernetes. Выберите Автоматический тип масштабирования и количество узлов от 1 до 5. Укажите среду запуска контейнеров — Docker.
    В сетевых настройках задайте автоматический IP-адрес и выберите зону доступности (кластер зональный, поэтому зона доступности только одна). Задайте SSH-ключ, чтобы иметь доступ к виртуальным машинам кластера. Настройки обновления идентичны настройкам мастера.
    Остальные настройки группы, которые мы не упомянули (вычислительные ресурсы, хранилище и т. д.), оставьте по умолчанию.
    Нажмите Создать группу узлов и дождитесь, пока операция выполнится.
    Task:
    Практическая работа. Первое приложение в кластере
    Decision:
    На прошлом уроке вы создали в консоли управления Yandex Cloud кластер Kubernetes и группу узлов в нём. Теперь с помощью командной строки вы развернете в кластере приложение — веб-сервер NGINX.
        Основное средство взаимодействия с кластером — инструмент kubectl. Установите его по инструкции.
        В консоли управления войдите в созданный кластер Managed Service for Kubernetes и нажмите кнопку Подключиться. В открывшемся окне скопируйте команду для подключения:
     yc managed-kubernetes cluster get-credentials \
       --id <идентификатор_кластера> \
       --external
    Чтобы проверить правильность установки и подключения, посмотрите на конфигурацию:
     kubectl config view
    Ответ получится примерно таким (IP-адрес сервера и название кластера будут отличаться):
     apiVersion: v1
     clusters:
     - cluster:
         certificate-authority-data: DATA+OMITTED
         server: https://178.154.206.242
       name: yc-managed-k8s-cat2oek6hbp7mnhhhr4m
     contexts:
     ...
    Создание манифеста
    Для описания настроек приложения в кластере создадим файл my-nginx.yaml. Такой файл называется манифестом.
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-nginx-deployment
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest 
    Рассмотрим, из чего он состоит.
        Директива apiVersion определяет, для какой версии Kubernetes написан манифест. От версии к версии обозначение может меняться.
     apiVersion: apps/v1
    Директива kind описывает механизм использования. Она может принимать значения Deployment, Namespace, Service, Pod, LoadBalancer и т. д. Для развёртывания приложения укажите значение Deployment.
     kind: Deployment
    Директива metadata определяет метаданные приложения: имя, метки (labels), аннотации.
    С помощью Меток можно идентифицировать, группировать объекты, выбирать их подмножества. Добавляйте и изменяйте метки при создании объектов или позднее, в любое время.
    Аннотации используют, чтобы добавить собственные метаданные к объектам.
    Укажем имя приложения:
     metadata:
       name: my-nginx-deployment
    В основном блоке spec содержится описание объектов Kubernetes.
    Директива replicas определяет масштабирование. Для первого запуска укажите, что приложению нужен один под. Позже вы посмотрите, как приложения масштабируются, и сможете увеличить число подов.
    Директива selector определяет, какими подами будет управлять контейнер (подробнее о ней можно прочитать в документации). Поды отбираются с помощью метки (label).
    Директива template определяет шаблон пода. Метка в шаблоне должна совпадать с меткой селектора — nginx.
    В шаблоне содержится ещё одна, собственная директива spec. Она задаёт настройки контейнеров, которые будет развёрнуты на поде. Нам нужен один контейнер. Используйте для него образ, созданный ранее с помощью Docker и помещённый в реестр Yandex Container Registry.
     spec: 
       matchLabels: 
         app: nginx
       replicas: 1
       selector: ~
       template: 
         metadata: 
           labels: 
             app: nginx
         spec: 
           containers: 
             - name: nginx
               image: "cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest"
        Настройки манифеста для развёртывания приложения есть в документации Kubernetes.
    Выполнение манифеста
        Для создания или обновления ресурсов в кластере используется команда apply. Файл манифеста указывается после флага -f.
     kubectl apply -f <путь_к_файлу_my-nginx.yaml>
    Если результат будет успешным, вы увидите сообщение:
     deployment.apps/my-nginx-deployment created
    Чтобы убедиться, что приложение создано, посмотрите список подов:
     kubectl get pods
    Дождитесь статуса Running:
     NAME                                   READY   STATUS    RESTARTS   AGE
     my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          5m27s
    Теперь получите более подробную информацию, выполнив ту же команду с флагом -o wide:
     kubectl get pods -o wide
    Вы увидите внутренний IP-адрес, который присвоен поду. Это пригодится, если нужно узнать, где именно развёрнуто приложение.
    Чтобы получить максимально подробную информацию о запущенном приложении, используйте команду describe:
     kubectl describe deployment/my-nginx-deployment
    Масштабирование
        Теперь увеличьте количество подов. Вручную это можно сделать двумя способами:
            изменить файл манифеста, указав в директиве replicas нужное число подов, и снова выполнить команду apply;
            если файла манифеста нет под рукой — использовать команду scale:
    kubectl scale --replicas=3 deployment/my-nginx-deployment 
    Если всё получится, в выводе команды kubectl get pods вы увидите сообщение:
    NAME                                   READY   STATUS    RESTARTS   AGE
    my-nginx-deployment-65b9b678b6-6whpp   1/1     Running   0          117s
    my-nginx-deployment-65b9b678b6-wtph9   1/1     Running   0          117s
    my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          14m 
    На следующей практической работе мы посмотрим, как обращаться извне к кластеру Kubernetes и развёрнутому в нём приложению.
    Кластер как код
    Как видите, управление кластерами Kubernetes отлично вписывается в концепцию Infrastructure as Code: вы можете описать конфигурацию кластера в текстовом файле — манифесте. Вы также можете разворачивать кластеры Kubernetes с помощью Terraform.
    Task:
    Практическая работа. Балансировка нагрузки
    Decision:
    Большинство веб-приложений созданы, чтобы взаимодействовать через интернет. Вы развернули в кластере приложение, но у вас пока нет к нему доступа из интернета. Чтобы исправить эту проблему, воспользуемся сервисом LoadBalancer.
    У созданного пода есть внутренний IP-адрес.
    Помните, мы говорили о том, что в кластере есть собственный сервис DNS? Он работает с внутренними IP-адресами объектов кластера, чтобы те могли взаимодействовать.
    Однако внутренний IP-адрес может меняться, когда ресурсы группы узлов обновляются. Чтобы обращаться к приложению извне, требуется неизменный публичный IP-адрес — это и будет IP-адрес балансировщика.
        Создайте файл-манифест load-balancer.yaml:
     apiVersion: v1
     kind: Service
     metadata:
       name: my-loadbalancer
     spec:
       selector:
          app: nginx
       ports:
       - port: 80
         targetPort: 80
       type: LoadBalancer
    Где:
    port — порт сетевого балансировщика, на котором будут обслуживаться пользовательские запросы;
    targetPort — порт контейнера, на котором доступно приложение;
    selector — метка селектора из шаблона подов в манифесте объекта Deployment.
    Выполните манифест:
     kubectl apply -f <путь_к_файлу_load-balancer.yaml>
    Вы увидите сообщение:
     service/my-loadbalancer created
        В консоли управления откройте раздел Load Balancer. Там должен появиться балансировщик нагрузки с префиксом k8s в имени и уникальным идентификатором кластера Kubernetes.
        Скопируйте IP-адрес балансировщика в адресную строку браузера. Вы увидите приветственную страницу NGINX.
    Если при создании ресурсов вы получаете ошибку failed to ensure cloud loadbalancer: failed to start cloud lb creation: Permission denied, убедитесь, что вашему сервисному аккаунту хватает прав. Подробнее читайте в документации. 
    Task:
    Практическая работа. Автомасштабирование в Yandex Managed Kubernetes
    Decision:
    В этой работе вы увидите, как в Kubernetes® выполняется горизонтальное автомасштабирование.
        Создайте манифест load-balancer-hpa.yaml.
        Для начала скопируйте в него настройки спецификаций, которые вы составляли на предыдущих уроках: из my-nginx.yaml (в примере ниже это раздел Deployment) и из load-balancer.yaml (раздел Service).
        Поскольку новый балансировщик должен отслеживать отдельную группу контейнеров, используйте для контейнеров другие метки (labels), например nginx-hpa.
    ---
    ### Deployment
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-loadbalancer-hpa
      labels:
        app: nginx-hpa
    spec:
      replicas: 1
      selector:
        matchLabels:
               app: nginx-hpa
      template:
        metadata:
                  name: nginx-hpa
                labels:
                  app: nginx-hpa
        spec:
                containers:
                  - name: nginx-hpa
                    image: k8s.gcr.io/hpa-example
    ---
    ### Service
    apiVersion: v1
    kind: Service
    metadata:
      name: my-loadbalancer-hpa
    spec:
      selector:
         app: nginx-hpa
      ports:
        - protocol: TCP
          port: 80
          targetPort: 80
      type: LoadBalancer 
        В разделе Deployment смените образ с Yandex Container Registry на k8s.gcr.io/hpa-example — это специальный тестовый образ из публичного репозитория, создающий высокую нагрузку на процессор. Так вам будет удобно отслеживать работу Horizontal Pod Autoscaler.
           ...
           spec:
              containers:
                  - name: nginx-hpa
                  image: k8s.gcr.io/hpa-example 
        Теперь добавьте в шаблон контейнера настройки requests и limits: мы попросим по умолчанию 256 мебибайтов памяти и 500 милли-CPU (половину ядра), а ограничим контейнер 500 мебибайтами и 1 CPU.
        ...
        spec:
         containers:
           - name: nginx-hpa
             image: k8s.gcr.io/hpa-example
             resources:
               requests:
                 memory: "256Mi"
                 cpu: "500m"
               limits:
                 memory: "500Mi"
                 cpu: "1" 
        Дополните манифест настройками для Horizontal Pod Autoscaler:
    apiVersion: autoscaling/v1
    kind: HorizontalPodAutoscaler
    metadata:
      name: my-hpa
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: my-nginx-deployment-hpa
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 20 
        В результате должен получиться такой манифест:
    ---
    ### Deployment
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-nginx-deployment-hpa
      labels:
        app: nginx-hpa
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx-hpa
      template:
        metadata:
          name: nginx-hpa
          labels:
            app: nginx-hpa
        spec:
          containers:
            - name: nginx-hpa
              image: k8s.gcr.io/hpa-example
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "500m"
                limits:
                  memory: "500Mi"
                  cpu: "1"
    ---
    ### Service
    apiVersion: v1
    kind: Service
    metadata:
      name: my-loadbalancer-hpa
    spec:
      selector:
        app: nginx-hpa
      ports:
        - protocol: TCP
          port: 80
          targetPort: 80
      type: LoadBalancer
    ---
    ### HPA
    apiVersion: autoscaling/v1
    kind: HorizontalPodAutoscaler
    metadata:
      name: my-hpa
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: my-nginx-deployment-hpa
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 20 
        Примените манифест:
    kubectl apply -f <путь_к_load-balancer-hpa.yaml> 
    Вы увидите три сообщения:
    deployment.apps/my-nginx-deployment-hpa created
    service/my-loadbalancer-hpa created
    horizontalpodautoscaler.autoscaling/my-hpa created 
        В консоли управления перейдите в раздел Network Load Balancer. Дождитесь, пока статус my-nginx-deployment-hpa станет Running, после чего посмотрите IP-адрес балансировщика. Убедитесь, что в браузере этот адрес доступен. В терминале сохраните IP-адрес в переменную. Например, так:
    LOAD_BALANCER_IP=<IP-адрес балансировщика> 
        Запустите в отдельном окне отслеживание интересующих вас компонентов кластера Kubernetes:
    while true; do kubectl get pod,svc,hpa,nodes -o wide; sleep 5; done  
        Теперь сымитируйте рабочую нагрузку на приложение. Для этого подойдёт утилита wget (установите её с помощью пакетного менеджера или с сайта).
    while true; do wget -q -O- http://$LOAD_BALANCER_IP; done  
    Вы увидите, что сначала увеличится число подов, а затем добавятся узлы. Число узлов ограничено настройками группы узлов кластера, которые вы задали при создании кластера (в нашем случае максимальное количество узлов — пять).
        Остановите цикл создания нагрузки на приложение (комбинация клавиш Ctrl + C). В окне консоли с отслеживанием компонентов кластера вы увидите, как удаляются узлы и поды без нагрузки.
    Task:
    Практическая работа. Сбой виртуальной машины
    Decision:
    Давайте посмотрим, как принципы построения отказоустойчивых систем реализованы в Yandex Cloud. В практических работах этой темы вы проверите четыре основных сценария отказов:
    сбой виртуальной машины,
    сбой всей зоны доступности,
    обновление приложения
    сбой приложения.
    Вы сымитируете эти отказы и понаблюдаете, как Yandex Cloud обеспечивает доступность приложения и восстанавливает инфраструктуру после сбоев.
    Начнем с самого простого сценария — сбоя виртуальной машины.
    Создайте группу из трёх ВМ в трёх зонах доступности под балансировщиком нагрузки. Используйте образ с ОС Ubuntu 18.04 (потом мы обновим его на более свежую версию ОС).
    Используйте спецификацию specification.yaml из практической работы по CLI Yandex Cloud, но адаптируйте её для того, чтобы на ней можно было проверить разные сценарии сбоев.
    Во-первых, будут задействованы все три зоны доступности, поэтому нужно немного исправить блок allocation_policy:
    allocation_policy:
        zones:
            - zone_id: ru-central1-a
            - zone_id: ru-central1-b
            - zone_id: ru-central1-c 
    Также пропишите подсети для каждой зоны (не забывайте подставлять идентификаторы ваших подсетей):
        network_interface_specs:
            - network_id: <идентификатор_сети>
              subnet_ids: 
                - <идентификатор_подсети_№1>
                - <идентификатор_подсети_№2>
                - <идентификатор_подсети_№3> 
              primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }} 
    Во-вторых, в секции #cloud-config укажите пользователя, которого нужно создать для входа в виртуальные машины по SSH (это понадобится позднее, на одной из следующих практических работ):
            users:
              - name: my-user
                groups: sudo
                lock_passwd: true
                sudo: 'ALL=(ALL) NOPASSWD:ALL'
                ssh-authorized-keys:
                  - ssh-rsa AAAAB3Nza... 
    Обновленный файл спецификации specification.yaml:
    name: my-group
    service_account_id: <идентификатор_сервисного_аккаунта>
    instance_template:
        platform_id: standard-v1
        resources_spec:
            memory: 2g
            cores: 2
        boot_disk_spec:
            mode: READ_WRITE
            disk_spec:
                image_id: <идентификатор_образа_Ubuntu_18.04> 
                type_id: network-hdd
                size: 32g
        network_interface_specs:
            - network_id: <идентификатор_сети>
              subnet_ids: 
                - <идентификатор_подсети_№1>
                - <идентификатор_подсети_№2>
                - <идентификатор_подсети_№3> 
              primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
        scheduling_policy:
            preemptible: false
        metadata:
          user-data: |-
            #cloud-config
              users:
                - name: my-user
                  groups: sudo
                  lock_passwd: true
                  sudo: 'ALL=(ALL) NOPASSWD:ALL'
                  ssh-authorized-keys:
                    - <содержимое_публичной_части_SSH-ключа>
              package_update: true
              runcmd:
                - [ apt-get, install, -y, nginx ]
                - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
     deploy_policy:
        max_unavailable: 1
        max_expansion: 0
    scale_policy:
        fixed_scale:
            size: 3
    allocation_policy:
        zones:
            - zone_id: ru-central1-a
            - zone_id: ru-central1-b
            - zone_id: ru-central1-c
     load_balancer_spec:
        target_group_spec:
            name: my-target-group
    Создайте группу по новой спецификации:
    yc compute instance-group create --file <путь_к_файлу_specification.yaml> 
    Если ранее вы удаляли балансировщик нагрузки, создайте его снова и привяжите к целевой группе:
    yc load-balancer network-load-balancer create \
      --region-id ru-central1 \
      --name my-load-balancer \
      --listener name=my-listener,external-ip-version=ipv4,port=80 \
      --target-group target-group-id=<идентификатор_целевой_группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80 
    В консоли управления убедитесь, что ресурсы созданы. Проверьте вывод по внешнему IP-адресу балансировщика — должна отображаться приветственная страница с идентификатором одной из виртуальных машин группы.
    Начните отслеживать состояние виртуальных машин группы и целевой группы балансировщика:
    while true; do \
    yc compute instance-group \
      --id <идентификатор_группы_ВМ> list-instances; \
    yc load-balancer network-load-balancer \
      --id <идентификатор_балансировщика> target-states \
      --target-group-id <идентификатор_целевой_группы>; \
    sleep 5; done 
    Информация выводится в виде таблиц:
    +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS         | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
    | ef34nv4tp3ha8gl6p3df | cl1m5ksvljnq5frekghi-uzex | 84.201.148.207 | 10.128.0.42 | RUNNING_ACTUAL [1m54s] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [13m]   |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]    |                |
    +----------------------+---------------------------+----------------+-------------+------------------------+----------------+
    +----------------------+-------------+---------+
    |      SUBNET ID       |   ADDRESS   | STATUS  |
    +----------------------+-------------+---------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
    | e2luooifg8ruecr7g6fk | 10.128.0.6  | HEALTHY |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
    +----------------------+-------------+---------+
    Сбой виртуальной машины может произойти из-за падения физического хоста, на котором она запущена. Иногда виртуальную машину могут удалить случайно, по ошибке. Чтобы сымитировать сбой, удалим одну из виртуальных машин в группе через консоль управления.
    Если бы это была единственная машина, на которую поступает трафик, система стала бы недоступна. Но у нас система развернута на нескольких виртуальных машинах, поэтому трафик будет перенаправлен на две оставшиеся. Через несколько секунд будет обнаружена проблема, и виртуальная машина будет выведена из-под балансировки. Об этом говорит статус UNHEALTHY.
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | RUNNING_ACTUAL [15m] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [32m] |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    +----------------------+-------------+-----------+
    |      SUBNET ID       |   ADDRESS   |  STATUS   |
    +----------------------+-------------+-----------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY   |
    | e2luooifg8ruecr7g6fk | 10.128.0.6  | UNHEALTHY |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY   |
    +----------------------+-------------+-----------+ 
    Далее подсеть перейдет в статус DRAINING — ресурс удаляется, и с него снимается трафик. Балансировщик перестает передавать трафик этому ресурсу.
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [0s] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    +----------------------+-------------+----------+
    |      SUBNET ID       |   ADDRESS   |  STATUS  |
    +----------------------+-------------+----------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
    | e2luooifg8ruecr7g6fk | 10.128.0.6  | DRAINING |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
    +----------------------+-------------+----------+
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [9s] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    +----------------------+-------------+----------+
    |      SUBNET ID       |   ADDRESS   |  STATUS  |
    +----------------------+-------------+----------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
    +----------------------+-------------+----------+ 
    После этого Instance Group начнет пересоздавать удалённую виртуальную машину. Процесс восстановления может занять некоторое время. Понаблюдаем за ним.
    Сначала новая виртуальная машина появится в группе в статусе CREATING_INSTANCE.
    +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
    | ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CREATING_INSTANCE [-1s] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m]    |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]     |                |
    +----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
    +----------------------+-------------+----------+
    |      SUBNET ID       |   ADDRESS   |  STATUS  |
    +----------------------+-------------+----------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
    +----------------------+-------------+----------+
    Далее виртуальная машина будет открыта для трафика (статус OPEN_TRAFFIC). Балансировщик начнет процесс включения машины в список доступных машин.
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [-1s] | Adding target(s)               |
    |                      |                           |                |             |                       | 10.128.0.32 to target group    |
    |                      |                           |                |             |                       | b7rh0bhm9f82dglb2p9r           |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    +----------------------+-------------+---------+
    |      SUBNET ID       |   ADDRESS   | STATUS  |
    +----------------------+-------------+---------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
    +----------------------+-------------+---------+
    +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        |         STATUS MESSAGE         |
    +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
    | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1s] | Adding target(s)               |
    |                      |                           |                |             |                      | 10.128.0.32 to target group    |
    |                      |                           |                |             |                      | b7rh0bhm9f82dglb2p9r           |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m] |                                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                                |
    +----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
    +----------------------+-------------+---------+
    |      SUBNET ID       |   ADDRESS   | STATUS  |
    +----------------------+-------------+---------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
    | e2luooifg8ruecr7g6fk | 10.128.0.32 | INITIAL |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
    +----------------------+-------------+---------+
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [18s] | Awaiting HEALTHY state for     |
    |                      |                           |                |             |                       | target(s) 10.128.0.32. Elapsed |
    |                      |                           |                |             |                       | time: 3s.                      |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
    +----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
    +----------------------+-------------+----------+
    |      SUBNET ID       |   ADDRESS   |  STATUS  |
    +----------------------+-------------+----------+
    | b0c4h992tbuodl5hudpu | 10.128.0.32 | INITIAL  |
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | INACTIVE |
    | b0c4h992tbuodl5hudpu | 10.128.0.9  | HEALTHY  |
    +----------------------+-------------+----------+
    +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          |         STATUS MESSAGE         |
    +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
    | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1m32s] | [NLB unhealthy]; Awaiting      |
    |                      |                           |                |             |                         | HEALTHY state for target(s)    |
    |                      |                           |                |             |                         | 10.128.0.32. Elapsed time: 1m  |
    |                      |                           |                |             |                         | 17s.                           |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m]    |                                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]     |                                |
    +----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
    +----------------------+-------------+---------+
    |      SUBNET ID       |   ADDRESS   | STATUS  |
    +----------------------+-------------+---------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
    | e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
    +----------------------+-------------+---------+ 
    И в завершение подсеть перейдет в статус HEALTHY, а машина — в статус RUNNING_ACTUAL, и трафик будет снова разделен между тремя машинами.
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    |     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    | ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | RUNNING_ACTUAL [-1s] |                |
    | ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m] |                |
    | ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                |
    +----------------------+---------------------------+----------------+-------------+----------------------+----------------+
    +----------------------+-------------+---------+
    |      SUBNET ID       |   ADDRESS   | STATUS  |
    +----------------------+-------------+---------+
    | b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
    | e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
    | e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
    +----------------------+-------------+---------+ 
    Восстановление произошло автоматически без ручного вмешательства.
    Обратите внимание! Эту группу виртуальных машин мы будем использовать и в трех следующих практических работах, не удаляйте её. Если вы будете делать большой перерыв между практическими работами, вы можете остановить группу (чтобы не расходовать средства на балансе), а затем запустить её снова.
    Task:
    Практическая работа. Сбой зоны доступности
    Decision:
    В этом сценарии рассмотрим ситуацию, когда произошел сбой сразу всей зоны доступности. Такие ситуации возникают крайне редко и могут быть связаны с какими-то масштабными стихийными бедствиями, однако и их стоит предусмотреть.
    Посмотрим, как будет решаться проблема неожиданного выхода из строя зоны доступности.
    В нашем примере (см. предыдущий урок) используются все три зоны доступности — ru-central1-a, ru-central1-b и ru-central1-c. В каждой зоне располагается одна ВМ.
        Установите такие настройки политики развертывания — пусть группу можно расширять на 1 ВМ и уменьшать на 1 ВМ:
        Теперь в настройках группы виртуальных машин уберите одну зону доступности, например ru-central1-c. Переключитесь на вкладку Список ВМ и посмотрите, что будет происходить.
        Для ВМ, которая располагалась в зоне ru-central1-c, отключается трафик (статус Closing traffic), а затем сама машина удаляется (статус Deleting instance). Одновременно в другой зоне доступности создаётся и запускается новая ВМ. Остальные машины в группе продолжают работать без изменений.
    Таким образом, даже при выходе из строя всей зоны доступности группа виртуальных машин продолжит работать и будет способна принимать прежнюю нагрузку.
    Task:
    Практическая работа. Обновление приложения
    Decision:
    На практической работе с CLI мы уже рассматривали обновление операционной системы для группы виртуальных машин. Любые приложения, установленные на ВМ, обновляются по тем же правилам. Давайте рассмотрим этот процесс ещё раз.
    Первый вариант обновления
    Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с ОС Ubuntu 20.04. Убедитесь, что параметры политики развёртывания такие: группу нельзя расширять, а уменьшать можно только на одну ВМ.
    Политика развёртывания группы виртуальных машин (вариант 1)
    Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например с fd8s2gbn4d5k2rcf12d9 на fd8ju9iqf6g5bcq77jns) и запустите обновление группы:
    yc compute instance-group update \
      --name my-group \
      --file <путь_к_файлу_specification.yaml> 
    В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
    Сначала вы увидите статус Running outdated. Это означает, что машины работают со старой версией приложения.
    Затем одна из машин начинает обновляться: для неё закрывается трафик (статус Closing traffic), она останавливается (статус Stopping instance), обновляется (статус Updating instance), затем трафик снова открывается (статус Opening traffic), и наконец статус меняется на Running actual. Обновление выполнено.
    Затем то же самое последовательно выполняется для остальных машин в группе.
    Порядок обновления зависит от политики развёртывания. Мы запретили увеличивать размер группы и указали, что одновременно неработоспособной может быть только одна машина. Именно так и произошло обновление: машины по одной выводились из строя, обновлялись и запускались снова.
    Второй вариант обновления
    Теперь давайте изменим настройки политики развёртывания.
    Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с Ubuntu и NGINX, созданный ранее и помещённый в Container Registry.
    Измените параметры развёртывания. Теперь группу можно расширять на одну ВМ, а уменьшать нельзя:
    Политика развёртывания группы виртуальных машин (вариант 2)
    Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например, снова с fd8ju9iqf6g5bcq77jns на fd8s2gbn4d5k2rcf12d9). Параметры обновления измените так:
    deploy_policy:
        max_unavailable: 0
        max_expansion: 1 
    Запустите обновление группы.
    В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
    Сначала вы увидите статус Running outdated. Затем создаётся новая машина (статус Creating instance), для неё открывается трафик (статус Opening traffic), статус машины меняется на Running actual, при этом одна из «устаревших» ВМ выводится из строя (статусы Closing traffic и Stopping instance).
    Затем то же самое последовательно выполняется для остальных машин в группе.
    Task:
    Практическая работа. Сбой приложения
    Decision:
    Последний сценарий, который мы рассмотрим, это сбой приложения. Ситуация, когда сама ВМ работоспособна, но по каким-то причинам произошла ошибка в приложении. Это может быть потеря соединения с базой данных или какой-то баг в запущенном приложении (например утечка памяти). Давайте сымитируем такой сценарий. На наших виртуальных машинах запущен только веб-сервер NGINX, давайте остановим его. Но сначала включим проверку состояния ВМ.
    В консоли управления откройте вкладку Обзор для вашей группы виртуальных машин, нажмите кнопку Изменить и активируйте проверку состояний. Сохраните изменения.
    В браузере откройте страницу с внешним IP-адресом балансировщика, привязанного к вашей группе, и посмотрите, на какую из машин выводится трафик. Узнайте внешний IP-адрес этой машины.
    В новой вкладке браузера откройте IP-адрес этой виртуальной машины и убедитесь, что выводится приветственная страница, т. е. сервер доступен.
    Помните, когда вы меняли файл конфигурации для группы машин, вы добавили в него пользователя my-user? Теперь он вам пригодится — из консоли зайдите на ВМ от его имени:
     ssh my-user@<внешний_IP-адрес_ВМ>
    Посмотрите список запущенных процессов:
     ps axu
    Убедитесь, что в списке есть процессы nginx:
    Теперь остановите эти процессы, чтобы сделать сервер недоступным:
     sudo killall nginx
    В браузере обновите страницу балансировщика. Вы увидите, что теперь трафик направляется на другую виртуальную машину группы. Это означает, что Instance Group обнаружил сбой приложения и переключил трафик.
    Теперь обновите страницу виртуальной машины, на которой вы остановили NGINX. Убедитесь, что сервер теперь недоступен.
    Откройте список машин вашей группы и проследите, как меняется состояние одной из машин.
    Сначала будет закрыт трафик (статус Closing traffic), затем виртуальная машина будет остановлена (статус Stopping instance), а затем перезапущена (статус Running actual).
    Убедитесь, что веб-сервер на этой ВМ снова доступен.
    Мы проверили четыре основных сценария сбоев и убедились, что Yandex Cloud автоматически отрабатывает их и восстанавливает работоспособность группы.
    Теперь вы можете удалить группу виртуальных машин, в этом курсе она больше не понадобится.
    Task:
    Практическая работа. Отправка собственных метрик
    Decision:
    Часто бывает полезно отслеживать более широкий набор метрик, чем тот, что доступен в Yandex Monitoring «из коробки».
    Предположим, вам интересно узнать, сколько людей заходит на ваш сайт и как их число зависит от времени дня или дня недели. Вы можете выгружать эти данные из Яндекс Метрики или вашей собственной аналитической системы и самостоятельно загружать в Yandex Monitoring с помощью API.
    Давайте попробуем сделать это с нашим сайтом.
    Отправка метрик через API
    Получите IAM-токен:
        Инструкция для аккаунта на Яндексе.
        Инструкция для сервисного аккаунта.
    Обратите внимание — токены устаревают через 12 часов после создания. Поэтому если вы сделаете паузу при выполнении данной практической работы, для продолжения лучше запросить новый токен.
    Сохраните токен в переменной окружения, так его будет проще использовать:
     export IAM_TOKEN=<IAM-токен>
    Создайте файл с телом запроса, например my-metrics.json. В свойстве metrics указывается список метрик для записи. Пусть это будет количество пользователей сайта. В массиве timeseries указываются значения на разные моменты времени (измените число на сегодняшнее в формате год-месяц-день).
    {
      "metrics": [
        {
          "name": "number_of_users",
          "labels": {
           "site": "aibolit"
          },
          "type": "IGAUGE",
          "timeseries": [
            {
              "ts": "2021-05-10T10:00:00Z",
              "value": "22"
            },
            {
              "ts": "2021-05-10T11:00:00Z",
              "value": "44"
            },
            {
              "ts": "2021-05-10T12:00:00Z",
              "value": "11"
            },
            {
              "ts": "2021-05-10T13:00:00Z",
              "value": "55"
            },
            {
              "ts": "2021-05-10T14:00:00Z",
              "value": "33"
            }
          ]
        }
      ]
    } 
    Отправьте запрос, указав в нем идентификатор каталога и имя сервиса custom (это имя указывается для всех пользовательских метрик):
     curl -X POST \
         -H "Content-Type: application/json" \
         -H "Authorization: Bearer ${IAM_TOKEN}" \
         -d '@<путь_к_файлу_my-metrics.json>' \
     'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=<идентификатор_каталога>&service=custom'
    Мониторинг пользовательских метрик
    Создайте на вашем дашборде новый виджет с графиком, назовите его «Число пользователей сайта».
    В виджете создайте запрос с параметрами service = Custom Metrics и name = number_of_users. Убедитесь, что в виджете выбран нужный период:
    Этот график станет нагляднее, если вместо точек отображать столбцы. Тип графика можно изменить с помощью кнопки в правом верхнем углу виджета:
    Мониторинг метрик Linux
    Другой пример — ваши приложения запущены на виртуальных машинах под Linux. По умолчанию вы можете посмотреть утилизацию ресурсов процессора или диска для ВМ в целом. Но вам будет полезно знать, сколько ресурсов потребляет каждое из них. В Yandex Monitoring вы можете отслеживать системные метрики Linux, такие как объём свободной памяти или загрузка процессора. Но для этого нужно дополнительно настроить отправку этих метрик с помощью Yandex Unified Agent, который мы уже упоминали.
    Установка Yandex Unified Agent
    Создайте виртуальную машину. На неё вы будете устанавливать Yandex Unified Agent. Можете использовать образ с ОС Ubuntu, который вы создали ранее и поместили в Container Registry. Назовите машину, например, for-ua.
    При создании используйте ваш сервисный аккаунт. Задайте логин (например ua-user) и ssh-ключ.
    Для сервисного аккаунта добавьте роль monitoring.editor.
    Посмотрите публичный IP-адрес машины for-ua и зайдите на неё по ssh:
     ssh ua-user@<публичный_адрес_ВМ>
    Теперь вы можете установить Yandex Unified Agent:
     ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent && chmod +x ./unified_agent'
    Также вы можете выбрать опцию Установить в поле Агент сбора метрик при создании ВМ, тогда Yandex Unified Agent будет установлен автоматически.
    Создайте файл config.yml с типовой спецификацией для доставки метрик Linux.
    В параметре folder_id укажите идентификатор вашего каталога.
    status:
    port: "16241"
    storages:
    - name: main
      plugin: fs
      config:
        directory: /var/lib/yandex/unified_agent/main
        max_partition_size: 100mb
        max_segment_size: 10mb
    channels:
    - name: cloud_monitoring
      channel:
        pipe:
          - storage_ref:
              name: main
        output:
          plugin: yc_metrics
          config:
            folder_id: "<идентификатор_каталога>"
            iam:
              cloud_meta: {}
    routes:
    - input:
        plugin: linux_metrics
        config:
          namespace: sys
      channel:
        channel_ref:
          name: cloud_monitoring
    - input:
        plugin: agent_metrics
        config:
          namespace: ua
      channel:
        pipe:
          - filter:
              plugin: filter_metrics
              config:
                match: "{scope=health}"
        channel_ref:
          name: cloud_monitoring
    import:
    - /etc/yandex/unified_agent/conf.d/*.yml 
    В секции status достаточно указать порт для просмотра статуса Yandex Unified Agent.
    Секция storage содержит список хранилищ, в которых будут находиться выгруженные данные. Для практической работы достаточно одного файлового хранилища (fs).
    Секция channels содержит список именованных каналов, к этим каналам можно обращаться по имени из других секций спецификации. Здесь обозначен один канал с именем cloud_monitoring. К нему идёт обращение из секции routes, которая содержит список маршрутов доставки метрик.
    Подробнее о конфигурировании Yandex Unified Agent вы можете почитать в документации.
    Скопируйте файл спецификации в виртуальную машину for-ua:
     scp config.yml ua-user@84.252.135.237:config.yml
    Теперь запустите Unified Agent с созданной спецификацией:
     sudo ./unified_agent --config config.yml
    Если запуск прошел успешно, в конце вы увидите сообщение такого вида:
     ... NOTICE agent started

    Настройка виджета для мониторинга метрик Linux
    Создайте на вашем дашборде новый виджет с графиком, назовите его «Метрики Linux».
    В виджете создайте запрос с параметром service = Custom Metrics. В параметре name выберите любой параметр, начинающийся с sys — всё это системные метрики, поставляемые Unified Agent. Например, name = sys.memory.MemAvailable.
    Теперь в виджете отображается график наличия свободной оперативной памяти в виртуальной машине for-ua.
    Decision:
    $ yc iam key create --service-account-name monitortest --output key.json
    id: ajean75uh879rndj8293
    service_account_id: ajehq9p412df22arccm1
    created_at: "2022-10-16T05:25:41.122300972Z"
    key_algorithm: RSA_2048
    $ yc config profile create monitortest-profile
    $ yc config set service-account-key key.json
    $ yc iam create-token
    $ export IAM_TOKEN=t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA
    $ vim my-metrics.json
    $ cat my-metrics.json
    {
      "metrics": [
        {
          "name": "number_of_users",
          "labels": {
           "site": "aibolit"
          },
          "type": "IGAUGE",
          "timeseries": [
            {
              "ts": "2021-05-10T10:00:00Z",
              "value": "22"
            },
            {
              "ts": "2021-05-10T11:00:00Z",
              "value": "44"
            },
            {
              "ts": "2021-05-10T12:00:00Z",
              "value": "11"
            },
            {
              "ts": "2021-05-10T13:00:00Z",
              "value": "55"
            },
            {
              "ts": "2021-05-10T14:00:00Z",
              "value": "33"
            }
          ]
        }
      ]
    }
    $ curl -X POST \
         -H "Content-Type: application/json" \
         -H "Authorization: Bearer ${t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA}" \
         -d '@/home/administrator/my-metrics.json' \
     'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=ajehq9p412df22arccm1&service=custom'
    Task:
    Практическая работа. Выгрузка метрик в формате Prometheus
    Decision:
    Как мы уже говорили раньше, метрики можно выгружать из Yandex Cloud Monitoring в сторонние приложения и сервисы. Пожалуй, чаще всего их выгружают для сервера Prometheus.
    На сегодняшний день Prometheus — один из самых популярных инструментов для мониторинга приложений и сервисов. В основе его лежит специализированная СУБД для анализа временных рядов, которая обеспечивает высокое быстродействие. В отличие от большинства систем мониторинга, Prometheus не ждёт, пока сторонние приложения передадут ему свои метрики, а сам опрашивает подключенные к нему приложения и собирает нужные данные.
    Prometheus и Yandex Cloud Monitoring решают схожие задачи — хранят значения разных метрик. Prometheus фактически является стандартом для обмена метриками. Поэтому даже используя сервисы Yandex Cloud, IT-администраторы часто хотят отслеживать их работу с помощью Prometheus. Чтобы не лишать специалистов привычных инструментов, Yandex Cloud Monitoring поддерживает выгрузку данных в формате Prometheus. Для этого используется метод prometheusMetrics.
    Для визуализации данных, собираемых Prometheus, можно использовать сервис Grafana (в нем можно зарегистрироваться бесплатно на тестовый период). Вы можете установить Grafana на свой компьютер, а можете работать в облачной версии.
    Посмотрим, как происходит выгрузка метрик в Prometheus и работа с ними в Grafana. Вы снова будете мониторить сайт клиники «Доктор Айболит».
    Подготовка
    Создайте API-ключ через консоль управления Yandex Cloud или CLI.
    Если вы создаете ключ в консоли управления, то перейдите в каталог, из которого будете выгружать метрики (например default). Затем перейдите на вкладку Сервисные аккаунты и выберите существующий аккаунт. Нажмите кнопку Создать новый ключ и выберите Создать API-ключ. В описании ключа можно указать, например, «для доступа к Prometheus». Сохраните секретную часть ключа в отдельный файл, например, prometheus-key.txt.
    Назначьте сервисному аккаунту роль monitoring.viewer на выбранный каталог.
    Создайте файл спецификации prometheus.yml (см. пример ниже, замените в нем значение параметра folderId на идентификатор каталога, а значение для bearer_token — на ключ доступа из файла prometheus-key.txt):
    global:
      scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    rule_files:
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
        - targets: ['localhost:9090']
     
      - job_name: 'yc-monitoring-export'
        metrics_path: '/monitoring/v2/prometheusMetrics'
        params:
          folderId:
          - '<идентификатор_каталога>' 
          service:
          - 'storage' 
        bearer_token: '<секретная_часть_API-ключа>'
        static_configs:
        - targets: ['monitoring.api.cloud.yandex.net']
          labels:
              folderId: '<идентификатор_каталога>'
              service: 'storage' 
    Запуск сервера Prometheus
    Если вы уже работаете с Prometheus, пропустите все шаги по установке — просто добавьте секцию scrape_configs из примера выше в спецификацию вашего сервера Prometheus и перезапустите сервер, а затем переходите к настройке Grafana.
    Для запуска сервера Prometheus используйте официальный Docker-образ prom/prometheus.
    Сначала загрузите образ. Для этого запустите Docker Desktop (в терминале выполните команду):
     docker pull prom/prometheus
    Чтобы на сервере сразу был ваш файл спецификации, создайте свой образ на основе prom/prometheus. Подготовьте Dockerfile с двумя командами:
     FROM prom/prometheus
     ADD prometheus.yml /etc/prometheus/
    Сохраните этот файл в тот же каталог, где находится prometheus.yml. Назовите его именем по умолчанию: Dockerfile.
    В терминале перейдите в каталог с Dockerfile. Создайте образ с вашей конфигурацией (используйте ваш идентификатор в Yandex Container Registry):
     docker build . -t cr.yandex/<идентификатор_реестра>/my-prometheus:latest -f Dockerfile
    Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper (чтобы Docker мог от вашего имени отправить образ в ваш реестр):
     yc container registry configure-docker
    Теперь отправьте образ в ваше хранилище в облаке:
     docker push cr.yandex/<идентификатор_реестра>/my-prometheus:latest
    Создайте виртуальную машину с помощью Container Optimized Image, вы уже делали это раньше в практической работе (в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный вами образ, остальные настройки оставьте по умолчанию и нажмите Применить).
    При создании виртуальной машины используйте ваш сервисный аккаунт. Задайте логин (например prom) и ssh-ключ.
    Назовите машину, например, for-prometheus.
    Проверьте статус сервера по адресу http://<публичный IP-адрес ВМ с Prometheus>:9090/targets. Через несколько минут после запуска статус процессов prometheus и yc-monitoring-export должен стать UP.
    Подайте нагрузку на ваш сайт:
    while true; do wget -q -O- <адрес_сайта>; done 
    Подождите несколько минут и проверьте, как поставляются метрики в Prometheus.
    В верхнем меню выберите пункт Graph. Нажмите на значок «Земли». Откроется меню с доступными метриками. Выберите метрику, которую вы хотите проверить, например, traffic и нажмите кнопку Execute.
    Переключитесь на вкладку Graph. Выберите текущее время, для наглядности уменьшите интервал запроса данных (например до 15 минут). Вскоре вы увидите график изменения выбранной метрики.
    Настройка Grafana
    Теперь посмотрим, как метрики визуализируются в системе Grafana.
    Если у вас еще нет аккаунта в Grafana, создайте его с помощью нескольких простых шагов, это бесплатно. Вам откроется интерфейс по адресу https://<ваш_логин>.grafana.net/.
    Добавление источника данных
    Настройте Prometheus в качестве источника данных. На главной странице нажмите кнопку Connect data. Из предложенного списка выберите источник Prometheus data source и нажмите кнопку Create Prometheus data source.
    В следующем окне в поле URL введите endpoint сервера Prometheus http://<публичный IP-адрес ВМ с Prometheus>:9090. Больше никакие настройки менять не нужно.
    Внизу нажмите кнопку Save & Test. Должна отобразиться надпись Data source is working.
    Добавление дашборда
    Вернитесь на главную страницу (нажав на логотип в левом верхнем углу) и нажмите Create your first dashboard. Откроется окно настройки дашборда.
    В нижней части экрана на вкладке Query выберите источник данных — Prometheus.
    Выберите метрику, которую вы хотите отслеживать. Нажмите на поле Metrics, в открывшемся списке выберите метрику traffic.
    Сверху отобразится график выбранной метрики.
    Вверху справа в поле Panel Title укажите название графика (например, «Трафик сайта»).
    Теперь сохраните настройки — в правом верхнем углу нажмите кнопку Save и укажите название дашборда (например, «Мой дашборд»).
    Вы научились отслеживать метрики Yandex.Cloud не только средствами Yandex Monitoring, но и с помощью сторонних систем, в том числе широко используемых Prometheus и Grafana. Но метрики можно использовать не только для визуальной оценки состояния облака и его ресурсов. В следующих двух уроках мы посмотрим, как можно облегчить работу специалиста DevOps и автоматизировать мониторинг.
    Task:
    Практическая работа. Создание алерта
    Decision:
    В этой практической работе вы создадите алерт для случая, если трафик на сайте вдруг начнет существенно расти. Снова используйте сайт клиники «Доктор Айболит», для которого настраивали графики на дашборде.
    Вы можете перейти на вкладку Алерты и там настроить алерт с нуля. А можете отталкиваться от графиков, которые уже выведены в виджете. Ниже рассматривается именно второй вариант.
    Создание алерта
    Вернитесь на созданный вами дашборд и в меню виджета «Трафик сайта» выберите пункт Создать алерт.
    Поскольку в виджете используются два запроса, вам будет предложено выбрать, для какого запроса вы хотите создать алерт. Выберите запрос с суммирующей функцией и нажмите Продолжить.
    Теперь задайте имя и, если хотите, описание алерта. Укажите значение для статусов Alarm и Warning.
    Откройте спойлер Показать дополнительные настройки. Там вы увидите, что система предложила вам использовать среднее значение за 5 минут. Оставьте эти параметры.
    Теперь нужно выбрать канал для получения алертов. У вас пока ещё нет настроенных каналов, поэтому система предложить вам создать его. Нажмите кнопку Добавить канал и далее Создать канал.
    Укажите имя канала, выберите метод — Email, SMS или Push-уведомления. Укажите получателей — себя. Затем нажмите кнопку Создать.
    В настройках алерта выберите только что созданный канал.
    Вы можете указать для одного алерта несколько каналов уведомлений. Например, если вы хотите получать алерты об увеличении трафика сайта не только в виде Push-уведомлений, но и по электронной почте, создайте еще один канал с методом Email и выберите также и его.
    Для каждого канала можно настроить режим повторения уведомлений. Например, в данном случае при превышении трафика будет отправлен один алерт по электронной почте, а алерты в виде push-уведомлений будут отправляться каждые 5 минут до тех пор, пока проблема не будет устранена.
    Нажмите кнопку Создать алерт.
    Вы увидите настройки созданного алерта, а сверху — его текущий статус OK.
    Нажмите слева на вкладку Алерты. Вы увидите ваш алерт, сейчас он единственный в списке. Когда алертов станет больше, вам понадобятся инструменты для работы с ними. Например, вы сможете отобрать из списка только алерты, имеющие статус Alarm или Warning, или временно деактивировать отдельные алерты.
    Срабатывание алерта
    Теперь посмотрим, как срабатывает алерт. Подайте трафик на сайт, который вы мониторите:
    while true; do wget -q -O- <адрес_сайта>; done 
    Подождите немного и понаблюдайте за ростом нагрузки. Через какое-то время трафик начнет превышать пороговое значение Warning, и вы начнете получать Push-уведомления.
    Если у администратора настроены другие каналы для алертов, он получил бы SMS или Email с предупреждением о пороговом значении трафика.
    Как видите, алерты позволяют вовремя привлекать внимание администратора и устранять даже потенциальные, ещё не случившиеся проблемы.
    Task:
    Практическая работа. Запускаем функцию с помощью CLI
    Decision:
    $ sudo apt install jq
    $ yc init
    $ yc config list
    token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
    cloud-id: b1gg01f1vt0rkid9qsuk
    folder-id: b1g0mvtp1aqh3kfsgubt
    $ export SERVICE_ACCOUNT=$(yc iam service-account create \
      --name service-account-for-cf \
      --description "service account for cloud functions" \
      --format json | jq -r .) 
    $ yc iam service-account list
    +----------------------+------------------------+
    |          ID          |          NAME          |
    +----------------------+------------------------+
    | ajehljbngf4kdbkaf5aq | service-account-for-cf |
    +----------------------+------------------------+
    $ echo $SERVICE_ACCOUNT
    $ echo "export SERVICE_ACCOUNT_ID=ajehljbngf4kdbkaf5aq" >> ~/.bashrc && . ~/.bashrc
    $ echo $SERVICE_ACCOUNT_ID
    $ echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
    $ echo $FOLDER_ID
    b1g0mvtp1aqh3kfsgubt
    $ yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_ID \
      --role editor 
    $ yc serverless function create --name my-first-function
    done (1s)
    id: d4erp0jk02hj3134gn7g
    folder_id: b1g0mvtp1aqh3kfsgubt
    created_at: "2022-10-18T11:51:03.815Z"
    name: my-first-function
    log_group_id: ckgaep2dmajejq7vjdvv
    http_invoke_url: https://functions.yandexcloud.net/d4erp0jk02hj3134gn7g
    status: ACTIVE
    $ vim index.py
    $ cat index.py
    def handler(event, context):
        return {
            'statusCode': 200,
            'body': 'Hello World!',
        }
    $ yc serverless function version create \
        --function-name my-first-function \
        --memory 256m \
        --execution-timeout 5s \
        --runtime python37 \
        --entrypoint index.handler \
        --service-account-id $SERVICE_ACCOUNT_ID \
        --source-path index.py
    id: d4e456dkn2shsmiqdq3i
    function_id: d4erp0jk02hj3134gn7g
    created_at: "2022-10-18T11:55:21.256Z"
    runtime: python37
    entrypoint: index.handler
    resources:
      memory: "268435456"
    execution_timeout: 5s
    service_account_id: ajehljbngf4kdbkaf5aq
    image_size: "4096"
    status: ACTIVE
    tags:
      - $latest
    log_group_id: ckgaep2dmajejq7vjdvv
    $ yc serverless function list
    +----------------------+-------------------+----------------------+--------+
    |          ID          |       NAME        |      FOLDER ID       | STATUS |
    +----------------------+-------------------+----------------------+--------+
    | d4erp0jk02hj3134gn7g | my-first-function | b1g0mvtp1aqh3kfsgubt | ACTIVE |
    +----------------------+-------------------+----------------------+--------+
    $ yc serverless function version list --function-name my-first-function
    +----------------------+----------------------+----------+---------------+---------+---------------------+
    |          ID          |     FUNCTION ID      | RUNTIME  |  ENTRYPOINT   |  TAGS   |     CREATED AT      |
    +----------------------+----------------------+----------+---------------+---------+---------------------+
    | d4e456dkn2shsmiqdq3i | d4erp0jk02hj3134gn7g | python37 | index.handler | $latest | 2022-10-18 11:55:21 |
    +----------------------+----------------------+----------+---------------+---------+---------------------+
    $ yc serverless function invoke d4erp0jk02hj3134gn7g
    {"statusCode": 200, "body": "Hello World!"}
    $ yc serverless function allow-unauthenticated-invoke my-first-function
    $ yc serverless function get my-first-function
    id: d4erp0jk02hj3134gn7g
    folder_id: b1g0mvtp1aqh3kfsgubt
    created_at: "2022-10-18T11:51:03.815Z"
    name: my-first-function
    log_group_id: ckgaep2dmajejq7vjdvv
    http_invoke_url: https://functions.yandexcloud.net/d4erp0jk02hj3134gn7g
    status: ACTIVE
    Task:
    Практическая работа. Создаём вашу первую функцию
    Decision:
    Мы уже достаточно сказали о том, что создавать облачные функции — просто. Давайте сделаем это на практике.
    Как добавить код функции
        На главной странице консоли управления в списке сервисов выберите Cloud Functions:
    На открывшейся странице нажмите кнопку Создать функцию:
    Укажите имя функции, введите короткое описание того, что она будет делать, и нажмите кнопку Создать:
    Затем выберите среду выполнения кода и нажмите кнопку Продолжить:
    По умолчанию сервис предлагает создать Hello World — файл с примером кода на выбранном языке программирования. Этот файл будет создан и автоматически загружен в контейнер. В поле Способ укажите Редактор кода и выберите файл index.go.
    По умолчанию сервис предлагает работать с редактором кода прямо в веб-интерфейсе (как на скриншоте выше). Однако вместо этого вы можете загрузить файл с кодом из бакета Object Storage (этот способ подойдёт для файлов больше 3,5 МБ) или загрузить ZIP-архив с кодом с локальной машины. Переключатель способа добавления кода находится прямо над окном редактора.
    Код вашей функции может находиться как в одном файле, так и в нескольких. Вы также можете создавать папки. При этом обязательно нужно указывать точку входа — часть кода, которая будет вызываться первой и принимать параметры вызова. Формат точки входа — <имя файла с функцией>.<имя обработчика вызова>. Например, index.Handler.
    Вверху справа нажмите кнопку Создать версию, чтобы сохранить текущее состояние функции.
    Сервис создаст версию функции и покажет справочную страницу о ней.
    Как протестировать созданную функцию
        Теперь в панели слева перейдите на вкладку Тестирование. В поле Шаблон данных выберите HTTPS-вызов. Сервис автоматически сгенерирует входные данные в формате JSON.
    Под полем с входными данными нажмите кнопку Запустить тест. Сервис выполнит HTTPS-вызов созданной функции и сформирует ответ (также в формате JSON).
    Task:
    Практическая работа. Запускаем функцию с помощью CLI
    Decision:
    В предыдущей практической работе вы познакомились с созданием функции через консоль управления. На этом уроке вы научитесь создавать функцию с помощью интерфейса командной строки (утилиты yc).
    Пользоваться консолью управления бывает очень удобно, но вести большой проект всё же лучше локально, с помощью среды разработки. Артефакты локальной разработки можно с лёгкостью переносить в облако с помощью консольных утилит. Выполняя последовательно шаги, вы изучите основные команды для создания функций в облаке.
    Шаг 1. Создание сервисного аккаунта
    Создание аккаунта
    Для начала убедитесь, что у вас установлена и инициализирована утилита yc.
    У вас уже есть сервисные аккаунты, созданные на предыдущих занятиях. Однако гораздо лучше, когда для каждой конкретной задачи (или блока задач) вы заводите отдельный сервисный аккаунт. Это обеспечивает прозрачность в управлении доступом и контроле за ролями в сервисах.
    Предварительно установите утилиту jq, она потребуется для выполнения задания:
    sudo apt install jq 
    Создайте сервисный аккаунт с именем service-account-for-cf:
    export SERVICE_ACCOUNT=$(yc iam service-account create \
      --name service-account-for-cf \
      --description "service account for cloud functions" \
      --format json | jq -r .) 
    Проверьте текущий список сервисных аккаунтов:
    yc iam service-account list
    echo $SERVICE_ACCOUNT 
    После проверки запишите идентификатор (ID) созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
    echo "export SERVICE_ACCOUNT_ID=<идентификатор_сервисного_аккаунта>" >> ~/.bashrc && . ~/.bashrc
    echo $SERVICE_ACCOUNT_ID 
    Назначение роли сервисному аккаунту
    Добавьте вновь созданному сервисному аккаунту роль editor:
    echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc 
    echo $FOLDER_ID
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_ID \
      --role editor 
    Не удаляйте файл ~/.bashrc после прохождения практической работы, он понадобится нам в дальнейшем.
    Шаг 2. Создание и настройка функции
    Создание функции
    Создайте функцию с именем my-first-function:
    yc serverless function create --name my-first-function 
    Вы получите URL, по которому можно будет сделать вызов функции http_invoke_url. По умолчанию функция будет непубличной.
    Загрузка кода функции
    Создайте файл  index.py :
    sudo nano index.py 
    Добавьте в index.py следующее содержимое:
    def handler(event, context):
        return {
            'statusCode': 200,
            'body': 'Hello World!',
        } 
    Успешное выполнение этой функции вернёт небольшую веб-страницу.
    Загрузите код функции в облако и создайте её версию. Для этого перейдите в папку с файлом index.py и выполните команду:
    yc serverless function version create \
        --function-name my-first-function \
        --memory 256m \
        --execution-timeout 5s \
        --runtime python37 \
        --entrypoint index.handler \
        --service-account-id $SERVICE_ACCOUNT_ID \
        --source-path index.py 
    Успешное выполнение команды приведёт к созданию версии функции. С помощью консоли управления убедитесь, что версия создана.
    Вызов функции
    Получите список функций, а затем — информацию о функции my-first-function:
    yc serverless function list
    yc serverless function version list --function-name my-first-function 
    В результате вызова последней команды из столбца FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
    yc serverless function invoke <идентификатор_функции> 
    По умолчанию функция создаётся непубличной. Чтобы сделать функцию my-first-function публичной, выполните следующую команду:
    yc serverless function allow-unauthenticated-invoke my-first-function 
    После этого вы сможете вызвать её в браузере. Получите параметр http_invoke_url для функции my-first-function:
    yc serverless function get my-first-function 
    Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции.
    Task:
    Практическая работа. Создание триггера от Object Storage
    Decision:
    В предыдущем практическом уроке вы познакомились с созданием одной функции с помощью интерфейса командной строки (yc). В этом уроке мы продолжим разработку этой функции: модифицируем её содержание, добавим переменные окружения и т.д.
    Важно выполнить предыдущий практический урок, так как вы будете опираться на знания и результаты, полученные в нём.
    Шаг 1. Модификация сервисного аккаунта
    Добавление роли сервисному аккаунту
    По итогам прохождения предыдущей практической работы у вас есть сервисный аккаунт с именем service-account-for-cf. Для работы с Object Storage добавьте этому сервисному аккаунту роль storage.editor:
    yc resource-manager folder add-access-binding $FOLDER_ID \
        --role storage.editor \
        --subject serviceAccount:$SERVICE_ACCOUNT_ID 
    Создание ключа доступа для сервисного аккаунта
    Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, а также в том случае, если на следующем шаге для создания бакета в Object Storage вы планируете использовать Terraform.
    Для создания ключа доступа необходимо вызвать следующую команду:
    yc iam access-key create --service-account-name service-account-for-cf 
    В результате вы получите примерно следующее:
    access_key:
        id: ajefraollq5puj2tir1o
        service_account_id: ajetdv28pl0a1a8r41f0
        created_at: "2021-08-23T21:13:05.677319393Z"
        key_id: BTPNvWthv0ZX2xVmlPIU
    secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
    Где:
        key_id — идентификатор ключа доступа, ACCESS_KEY.
        secret — секретный ключ, SECRET_KEY.
    Переменные ACCESS_KEY и SECRET_KEY будут использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3 на следующих этапах.
    Шаг 2. Object Storage
    Самый простой способ создания бакета в Object Storage — через консоль управления. Более сложный, позволяющий автоматизировать разработку, — использование Terraform. Вы можете выбрать любой из них.
    Способ 1. Консоль управления
    В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
    На странице создания бакета:
        Введите имя бакета, пусть это будет bucket-for-trigger.
        При необходимости ограничьте максимальный размер бакета, установив значение, например, 1 ГБ.
        Выберите тип доступа, в нашем уроке установим значения в Публичный во всех случаях.
        Выберите класс хранилища, по умолчанию используется Стандартное.
    Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
    Способ 2. Terraform
    Прежде всего необходимо получить OAuth-токен для работы с Yandex Cloud. Для этого можно сделать запрос к сервису Яндекс.OAuth. Подробнее прочитать можно в документации.
    Сохраните OAuth-токен в переменную OAuth, но никому не передавайте. Также вам потребуются значения переменных: идентификатор облака — CLOUD_ID и идентификатор каталога FOLDER_ID (сохранен в переменную ранее).
    Также на предыдущем шаге вы получили ключ доступа для сервисного аккаунта. Нам потребуется идентификатор ключа доступа ACCESS_KEY и секретный ключ SECRET_KEY.
    В файл main.tf, представленный далее, внесём все собранные переменные. Важно: переменная BUCKET_NAME содержит имя создаваемого бакета в Object Storage, куда будем загружать файлы. Допустим, переменная будет равна bucket-for-trigger. Сохраним все значения:
    terraform {
      required_providers {
        yandex = {
          source = "yandex-cloud/yandex"
        }
      }
      required_version = ">= 0.13"
    }
    provider "yandex" {
      token     = "<OAuth>"
      cloud_id  = "<CLOUD_ID>"
      folder_id = "<FOLDER_ID>"
    }
    resource "yandex_storage_bucket" "bucket" {
      access_key = "<ACCESS_KEY>"
      secret_key = "<SECRET_KEY>"
      bucket = "<BUCKET_NAME>"
    } 
    После внесения правок, находясь в каталоге с файлом main.tf, последовательно выполните следующие команды:
    terraform init
    terraform plan
    terraform apply 
    Успешное выполнение команд приведёт к созданию бакета bucket-for-trigger в объектном хранилище в вашем рабочем каталоге.
    Шаг 3. Модификация функции
    В предыдущей практической работе мы создали функцию с именем my-first-function с помощью следующей команды:
    yc serverless function create --name my-first-function 
    При создании функции вы получили URL, по которому можно будет сделать вызов функции http_invoke_url.
    Загрузка кода новой версии
    Новая версия функции имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо файлы index.py и requirements.txt заархивировать и получить файл my-first-function.zip.
    Новая версия index.py:
    import os
    import datetime
    import boto3
    import pytz
    ACCESS_KEY = os.getenv("ACCESS_KEY")
    SECRET_KEY = os.getenv("SECRET_KEY")
    BUCKET_NAME = os.getenv("BUCKET_NAME")
    TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
    TEMP_FILENAME = "/tmp/temp_file"
    TEXT_FOR_TEMP_FILE = "This is text file"
    def write_temp_file():
        temp_file = open(TEMP_FILENAME, 'w')
        temp_file.write(TEXT_FOR_TEMP_FILE)
        temp_file.close()
        print("\U0001f680 Temp file is written")
    def get_now_datetime_str():
        now = datetime.datetime.now(pytz.timezone(TIME_ZONE))    
        return now.strftime('%Y-%m-%d__%H-%M-%S')
    def get_s3_instance():
        session = boto3.session.Session()
        return session.client(
            aws_access_key_id=ACCESS_KEY,
            aws_secret_access_key=SECRET_KEY,
            service_name='s3',
            endpoint_url='https://storage.yandexcloud.net'
        )
    def upload_dump_to_s3():
        print("\U0001F4C2 Starting upload to Object Storage")
        get_s3_instance().upload_file(
            Filename=TEMP_FILENAME,
            Bucket=BUCKET_NAME,
            Key=f'file-{get_now_datetime_str()}.txt'
        )
        print("\U0001f680 Uploaded")
    def remove_temp_files():
        os.remove(TEMP_FILENAME)
        print("\U0001F44D That's all!")
    def handler(event, context):
        write_temp_file()
        upload_dump_to_s3()
        remove_temp_files()
        return {
            'statusCode': 200,
            'body': 'File is uploaded',
        } 
    Первая версия requirements.txt:
    boto3==1.13.10
    botocore==1.16.10
    python-dateutil==2.8.1
    pytz==2020.1 
    Находясь в каталоге с файлом my-first-function.zip вызовите следующую команду, это позволит вам загрузить код функции в облако и создать её версию:
    yc serverless function version create \
      --function-name my-first-function \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint index.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-path my-first-function.zip 
    Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой версии необходимо подготовить несколько переменных. Переменные ACCESS_KEY и SECRET_KEY вы получили на первом шаге, а значение BUCKET_NAME на втором:
    echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
    echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
    echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
    Определим идентификатор (ID) для последней загруженной версии функции:
    yc serverless function version list --function-name my-first-function 
    Создадим новую версию функции, задав при этом переменные окружения. Для этого выставим значение параметра source-version-id равное полученному ID в следующей команде:
    yc serverless function version create \
      --function-name my-first-function \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint index.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-version-id <ID> \
      --environment ACCESS_KEY=$ACCESS_KEY \
      --environment SECRET_KEY=$SECRET_KEY \
      --environment BUCKET_NAME=$BUCKET_NAME 
    Успешное выполнение команды приведёт к созданию версии функции.
    Вызов функции
    Получите список функций и информацию о функции my-first-function:
    yc serverless function list
    yc serverless function version list --function-name my-first-function 
    В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
    yc serverless function invoke <идентификатор_функции> 
    В предыдущей практической работе мы сделали функцию my-first-function публичной с помощью команды:
    yc serverless function allow-unauthenticated-invoke my-first-function 
    Теперь мы можем сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
    yc serverless function get my-first-function 
    Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции. Во время её вызова в Object Storage будет создан новый файл.
    Шаг 4. Создание триггера
    Создание функции
    Для создания триггера нам необходима функция, которую триггер будет запускать. Аналогично предыдущему шагу создадим функцию my-trigger-function и её версию на основе файла index.py.
    def handler(event, context):
        print("\U0001F4C2 Starting function after trigger")
        print(event)     
        return {
            'statusCode': 200,
            'body': 'File is uploaded',
        } 
    Находясь в каталоге с файлом index.py, вызовите следующие команды:
    yc serverless function create --name my-trigger-function
    yc serverless function version create \
      --function-name my-trigger-function \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint index.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-path index.py
    yc serverless function version list --function-name my-trigger-function 
    Создание триггера
    Чтобы создать триггер my-first-trigger, который вызывает функцию my-trigger-function при создании нового объекта в бакете BUCKET_NAME, выполните команду:
    yc serverless trigger create object-storage \
      --name my-first-trigger \
      --bucket-id $BUCKET_NAME \
      --events 'create-object' \
      --invoke-function-name my-trigger-function \
      --invoke-function-service-account-id $SERVICE_ACCOUNT_ID 
    Вызов цепочки событий
    Чтобы запустить цепочку событий, вызовем первую функцию my-first-function. Получите список функций и информацию о функции my-first-function:
    yc serverless function list
    yc serverless function version list --function-name my-first-function 
    В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью команды:
    yc serverless function invoke <идентификатор_функции> 
    После этого вы можете сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
    yc serverless function get my-first-function 
    Введите значение параметра http_invoke_url в браузере. Во время вызова функции в Object Storage будет создан новый объект. Сразу после этого сработает триггер my-first-trigger, который вызовет функцию my-trigger-function. В итоге, наша вторая функция запишет в логи содержание переменной event. Убедиться в этом вы сможете как в UI, так и через CLI.
    yc serverless function logs my-trigger-function 
    Task:
    Практическая работа. Навык Алисы
    Decision:
    В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, добавили ему роли editor и storage.editor и создали ключ доступа.
    Также вы создали бакет в Object Storage с именем bucket-for-trigger, триггер my-first-trigger для его обработки и вызываемую им функцию my-trigger-function.
    Ещё была создана функция my-first-function, её использовали для того, чтобы запустить цепочку событий. Публичный вызов этой функции приводил к созданию нового объекта в бакете в Object Storage. Это запускало вызов триггера my-first-trigger, который стартовал функцию my-trigger-function. В итоге последняя функция записывала в логи содержание переменной event.
    Если вы удалили бакет и сервисный аккаунт, необходимо вернуться к предыдущим урокам и повторить их создание.
    Шаг 1. Создание функции
    На предыдущем уроке мы создали функцию с именем my-first-function. Поменяем её исходный код так, чтобы обрабатывать запросы от Алисы.
    На основе функции будет создан навык Попугай, который повторяет все, что ему написал или сказал пользователь.
    Функция parrot
    Создадим новую функцию с именем parrot с помощью команды:
    yc serverless function create \
      --name parrot \
      --description "function for Alice" 
    По умолчанию функция не является публичной.
    Загрузка кода новой версии
    Функция имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо заархивировать файлы parrot.py и requirements.txt и получить файл parrot.zip.
    Содержание функции parrot.py:
    import os
    import datetime
    import boto3
    import pytz
    ACCESS_KEY = os.getenv("ACCESS_KEY")
    SECRET_KEY = os.getenv("SECRET_KEY")
    BUCKET_NAME = os.getenv("BUCKET_NAME")
    TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
    TEMP_FILENAME = "/tmp/temp_file"
    TEXT_FOR_TEMP_FILE = "This is text file"
    def write_temp_file(text_for_s3):
        TEXT_FOR_TEMP_FILE = text_for_s3
        temp_file = open(TEMP_FILENAME, 'w')    
        temp_file.write(TEXT_FOR_TEMP_FILE)
        temp_file.close()
        print("\U0001f680 Temp file is written")
    def get_now_datetime_str():
        now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
        return now.strftime('%Y-%m-%d__%H-%M-%S')
    def get_s3_instance():
        session = boto3.session.Session()
        return session.client(
            aws_access_key_id=ACCESS_KEY,
            aws_secret_access_key=SECRET_KEY,
            service_name='s3',
            endpoint_url='https://storage.yandexcloud.net'
        )
    def upload_dump_to_s3():
        print("\U0001F4C2 Starting upload to Object Storage")
        get_s3_instance().upload_file(
            Filename=TEMP_FILENAME,
            Bucket=BUCKET_NAME,
            Key=f'file-{get_now_datetime_str()}.txt'
        )
        print("\U0001f680 Uploaded")
    def remove_temp_files():
        os.remove(TEMP_FILENAME)
        print("\U0001F44D That's all!")
    def handler(event, context):
        """
        Entry-point for Serverless Function.
        :param event: request payload.
        :param context: information about current execution context.
        :return: response to be serialized as JSON.
        """
        text = 'Hello! I\'ll repeat anything you say to me.'
        if 'request' in event and \
                'original_utterance' in event['request'] \
                and len(event['request']['original_utterance']) > 0:
            text = event['request']['original_utterance']
            write_temp_file(text)
            upload_dump_to_s3()
            remove_temp_files()
        return {
            'version': event['version'],
            'session': event['session'],
            'response': {
                # Respond with the original request or welcome the user if this is the beginning of the dialog and the request has not yet been made.
                'text': text,
                # Don't finish the session after this response.
                'end_session': 'false'
            },
        }
    Содержание файла зависимостей requirements.txt:
    boto3==1.13.10
    botocore==1.16.10
    python-dateutil==2.8.1
    pytz==2020.1 
    Находясь в каталоге с файлом parrot.zip, вызовите приведенную ниже команду. Это позволит вам загрузить код функции в облако и создать её версию:
    yc serverless function version create \
      --function-name=parrot \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=parrot.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-path parrot.zip 
    Шаг 2. Создание новой версии функции
    Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой новой версии функции необходимы переменные.
    Если переменные среды не сохранились, то в консоли управления можно посмотреть имя бакета, а ACCESS_KEY и SECRET_KEY скопировать из предыдущей функции my-first-function:
    echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
    echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
    echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
    Определим идентификатор (ID) последней загруженной версии функции:
    yc serverless function version list --function-name parrot 
    Создадим новую версию функции, задав переменные окружения. Для этого выставим значение параметра source-version-id равное полученному идентификатору версии функции (ID) в следующей команде:
    yc serverless function version create \
      --function-name parrot \
      --memory 256m \
      --execution-timeout 5s \
      --runtime python37 \
      --entrypoint parrot.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --source-version-id <идентификатор_версии_функции> \
      --environment ACCESS_KEY=$ACCESS_KEY \
      --environment SECRET_KEY=$SECRET_KEY \
      --environment BUCKET_NAME=$BUCKET_NAME 
    Успешное выполнение команды приведёт к созданию версии функции.
    Шаг 3. Вызов функции и её тестирование
    По умолчанию функция создаётся непубличной. Чтобы сделать функцию parrot публичной, вызовите следующую команду:
    yc serverless function allow-unauthenticated-invoke parrot 
    Протестируйте функцию parrot, чтобы проверить правильность кода перед созданием связки с Алисой. В консоли управления на странице сервиса Cloud Functions выберите созданную функцию и перейдите на вкладку Тестирование. В поле Шаблон данных данных укажите Навык Алисы и нажмите кнопку Запустить тест.
    В блоке Результат тестирования убедитесь, что функция выполнена и приведен ответ.
    Перейдите по ссылке https://dialogs.yandex.ru/developer/ и создайте новый диалог Алисы (подробности о создании навыков вы можете узнать из документации):
        Нажмите кнопку Создать диалог. Выберите тип диалога Навык в Алисе, у вас откроется форма на вкладке Настройки.
    Заполните имя навыка, оно должно состоять минимум из двух слов, например My parrot.
    В блоке Backend выберите вариант Функция в Яндекс.Облаке и в выпадающем списке выберите созданную вами функцию parrot.
    В блоке Тип доступа в выпадающем списке выберите Приватный.
    В блоке Публикация в каталоге выберите Примеры запросов, например Запусти навык - My parrot, Имя разработчика, Категорию, Описание и Иконку.
    Нажмите кнопку Сохранить и перейдите на вкладку Тестирование.
    Если вы сделали всё правильно, то на экране появится приветствие навыка. Далее навык будет повторять всё, что вы ему напишете. При этом фразы, которые вы отправите Алисе, будут сохраняться в новом файле в бакете. Вы можете это проверить в консоли управления.
    Task:
    Практическая работа. Проверка доступности
    Decision:
    На этом практическом занятии вы создадите функцию для проверки доступности сайта yandex.ru, которая будет измерять время ответа. Результаты работы функции будут передаваться в базу данных сервиса Yandex Managed Service for PostgreSQL с использованием подключения к управляемой БД из функции. Также вы запустите триггер-таймер, который будет регулярно производить опрос сайта yandex.ru.
    Шаг 1. Дополнительная роль для сервисного аккаунта
    В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, назначили ему роли editor и  storage.editor и создали ключ доступа. Чтобы подключаться к управляемым БД из функции, нужно добавить сервисному аккаунту роль serverless.mdbProxies.user.
    Для этого выполните следующую команду:
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --role serverless.mdbProxies.user \
      --subject serviceAccount:$SERVICE_ACCOUNT_ID 
    Шаг 2. Создание базы данных
    Создание кластера PostgreSQL
    Конечно, кластер PostgreSQL можно создать с помощью консоли управления, но в этой практической работе мы используем CLI. Прежде всего, давайте определим подсеть, в которой будет расположен кластер. Разместим кластер в зоне ru-central1-c и с помощью следующей команды узнаем идентификатор(ID) соответствующей подсети:
    yc vpc subnet list 
    Создадим кластер версии PostgreSQL 13 с именем my-pg-database. Установим тип хоста burstable b2.nano — это самый дешёвый и простой вариант хоста. Из-за невысокой производительности он подходит только для тестовых целей. Используем для хоста жёсткий диск (HDD) размером 10 ГБ.
    Сразу создадим пользователя с именем user1 и паролем user1user1, а также базу данных db1. Для удобства администрирования откроем доступ из консоли управления. Используйте опцию websql-access — это позволит выполнять SQL-запросы прямо в консоли управления. Чтобы открыть возможность подключения к PostgreSQL из функции, необходимо подключить опцию serverless-access.
    Следующая команда за несколько минут создаст кластер PostgreSQL (не забудьте подставить идентификатор вашей подсети):
    yc managed-postgresql cluster create \
      --name my-pg-database \
      --description 'For Serverless' \
      --postgresql-version 13 \
      --environment production \
      --network-name default \
      --resource-preset b2.nano \
      --host zone-id=ru-central1-c,subnet-id=<идентификатор_подсети> \
      --disk-type network-hdd \
      --disk-size 10 \
      --user name=user1,password=user1user1 \
      --database name=db1,owner=user1 \
      --websql-access \
      --serverless-access 
    После успешного создания кластера проверьте результат:
    yc managed-postgresql cluster list
    yc managed-postgresql cluster get <имя или идентификатор кластера> 
    Создание таблицы для хранения данных
    При создании кластера мы использовали опцию websql-access, что открывает нам возможности по исполнению SQL-команд в консоли управления. Воспользуемся этим и сделаем таблицу в созданной нами базе данных. В эту таблицу мы будем складывать результаты выполнения функции. В консоли управления перейдите в каталог, в котором создан кластер PostgreSQL. Откройте сервис Managed Service for PostgreSQL и перейдите в кластер my-pg-database.
    В боковом меню перейдите на вкладку SQL. Для базы данных db1введите имя user1 и пароль user1user1, нажмите кнопку Подключиться.
    В открывшемся окне введите SQL-запрос и исполните его:
    CREATE TABLE measurements (
        result integer,
        time float
    ); 
    Успешное выполнение команды создаст таблицу, куда мы будем складывать результаты.
    Шаг 3. Подключение к управляемой БД из функции
    Создание подключения
    В консоли управления перейдите в каталог, в котором хотите создать подключение. Откройте сервис Cloud Functions. В боковом меню перейдите на вкладку Подключения к БД. Нажмите кнопку Создать подключение.
        Введите имя, описание подключения и в выпадающем списке выберите тип подключения — PostgreSQL.
        Укажите кластер — my-pg-database.
        Укажите базу данных — db1.
        Введите данные пользователя БД: имя user1 и пароль user1user1.
        Нажмите кнопку Создать.
    Выберите созданное подключение. На вкладке Обзор скопируйте параметры Идентификатор и Точка входа. Они будут использованы в функции на следующем шаге.
    Шаг 4. Создание функции
    Перед созданием функции определите переменные для инициации подключения: CONNECTION_ID — идентификатор подключения, DB_USER — имя пользователя БД, DB_HOST — точка входа. Используйте следующие команды:
    echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
    Они будут использованы в функции function-for-postgresql.py. Код функции:
    import datetime
    import logging
    import requests
    import os
    #Эти библиотеки нужны для работы с PostgreSQL
    import psycopg2
    import psycopg2.errors
    CONNECTION_ID = os.getenv("CONNECTION_ID")
    DB_USER = os.getenv("DB_USER")
    DB_HOST = os.getenv("DB_HOST")
    # Настраиваем функцию для записи информации в журнал функции
    # Получаем стандартный логер языка Python
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
    verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
    #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
    def log(logString):
        if verboseLogging:
            logger.info(logString)
    #Запись в базу данных
    def save(result, time, context):
        connection = psycopg2.connect(
            database=CONNECTION_ID, # Идентификатор подключения
            user=DB_USER, # Пользователь БД
            password=context.token["access_token"],
            host=DB_HOST, # Точка входа
            port=6432,
            sslmode="require")
        cursor = connection.cursor()    
        postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
        record_to_insert = (result, time)
        cursor.execute(postgres_insert_query, record_to_insert)
        connection.commit()
    # Это обработчик. Он будет вызван первым при запуске функции
    def entry(event, context):
        #Выводим в журнал значения входных параметров event и context
        log(event)
        log(context)
        # Тут мы запоминаем текущее время, отправляем запрос к yandex.ru и вычисляем время выполнения запроса
        try:
            now = datetime.datetime.now()
            #здесь указано два таймаута: 1c для установки связи с сервисом и 3 секунды на получение ответа
            response = requests.get('https://yandex.ru', timeout=(1.0000, 3.0000))
            timediff = datetime.datetime.now() - now
            #сохраняем результат запроса
            result = response.status_code
        #если в процессе запроса сработали таймауты, то в результат записываем соответствующие коды
        except requests.exceptions.ReadTimeout:
            result = 601
        except requests.exceptions.ConnectTimeout:
            result = 602
        except requests.exceptions.Timeout:
            result = 603
        log(f'Result: {result} Time: {timediff.total_seconds()}')    
        save(result, timediff.total_seconds(), context)
        #возвращаем результат запроса
        return {
            'statusCode': result,
            'headers': {
                'Content-Type': 'text/plain'
            },
            'isBase64Encoded': False
        } 
    Перейдем в директорию с кодом функции и создадим нашу функцию function-for-postgresql. При этом сразу зададим все необходимые переменные и сервисный аккаунт:
    yc serverless function create \
      --name  function-for-postgresql \
      --description "function for postgresql"
    yc serverless function version create \
      --function-name=function-for-postgresql \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=function-for-postgresql.entry \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --environment VERBOSE_LOG=True \
      --environment CONNECTION_ID=$CONNECTION_ID \
      --environment DB_USER=$DB_USER \
      --environment DB_HOST=$DB_HOST \
      --source-path function-for-postgresql.py 
    Проверим работоспособность функции:
    yc serverless function version list --function-name function-for-postgresql
    yc serverless function invoke --name function-for-postgresql 
    Успешный вызов функции приведёт к измерению времени ответа сайта и формированию записи в базе данных.
    Шаг 5. Создание триггера
    Создание триггера-таймера
    Проверять доступность сайта лучше в автоматическом режиме через равные промежутки времени. Для этой задачи создайте триггер-таймер. Он будет использовать cron-выражения:
    yc serverless trigger create timer \
      --name trigger-for-postgresql \
      --invoke-function-name function-for-postgresql \
      --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
      --cron-expression '* * * * ? *' 
    Cron-выражение * * * * ? * означает вызов функции function-for-postgresql один раз в минуту. Успешное выполнение функции раз в минуту будет создавать запись в базе данных, в чём вы можете убедиться, просмотрев записи в таблице.
    Убедились? Поздравляем: вы успешно создали функцию, которая через заданный промежуток времени выполняется по триггеру, чтобы проверить доступность yandex.ru и записать результат проверки в базу данных.
    Удаление триггера-таймера
    После завершения практической работы не забудьте удалить созданный триггер trigger-for-postgresql, иначе он будет продолжать работать:
    yc serverless trigger delete trigger-for-postgresql 
    Поздравляем, вы успешно закончили вторую тему. Пройдите короткий тест — и мы перейдём к изучению сервиса API Gateway.
    Не удаляйте созданный кластер PostgreSQL, он понадобится для следующей практической работы.
    Task:
    Ниже — пример статического ответа. Какие элементы из обязательных в нём пропущены или в них допущена ошибка?
    openapi: 3.0.0
    info:
      title: Test API
      version: 1.0.0
    paths:
      /enter:
        get:
          x-yc-apigateway-integration:
            http_code: 200
            http_headers:
              Content-Type: text/plain
            content:
              text/plain: |
                Enter the world!
        operationId: enter 
    Decision:
    -enter:
    -get:
    -x-yc-apigateway-integration:
    +operationId:
    Task:
    Практическая работа. Создание HTTP API с помощью Cloud Functions и API Gateway
    Decision:
    На предыдущем практическом занятии мы создали простую систему, которая проверяет доступность сайта yandex.ru и измеряет время ответа на запрос. Полученную информацию функция записывала в базу данных PostgreSQL. На этом уроке мы доработаем начатый проект и добавим REST API, который позволит получать до 50 результатов проверки из базы данных.
    Шаг 1. Проверить наличие сервисного аккаунта
    Для работы нам понадобится сервисный аккаунт с именем service-account-for-cf и ролями editor, serverless.mdbProxies.user, который мы создали ранее.
    Шаг 2. Yandex API Gateway
    Создание спецификации
    В рабочем каталоге создадим спецификацию hello-world.yaml:
    openapi: "3.0.0"
    info:
      version: 1.0.0
      title: Test API
    paths:
      /hello:
        get:
          summary: Say hello
          operationId: hello
          parameters:
            - name: user
              in: query
              description: User name to appear in greetings
              required: false
              schema:
                type: string
                default: 'world'
          responses:
            '200':
              description: Greeting
              content:
                'text/plain':
                  schema:
                    type: "string"
          x-yc-apigateway-integration:
            type: dummy
            http_code: 200
            http_headers:
              'Content-Type': "text/plain"
            content:
              'text/plain': "Hello, {user}!\n" 
    Мы можем создать API-шлюз с помощью консоли управления, но сейчас воспользуемся CLI.
    Инициализация спецификации
    Чтобы развернуть API-шлюз, используем спецификацию hello-world.yaml:
    yc serverless api-gateway create \
      --name hello-world \
      --spec=hello-world.yaml \
      --description "hello world" 
    В результате успешного создания API-шлюза получим значение параметра domain:
    yc serverless api-gateway list
    yc serverless api-gateway get --name hello-world 
    Скопируем служебный домен, чтобы проверить работоспособность API-шлюза. Вставим его в адресную строку браузера и допишем в конец /hello. Должно получиться следующее:
    https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello 
    Теперь протестируем запрос с параметрами. Добавьте к предыдущему запросу ?user=my_user. Должно получиться следующее:
    https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello?user=my_user 
    В первом случае в окне браузера вы увидите «Hello, world!», во втором «Hello, my_user!».
    Шаг 3. Создание функции
    Работа с библиотеками и переменными
    До этого момента мы использовали рантайм python37, который не требовал явного указания библиотек, но начиная с версии python39, нужно указывать библиотеки явно. Для работы с requirements.txt можно воспользоваться удобной Python-библиотекой pipreqs: чтобы сгенерировать requirements.txt с помощью pipreqs, достаточно указать рабочий каталог. В большинстве интерпретаторов Linux для указания текущего каталога предусмотрена переменная $PWD. Если файл requirements.txt уже существует, актуализируйте его с помощью флага --force, например:
    pip install pipreqs
    pipreqs $PWD --print
    pipreqs $PWD --force 
    Чтобы создать функцию, проверим доступность переменных для инициации подключения CONNECTION_ID, DB_USER, DB_HOST, которые мы создали в предыдущей работе с помощью следующих команд:
    echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
    Создание функции
    Создадим функцию function-for-user-requests.py:
    import json
    import logging
    import requests
    import os
    #Эти библиотеки нужны для работы с PostgreSQL
    import psycopg2
    import psycopg2.errors
    import psycopg2.extras
    CONNECTION_ID = os.getenv("CONNECTION_ID")
    DB_USER = os.getenv("DB_USER")
    DB_HOST = os.getenv("DB_HOST")
    # Настраиваем функцию для записи информации в журнал функции
    # Получаем стандартный логер языка Python
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения
    verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
    #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
    def log(logString):
        if verboseLogging:
            logger.info(logString)
    #Запись в базу данных
    def save(result, time, context):
        connection = psycopg2.connect(
            database=CONNECTION_ID, # Идентификатор подключения
            user=DB_USER, # Пользователь БД
            password=context.token["access_token"],
            host=DB_HOST, # Точка входа
            port=6432,
            sslmode="require")
        cursor = connection.cursor()   
        postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
        record_to_insert = (result, time)
        cursor.execute(postgres_insert_query, record_to_insert)
        connection.commit()
    #Формируем запрос
    def generateQuery():
        select = f"SELECT * FROM measurements LIMIT 50"
        result = select
        return result
    #Получаем подключение
    def getConnString(context):
        """
        Extract env variables to connect to DB and return a db string
        Raise an error if the env variables are not set
        :return: string
        """
        connection = psycopg2.connect(
            database=CONNECTION_ID, # Идентификатор подключения
            user=DB_USER, # Пользователь БД
            password=context.token["access_token"],
            host=DB_HOST, # Точка входа
            port=6432,
            sslmode="require")   
        return connection
    def handler(event, context):
        try:
            secret = event['queryStringParameters']['secret']
            if secret != 'cecfb23c-bc86-4ca2-b611-e79bc77e5c31':
                raise Exception()
        except Exception as error:
            logger.error(error)
            statusCode = 401
            return {
                'statusCode': statusCode
            }
        sql = generateQuery()
        log(f'Exec: {sql}')
        connection = getConnString(context)
        log(f'Connecting: {connection}')
        cursor = connection.cursor()
        try:
            cursor.execute(sql)
            statusCode = 200
            return {
                'statusCode': statusCode,
                'body': json.dumps(cursor.fetchall()),
            }
        except psycopg2.errors.UndefinedTable as error:
            connection.rollback()
            logger.error(error)
            statusCode = 500
        except Exception as error:
            logger.error(error)
            statusCode = 500
        cursor.close()
        connection.close()
        return {
            'statusCode': statusCode,
            'body': json.dumps({
                'event': event,
            }),
        }
    Обратите внимание, в коде функции мы заложили параметр secret и его значение cecfb23c-bc86-4ca2-b611-e79bc77e5c31, при котором функция будет выполняться. Таким образом мы обеспечиваем дополнительную защиту при доступе к БД.
    При создании функции сразу зададим все необходимые переменные и сервисный аккаунт:
    yc serverless function create \
      --name function-for-user-requests \
      --description "function for response to user"
    yc serverless function version create \
      --function-name=function-for-user-requests \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=function-for-user-requests.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --environment VERBOSE_LOG=True \
      --environment CONNECTION_ID=$CONNECTION_ID \
      --environment DB_USER=$DB_USER \
      --environment DB_HOST=$DB_HOST \
      --source-path function-for-user-requests.py 
    Шаг 4. Обновление спецификации API Gateway
    Наша функция готова, но по умолчанию она не является публичной. Предоставим доступ к этой функции с помощью API-шлюза — обновим ранее созданную спецификацию hello-world.yaml. Не забудьте вставить в файл идентификаторы вашей функции и вашего сервисного аккаунта:
    openapi: "3.0.0"
    info:
      version: 1.0.0
      title: Updated API
    paths:
      /results:
        get:
          x-yc-apigateway-integration:
            type: cloud-functions
            function_id: <идентификатор функции>
            service_account_id: <идентификатор сервисного аккаунта>
          operationId: function-for-user-requests 
    Вызовем перезагрузку нашей спецификации:
    yc serverless api-gateway update \
      --name hello-world \
      --spec=hello-world.yaml 
    Для тестирования вызовем функцию в браузере сначала без параметра secret, а затем — с ним:
    https://<идентификатор API Gateway>.apigw.yandexcloud.net/results
    https://<идентификатор API Gateway>.apigw.yandexcloud.net/results?secret=cecfb23c-bc86-4ca2-b611-e79bc77e5c31 
    В ответе увидим результаты тестирования сервиса yandex.ru из базы данных.
    Иногда приходится тестировать функцию в процессе разработки: для этого в консоли управления на странице функции перейдите на вкладку Тестирование, в поле Шаблон данных выберите HTTPS-вызов. Нажмите кнопку Запустить тест, и вы увидите код ошибки.
    Код функции проверяет параметр secret для авторизации, то есть при вызове вы должны передать секретную последовательность, чтобы функция выдала результат. Добавим secret в параметры запроса в поле Входные данные:
        "queryStringParameters": {
            "a": "2",
            "b": "1",
            "secret": "cecfb23c-bc86-4ca2-b611-e79bc77e5c31"
        }, 
    Запустим тест ещё раз. В ответе отобразятся данные из базы, как и с запросами через браузер.
    Task:
    При работе в serverless-режиме для доступа к данным YDB вы можете использовать:
    Decision:
    +YQL-запросы через консоль управления и YDB CLI
    +YQL-запросы через консоль управления, YDB CLI, YDB API
    +Document API и cURL (для документных таблиц)
    +Document API через cURL, AWS CLI, AWS SDK (для документных таблиц)
    Task:
    Рассчитайте стоимость следующего запроса YQL. Ответ введите в текстовое поле ниже.
    $to_update = (
        SELECT series_id,
               season_id,
               episode_id,
               Utf8("Yesterday's Jam UPDATED") AS title
        FROM episodes
        WHERE series_id = 1 AND season_id = 1 AND episode_id = 1
    );
    SELECT * FROM episodes WHERE series_id = 1 AND season_id = 1;
    UPDATE episodes ON
    SELECT * FROM $to_update; 
    На основании статистики запроса:
    {
        "processCpuTimeUs": "11254",
        "queryPhases": [
            {
                "affectedShards": "1",
                "cpuTimeUs": "590",
                "durationUs": "8939",
                "tableAccess": [
                    {
                        "name": "episodes",
                        "partitionsCount": "1",
                        "reads": {
                            "bytes": "24",
                            "rows": "1"
                        }
                    }
                ]
            },
            {
                "affectedShards": "1",
                "cpuTimeUs": "510",
                "durationUs": "4131",
                "tableAccess": [
                    {
                        "name": "episodes",
                        "partitionsCount": "1",
                        "reads": {
                            "bytes": "48",
                            "rows": "2"
                        }
                    }
                ]
            },
            {
                "affectedShards": "1",
                "cpuTimeUs": "380",
                "durationUs": "4056",
                "tableAccess": [
                    {
                        "name": "episodes",
                        "partitionsCount": "1",
                        "reads": {
                            "bytes": "257",
                            "rows": "6"
                        }
                    }
                ]
            },
            {
                "affectedShards": "1",
                "cpuTimeUs": "463",
                "durationUs": "8651",
                "tableAccess": [
                    {
                        "name": "episodes",
                        "partitionsCount": "1",
                        "updates": {
                            "bytes": "47",
                            "rows": "1"
                        }
                    }
                ]
            }
        ]
    } 
    Сколько получилось?
    Task:
    Практика. Загрузка данных, выполнение запросов AWS CLI
    Decision:
    а предыдущем уроке мы рассмотрели, как работать с YDB через Document API — низкоуровневый HTTP API, совместимый с AWS DynamoDB API. В этом уроке рассмотрим операции создания таблицы, записи, чтения, изменения и удаления данных в таблице с помощью AWS CLI.
    Сервисный аккаунт и ключ доступа
    Для работы инструментов AWS вам понадобится создать сервисный аккаунт в облаке.
    Выберите вкладку Сервисные аккаунты в каталоге, где расположена БД.
    Нажмите кнопку Создать сервисный аккаунт.
    Введите имя сервисного аккаунта. Чтобы назначить сервисному аккаунту роль на текущий каталог, нажмите Добавить роль и выберите роль, например editor.
    Нажмите кнопку Создать.
    Выберите созданный сервисный аккаунт и нажмите на строку с его именем. Нажмите кнопку Создать новый ключ на верхней панели. Выберите пункт Создать статический ключ доступа.
    Сохраните идентификатор и секретный ключ.
    Работа с AWS CLI
    Установите AWS CLI с сайта https://aws.amazon.com/ru/cli/.
    Для Windows: загрузите и запустите 64- или 32-разрядный установщик.
    Для Mac и Linux: установите AWS CLI с помощью утилиты pip (требуется Python 2.6.5 или более поздней версии).
    pip install awscli 
    Для настройки AWS CLI  запустите команду:
    aws configure 
    Введите сохраненные значения идентификатора ключа AWS Access Key ID и ключа AWS Secret Access Key и укажите ru-central1 в качестве Default region name.
    Убедитесь, что в качестве переменной окружения ENDPOINT указано корректное значение эндпойнта вашей базы данных, либо добавьте его, как вы это делали в прошлом уроке: сохраните значение эндпойнта, указанное в строке Document API эндпоинт, в переменной окружения с помощью команды
    export ENDPOINT=<значение endpoint> 
    Создание таблицы
    Создайте таблицу с помощью команды:
    aws dynamodb create-table \
      --table-name docapitest/series \
      --attribute-definitions \
      AttributeName=series_id,AttributeType=N \
      AttributeName=title,AttributeType=S \
      --key-schema \
      AttributeName=series_id,KeyType=HASH \
      AttributeName=title,KeyType=RANGE \
      --endpoint $ENDPOINT 
    Убедитесь, что в директории docapitest появилась таблица series.
    Добавление данных в таблицу
    Добавьте в таблицу две строки c помощью команд:
    aws dynamodb put-item \
      --table-name docapitest/series \
      --item '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}, "series_info": {"S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."}, "release_date": {"S": "2006-02-03"}}' \
      --endpoint $ENDPOINT 
    и
    aws dynamodb put-item \
      --table-name docapitest/series \
      --item '{"series_id": {"N": "2"}, "title": {"S": "Silicon Valley"}, "series_info": {"S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."}, "release_date": {"S": "2014-04-06"}}' \
      --endpoint $ENDPOINT 
    Чтение данных из таблицы
    Для того чтобы прочитать данные из таблицы, выполните команду:
    aws dynamodb get-item --consistent-read \
      --table-name docapitest/series \
      --key '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}}' \
      --endpoint $ENDPOINT 
    В качестве вывода вы увидите:
    {
        "Item": {
            "release_date": {
                "S": "2006-02-03"
            },
            "series_id": {
                "N": "1"
            },
            "series_info": {
                "S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."
            },
            "title": {
                "S": "IT Crowd"
            }
        }
    } 
    Для того, чтобы выбрать данные из таблицы series по ключу series_id, выполните следующую команду:
    aws dynamodb query \
      --table-name docapitest/series \
      --key-condition-expression "series_id = :name" \
      --expression-attribute-values '{":name":{"N":"2"}}' \
      --endpoint $ENDPOINT 
    В качестве результата вы увидите:
    {
        "Items": [
            {
                "release_date": {
                    "S": "2014-04-06"
                },
                "series_id": {
                    "N": "2"
                },
                "series_info": {
                    "S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."
                },
                "title": {
                    "S": "Silicon Valley"
                }
            }
        ],
        "Count": 1,
        "ScannedCount": 1,
        "ConsumedCapacity": null
    } 
    Удаление таблицы
    aws dynamodb delete-table \
      --table-name docapitest/series \
      --endpoint $ENDPOINT 
    Task:
    Практическая работа. Запуск тестового приложения
    Decision:
    В предыдущем уроке вы прошли подготовительные этапы: создали и настроили сервисный аккаунт, выпустили статический ключ, а также научились работать с таблицами и данными с помощью низкоуровневого API и CLI.
    В этом уроке вы продолжите работу с инструментами AWS и с помощью AWS SDK для языка Python научитесь выполнять такие базовые операции, как создание таблиц БД, запись и чтение данных.
    Для выполнения работы вам понадобится Python версии 3.6 и выше и библиотека boto3.
    Установить эту библиотеку можно с помощью команды:
    pip install boto3 
    Создание таблицы
    Создайте файл с именем SeriesCreateTable.py и скопируйте в него исходный код программы:
    import boto3
    def create_series_table():
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.create_table(
            TableName = 'docapitest/series', # Series — имя таблицы 
            KeySchema = [
                {
                    'AttributeName': 'series_id',
                    'KeyType': 'HASH'  # Ключ партицирования
                },
                {
                    'AttributeName': 'title',
                    'KeyType': 'RANGE'  # Ключ сортировки
                }
            ],
            AttributeDefinitions = [
                {
                    'AttributeName': 'series_id',
                    'AttributeType': 'N'  # Целое число
                },
                {
                    'AttributeName': 'title',
                    'AttributeType': 'S'  # Строка
                },
            ]
        )
        return table
    if __name__ == '__main__':
        series_table = create_series_table()
        print("Table status:", series_table.table_status) 
    Отредактируйте исходный код файла и укажите значение endpoint_url вашей базы. Затем запустите написанный код:
    python SeriesCreateTable.py 
    С помощью консоли управления убедитесь, что  в директории docapitest появилась таблица series.
    Первоначальная загрузка данных
    Для того чтобы вставить данные в созданную таблицу series, создайте файл с именем SeriesLoadData.py и скопируйте в него следующий исходный код программы:
    from decimal import Decimal
    import json
    import boto3
    def load_series(series):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document API эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        for serie in series:
            series_id = int(serie['series_id'])
            title = serie['title']
            print("Series added:", series_id, title)
            table.put_item(Item = serie)
    if __name__ == '__main__':
        with open("seriesdata.json") as json_file:
            serie_list = json.load(json_file, parse_float = Decimal)
        load_series(serie_list) 
    Отредактируйте файл SeriesLoadData.py и укажите значение endpoint_url вашей базы.
    Для загрузки данных приложение будет использовать данные, которые записаны в файл seriesdata.json. Создайте этот файл и скопируйте в него описание сериалов:
    [{
        "series_id": 1,
        "title": "IT Crowd",
        "info": {
          "release_date": "2006-02-03T00:00:00Z",
          "series_info": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry"
        }
      },
      {
        "series_id": 2,
        "title": "Silicon Valley",
        "info": {
          "release_date": "2014-04-06T00:00:00Z",
          "series_info": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley"
        }
      },
      {
        "series_id": 3,
        "title": "House of Cards",
        "info": {
          "release_date": "2013-02-01T00:00:00Z",
          "series_info": "House of Cards is an American political thriller streaming television series created by Beau Willimon. It is an adaptation of the 1990 BBC miniseries of the same name and based on the 1989 novel of the same name by Michael Dobbs"
        }
      },
      {
        "series_id": 3,
        "title": "The Office",
        "info": {
          "release_date": "2005-03-24T00:00:00Z",
          "series_info": "The Office is an American mockumentary sitcom television series that depicts the everyday work lives of office employees in the Scranton, Pennsylvania, branch of the fictional Dunder Mifflin Paper Company"
        }
      },
      {
        "series_id": 3,
        "title": "True Detective",
        "info": {
          "release_date": "2014-01-12T00:00:00Z",
          "series_info": "True Detective is an American anthology crime drama television series created and written by Nic Pizzolatto. The series, broadcast by the premium cable network HBO in the United States, premiered on January 12, 2014"
        }
      },
      {
        "series_id": 4,
        "title": "The Big Bang Theory",
        "info": {
          "release_date": "2007-09-24T00:00:00Z",
          "series_info": "The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom served as executive producers on the series, along with Steven Molaro"
        }
      },
      {
        "series_id": 5,
        "title": "Twin Peaks",
        "info": {
          "release_date": "1990-04-08T00:00:00Z",
          "series_info": "Twin Peaks is an American mystery horror drama television series created by Mark Frost and David Lynch that premiered on April 8, 1990, on ABC until its cancellation after its second season in 1991 before returning as a limited series in 2017 on Showtime"
        }
      }
    ] 
    Запустите программу (для её успешного выполнения может понадобиться указать в скрипте SeriesLoadData.py полный путь к файлу seriesdata.json):
    python SeriesLoadData.py 
    В результате выполнения вы увидите вывод программы:
    Series added: 1 IT Crowd
    Series added: 2 Silicon Valley
    Series added: 3 House of Cards
    Series added: 3 The Office
    Series added: 3 True Detective
    Series added: 4 The Big Bang Theory
    Series added: 5 Twin Peaks 
    Работа с записями
    Создание записи
    Теперь создайте файл SeriesItemPut.py и скопируйте в него следующий код:
    from pprint import pprint
    import boto3
    def put_serie(series_id, title, release_date, series_info):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        response = table.put_item(
          Item = {
                'series_id': series_id,
                'title': title,
                'info': {
                    'release_date': release_date,
                    'series_info': series_info
                }
            }
        )
        return response
    if __name__ == '__main__':
        serie_resp = put_serie(3, "Supernatural", "2015-09-13",
                              "Supernatural is an American television series created by Eric Kripke")
        print("Series added successfully:")
        pprint(serie_resp, sort_dicts = False) 
    В результате выполнения этого кода в таблице добавится запись о сериале Supernatural.
    Чтение записи
    Создайте файл SeriesItemGet.py и скопируйте в него следующий код:
    from pprint import pprint
    import boto3
    from botocore.exceptions import ClientError
    def get_serie(title, series_id):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        try:
            response = table.get_item(Key = {'series_id': series_id, 'title': title})
        except ClientError as e:
            print(e.response['Error']['Message'])
        else:
            return response['Item']
    if __name__ == '__main__':
        serie = get_serie("Supernatural", 3,)
        if serie:
            print("Record read:")
            pprint(serie, sort_dicts = False) 
    Результатом будет сообщение в формате JSON с данными о сериале.
    Обновление записи
    В файле SeriesItemUpdate.py разместите код обновления записи:
    from decimal import Decimal
    from pprint import pprint
    import boto3
    def update_serie(title, series_id, release_date,  rating):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        response = table.update_item(
            Key = {
                'series_id': series_id,
                'title': title
            },
            UpdateExpression = "set info.release_date = :d, info.rating = :r ",
            ExpressionAttributeValues = {
                ':d': release_date,
                ':r': Decimal(rating)
            },
            ReturnValues = "UPDATED_NEW"
        )
        return response
    if __name__ == '__main__':
        update_response = update_serie(
            "Supernatural", 3, "2005-09-13", 8)
        print("Series updated:")
        pprint(update_response, sort_dicts = False) 
    Результатом будет сообщение в формате JSON с измененными данными.
    Удаление записи
    Создайте файл SeriesItemDelete.py и скопируйте в него следующий код:
    from decimal import Decimal
    from pprint import pprint
    import boto3
    from botocore.exceptions import ClientError
    def delete_underrated_serie(title, series_id, rating):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        try:
            response = table.delete_item(
                Key = {
                    'series_id': series_id,
                    'title': title
                },
                ConditionExpression = "info.rating <= :val",
                ExpressionAttributeValues = {
                    ":val": Decimal(rating)
                }
            )
        except ClientError as e:
            if e.response['Error']['Code'] == "ConditionalCheckFailedException":
                print(e.response['Error']['Message'])
            else:
                raise
        else:
            return response
    if __name__ == '__main__':
        print("Deleting...")
        delete_response = delete_underrated_serie("Supernatural", 3, 8)
        if delete_response:
            print("Series data deleted:")
            pprint(delete_response, sort_dicts = False) 
    Убедитесь, что данные о сериале Supernatural удалены из таблицы.
    Поиск по ключам партицирования и сортировки
    Код поиска разместите в новом файле SeriesQuery.py:
    from pprint import pprint
    import boto3
    from boto3.dynamodb.conditions import Key
    def query_and_project_series(series_id, title_range):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        response = table.query(
            ProjectionExpression = "series_id, title, info.release_date",
            KeyConditionExpression = Key('series_id').eq(series_id) & Key('title').begins_with(title_range)
        )
        return response['Items']
    if __name__ == '__main__':
        query_id = 3
        query_range = 'T'
        print(f"Series with ID = {query_id} and names beginning with "
              f"{query_range}")
        series = query_and_project_series(query_id, query_range)
        for serie in series:
            print(f"\n{serie['series_id']} : {serie['title']}")
            pprint(serie['info']) 
    Результатом будет сообщение:
    Series with ID = 3 and names beginning with T
    3 : The Office
    {'release_date': '2005-03-24T00:00:00Z'}
    3 : True Detective
    {'release_date': '2014-01-12T00:00:00Z'} 
    Запуск операции Scan
    Создайте файл SeriesTableScan.py и скопируйте в него следующий код:
    from pprint import pprint
    import boto3
    from boto3.dynamodb.conditions import Key
    def scan_series(id_range, display_series):
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
        table = ydb_docapi_client.Table('docapitest/series')
        scan_kwargs = {
            'FilterExpression': Key('series_id').between(*id_range),
            'ProjectionExpression': "series_id, title, info.release_date"
        }
        done = False
        start_key = None
        while not done:
            if start_key:
                scan_kwargs['ExclusiveStartKey'] = start_key
            response = table.scan(**scan_kwargs)
            display_series(response.get('Items', []))
            start_key = response.get('LastEvaluatedKey', None)
            done = start_key is None
    if __name__ == '__main__':
        def print_series(series):
            for serie in series:
                print(f"\n{serie['series_id']} : {serie['title']}")
                pprint(serie['info'])
        query_range = (1, 3)
        print(f"Series with IDs from {query_range[0]} to {query_range[1]}...")
        scan_series(query_range, print_series) 
    Результатом будет сообщение:
    Series with IDs from 1 to 3...
    3 : House of Cards
    {'release_date': '2013-02-01T00:00:00Z'}
    3 : The Office
    {'release_date': '2005-03-24T00:00:00Z'}
    3 : True Detective
    {'release_date': '2014-01-12T00:00:00Z'}
    1 : IT Crowd
    {'release_date': '2006-02-03T00:00:00Z'}
    2 : Silicon Valley
    {'release_date': '2014-04-06T00:00:00Z'} 
    Удаление таблицы
    Создайте файл SeriesTableDelete.py и скопируйте в него следующий код:
    import boto3
    def delete_serie_table():
        ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")

        table = ydb_docapi_client.Table('docapitest/series')
        table.delete()
    if __name__ == '__main__':
        delete_serie_table()
        print("Table Series deleted") 
    Убедитесь, что таблица удалена из базы данных.
	Task:
    Практическая работа. Проверка доступности веб-ресурсов
    Decision:
    В этом уроке вы доработаете систему проверки доступности веб-ресурсов, которую создали на предыдущих практических занятиях. В текущем варианте она проверяет только доступность сайта yandex.ru. Теперь давайте добавим в неё возможность ставить задачи по проверке доступности других веб-ресурсов.
    Общая архитектура системы
    У системы есть два метода:
        CheckUrl — ставит задачу на проверку указанного URL.
        GetResult — считывает результаты проверки.
    Метод CheckUrl обрабатывается функцией, которая будет складывать все запросы в очередь. Функция-обработчик будет вызываться раз в секунду, считывать URL из очереди,  проверять его доступность и записывать результат в базу данных. Оттуда этот результат можно будет получить с помощью метода GetResult.
    Мы не будем менять уже созданные функции и таблицу в PostgreSQL, сделаем новые.
    Работать с YMQ из функций мы будем с помощью библиотеки boto3. Чтобы её использовать, нужно создать сервисный аккаунт с секретным ключом доступа, а затем настроить зависимости функции. Сделаем это после того, как создадим очередь.
    Шаг 1. Проверить наличие сервисного аккаунта
    Если вы ранее создавали сервисный аккаунт с именем service-account-for-cf, добавляли вновь созданному сервисному аккаунту роли editor и другие, то вам остаётся только создать ключ доступа:
    yc iam access-key create --service-account-name service-account-for-cf 
    В результате вы получите примерно следующее:
        access_key:
            id: ajefraollq5puj2tir1o
            service_account_id: ajetdv28pl0a1a8r41f0
            created_at: "2021-08-23T21:13:05.677319393Z"
            key_id: BTPNvWthv0ZX2xVmlPIU
        secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
    Здесь key_id — это идентификатор ключа доступа ACCESS_KEY. А secret — это секретный ключ SECRET_KEY. Переменные ACCESS_KEY и SECRET_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
    Шаг 2. Создание очереди Yandex Message Queue
    Вы можете создать очередь одним из трёх способов:
        через консоль управления;
        с помощью консольной утилиты aws;
        с помощью Terraform.
    В этом уроке мы будем использовать консоль управления. Откройте раздел Message Queue и нажмите кнопку Создать очередь.
    В настройках создаваемой очереди задайте имя очереди my-first-queue, затем выберите тип очереди Стандартная и нажмите кнопку Создать.
    Очередь создана.
    Теперь зайдите в настройки очереди, чтобы посмотреть параметры подключения к ней. Нам потребуется значение URL.
    Шаг 3. Создание функции
    Для создания функции зададим ряд переменных:
        VERBOSE_LOG — определяет, пишет ли функция подробности своего выполнения в журнал.
        AWS_ACCESS_KEY_ID — значение «Идентификатор ключа» из сервисного аккаунта, который мы сделали ранее.
        AWS_SECRET_ACCESS_KEY — значение «Секретный ключ» из того же сервисного аккаунта.
        QUEUE_URL — URL на очередь, его можно получить на обзорной странице созданной ранее очереди.
    Чтобы задать переменные, в консоли выполните следующие команды:
    echo "export VERBOSE_LOG=True" >> ~/.bashrc && . ~/.bashrc
    echo "export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>" >> ~/.bashrc && . ~/.bashrc
    echo "export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
    echo "export QUEUE_URL=<QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc 
    Воспользуйтесь командой pipreqs $PWD --force для формирования файла requirements.txt. Затем создайте функцию my-url-receiver-function.py:
    import logging
    import os
    import boto3
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
    queue_url = os.environ['QUEUE_URL']
    def log(logString):
        if verboseLogging:
            logger.info(logString)
    def handler(event, context):
        # Get url
        try:
            url = event['queryStringParameters']['url']
        except Exception as error:
            logger.error(error)
            statusCode = 400
            return {
                'statusCode': statusCode
            }
        # Create client
        client = boto3.client(
            service_name='sqs',
            endpoint_url='https://message-queue.api.cloud.yandex.net',
            region_name='ru-central1'
        )
        # Send message to queue
        client.send_message(
            QueueUrl=queue_url,
            MessageBody=url
        )
        log('Successfully sent test message to queue')
        statusCode = 200
        return {
            'statusCode': statusCode
        } 
    Перейдите в директорию с исходными файлами и упакуйте файлы с функцией и требованиями в ZIP-архив. При этом сразу задайте все необходимые переменные и сервисный аккаунт:
    zip my-url-receiver-function my-url-receiver-function.py requirements.txt
    yc serverless function create \
      --name  my-url-receiver-function \
      --description "function for url"
    yc serverless function version create \
      --function-name=my-url-receiver-function \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=my-url-receiver-function.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --environment VERBOSE_LOG=$VERBOSE_LOG \
      --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
      --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
      --environment QUEUE_URL=$QUEUE_URL \
      --source-path my-url-receiver-function.zip 
    Тестирование функции
    Находясь в вашем рабочем каталоге, перейдите в раздел Cloud Functions консоли управления и выберите ранее созданную функцию my-url-receiver-function. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон HTTPS-вызов и замените раздел queryStringParameters:
        "queryStringParameters": {
            "a": "2",
            "b": "1",
        },     
    на аналогичный, но с параметром url с любым сайтом. Важно указывать ссылку целиком.
        "queryStringParameters": {
            "url": "https://ya.ru/"
        },     
    Нажмите кнопку Запустить тест.
    Если вы всё сделали правильно, то увидите код статуса 200. При этом в очереди увеличится количество сообщений.
    Шаг 4. Обновление спецификации API Gateway
    Функция готова, но по умолчанию она не является публичной. Предоставим доступ к ней с помощью API-шлюза. Для этого необходимо обновить ранее созданную спецификацию hello-world.yaml. Если у вас нет её под рукой, выгрузите её из облака:
    yc serverless api-gateway get-spec \
      --name hello-world >> hello-world-new.yaml 
    Внесите изменения, добавив секцию о ранее созданной функции:
        /check:
            get:
                x-yc-apigateway-integration:
                    type: cloud-functions
                    function_id: <идентификатор функции>
                    service_account_id: <идентификатор сервисного аккаунта>
                operationId: add-url 
    Обновите конфигурацию:
    yc serverless api-gateway update \
      --name hello-world \
      --spec=hello-world-new.yaml 
    Для тестирования выполните вызов функции в браузере:
    https://<идентификатор API Gateway>.apigw.yandexcloud.net/check?url=https://ya.ru/ 
    После каждого запроса количество сообщений в очереди будет увеличиваться на одно.
    Шаг 5. Создание функции для чтения из очереди
    В предыдущих работах мы создавали функцию, использующую подключение к БД. Здесь мы повторим этот опыт.
    Проверим, что нам доступны переменные для инициации подключения: CONNECTION_ID, DB_USER, DB_HOST. Мы создавали их в предыдущей работе с помощью следующих команд:
    echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
    echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
    Также для работы с очередью нам потребуются переменные VERBOSE_LOG, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY и QUEUE_URL, заданные на предыдущих шагах.
    Создадим функцию function-for-url-from-mq.py и воспользуемся командой pipreqs $PWD --force, чтобы сформировать для нее файл requirements.txt.
    import logging
    import os
    import boto3
    import datetime
    import requests
    #Эти библиотеки нужны для работы с PostgreSQL
    import psycopg2
    import psycopg2.errors
    import psycopg2.extras
    CONNECTION_ID = os.getenv("CONNECTION_ID")
    DB_USER = os.getenv("DB_USER")
    DB_HOST = os.getenv("DB_HOST")
    QUEUE_URL = os.environ['QUEUE_URL']
    # Настраиваем функцию для записи информации в журнал функции
    # Получаем стандартный логер языка Python
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    # Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
    verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
    #Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
    def log(logString):
        if verboseLogging:
            logger.info(logString)
    #Получаем подключение
    def getConnString(context):
        """
        Extract env variables to connect to DB and return a db string
        Raise an error if the env variables are not set
        :return: string
        """
        connection = psycopg2.connect(
            database=CONNECTION_ID, # Идентификатор подключения
            user=DB_USER, # Пользователь БД
            password=context.token["access_token"],
            host=DB_HOST, # Точка входа
            port=6432,
            sslmode="require")
        return connection
    """
        Create SQL query with table creation
    """
    def makeCreateDataTableQuery(table_name):
        query = f"""CREATE TABLE public.{table_name} (
        url text,
        result integer,
        time float
        )"""
        return query
    def makeInsertDataQuery(table_name, url, result, time):
        query = f"""INSERT INTO {table_name} 
        (url, result,time)
        VALUES('{url}', {result}, {time})
        """
        return query
    def handler(event, context):
        # Create client
        client = boto3.client(
            service_name='sqs',
            endpoint_url='https://message-queue.api.cloud.yandex.net',
            region_name='ru-central1'
        )
        # Receive sent message
        messages = client.receive_message(
            QueueUrl=QUEUE_URL,
            MaxNumberOfMessages=1,
            VisibilityTimeout=60,
            WaitTimeSeconds=1
        ).get('Messages')
        if messages is None:
            return {
                'statusCode': 200
            }
        for msg in messages:
            log('Received message: "{}"'.format(msg.get('Body')))
        # Get url from message
        url = msg.get('Body');
        # Check url
        try:
            now = datetime.datetime.now()
            response = requests.get(url, timeout=(1.0000, 3.0000))
            timediff = datetime.datetime.now() - now
            result = response.status_code
        except requests.exceptions.ReadTimeout:
            result = 601
        except requests.exceptions.ConnectTimeout:
            result = 602
        except requests.exceptions.Timeout:
            result = 603
        log(f'Result: {result} Time: {timediff.total_seconds()}')
        connection = getConnString(context)
        log(f'Connecting: {connection}')    
        cursor = connection.cursor()
        table_name = 'custom_request_result'
        sql = makeInsertDataQuery(table_name, url, result, timediff.total_seconds())
        log(f'Exec: {sql}')
        try:
            cursor.execute(sql)
        except psycopg2.errors.UndefinedTable as error:
            log(f'Table not exist - create and repeate insert')
            connection.rollback()
            logger.error(error)
            createTable = makeCreateDataTableQuery(table_name)
            log(f'Exec: {createTable}')
            cursor.execute(createTable)
            connection.commit()
            log(f'Exec: {sql}')
            cursor.execute(sql)
        except Exception as error:
            logger.error( error)
        connection.commit()
        cursor.close()
        connection.close()
        # Delete processed messages
        for msg in messages:
            client.delete_message(
                QueueUrl=QUEUE_URL,
                ReceiptHandle=msg.get('ReceiptHandle')
            )
            print('Successfully deleted message by receipt handle "{}"'.format(msg.get('ReceiptHandle')))
        statusCode = 200
        return {
            'statusCode': statusCode
        } 
    При создании сразу задайте все необходимые переменные и сервисный аккаунт:
    zip function-for-url-from-mq function-for-url-from-mq.py requirements.txt
    yc serverless function create \
      --name function-for-url-from-mq \
      --description "function for url from mq"
    yc serverless function version create \
      --function-name=function-for-url-from-mq \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=function-for-url-from-mq.handler \
      --service-account-id $SERVICE_ACCOUNT_ID \
      --environment VERBOSE_LOG=True \
      --environment CONNECTION_ID=$CONNECTION_ID \
      --environment DB_USER=$DB_USER \
      --environment DB_HOST=$DB_HOST \
      --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
      --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
      --environment QUEUE_URL=$QUEUE_URL \
      --source-path function-for-url-from-mq.zip 
    Протестируйте функцию.
    После её выполнения количество сообщений в очереди уменьшится, а в базе данных появится новая таблица с результатами тестирования доступности функции.
    Шаг 6. Создание триггера
    Создадим триггер, который будет вызывать функцию обработки сообщений из очереди один раз в минуту. Он будет использовать cron-выражение:
    yc serverless trigger create timer \
      --name trigger-for-mq \
      --invoke-function-name function-for-url-from-mq \
      --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
      --cron-expression '* * * * ? *' 
    Cron-выражение * * * * ? * означает вызов функции function-for-url-from-mq один раз в минуту. Подробнее про cron-выражения можно прочитать в документации.
    Теперь у нас есть функция, которая раз в минуту будет пробовать взять из очереди URL и проверить его. Также есть метод REST API, который позволяет записывать URL в очередь независимо от работы обработчика. Мы можем вызывать созданный метод как угодно часто. Очередь будет просто накапливаться, а затем обработчик будет постепенно её разбирать.
    В итоге вы получили асинхронную систему проверки доступности URL с доступом по REST API. Вы не создали ни одной виртуальной машины, но решили вопросы масштабирования и отказоустойчивости системы.
    Удаление триггера-таймера
    По завершении практической работы не забудьте удалить созданный вами триггер trigger-for-mq, иначе он будет работать, пока не исчерпает деньги на аккаунте:
    yc serverless trigger delete trigger-for-mq 
    Не забудьте удалить или остановить все созданные вами ресурсы: триггеры, очереди YMQ и кластер базы данных.
    Следующий практический урок завершает тему. Вы попробуете создать онлайн-сервис, конвертирующий произвольные видеофайлы в GIF-анимацию. Для этого вы объедините в одно решение сервисы Yandex Cloud Functions, Yandex Message Queue, Yandex Database и Yandex Object Storage. А заодно закрепите использование консольных инструментов yc и aws.
    Task:
    Практическая работа. Однократная отправка сообщений
    Decision:
    В этой практической работе мы реализуем проект, который позволит пользователям конвертировать видеофайлы в GIF. Такая задача хорошо подходит для Cloud Functions, потому что конвертирование отнимает немало ресурсов процессора, и чем качественнее видео, тем больше ресурсов требуется на его обработку.
    Почему для решения этой задачи нужны очереди?
    Представим, что мы попытались решить эту задачу «в лоб». Пользователь заходит на страницу и вводит ссылку на видеофайл. Сервис скачивает его, конвертирует и отдает ссылку на GIF. Возникают две серьёзные проблемы:
        Синхронное соединение не всегда стабильно. Чем дольше вы его держите, тем выше вероятность, что оно разорвётся. В этом случае всё придётся сделать заново. А если соединение нестабильно, то пользователь может и не дождаться результата.
        Задача ресурсоёмкая: если сервисом одновременно воспользуются много пользователей с большими видеороликами, мощностей может не хватить.
    Чтобы избежать этих проблем, в архитектуру сервиса необходимо встроить очередь.
    Шаг 1. Сервисный аккаунт и Lockbox
    Создание сервисного аккаунта
    Создайте сервисный аккаунт с именем ffmpeg-account-for-cf:
    export SERVICE_ACCOUNT=$(yc iam service-account create --name ffmpeg-account-for-cf \
      --description "service account for serverless" \
      --format json | jq -r .) 
    Проверьте текущий список сервисных аккаунтов:
    yc iam service-account list 
    После проверки запишите ID созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
    echo "export SERVICE_ACCOUNT_FFMPEG_ID=<ID>" >> ~/.bashrc && . ~/.bashrc
    echo $SERVICE_ACCOUNT_FFMPEG_ID 
    Назначение роли сервисному аккаунту
    Добавим вновь созданному сервисному аккаунту роли storage.viewer, storage.uploader, ymq.reader, ymq.writer, ydb.admin, serverless.functions.invoker, и lockbox.payloadViewer:
    echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
    echo $FOLDER_ID
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role storage.viewer
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role storage.uploader
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role ymq.reader
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role ymq.writer
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role ydb.admin
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role serverless.functions.invoker
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role lockbox.payloadViewer
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --role editor 
    Вы можете назначить несколько ролей и с помощью команды set-access-binding. Но эта команда полностью перезаписывает права доступа к ресурсу и все текущие роли на него будут удалены! Поэтому сначала убедитесь, что ресурсу не назначены роли, которые вы не хотите потерять:
    yc resource-manager folder list-access-bindings $FOLDER_ID
    yc resource-manager folder set-access-bindings $FOLDER_ID \
      --access-binding role=storage.viewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=storage.uploader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=ymq.reader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=ymq.writer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=ydb.admin,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=serverless.functions.invoker,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=lockbox.payloadViewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
      --access-binding role=editor,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID 
    Создание ключа доступа для сервисного аккаунта
    Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, работы с Yandex Message Queue и т. д. Для создания ключа доступа необходимо вызвать следующую команду:
    yc iam access-key create --service-account-name ffmpeg-account-for-cf 
    В результате вы получите примерно следующее:
        access_key:
            id: ajefraollq5puj2tir1o
            service_account_id: ajetdv28pl0a1a8r41f0
            created_at: "2021-08-23T21:13:05.677319393Z"
            key_id: BTPNvWthv0ZX2xVmlPIU
        secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
    Здесь key_id — это идентификатор ключа доступа ACCESS_KEY_ID. А secret — это секретный ключ SECRET_ACCESS_KEY. Переменные ACCESS_KEY_ID и SECRET_ACCESS_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
    Создание элемента в сервисе Lockbox
    В сервисе Lockbox (находится на стадии Preview) создайте ваш первый секрет, состоящий из набора версий, в которых хранятся ваши данные. Версия содержит наборы ключей и значений:
        Ключ — несекретное название для значения, по которому вы будете его идентифицировать.
        Значение — это секретные данные.
    Версия не изменяется. Для любого изменения количества пар ключей-значений или их содержимого необходимо создать новую версию. Создадим секрет с именем ffmpeg-sa-key и парой ключей ACCESS_KEY_ID и SECRET_ACCESS_KEY:
    yc lockbox secret create --name ffmpeg-sa-key \
      --folder-id $FOLDER_ID \
      --description "keys for serverless" \
      --payload '[{"key": "ACCESS_KEY_ID", "text_value": <ACCESS_KEY_ID>}, {"key": "SECRET_ACCESS_KEY", "text_value": "<SECRET_ACCESS_KEY>"}]' 
    Получим и запишем значение SECRET_ID, оно нам потребуется при создании функции:
    yc lockbox secret list
    yc lockbox secret get --name ffmpeg-sa-key
    echo "export SECRET_ID=<SECRET_ID>" >> ~/.bashrc && . ~/.bashrc
    echo $SECRET_ID 
    Шаг 2. Создание очереди Yandex Message Queue
    Для создания очереди Yandex Message Queue вы можете использовать три разных способа:
        консоль управления;
        консольная утилита aws;
        Terraform.
    Создание очереди с помощью утилиты aws
    Воспользуемся AWS CLI. Для начала задайте конфигурацию с помощью команды aws configure. При этом от вас потребуется ввести:
        AWS Access Key ID — идентификатор ключа доступа key_id сервисного аккаунта, полученный на предыдущем шаге.
        AWS Secret Access Key — секретный ключ secret сервисного аккаунта, полученный на предыдущем шаге.
        Default region name — используйте значение ru-central1.
    По завершению конфигурации вы сможете создать очередь:
    aws configure
    aws sqs create-queue --queue-name ffmpeg --endpoint https://message-queue.api.cloud.yandex.net/ 
    В результате успешного выполнения предыдущей команды в ответ вы получите URL:
        {
            "QueueUrl": "https://message-queue.api.cloud.yandex.net/b1ga4gj7agij03ln6aov/dj6000000003kv2t02b3/ffmpeg"
        } 
    Запишем значения URL в переменную YMQ_QUEUE_URL. Она потребуется нам при создании функции:
    echo "export YMQ_QUEUE_URL=<YMQ_QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc
    echo $YMQ_QUEUE_URL 
    Ещё вам потребует значение атрибута QueueArn, получим его:
    aws sqs get-queue-attributes \
      --endpoint https://message-queue.api.cloud.yandex.net \
      --queue-url $YMQ_QUEUE_URL \
      --attribute-names QueueArn 
    В результате вы получите ответ вида:
        {
            "Attributes": {
                "QueueArn": "yrn:yc:ymq:ru-central1:b1gl21bkgss4msekt08i:ffmpeg"
            }
        } 
    Сохраним значение QueueArn в переменную YMQ_QUEUE_ARN:
    echo "export YMQ_QUEUE_ARN=<YMQ_QUEUE_ARN>" >> ~/.bashrc && . ~/.bashrc
    echo $YMQ_QUEUE_ARN 
    Шаг 3. Создание базы данных в сервисе Yandex Database
    Создадим базу данных YDB с именем ffmpeg и типом serverless, используя для этого флаг --serverless:
    yc ydb database create ffmpeg \
      --serverless \
      --folder-id $FOLDER_ID
    yc ydb database list 
    Сразу получим и сохраним document_api_endpoint в значение переменной DOCAPI_ENDPOINT:
    yc ydb database get --name ffmpeg
    echo "export DOCAPI_ENDPOINT=<DOCAPI_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
    echo $DOCAPI_ENDPOINT 
    Как только база данных создана, воспользуемся ранее использованной утилитой AWS CLI для создания документной таблицы в этой базе данных. Всю конфигурацию возьмем из файла tasks.json:
    {
      "AttributeDefinitions": [
        {
          "AttributeName": "task_id",
          "AttributeType": "S"
        }
      ],
      "KeySchema": [
        {
          "AttributeName": "task_id",
          "KeyType": "HASH"
        }
      ],
      "TableName": "tasks"
    } 
    Находясь в одном каталоге с файлом tasks.json, вызовите следующую команду для создания таблицы:
    aws dynamodb create-table \
      --cli-input-json file://tasks.json \
      --endpoint-url $DOCAPI_ENDPOINT \
      --region ru-central1 
    В консоли управления убедитесь, что БД ffmpeg создана, и в ней есть пустая таблица tasks.
    Шаг 4. Создание бакета в сервисе Object Storage
    Самый простой способ создания бакета в Object Storage — это использование консоли управления.
    В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет. На странице создания бакета введите имя, в нашем примере это будет storage-for-ffmpeg, остальные параметры не меняйте.
    Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
    Сохраним название бакета для дальнейшего использования:
    echo "export S3_BUCKET=<имя бакета>" >> ~/.bashrc && . ~/.bashrc
    echo $S3_BUCKET 
    Шаг 5. Создание функций
    При создании функций нам потребуется ряд переменных:
        SECRET_ID — идентификатор секрета (можно получить из таблицы со списком секретов);
        YMQ_QUEUE_URL — URL очереди (можно получить на странице обзора);
        DOCAPI_ENDPOINT — его можно получить на странице обзора БД, нужен именно Document API;
        S3_BUCKET — имя бакета, в нашем случае это storage-for-ffmpeg.
    Проверим заданные ранее переменные:
    echo $SERVICE_ACCOUNT_FFMPEG_ID
    echo $SECRET_ID
    echo $YMQ_QUEUE_URL
    echo $DOCAPI_ENDPOINT
    echo $S3_BUCKET 
    Для обработки видео нам потребуется утилита FFmpeg. Скачайте статический релизный бинарный файл для Linux amd64 на сайте ffmpeg.org. Обычно он находится в разделе FFmpeg Static Builds и называется примерно так: ffmpeg-release-amd64-static.tar.xz. Распакуйте архив. Из него вам понадобится только файл ffmpeg. Поскольку есть ограничение на размер  файла, который можно приложить через консоль, загрузим код функций и ffmpeg в Object Storage.
    Исходный код в файле index.py содержит обе необходимые нам функции:
    import json
    import os
    import subprocess
    import uuid
    from urllib.parse import urlencode
    import boto3
    import requests
    import yandexcloud
    from yandex.cloud.lockbox.v1.payload_service_pb2 import GetPayloadRequest
    from yandex.cloud.lockbox.v1.payload_service_pb2_grpc import PayloadServiceStub
    boto_session = None
    storage_client = None
    docapi_table = None
    ymq_queue = None
    def get_boto_session():
        global boto_session
        if boto_session is not None:
            return boto_session
        # initialize lockbox and read secret value
        yc_sdk = yandexcloud.SDK()
        channel = yc_sdk._channels.channel("lockbox-payload")
        lockbox = PayloadServiceStub(channel)
        response = lockbox.Get(GetPayloadRequest(secret_id=os.environ['SECRET_ID']))
        # extract values from secret
        access_key = None
        secret_key = None
        for entry in response.entries:
            if entry.key == 'ACCESS_KEY_ID':
                access_key = entry.text_value
            elif entry.key == 'SECRET_ACCESS_KEY':
                secret_key = entry.text_value
        if access_key is None or secret_key is None:
            raise Exception("secrets required")
        print("Key id: " + access_key)
        # initialize boto session
        boto_session = boto3.session.Session(
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key
        )
        return boto_session
    def get_ymq_queue():
        global ymq_queue
        if ymq_queue is not None:
            return ymq_queue
        ymq_queue = get_boto_session().resource(
            service_name='sqs',
            endpoint_url='https://message-queue.api.cloud.yandex.net',
            region_name='ru-central1'
        ).Queue(os.environ['YMQ_QUEUE_URL'])
        return ymq_queue
    def get_docapi_table():
        global docapi_table
        if docapi_table is not None:
            return docapi_table
        docapi_table = get_boto_session().resource(
            'dynamodb',
            endpoint_url=os.environ['DOCAPI_ENDPOINT'],
            region_name='ru-central1'
        ).Table('tasks')
        return docapi_table
    def get_storage_client():
        global storage_client
        if storage_client is not None:
            return storage_client
        storage_client = get_boto_session().client(
            service_name='s3',
            endpoint_url='https://storage.yandexcloud.net',
            region_name='ru-central1'
        )
        return storage_client
    # API handler
    def create_task(src_url):
        task_id = str(uuid.uuid4())
        get_docapi_table().put_item(Item={
            'task_id': task_id,
            'ready': False
        })
        get_ymq_queue().send_message(MessageBody=json.dumps({'task_id': task_id, "src": src_url}))
        return {
            'task_id': task_id
        }
    def get_task_status(task_id):
        task = get_docapi_table().get_item(Key={
            "task_id": task_id
        })
        if task['Item']['ready']:
            return {
                'ready': True,
                'gif_url': task['Item']['gif_url']
            }
        return {'ready': False}
    def handle_api(event, context):
        action = event['action']
        if action == 'convert':
            return create_task(event['src_url'])
        elif action == 'get_task_status':
            return get_task_status(event['task_id'])
        else:
            return {"error": "unknown action: " + action}
    # Converter handler
    def download_from_ya_disk(public_key, dst):
        api_call_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?' + \
                       urlencode(dict(public_key=public_key))
        response = requests.get(api_call_url)
        download_url = response.json()['href']
        download_response = requests.get(download_url)
        with open(dst, 'wb') as video_file:
            video_file.write(download_response.content)
    def upload_and_presign(file_path, object_name):
        client = get_storage_client()
        bucket = os.environ['S3_BUCKET']
        client.upload_file(file_path, bucket, object_name)
        return client.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': object_name}, ExpiresIn=3600)
    def handle_process_event(event, context):
        for message in event['messages']:
            task_json = json.loads(message['details']['message']['body'])
            task_id = task_json['task_id']
            # Download video
            download_from_ya_disk(task_json['src'], '/tmp/video.mp4')
            # Convert with ffmpeg
            subprocess.run(['ffmpeg', '-i', '/tmp/video.mp4', '-r', '10', '-s', '320x240', '/tmp/result.gif'])
            result_object = task_id + ".gif"
            # Upload to Object Storage and generate presigned url
            result_download_url = upload_and_presign('/tmp/result.gif', result_object)
            # Update task status in DocAPI
            get_docapi_table().update_item(
                Key={'task_id': task_id},
                AttributeUpdates={
                    'ready': {'Value': True, 'Action': 'PUT'},
                    'gif_url': {'Value': result_download_url, 'Action': 'PUT'},
                }
            )
        return "OK" 
    Сгенерируйте файл requirements.txt:
    pipreqs $PWD --force 
    Находясь в директории с исходными файлами, упакуем все нужные файлы в ZIP-архив.
    zip src.zip index.py requirements.txt ffmpeg 
    В Object Storage для простоты используем тот же бакет, куда далее будем складывать видео. На вкладке Объекты, вверху справа нажмите кнопку Загрузить и выберите созданный архив.
    Создадим функции ffmpeg-api и ffmpeg-converter, при этом сразу зададим все необходимые переменные и сервисный аккаунт:
    yc serverless function create \
      --name ffmpeg-api \
      --description "function for ffmpeg-api"
    yc serverless function create \
      --name ffmpeg-converter \
      --description "function for ffmpeg-converter"
    yc serverless function version create \
      --function-name ffmpeg-api \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=index.handle_api \
      --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
      --environment SECRET_ID=$SECRET_ID \
      --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
      --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
      --package-bucket-name $S3_BUCKET \
      --package-object-name src.zip
    yc serverless function version create \
      --function-name ffmpeg-converter \
      --memory=2048m \
      --execution-timeout=600s \
      --runtime=python37 \
      --entrypoint=index.handle_process_event \
      --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
      --environment SECRET_ID=$SECRET_ID \
      --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
      --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
      --environment S3_BUCKET=$S3_BUCKET \
      --package-bucket-name $S3_BUCKET \
      --package-object-name src.zip 
    Тестирование функции
    В консоли управления перейдите из рабочего каталога в раздел Cloud Functions и выберите ранее созданную функцию ffmpeg-api. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон данных Без шаблона и добавьте во вводные данные JSON:
    {"action":"convert", "src_url":"https://disk.yandex.ru/i/38RbVC0spb_jQQ"} 
    Нажмите кнопку Запустить тест. Этим самым мы загрузим файл в хранилище и создадим задачу в БД. Если всё сделано правильно, то вы увидите такой результат:
        {
            "task_id": "133e05c2-1b98-41cc-9aab-b816d71af343"
        } 
    Воспользуемся полученным идентификатором задачи task_id для получения статуса из базы данных. Для этого внесите в вводные данные JSON следующие изменения:
    {"action":"get_task_status", "task_id":"<идентификатор задачи>"} 
    Нажмите кнопку Запустить тест. Так как мы ещё не обрабатывали задачи в очереди, результат очевиден:
        {
            "ready": false
        } 
    Шаг 6. Создание триггера
    Теперь создайте триггер, который будет вызывать функцию обработки сообщений из очереди. После создания триггер начинает работать через пять минут. Он будет брать по одному сообщению и раз в 10 секунд отправлять в функцию:
    yc serverless trigger create message-queue \
      --name ffmpeg \
      --queue $YMQ_QUEUE_ARN \
      --queue-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
      --invoke-function-name ffmpeg-converter  \
      --invoke-function-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
      --batch-size 1 \
      --batch-cutoff 10s 
    С этого момента очередь начнёт обрабатываться. Можно проверить, готова ли задача, и, если это так, запросить по URL результат обработки из Object Storage.
    Теперь у нас есть функция, которая выполняет функцию API, через которую мы можем ставить задачи в очередь на исполнение. Триггер раз в 10 секунд берет по одному сообщению в очереди и передает функции обработчику. Функция-обработчик формирует результат и обновляет данные в базе данных. При этом мы получаем сконвертированные GIF-файлы из видео.
    Протестируйте систему, используя полученный ранее идентификатор задачи task_id для получения статуса из базы данных. Для этого внесите изменения в вводные данные JSON:
    {"action":"get_task_status", "task_id":"133e05c2-1b98-41cc-9aab-b816d71af343"} 
    Нажмите кнопку Запустить тест. Если задача уже успела обработаться, то вы получите URL.
    Удаление триггера
    По завершении работы не забудьте удалить созданный триггер ffmpeg, иначе он будет продолжать работать:
    yc serverless trigger delete ffmpeg 
    Не забудьте также удалить или остановить все созданные вами ресурсы.
    Task:
    Практическая работа. Сокращатель ссылок
    Decision:
    В рамках этого курса вы изучили несколько ключевых сервисов Yandex Cloud, относящихся к группе Serverless. Давайте объединим их для решения ещё одной практической задачи и создадим сервис, который конвертирует длинные ссылки в короткие.
    Шаг 1. Сервисный аккаунт
    Создание аккаунта
    Создайте сервисный аккаунт с именем serverless-shortener:
     export SERVICE_ACCOUNT_SHORTENER_ID=$(yc iam service-account create --name serverless-shortener \
      --description "service account for serverless" \
      --format json | jq -r .) 
    Проверьте текущий список сервисных аккаунтов:
    yc iam service-account list 
    После проверки запишите идентификатор созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_SHORTENER_ID:
    echo "export SERVICE_ACCOUNT_SHORTENER_ID=<идентификатор сервисного аккаунта>" >> ~/.bashrc && . ~/.bashrc
    echo $SERVICE_ACCOUNT_SHORTENER_ID 
    Назначение ролей
    Добавьте созданному сервисному аккаунту роли editor, storage.viewer и ydb.admin:
    echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
    echo $FOLDER_ID
    echo "export OAUTH_TOKEN=$(yc config get token)" >> ~/.bashrc && . ~/.bashrc
    echo $OAUTH_TOKEN
    echo "export CLOUD_ID=$(yc config get cloud-id)" >> ~/.bashrc && . ~/.bashrc
    echo $CLOUD_ID
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
      --role editor
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
      --role ydb.admin
    yc resource-manager folder add-access-binding $FOLDER_ID \
      --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
      --role storage.viewer 
    Шаг 2. Создание бакета в Object Storage
    Сделаем для нашего сервиса веб-интерфейс. Поскольку это будет статическая веб-страница, разместим её в объектном хранилище.
    В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
    На странице создания бакета:
        Введите имя бакета. В нашем примере это будет storage-for-serverless-shortener.
        Ограничьте максимальный размер бакета (например 1 ГБ).
        Выберите тип доступа Публичный во всех случаях.
        Выберите класс хранилища Стандартное.
    Нажмите кнопку Создать бакет для завершения операции.
    Создайте и загрузите файл index.html в созданный бакет — это будет стартовая страничка для нашего сокращателя:
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Сокращатель URL</title>
        <!-- предостережет от лишнего GET запроса на адрес /favicon.ico -->
        <link rel="icon" href="data:;base64,iVBORw0KGgo=">
    </head>
    <body>
    <h1>Добро пожаловать</h1>
    <form action="javascript:shorten()">
        <label for="url">Введите ссылку:</label><br>
        <input id="url" name="url" type="text"><br>
        <input type="submit" value="Сократить">
    </form>
    <p id="shortened"></p>
    </body>
    <script>
        function shorten() {
            const link = document.getElementById("url").value
            fetch("/shorten", {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: link
            })
                .then(response => response.json())
                .then(data => {
                    const url = data.url
                    document.getElementById("shortened").innerHTML = `<a href=${url}>${url}</a>`
                })
                .catch(error => {
                    document.getElementById("shortened").innerHTML = `<p>Произошла ошибка ${error}, попробуйте еще раз</p>`
                })
        }
    </script>
    </html> 
    Шаг 3. Создание базы данных
        Создадим бессерверную базу данных YDB с именем for-serverless-shortener. Чтобы не переключаться из терминала, снова воспользуемся CLI. Обязательно укажите флаг --serverless для выбора типа создаваемой базы данных:
    yc ydb database create for-serverless-shortener \
      --serverless \
      --folder-id $FOLDER_ID
    yc ydb database list 
        Выполните команду:
    yc ydb database get --name for-serverless-shortener 
    В выводе вы увидите значение endpoint. Оно состоит из двух частей: собственно эндпоинта (обычно это ydb.serverless.yandexcloud.net:2135) и пути базы данных (он указывается после ключевого слова database и начинается с символа /, например /ru-central1/...). Сохраним адрес эндпоинта в переменную YDB_ENDPOINT, а путь базы данных — в переменную YDB_DATABASE. Они пригодятся нам для подключения функции.
    yc ydb database get --name for-serverless-shortener
    echo "export YDB_ENDPOINT=<YDB_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
    echo $YDB_ENDPOINT
    echo "export YDB_DATABASE=<YDB_DATABASE>" >> ~/.bashrc && . ~/.bashrc
    echo $YDB_DATABASE 
        Для дальнейшей работы нам понадобится утилита ydb:
    curl https://storage.yandexcloud.net/yandexcloud-ydb/install.sh | bash 
        С помощью CLI создадим авторизованный ключ сервисного аккаунта serverless-shortener:
        yc iam key create \
        --service-account-name serverless-shortener \
        --output serverless-shortener.sa 
    Сохраним путь к файлу с ключом в переменную окружения:
    echo "export SA_KEY_FILE=$PWD/serverless-shortener.sa" >> ~/.bashrc && . ~/.bashrc
    echo $SA_KEY_FILE 
        Проверим работоспособность с помощью команды:
    ydb \
      --endpoint $YDB_ENDPOINT \
      --database $YDB_DATABASE \
      --sa-key-file $SA_KEY_FILE \
      discovery whoami \
      --groups 
        Сохраним в файл links.yql SQL-скрипт для создания таблицы:
    CREATE TABLE links
    (
        id Utf8,
        link Utf8,
        PRIMARY KEY (id)
    );
    COMMIT; 
        Запустите создание таблицы, а затем проверьте результат:
    ydb \
      --endpoint $YDB_ENDPOINT \
      --database $YDB_DATABASE \
      --sa-key-file $SA_KEY_FILE \
      scripting yql --file links.yql
    ydb \
      --endpoint $YDB_ENDPOINT \
      --database $YDB_DATABASE \
      --sa-key-file $SA_KEY_FILE \
      scheme describe links 
    Шаг 4. Создание функции
        В рабочем каталоге создайте файл index.py:
    from kikimr.public.sdk.python import client as ydb
    import urllib.parse
    import hashlib
    import base64
    import json
    import os
    def decode(event, body):
        # тело запроса может быть закодировано
        is_base64_encoded = event.get('isBase64Encoded')
        if is_base64_encoded:
            body = str(base64.b64decode(body), 'utf-8')
        return body
    def response(statusCode, headers, isBase64Encoded, body):
        return {
            'statusCode': statusCode,
            'headers': headers,
            'isBase64Encoded': isBase64Encoded,
            'body': body,
        }
    def get_config():
        endpoint = os.getenv("endpoint")
        database = os.getenv("database")
        if endpoint is None or database is None:
            raise AssertionError("Нужно указать обе переменные окружения")
        credentials = ydb.construct_credentials_from_environ()
        return ydb.DriverConfig(endpoint, database, credentials=credentials)
    def execute(config, query, params):
        with ydb.Driver(config) as driver:
            try:
                driver.wait(timeout=5)
            except TimeoutError:
                print("Connect failed to YDB")
                print("Last reported errors by discovery:")
                print(driver.discovery_debug_details())
                return None
            session = driver.table_client.session().create()
            prepared_query = session.prepare(query)
            return session.transaction(ydb.SerializableReadWrite()).execute(
                prepared_query,
                params,
                commit_tx=True
            )
    def insert_link(id, link):
        config = get_config()
        query = """
            DECLARE $id AS Utf8;
            DECLARE $link AS Utf8;

            UPSERT INTO links (id, link) VALUES ($id, $link);
            """
        params = {'$id': id, '$link': link}
        execute(config, query, params)
    def find_link(id):
        print(id)
        config = get_config()
        query = """
            DECLARE $id AS Utf8;

            SELECT link FROM links where id=$id;
            """
        params = {'$id': id}
        result_set = execute(config, query, params)
        if not result_set or not result_set[0].rows:
            return None
        return result_set[0].rows[0].link
    def shorten(event):
        body = event.get('body')
        if body:
            body = decode(event, body)
            original_host = event.get('headers').get('Origin')
            link_id = hashlib.sha256(body.encode('utf8')).hexdigest()[:6]
            # в ссылке могут быть закодированные символы, например, %. это помешает работе api-gateway при редиректе,
            # поэтому следует избавиться от них вызовом urllib.parse.unquote
            insert_link(link_id, urllib.parse.unquote(body))
            return response(200, {'Content-Type': 'application/json'}, False, json.dumps({'url': f'{original_host}/r/{link_id}'}))
        return response(400, {}, False, 'В теле запроса отсутствует параметр url')
    def redirect(event):
        link_id = event.get('pathParams').get('id')
        redirect_to = find_link(link_id)
        if redirect_to:
            return response(302, {'Location': redirect_to}, False, '')
        return response(404, {}, False, 'Данной ссылки не существует')
    # эти проверки нужны, поскольку функция у нас одна
    # в идеале сделать по функции на каждый путь в api-gw
    def get_result(url, event):
        if url == "/shorten":
            return shorten(event)
        if url.startswith("/r/"):
            return redirect(event)
        return response(404, {}, False, 'Данного пути не существует')
    def handler(event, context):
        url = event.get('url')
        if url:
            # из API-gateway url может прийти со знаком вопроса на конце
            if url[-1] == '?':
                url = url[:-1]
            return get_result(url, event)
        return response(404, {}, False, 'Эту функцию следует вызывать при помощи api-gateway') 
    Создайте файл requirements.txt:
    pipreqs $PWD --force 
    Находясь в директории с исходными файлами, упакуйте все нужные файлы в zip-архив:
    zip src.zip index.py requirements.txt 
        В переменные окружения функции необходимо добавить:
        endpoint — нужно указать протокол grpcs:// и добавить значение Эндпоинт из секции YDB эндпоинт, обычно получается grpcs://ydb.serverless.yandexcloud.net:2135.
        database — это значение поля База данных из секции YDB эндпоинт (начинается с /ru-central1/....).
        USE_METADATA_CREDENTIALS — выставите значение переменной в 1.
        Создадим нашу функцию for-serverless-shortener. При этом сразу зададим все необходимые переменные, сервисный аккаунт и сделаем ее публичной:
    yc serverless function create \
      --name for-serverless-shortener \
      --description "function for serverless-shortener"
    yc serverless function version create \
      --function-name for-serverless-shortener \
      --memory=256m \
      --execution-timeout=5s \
      --runtime=python37 \
      --entrypoint=index.handler \
      --service-account-id $SERVICE_ACCOUNT_SHORTENER_ID \
      --environment USE_METADATA_CREDENTIALS=1 \
      --environment endpoint=grpcs://ydb.serverless.yandexcloud.net:2135 \
      --environment database=$YDB_DATABASE \
      --source-path src.zip
    yc serverless function allow-unauthenticated-invoke for-serverless-shortener 
    Шаг 5. Конфигурирование Yandex API Gateway
        Создадим спецификацию for-serverless-shortener.yml со следующим содержанием:
    openapi: 3.0.0
    info:
      title: for-serverless-shortener
      version: 1.0.0
    paths:
      /:
        get:
          x-yc-apigateway-integration:
            type: object_storage
            bucket:             <bucket_name>        # <-- имя бакета
            object:             <html_file>          # <-- имя html-файла
            presigned_redirect: false
            service_account:    <service_account_id> # <-- идентификатор сервисного аккаунта
          operationId: static
      /shorten:
        post:
          x-yc-apigateway-integration:
            type: cloud_functions
            function_id:  <function_id>               # <-- идентификатор функции
          operationId: shorten
      /r/{id}:
        get:
          x-yc-apigateway-integration:
            type: cloud_functions
            function_id:  <function_id>               # <-- идентификатор функции
          operationId: redirect
          parameters:
            - description: id of the url
              explode: false
              in: path
              name: id
              required: true
              schema:
                type: string
              style: simple 
    Не забудьте подставить в спецификацию актуальные для вас значения переменных.
        Используем спецификацию для инициализации:
    yc serverless api-gateway create \
      --name for-serverless-shortener \
      --spec=for-serverless-shortener.yml \
      --description "for serverless shortener" 
    В результате успешного создания API-шлюза получим значение параметра domain:
    yc serverless api-gateway list
    yc serverless api-gateway get --name for-serverless-shortener 
        Чтобы проверить работоспособность API-шлюза и созданного приложения целиком, скопируйте служебный домен (вида https://<идентификатор API Gateway>.apigw.yandexcloud.net/) и вставьте адрес в браузер.
    Добавляйте адреса сайтов в форму, они будут сохранятся в базу данных. А вам будет доступна ссылка, за которой будет скрываться оригинальный адрес. Ваше приложение полностью работоспособно. Теперь вы умеете использовать serverless-стеком технологий Yandex Cloud.
    Итак, вы создали приложение с использованием Cloud Functions, API Gateway, Object Storage и Yandex Database. Конечно, вы можете развивать его и дальше, расширяя функциональность.
    Вводный курс по serverless-разработке на этом завершён. Вам осталось пройти лишь заключительный тест, который проверит ваши знания по всем рассмотренным в курсе сервисам.
    Task:
    Практическая работа. Права доступа и роли для сервисного аккаунта
    Decision:
    В этом уроке вы научитесь работать с сервисными аккаунтами и назначать для них роли и права доступа к объектам. В качестве объекта будет выступать созданный в сервисе KMS ключ шифрования (подробнее об этом сервисе вы узнаете на одном из следующих занятий). Предположим, что перед вами стоит задача использовать сервисный аккаунт для ротации ключей.
    Чтобы решить эту задачу, понадобится выполнить следующие шаги:
        Создать сервисный аккаунт.
        Получить права на управление этим сервисным аккаунтом.
        Создать статические ключи доступа и привязать их к сервисному аккаунту, чтобы он мог пройти авторизацию в сервисе KMS.
        Создать в сервисе KMS ключ шифрования и назначить сервисному аккаунту роль kms.admin для управления этим ключом.
        И, наконец, ротировать ключ, то есть создать его новую версию с такими же параметрами, из-под сервисного аккаунта.
    Нужно заметить, что через консоль управления сервисному аккаунту можно назначить роль только на каталог, в котором он был создан. Роли на все остальные ресурсы назначаются с помощью CLI или API. Поэтому для выполнения этой практической работы вам понадобится вспомнить, как пользоваться утилитой yc, чему вы уже научились в курсе «DevOps и автоматизация».
    Ну что же, поехали!
    ШАГ 1
    Для начала создадим сервисный аккаунт. В консоли управления войдите в каталог облака, в котором вы будете выполнять эту практическую работу, и перейдите на вкладку Сервисные аккаунты. Нажмите кнопку Создать сервисный аккаунт.
    В открывшемся окне задайте для нового сервисного аккаунта имя и, при желании, добавьте описание. Здесь аккаунту также можно добавить роли на каталог, в котором он создаётся.
    Оставьте поле с ролями пустым и нажмите Создать.
    ШАГ 2
    Настройте для вашего аккаунта на Яндексе доступ на авторизацию под созданным сервисным аккаунтом.
        Для начала убедитесь, что вы авторизованы в аккаунте с ролью admin. Чтобы это проверить, выполните команду
    yc iam role list 
    Вы увидите список ролей вашего аккаунта. Примерно такой (роль admin должна в нем присутствовать):
        Узнайте идентификатор своего аккаунта. Он понадобится, чтобы добавить вашему аккаунту роль editor на созданный сервисный аккаунт (сервисный аккаунт тоже является ресурсом, и для работы с ним нужна соответствующая роль). Воспользуйтесь для этого командой:
    yc iam user-account get <имя_вашего_аккаунта> 
        Кроме того, нужно узнать идентификатор созданного сервисного аккаунта. Это можно сделать в разделе Сервисные аккаунты консоли управления. Выбрав нужный аккаунт в списке, вы перейдёте на страницу с детальной информацией о нем.
        Теперь предоставьте вашему пользовательскому аккаунту права на управление созданным сервисным аккаунтом. Для этого нужно выполнить команду
    yc iam service-account add-access-binding <ID_сервисного_аккаунта> \
    --role editor --subject userAccount:<ID_пользовательского_аккаунта> 
    ШАГ 3
    Настройте аутентификацию под сервисным аккаунтом с вашей машины.
        Сначала нужно создать статические ключи доступа и сохранить их в json-файле (например key.json).
    Воспользуйтесь для этого командой
    yc --folder-name <имя_каталога> iam key create \
    --service-account-name <имя_сервисного_аккаунта> --output key.json 
    После выполнения команды вы получите идентификатор созданной ключевой пары. Используя статические ключи доступа, можно получить IAM-токен для авторизации в сервисах.
        Теперь нужно создать профиль, от имени которого будут выполняться операции из-под сервисного аккаунта. Для этого придумайте имя этого профиля (например yc-lab23-profile) и выполните команду:
    yc config profile create <имя_профиля> 
    Привяжите к этому профилю ранее созданный статический ключ доступа с помощью команды:
    yc config set service-account-key key.json 
        Чтобы убедиться, что всё сделано правильно, выведите информацию об авторизации и ключах доступа
    yc config list 
    Вы должны получить примерно такой результат:
    ШАГ 4
        Теперь нужно создать ключ шифрования, ротацией которого вы будете управлять с помощью сервисного аккаунта. Для этого перейдите в дашборд каталога в консоли управления, нажмите кнопку Создать ресурс и выберите Ключ шифрования.
    В открывшемся окне задайте для ключа имя и нажмите кнопку Создать. Новый ключ появится в списке в открывшемся разделе Ключи.
        Чтобы назначить сервисному аккаунту роль для какого-либо ресурса, нужно знать идентификатор этого ресурса. Нажмите строку с созданным ключом, чтобы перейти на страницу детальной информации о нём, и скопируйте ID ключа.
        Перейдем к назначению сервисному аккаунту роли kms.admin для управления созданным ключом шифрования. Перед этим нужно сначала вернуться в профиль вашего аккаунта.
    yc config profile activate <имя_профиля> 
    Выполните команду:
    yc --folder-name <имя_каталога> kms symmetric-key \
    add-access-binding <ID_ключа_шифрования> --role kms.admin \
    --subject serviceAccount:<ID_сервисного_аккаунта> 
    Теперь с помощью сервисного аккаунта вы можете управлять этим ключом шифрования.
    ШАГ 5
        В CLI переключитесь обратно в профиль сервисного аккаунта:
    yc config profile activate <имя_профиля_сервисного_аккаунта> 
    Ротируйте ключ шифрования:
    yc kms symmetric-key rotate <ID_ключа> 

        После выполнения команды перейдите на страницу детальной информации о ключе и откройте вкладку Операции.
    Вы увидите, что операция по ротации ключа выполнена под вашим сервисным аккаунтом. Значит, всё получилось и задача решена!
    Task:
    Лучшие практики по работе с учетными записями
    Decision:
    В последних уроках этой и следующих тем мы будем рассказывать о лучших практиках обеспечения безопасности. Возможно, не все из них будут применимы для вашего проекта, но это полезные практические приемы, и о них в любом случае стоит знать.
    Управление привилегированными пользователями
    К привилегированным пользователям относятся учетные записи со следующими ролями:
        resource-manager.clouds.owner;
        billing.accounts.owner;
        роль admin, назначенная всему облаку;
        роль admin, назначенная каталогу;
        роль admin, назначенная платежному аккаунту.
    Привилегированными они называются потому, что могут делать в облаке гораздо больше, чем обычный пользователь. Поэтому и результат их действий может оказаться крайне неприятным.
    Пользователь, который создает облако, автоматически получает роль resource-manager.clouds.owner. Она позволяет выполнять любые операции с облаком и ресурсами в нем, а также выдавать доступ другим пользователям путем назначения и отзыва ролей.
    Если ваша компания использует федерацию удостоверений, то рекомендуется назначить эту роль одному или нескольким сотрудникам. Их федеративные аккаунты должны быть надёжно защищены с помощью:
        двухфакторной аутентификации;
        запрета на аутентификацию с посторонних устройств;
        мониторинга попыток входа и заданных порогов предупреждений.
    Для аккаунта Яндекс ID, под которым создано облако, нужно назначить сложный пароль, а использовать его — только в случае крайней необходимости (например, если федерация сломалась).
    Роли admin на облака, каталоги и платежные аккаунты рекомендуется назначать только федеративным учетным записям. При этом число таких учетных записей должно быть минимально необходимым, а потребность пользователей в такой роли следует регулярно перепроверять.
    Еще одна важная роль — это billing.accounts.owner, которая автоматически выдается при создании платёжного аккаунта и не может быть переназначена другому пользователю. Она позволяет выполнять любые действия с платёжным аккаунтом. Роль billing.accounts.owner может быть назначена только аккаунту Яндекс ID. Аккаунт с этой ролью используется при настройке способов оплаты и подключении облаков.
    Безопасности такого аккаунта следует уделять повышенное внимание, поскольку он обладает значительными полномочиями и не может быть подключен к федерации корпоративных аккаунтов.
    Наиболее правильным подходом можно считать отказ от регулярного использования аккаунта с этой ролью, то есть его нужно использовать только при первоначальной настройке и при внесении изменений. На время активного использования этого аккаунта включите двухфакторную аутентификацию (2FA) в Яндекс ID. Затем, если вы не используете способ оплаты банковской картой (доступный только для данной роли), назначьте этому аккаунту сложный пароль, сгенерированный с помощью специализированного ПО, отключите 2FA и не используйте этот аккаунт без необходимости.
    После каждого использования генерируйте новый пароль. Отключение 2FA для этого аккаунта важно в ситуации, если аккаунт не закреплён за конкретным сотрудником. Это позволяет избежать привязки критически важного аккаунта к личному устройству.
    Использование ресурсной модели
    Если система должна соответствовать требованиям PCI DSS, то при разработке модели доступа для создаваемой инфраструктуры рекомендуется использовать следующий подход:
        все ресурсы, которые входят в область соответствия PCI DSS, нужно поместить в отдельное облако;
        группы ресурсов, которые требуют разного административного доступа, помещают в разные каталоги (например, DMZ, security, backoffice и т.д.);
        общие ресурсы (например, сеть и группы безопасности) помещают в отдельный каталог для разделяемых ресурсов.
    Последние два пункта стоит иметь в виду при построении любой сложной инфраструктуры в облаке, даже если требования PCI DSS вас пока не беспокоят.
    Использование сервисных аккаунтов
    При использовании сервисных аккаунтов рекомендуется:
        для назначения сервисного аккаунта на виртуальную машину и получения токена использовать сервис метаданных;
        настроить на виртуальной машине локальный файрвол;
        обеспечить безопасное хранение и управление ключами сервисного аккаунта;
        следовать принципу минимальных привилегий и назначать сервисному аккаунту только те роли, которые необходимы для работы приложения;
        следовать принципу минимальных привилегий и в отношении доступа к самому сервисному аккаунту, то есть выдавать роли на него минимальному кругу пользователей и только при необходимости.
    Task:
    Практическая работа. Организация защищённого канала
    Decision:
    Защита данных, передаваемых между вашей локальной инфраструктурой и облаком, — важный элемент информационной безопасности. А удалённая работа, которая получила распространение в период пандемии коронавируса и сейчас закрепилась в практиках многих компаний, сделала эту задачу ещё более актуальной.
    Чтобы защитить передаваемую информацию, используют VPN (Virtual Private Network) — технологию, позволяющую развернуть защищённое сетевое соединение «поверх» незащищённой сети (чаще всего это интернет). VPN-соединение представляет собой канал передачи данных между двумя узлами. Этот канал обычно называют VPN-туннелем. Если за одним из узлов находится целая сеть, то его называют VPN-шлюзом.
    Механизм работы VPN:
        Перед созданием туннеля узлы идентифицируют друг друга, чтобы удостовериться, что шифрованные данные будут отправлены на нужный узел.
        На обоих узлах нужно заранее определить, какие протоколы будут использоваться для шифрования и обеспечения целостности данных.
        Узлы сверяют настройки, чтобы договориться об используемых алгоритмах. Если настройки разные, туннель не создаётся.
        Если сверка прошла успешно, то создаётся ключ, который используется для симметричного шифрования.
    Этот механизм регламентируют несколько стандартов. Один из самых популярных — IPSec (Internet Protocol Security).
    На этом уроке вы научитесь настраивать IPSec VPN-туннель между двумя VPN-шлюзами с помощью демона strongSwan. Один шлюз вы настроите на виртуальной машине в Yandex Cloud, второй — на своей локальной машине или виртуальной машине в другой облачной сети.
    Шаг 1. Создание ресурсов
    Для практической работы вам понадобится сеть и подсеть в Yandex Cloud, а также созданная в этой подсети тестовая ВМ без публичного IP-адреса. Создайте эти ресурсы, если у вас их нет.
    Теперь создадим IPSec-инстанс — ВМ, которая будет служить шлюзом для IPSec-туннеля. Чтобы это сделать:
        Откройте ваш каталог, нажмите кнопку Создать ресурс и выберите пункт Виртуальная машина.
        В поле Имя задайте имя ВМ, например ipsec-instance.
        Выберите зону доступности, где находится подсеть, к которой будет подключён IPSec-инстанс, и тестовая ВМ.
        В разделе Выбор образа/загрузочного диска перейдите в блок Cloud Marketplace и выберите образ IPSec-инстанс.
        В блоке Сетевые настройки выберите нужную сеть, подсеть и назначьте ВМ публичный IP-адрес из списка или автоматически.
    Важно использовать только статические публичные IP-адреса из списка или сделать IP-адрес ВМ статическим после её создания. Динамический IP-адрес может измениться после перезагрузки ВМ, и туннель перестанет работать.
        В блоке Доступ укажите логин и SSH-ключ для доступа к ВМ.
        Нажмите кнопку Создать ВМ.
    Виртуальная машина готова.
    Шаг 2. Настраиваем IPSec
    Теперь настроим шлюз с публичным IP-адресом, который будет устанавливать IPSec-соединение с удалённым шлюзом (вашей локальной машиной или ВМ в другой облачной сети).
    Вы можете создать в своём каталоге ещё одну облачную сеть с подсетью, создать в ней IPSec-инстанс из образа и использовать его в качестве удалённого шлюза. Либо можно использовать в качестве шлюза машину в вашей локальной сети. Вам понадобится публичный IP-адрес удалённого шлюза и CIDR подсети.
    Допустим, публичный IP-адрес вашего шлюза — 130.193.32.25, а за ним находится подсеть c префиксом подсети CIDR 10.128.0.0/24. Шлюз будет устанавливать IPSec-соединение с удалённым шлюзом с IP-адресом, например, 1.1.1.1, за которым находится подсеть с префиксом подсети CIDR 192.168.0.0/24.
        Подключитесь к ВМ IPSec-инстанс по SSH:
    ssh <имя пользователя>@130.193.32.25 
        Откройте конфигурацию IPSec:
    sudo nano /etc/ipsec.conf 
        В разделе config setup файла конфигурации задайте следующие параметры:
    config setup
            charondebug="all"
            uniqueids=yes
            strictcrlpolicy=no 
        Добавьте новый раздел с описанием тестового соединения, например conn cloud-to-hq.
        Задайте параметры тестового соединения:
    leftid — публичный IP-адрес IPSec-инстанса.
    leftsubnet — CIDR подсети, к которой подключён IPSec-инстанс.
    right — публичный IP-адрес шлюза на другом конце VPN-туннеля.
    rightsubnet — CIDR подсети, к которой подключён VPN-шлюз на другом конце VPN-туннеля.
    Параметры ike и esp — это алгоритмы шифрования, которые поддерживаются на удалённом шлюзе. Перечень поддерживаемых алгоритмов можно посмотреть на сайте strongSwan: IKEv1 и IKEv2.
        Укажите остальные настройки, консультируясь с документацией strongSwan и учитывая настройки удалённого шлюза.
        У вас должна получиться примерно такая конфигурация:
    conn cloud-to-hq
            authby=secret
            left=%defaultroute
            leftid=130.193.32.25
            leftsubnet=10.128.0.0/24
            right=1.1.1.1
            rightsubnet=192.168.0.0/24
            ike=aes256-sha2_256-modp1024!
            esp=aes256-sha2_256!
            keyingtries=0
            ikelifetime=1h
            lifetime=8h
            dpddelay=30
            dpdtimeout=120
            dpdaction=restart
            auto=start 
        Сохраните изменения и закройте файл.
        Откройте файл /etc/ipsec.secrets и укажите в нём пароль для установки соединения:
    130.193.32.25 1.1.1.1 : PSK "<пароль>" 
        Перезапустите strongSwan:
    sudo systemctl restart strongswan-starter 
    Шаг 3. Настраиваем статическую маршрутизацию
    Теперь нужно настроить маршрутизацию между IPSec-инстансом и тестовой ВМ без публичного IP-адреса. Для этого создадим таблицу маршрутизации и добавим в неё статические маршруты.
        Откройте сервис Virtual Private Cloud в каталоге, где требуется создать статический маршрут.
        Выберите раздел Таблицы маршрутизации в панели слева и нажмите кнопку Создать таблицу маршрутизации.
        Задайте имя таблицы маршрутизации, выберите сеть, в которой требуется её создать, и нажмите кнопку Добавить маршрут.
        В открывшемся окне введите префикс подсети назначения на удалённой стороне, в примере это 192.168.0.0/24.
        В поле Next hop укажите внутренний IP-адрес IPSec-шлюза и нажмите кнопку Добавить.
        Нажмите кнопку Создать таблицу маршрутизации.
        Чтобы использовать статические маршруты, нужно привязать таблицу маршрутизации к подсети. Для этого в разделе Подсети, в строке нужной подсети, нажмите кнопку … и в открывшемся меню выберите пункт Привязать таблицу маршрутизации.
        В открывшемся окне выберите созданную таблицу и нажмите кнопку Привязать. Созданный маршрут можно применять и к другим подсетям этой сети.
    Шаг 4. Настраиваем IPSec на другом шлюзе
    Для работы VPN-туннеля нужно настроить второй шлюз.
        Настройте strongSwan аналогично первому IPSec-шлюзу, но с зеркальными настройками IP-адресов и подсетей в файле /etc/ipsec.conf. Должна получиться такая конфигурация:
    conn hq-to-cloud
            authby=secret
            left=%defaultroute
            leftid=1.1.1.1
            leftsubnet=192.168.0.0/24
            right=130.193.32.25
            rightsubnet=10.128.0.0/24
            ike=aes256-sha2_256-modp1024!
            esp=aes256-sha2_256!
            keyingtries=0
            ikelifetime=1h
            lifetime=8h
            dpddelay=30
            dpdtimeout=120
            dpdaction=restart
            auto=start 
        Укажите пароль для соединения в файле /etc/ipsec.secrets, указав IP-адреса шлюзов в обратном порядке:
    1.1.1.1 130.193.32.25 : PSK "<пароль>" 
        Перезапустите strongSwan:
    sudo systemctl restart strongswan-starter 
    Шаг 5. Проверяем, что всё работает
    Чтобы убедиться, что туннель между шлюзами установлен, выполните на любом из шлюзов команду:
    sudo ipsec status 
    Если всё в порядке, то у вас должно появиться примерно такое сообщение:
    Security Associations (1 up, 0 connecting):
    hq-to-cloud[3]: ESTABLISHED 29 minutes ago, 10.128.0.26[130.193.33.12]...192.168.0.23[1.1.1.1]
    hq-to-cloud{3}:  INSTALLED, TUNNEL, reqid 3, ESP in UDP SPIs: c7fa371d_i ce8b91ad_o
    hq-to-cloud{3}:   10.128.0.0/24 === 192.168.0.0/24 
    Статус ESTABLISHED означает, что туннель между шлюзами создан.
    Сведения об установке и работе соединения находятся в логах strongSwan. Просмотреть логи можно с помощью команды:
    sudo journalctl -u strongswan-starter 
    Проверить статус демона strongSwan можно командой:
    systemctl status strongswan-starter 
    Осталось проверить связность соединения. Для этого создайте ещё одну тестовую виртуальную машину за вторым шлюзом, а затем пропингуйте одну тестовую машину с другой.
    Поздравляем, ваше соединение с облаком безопасно!
    Если вы не планируете использовать созданный VPN-туннель, удалите ненужные ресурсы.
    Task:
    Практическая работа. Выпуск сертификата для сайта
    Decision:
    В этой практической работе мы зарегистрируем домен, привяжем его к бакету в объектном хранилище и настроим для этого домена автоматический выпуск сертификата с помощью Certificate Manager.
    Шаг 1
    Если у вас нет своего домена, зарегистрируйте временный домен, например, на сайте freenom.com:
        Проверьте на сайте доступность имени, которое вы придумали для своего домена.
        Введите имя вместе с доменом верхнего уровня, например testpracticum2022.ml, иначе при попытке зарезервировать домен сервис будет сообщать, что домен занят.
        Если это имя доступно, добавьте домен в корзину и укажите свой email для подтверждения.
        Проверьте почту и подтвердите регистрацию домена.
        Обновите страницу с заказом.
        После подтверждения регистрации домена зайдите в объектное хранилище (Object Storage) и создайте новый публичный бакет. Его название должно совпадать с полным названием домена.
        Переключите доступ на чтение объектов в Публичный. Загрузите в бакет файлы статического сайта (вы можете воспользоваться файлами из практической работы курса «Хранение и анализ данных».
        Выберите на панели управления раздел Веб-сайт, переключите бакет в режим Хостинг и нажмите Сохранить.
    Шаг 2
    Настроить защищённый доступ к бакету можно двумя способами: загрузить сертификат прямо в бакет или с помощью Certificate Manager. Воспользуемся вторым способом.
        В консоли управления перейдите в сервис Certificate Manager. Для выпуска сертификата с помощью этого сервиса подтвердите владение доменом: в разделе Сертификаты нажмите кнопку Добавить сертификат и выберите Сертификат Let’s Encrypt.
        В открывшемся окне задайте имя создаваемого сертификата и заполните поле с именем вашего домена. Нажмите кнопку Создать.
    Сервис автоматически направит запрос на создание сертификата, а домен перейдёт в статус проверки.
        Для выпуска сертификата необходимо подтвердить статус владения доменом. Откройте страницу с деталями запроса на сертификат:
    На этой странице для нас важны два поля: имя DNS-записи и её значение. Если вы создавали домен на freenom.com, то перейдите в личный кабинет на этом сайте, выберите раздел Services → My Domains и нажмите кнопку Manage Domains:
    Выберите Manage Freenom DNS:
    В открывшемся редакторе добавьте TXT-запись для подтверждения владения доменом. В качестве названия записи задайте _acme-challenge без полного названия домена. В качестве значения TXT-записи — значение со страницы проверки прав на домен в консоли управления Yandex Cloud.
    Аналогично внесите значение CNAME-записи со страницы проверки прав на домен в консоли управления Yandex Cloud.
    Добавьте также запись CNAME для привязки поддомена WWW к вашему бакету:
    В поле Target укажите полное имя бакета, включая .website.yandexcloud.net. Сохраните сделанные изменения.
    Если вы используете собственный домен, задайте параметры DNS в настройках вашего DNS-сервера. Для применения настроек DNS потребуется некоторое время — обычно до 15 минут.
    После окончания проверки домена Certificate Manager автоматически выпустит сертификат.
    Шаг 3
    Теперь настроим доступ к сайту, то есть к созданному бакету, по протоколу HTTPS с помощью сертификата. Для этого перейдите в раздел HTTPS и нажмите кнопку Настроить.
    В поле Источник выберите Certificate Manager, в поле Сертификат — ранее выпущенный сертификат. Нажмите кнопку Сохранить.
    Теперь ваш сайт доступен по протоколу HTTPS. Чтобы проверить это, откройте его в браузере. В адресной строке браузера должен отображаться значок защищённого соединения.
    Task:
    Практическая работа. Создание и ротация ключей шифрования
    Decision:
    На прошлом уроке вы познакомились с возможностями сервиса управления ключами шифрования KMS. В этой практической работе вы научитесь создавать ключи шифрования и управлять ими, а также использовать эти ключи для шифрования и расшифрования данных.
    Шаг 1
    Перейдите в панель управления Yandex Cloud, нажмите кнопку Создать ресурс и выберите из выпадающего списка пункт Ключ шифрования.
    Задайте для создаваемого ключа имя (например yc-lab-key1), заполните поле Описание (это необязательно) и выберите алгоритм шифрования. Предположим, что ключ нужно ротировать каждый день. Для этого в поле Период ротации, дни выберите вариант Своё значение и введите число 1 в поле справа.
    Нажмите кнопку Создать. Когда операция создания ключа завершится, новый ключ появится в списке.
    Нажав на строку с ключом, вы перейдёте на страницу детальной информации. На ней приведены все параметры ключа, а также список его версий. Обратите внимание, что ID (идентификатор) ключа и ID конкретной версии ключа отличаются. Важно их не путать.
    Шаг 2
    Давайте используем созданный ключ для шифрования и расшифрования данных. Создайте у себя на диске файл (например, текстовый файл с именем plain.txt). Добавьте в него любой текст и сохраните содержимое. Напомним, что размер файла не должен превышать 32 килобайта.
    Запустите утилиту командной строки (bash или cmd) и перейдите в каталог с файлом plain.txt. Зашифруйте этот файл с помощью утилиты yc, а результат операции шифрования выведите в файл encrypted.txt. Для этого выполните команду:
    yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted.txt 
    После выполнения команды будет создан файл encrypted.txt, который содержит зашифрованный текст. Утилита yc также выведет информацию о том, каким ключом и какой его версией файл был зашифрован.
    Шаг 3
    Теперь расшифруйте этот файл, а результат операции выведите в файл decrypted.txt. Для этого выполните команду:
    yc kms symmetric-crypto decrypt --id <ID ключа> --ciphertext-file encrypted.txt --plaintext-file decrypted.txt 
    В результате выполнения команды будет создан файл decrypted.txt с идентичным исходному файлу (plain.txt) содержимым.
    Если расшифровать файл не удалось, утилита выдаст сообщение об ошибке.
    Шаг 4
    Создайте новую версию ключа. Для этого перейдите на страницу детальной информации о ключе и нажмите кнопку Ротировать. Новая версия ключа появится в списке версий и станет основной (Primary). Обратите внимание, что идентификаторы версий отличаются друг от друга.
    Шаг 5
    Запланируйте удаление первой версии ключа. Для этого в списке версий нажмите на значок … в строке с этой версией, а затем выберите пункт Запланировать удаление.
    В появившемся окне установите время, по истечении которого ключ будет удалён, и нажмите Запланировать. Версия ключа не может быть удалена моментально, минимальный период времени для её удаления составляет один день.
    После этого в списке версий удаляемый ключ будет помечен как запланированный на удаление (Scheduled For Destruction). Теперь этой версией ключа невозможно расшифровать файлы, которые были зашифрованы с её помощью.
    Провести ротацию ключа можно и из командной строки. Для этого используется команда:
    yc kms symmetric-key rotate <ID ключа> 
    Шаг 6
    Зашифруйте исходный файл plain.txt с помощью новой версии ключа. Результат запишите в файл encrypted_with_new_key.txt.
    yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted_with_new_key.txt 
    Теперь у вас есть два файла:
        encrypted.txt, зашифрованный версией ключа, которая помечена на удаление;
        encrypted_with_new_version.txt, зашифрованный новой версией ключа.
    Попробуйте расшифровать данные из обоих файлов. Вы увидите, что расшифровать первый файл не получилось, а файл, который зашифрован второй версией ключа, расшифрован.
    Запланированное удаление первой версии ключа можно отменить. Это позволит расшифровать данные из первого файла.
    В строке версии ключа, которая запланирована на удаление, нажмите значок …, а затем кнопку кнопку Отменить удаление. Эта версия снова получит статус активной. Проверьте, что она работает, расшифровав файл encrypted.txt.
    Task:
    Практическая работа. Калькулятор расходов
    Decision:
    Любая стройка начинается со сметы. Облачная архитектура не исключение — прежде чем разворачивать систему, нужно просчитать её стоимость.
    Чтобы сделать это в Yandex Cloud, откройте калькулятор тарифов.
    Задание 1. Работа с калькулятором
    Давайте для начала посчитаем, во сколько рублей в месяц обойдётся система со следующими параметрами:
    Compute Cloud: Ubuntu 20.04 LTS, Intel Broadwell, 4 vCPU 100%, 16 ГБ RAM, SSD 100 ГБ, без публичного IP-адреса.
    Managed Service for PostgreSQL: Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
    Техподдержка: тариф «Бизнес» при потреблении 50 тыс. ₽ в месяц.
    Сервисы Yandex.Cloud, стоимость которых можно рассчитать, указаны на верхней панели калькулятора:
    Выберите Compute Cloud и задайте параметры виртуальной машины. Должно получиться вот так:
    Как видите, справа указана стоимость сервиса с детализацией. Если выбрать ещё один сервис на панели вверху — его стоимость добавится в общий лист, а внизу отобразится общая сумма за все сервисы.
    Давайте это проверим. Выберите PostgreSQL и укажите нужную конфигурацию: один кластер Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
    Остался последний сервис — техническая поддержка.
    Плата за тарифы «Стандарт» и «Бизнес» зависит от количества потребляемых ресурсов. Тариф «Премиум» может быть дополнен различными услугами и рассчитывается индивидуально.
    Узнаем стоимость тарифа «Бизнес» при потреблении ресурсов на 50 тыс. ₽ в месяц:
    Это было легко!
    Теперь вы знаете, сколько стоит система из трёх сервисов, и можете запустить её — или поэкспериментировать с конфигурациями, чтобы добиться приемлемой стоимости.
    Задание 2. Расчёт вручную
    А теперь задача со звёздочкой. В калькулятор тарифов пока добавлены не все сервисы Yandex Cloud. Стоимость некоторых придётся рассчитывать вручную. Потренируемся это делать.
    Узнаем, например, сколько стоит сервис Monitoring. Прокрутите страницу калькулятора вниз и в блоке Инфраструктура и сеть выберите Monitoring.
    Да, вы попадёте в документацию, но такова жизнь. Здесь указаны цены ресурсов. Со временем они могут меняться, поэтому для решения задачи ниже берите цены за август 2022 года:
    Ваша задача: рассчитать стоимость месячного использования сервиса при записи 35 метрик с частотой два значения в минуту.
    Подсказка: используйте раздел «Пример расчёта стоимости» как шпаргалку.
    Укажите полученную сумму:
    -65,03 ₽
    -3 021 ₽
    +21,168 ₽
    -205,07 ₽
    Task:
    Практическая работа. Поиск самой затратной виртуальной машины
    Decision:
    Примечание
    Перед началом этого урока создайте четыре виртуальные машины — с 5, 20, 50 и 100% vCPU — и сделайте небольшой перерыв. Нужно как минимум 15 минут, чтобы вы увидели их потребление на дашбордах.
    В боевых проектах вам часто придётся искать самую большую статью расходов, а потом придумывать, что с ней делать. Сейчас вы научитесь маркировать ВМ метками, чтобы понять, какая из них обходится дороже всего. Такими же метками можно размечать ресурсы и других сервисов Yandex Cloud.
    Если у вас ещё нет интерфейса командной строки Yandex Cloud
    Воспользуйтесь инструкцией, чтобы установить и инициализировать интерфейс командной строки (CLI) Yandex Cloud.
    Универсальный шаблон команды добавления метки выглядит следующим образом:
     yc <имя сервиса> <тип ресурса> add-labels <имя или идентификатор ресурса> --labels <имя метки>=<значение метки>
    Давайте уточним его для нашей задачи. Имя сервиса — compute, тип ресурса — instance. Идентификатор ресурса вы найдёте в консоли на странице обзора ВМ:
    Имя метки и значение метки задайте на свой вкус: можно использовать латиницу, цифры, дефисы и подчёркивания (не больше 63 символов).
    Если вы сделали всё правильно, то увидите результат выполнения команды:
    Повторите те же действия для трёх оставшихся ВМ.
    Когда закончите с метками, сделайте паузу. Системе понадобится несколько минут, чтобы отобразить данные размеченных машин в Yandex DataLens.
    В DataLens откройте дашборд Yandex Cloud Billing Dashboard и перейдите на вкладку Labels. Найти самую дорогую ВМ будет несложно:
    Устанавливайте метки на те ресурсы, которые требуют пристального внимания, и тогда вам будет легче за ними следить.
    Однако самостоятельно следить за потреблением в режиме нон-стоп просто невозможно. Чтобы всегда оставаться в курсе потребления и не уйти в минус при его резком всплеске, нужен автоматический помощник. О нём мы расскажем на следующем уроке.
    После практической работы не забудьте удалить ненужные виртуальные машины!

CI/CD
	Task:   
	Сделай CI/CD Pipeline который созадет и деплоит dicker image в cloud
	Task:   
	Сделай CI/CD Pipeline который деплоит java приложение в кибернетес
  	Source:
  	1. https://www.youtube.com/playlist?list=PLg5SS_4L6LYuu1RAT4l1tCkZrJzkIaNgL

K8s
	Task:
	Как автоматически деплоить helm cart?
	Decision:
	k8s add-on helm operator + argocd/fluxcd
	k8s add-on github action + terraform + helm provider 
  	Source:
  	1. https://www.youtube.com/playlist?list=PLg5SS_4L6LYuu1RAT4l1tCkZrJzkIaNgL

Terraform
	Task:
	напиши terraform модуль, который создаст простой EC2 instance в каждом полученном subnet ID и выдаст output в таком формате
	{instance_id : instance_private_ip}
	например
	{
	    i-2565432 : 10.10.10.1
	    i-4635656 : 20.20.20.2
	}  
  	Source:
  	1. https://www.youtube.com/playlist?list=PLg5SS_4L6LYuu1RAT4l1tCkZrJzkIaNgL

Git
    Task:
    Добавляем новый репозиторий
    Decision:
    $ mkdir tProjects
    $ cd tProjects/
    $ vim tFile1
    $ cat tFile1
    $ git init
    $ git add .
    $ git commit -m "first commit"
    $ git branch -M main
    $ git remote add origin https://github.com/David138it/tProject.git
    $ git push -u origin main
    Task:
    Вносим новые изменения
    Decision:
    $ git status
    $ git add .
    $ git commit -m "Add information in the file"
    $ git push -u origin main
    Task:
    Скопируем проект в другом компе
    Decision:
    $ git clone https://github.com/David138it/tProject
    $ cd tProject/
    Task:
    Возвращаемся в первый комп и там уже вносим копируем данные другого компа 
    Decision:
    $ git fetch origin
    $ cat tFile1 
    $ git merge
    $ cat tFile1
    Source:
    1. https://docs.github.com/ru/repositories/creating-and-managing-repositories/cloning-a-repository
	Task:   
	Как сделать rebase на main branch? и что это значит?
	Decision:
	$ git checkout your-branch
	$ git rebase main
	Когда вы сделаете готовый branch, вы хотите все обнволения main добавить себе в branch
    Source:
    1. https://www.youtube.com/playlist?list=PLg5SS_4L6LYuu1RAT4l1tCkZrJzkIaNgL
	Task:
	Я не хочу кидать в гитхаб репозитория папку Env. Показать последний коммит, последние 2 коммита и статистику внесенных ими изменений.
	Decision:
	$ vim .gitignore
	$ cat .gitignore
	Env/
	Env/*
	$ git show HEAD^
	$ git log -2 --stat
	$ git add .
	Task:
	Есть master (публичная версия сайта), в двух параллельных ветках (branch_1 и branch_2) было отредактировано одно и то же место одного и того же файла, первую ветку (branch_1) влили в master, попытка влить вторую вызывает конфликт.переключаемся на ветку master
	Decision:
	$ git checkout master
	$ git checkout -b branch_1
	subl .
	$ git commit -a -m "Правка 1"
	$ git checkout master
	$ git checkout -b branch_2
	subl .
	$ git commit -a -m "Правка 2"
	$ git checkout master
	$ git merge branch_1
	$ git merge branch_2
	subl .
	$ git commit -a -m "Устранение конфликта"
	Task:
	Если в разных ветках, в одних и тех же файлов, разные изменения, значит будет конфликт. Для этого установим утилиту kdiff3 с официального сайта. Укажем утилиту в Гит
	Decision:
	$ git config --global merge.tool kdiff3
	$ git merge master
	$ git mergetool
	$ git status
	Task:
	Удалим не нужные файлы, которые появились из-за бекапа и коммитим переделанный файл
	Decision:
	$ git merge new_vetka создать новую ветку с указанным именем
	$ git branch new_branch
	$ git branch -d hotfix
	$ git branch —merged
	$ git branch —no-merged
	$ git branch -a
	Task:
	Есть master (публичная версия сайта), хотим масштабно что-то поменять (переверстать «шапку»), но по ходу работ возникает необходимость подправить критичный баг (неправильно указан контакт в «подвале»).создадим новую ветку для задачи изменения «шапки» и перейдём в неё. устраняем баг и сохраняем разметку.  «подвала». удаляем ветку footer_hotfix
	Decision:
	$ git checkout -b new_page_header
	subl inc/header.html
	$ git commit -a -m "Новая шапка: смена логотипа"
	$ git checkout master
	$ git checkout -b footer_hotfix
    $ subl inc/footer.html
	$ git commit -a -m "Исправление контакта в подвале"
	$ git checkout master
	$ git merge footer_hotfix	
	$ git branch -d footer_hotfix
	$ git checkout new_page_header
	$ subl inc/header.html
	$ git commit -a -m "Новая шапка: смена навигации"
	$ git checkout master
	$ git merge new_page_header
	$ git branch -d new_page_header
	Source:
    1. https://www.youtube.com/playlist?list=PLoonZ8wII66iUm84o7nadL-oqINzBLk5g

Docker
	Task:
	Adding the Ubuntu using wsl
	Decision:
	C:\Users\dd>powershell "start-process powershell -verb runas"
	PS C:\Windows\system32> wsl --list --online
	Ниже приведен список допустимых распределений, которые можно установить.
	Распределение по умолчанию помечено знаком «*».
	Установите с помощью команды wsl --install -d <Distro>
	  NAME            FRIENDLY NAME
	* Ubuntu          Ubuntu
	  Debian          Debian GNU/Linux
	  kali-linux      Kali Linux Rolling
	  openSUSE-42     openSUSE Leap 42
	  SLES-12         SUSE Linux Enterprise Server v12
	  Ubuntu-16.04    Ubuntu 16.04 LTS
	  Ubuntu-18.04    Ubuntu 18.04 LTS
	  Ubuntu-20.04    Ubuntu 20.04 LTS
	PS C:\Windows\system32> wsl --install -d Ubuntu-20.04
	  Выполняется установка: Платформа виртуальной машины
	  Установка "Платформа виртуальной машины" выполнена.
	  Выполняется установка: Подсистема Windows для Linux
	  Установка "Подсистема Windows для Linux" выполнена.
	  Загрузка: Ядро WSL
	  Выполняется установка: Ядро WSL
	  Установка "Ядро WSL" выполнена.
	  Загрузка: Ubuntu 20.04 LTS
	  Требуемая операция выполнена успешно. Чтобы сделанные изменения вступили в силу, следует перезагрузить систему.
	PS C:\Windows\system32> Restart-Computer
	  Installing, this may take a few minutes...
	  Please create a default UNIX user account. The username does not need to match your Windows username.
	  For more infor-tation visit: https://aka.ms/wslusers
	Enter new UNIX username: Admin
	  adduser: Please enter a username matching the regular expression configured
	  via the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'
	  option to relax this check or reconfigure NAME_REGEX.
	Enter new UNIX username: admin
	  adduser: The group `admin' already exists.
	Enter new UNIX username: ar
	New password:
	Retype new password:
	  passwd: password updated successfully
	  Installation successful!
	  To run a command as ar (user "root"), use "sudo <command>".
	  See "man sudo_root" for details.
	  Welcome to Ubuntu 20.04 LTS (GNU/Linux 5.10.16.3-mi-crosoft-standard-WSL2 x86_64)
	   * Documentation:  https://help.ubuntu.com
	   * Management:     https://landscape.canoni-cal.com
	   * Support:        https://ubuntu.com/advantage
	    System infor-tation as of Wed Feb 23 10:10:50 +08 2022
	    System load:  0.29               Processes:             8
	    Usage of /:   0.4% of 250.98GB   Users logged in:       0
	    Memory usage: 1%                 IPv4 address for eth0: 172.17.195.253
	    Swap usage:   0%
	  0 updates can be installed immediately.
	  0 of these updates are security updates.
	  The list of available updates is more than a week old.
	  To check for new updates run: sudo apt update
	  This message is shown once once a day. To disable it please create the
	  /home/ar/.hushlogin file.
	Task:
	WslRegisterDistribution failed with error: 0x8007019e
	The Windows Subsystem for Linux optional component is not enabled. Please enable it and try again.
	See https://aka.ms/wslinstall for details.
	Press any key to continue...
	Decision:
	панель управление -> Включение или отключение компонентов Windows -> Подсистема Windows для Linux + -> Ok -> reboot -> Пуск -> Ubuntu 20.04

	Installing, this may take a few minutes...
	Не удалось завершить операцию.  Следует повторить ее.
	Please create a default UNIX user account. The username does not need to match your Windows username.
	For more infor-tation visit: https://aka.ms/wslusers
	Enter new UNIX username: ...
	New password:
	Retype new password:
	passwd: password updated successfully
	Installation successful!
	To run a command as administrator (user "root"), use "sudo <command>".
	See "man sudo_root" for details.
	Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 4.4.0-19041-Mi-crosoft x86_64)
	 * Documentation:  https://help.ubuntu.com
	 * Management:     https://landscape.canoni-cal.com
	 * Support:        https://ubuntu.com/advantage
	  System infor-tation as of Sun Dec  5 09:59:49 +08 2021
	  System load:    0.52      Processes:              7
	  Usage of /home: unknown   Users logged in:        0
	  Memory usage:   49%       IPv4 address for eth2:  192.168.56.1
	  Swap usage:     0%        IPv4 address for wifi0: 192.168.0.105
	1 update can be applied immediately.
	To see these additional updates run: apt list --upgradable
	The list of available updates is more than a week old.
	To check for new updates run: sudo apt update
	This message is shown once a day. To disable it please create the
	/home/.../.hushlogin file.
	...@A$
	Source:
	1. https://docs.microsoft.com/ru-ru/windows/wsl/
	Task:
	Set up Docker
	Decision:
	$ sudo apt update
	$ sudo apt install apt-transport-https ca-certifi-cates curl software-properties-common
	$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
	OK
	$ sudo add-apt-repository "deb [arch=amd64]
	$ sudo apt update
	$ apt-cache poli-cy docker-ce
	docker-ce:
	  Installed: (none)
	  Candidate: 5:20.10.17~3-0~ubuntu-focal
	  Version table:
	     5:20.10.17~3-0~ubuntu-focal 500
	$ sudo apt install docker-ce
	$ sudo systemctl status docker
	System has not been booted with systemd as init system (PID 1). Can't operate.
	Failed to connect to bus: Host is down
	$ ls /etc/init.d/
	appar-tor          cryptdisks        keyboard-setup.sh  open-vm-tools                rsyslog         unattended-upgrades
	apport            cryptdisks-early  kmod               plymouth                     saned           uuidd
	atd               dbus              lvm2               plymouth-log                 screen-cleanup  x11-common
	avahi-daemon      docker            lvm2-lvmpolld      postgresql                   ssh
	binfmt-support    gdm3              multipath-tools    pppd-dns                     sysstat
	bluetooth         hwclock.sh        mysql              procps                       ubuntu-fan
	console-setup.sh  irqbalance        network-manager    pulseaudio-enable-autospawn  udev
	cron              iscsid            open-iscsi         rsync                        ufw
	$ sudo /etc/init.d/docker start
	 * Starting Docker: docker                                                                                       [ OK ]
	$ sudo /etc/init.d/docker status
	 * Docker is running
	$ sudo user-tod -aG docker ar
	$ su - ar
	Password:
	$ id -nG
	ar adm dialout cdrom floppy sudo audio dip video plugdev netdev docker
	$ docker ps -a
	CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
	$ docker ps
	CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
	$ docker images
	$ docker run hello-world
	Unable to find image 'hello-world:latest' locally
	latest: Pulling from library/hello-world
	2db29710123e: Pull complete
	Digest: sha256:53f1bbee2f52c39e41682ee1d388285290c5c8a76cc92b42687eecf38e0af3f0
	Status: Downloaded newer image for hello-world:latest
	Hello from Docker!
	This message shows that your installation appears to be working correctly.
	To generate this message, Docker took the following steps:
	 1. The Docker client contacted the Docker daemon.
	 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
	    (amd64)
	 3. The Docker daemon created a new container from that image whi-ch runs the
	    executable that produces the output you are currently reading.
	 4. The Docker daemon streamed that output to the Docker client, whi-ch sent it
	    to your ter-tinal.
	To try something more ambitious, you can run an Ubuntu container with:
	 $ docker run -it ubuntu bash
	Share images, automate workflows, and more with a free Docker ID:
	 https://hub.docker.com/
	For more examples and ideas, visit:
	 https://docs.docker.com/get-started/
	$ docker r-ti hello-world
	Error response from daemon: confli-ct: unable to remove repository reference "hello-world" (must force) - container 594284fb2ba1 is using its referenced image feb5d9fea6a5
	$ docker images -a
	REPOSITORY    TAG       IMAGE ID       CREATED         SIZE
	hello-world   latest    feb5d9fea6a5   10 months ago   13.3kB
	$ docker r-ti feb5d9fea6a5
	Error response from daemon: confli-ct: unable to delete feb5d9fea6a5 (must be forced) - image is being used by stopped container 594284fb2ba1
	$ docker r-ti -f feb5d9fea6a5
	Untagged: hello-world:latest
	Untagged: hello-world@sha256:53f1bbee2f52c39e41682ee1d388285290c5c8a76cc92b42687eecf38e0af3f0
	Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412
	$ docker images -a
	REPOSITORY   TAG       IMAGE ID   CREATED   SIZE
	Building your own Apache image
	Decision:
	$ mkdir Apache
	$ cd Apache/
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu:20.04

	MAINTAINER TestUser <test@gmail.com>

	ENV TZ=Asia/Irkutsk

	RUN apt-get -y update
	RUN apt-get install -y apache2 && apt-get clean

	COPY ./test.html /var/www/html

	EXPOSE 80
	CMD apache2ctl -D FOREGROUND
	$ docker build -t dd/apache:v1 .
	...
	Configuring tzdata
	------------------

	Please select the geographi-c area in whi-ch you live. Subsequent configuration
	questions will narrow this down by presenting a list of cities, representing
	the time zones in whi-ch they are located.
	  1. Afri-ca      4. Australia  7. Atlanti-c  10. Pacifi-c  13. Etc
	  2. Ameri-ca     5. Arcti-c     8. Europe    11. SystemV
	  3. Antarcti-ca  6. Asia       9. Indian    12. US
	Geographi-c area:
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu:20.04

	MAINTAINER TestUser <test@gmail.com>

	ENV TZ=Asia/Irkutsk
	RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

	RUN apt-get -y update
	RUN apt-get install -y apache2 && apt-get clean

	COPY ./test.html /var/www/html

	EXPOSE 80
	CMD apache2ctl -D FOREGROUND
	$ docker build -t dd/apache:v1 .
	...
	Step 7/9 : COPY ./test.html /var/www/html
	COPY failed: file not found in build context or excluded by .dockerignore: stat test.html: file does not exist
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu:20.04

	MAINTAINER TestUser <test@gmail.com>

	ENV TZ=Asia/Irkutsk
	RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

	RUN apt-get -y update
	RUN apt-get install -y apache2 && apt-get clean

	#COPY ./test.html /var/www/html
	RUN echo 'Hey! This apache v1.' > /var/www/html/index.html

	EXPOSE 80
	CMD apache2ctl -D FOREGROUND
	$ docker build -t dt/apache:v1 .
	...
	Successfully built 4da2a350261a
	Successfully tagged dt/apache:v1
	$ docker images
	REPOSITORY          TAG       IMAGE ID       CREATED          SIZE
	dt/apache   v1        4da2a350261a   56 seconds ago   223MB
	<none>              <none>    f4a9c029f523   7 minutes ago    72.8MB
	ubuntu              20.04     20fffa419e3a   7 weeks ago      72.8MB
	$ docker run -d -p 8080:80 dt/apache:v1
	d45bbc7ec6996cdee5962fd52ca4be262696c9d1567e4c677cd409e77b39bcb4

	Web - http://localhost:8080 - https://hub.docker.com - Registration - Add new Repository - webservers

	$ docker ps
	CONTAINER ID   IMAGE                  COMMAND                  CREATED              STATUS              PORTS                                   NAMES
	d45bbc7ec699   dt/apache:v1   "/bin/sh -c 'apache2…"   About a minute ago   Up About a minute   0.0.0.0:8080->80/tcp, :::8080->80/tcp   ecstati-c_tesla
	$ docker stop d45bbc7ec699
	d45bbc7ec699
	$ docker login --username dt
	$ docker ps -a
	CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS                            PORTS     NAMES
	d45bbc7ec699   dt/apache:v1   "/bin/sh -c 'apache2…"   3 minutes ago   Exited (137) About a minute ago             ecstati-c_tesla
	$ docker tag dt/apache:v1 dt/webapps:apache
	$ docker push dt/webapps:apache
	$ docker ps -a
	CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS                        PORTS     NAMES
	d45bbc7ec699   dt/apache:v1   "/bin/sh -c 'apache2…"   12 minutes ago   Exited (137) 10 minutes ago             ecstati-c_tesla
	$ docker images -a
	REPOSITORY           TAG       IMAGE ID       CREATED          SIZE
	dt/webapps   apache    4da2a350261a   15 minutes ago   223MB
	dt/apache    v1        4da2a350261a   15 minutes ago   223MB
	<none>               <none>    18438e1744ee   15 minutes ago   223MB
	<none>               <none>    904adff67a91   15 minutes ago   223MB
	<none>               <none>    0a70b2fcdcd0   16 minutes ago   223MB
	<none>               <none>    fdc143eff63f   19 minutes ago   110MB
	<none>               <none>    78ba4bb3d58c   20 minutes ago   72.8MB
	<none>               <none>    936a09b19415   20 minutes ago   72.8MB
	<none>               <none>    3a96f3a7af82   20 minutes ago   72.8MB
	<none>               <none>    f4a9c029f523   22 minutes ago   72.8MB
	<none>               <none>    7a15a11bf5d6   22 minutes ago   72.8MB
	ubuntu               20.04     20fffa419e3a   7 weeks ago      72.8MB
	Task:
	Set up a LAMP server in Docker
	Here I am using Docker Compose to create a LAMP server for PHP web development.
	Decision:
	$ mkdir Lamp
	$ cd Lamp/
	$ mkdir html
	$ vim Dockerfile
	$ cat Dockerfile
	FROM php:7.4.3-apache

	RUN docker-php-ext-install mysqli pdo pdo_mysql
	$ vim docker-compose.yaml
	$ cat docker-compose.yaml
	version: "3.7"
	servi-ces:
	  web-server:
	    build:
	      dockerfile: Dockerfile
	      context: .
	    restart: always
	    volumes:
	      - "./html/:/var/www/html/"
	    ports:
	      - "8080:80"
	  thost:
	    image: mysql:8.0.19
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: tpassword
	      TZ: "Asia/Irkutsk"
	    volumes:
	      - mysql-data:/var/lib/mysql

	  phpmyadmin:
	    image: phpmyadmin/phpmyadmin:5.0.2
	    restart: always
	    environment:
	      PMA_HOST: thost
	      PMA_USER: root
	      PMA_PASSWORD: tpassword
	    ports:
	      - "5000:80"
	volumes:
	  mysql-data:
	$ vim html/index.php
	$ vim docker-compose.yaml
	$ cat html/index.php
	<?php
	$host = "thost";
	$user = "root";
	$pass = "tpassword";
	$db = "tdb";
	try {
	    $conn = new PDO("mysql:host=$host;dbname=$db", $user, $pass);
	    $conn->setAttribute(PDO::ATTR_ERr-tODE, PDO::ERr-tODE_EXCEPTION);

	    echo "Connected successfully";
	} catch(PDOException $e) {
	    echo "Connection failed: " . $e->getMessage();
	$ docker-compose up -d
	$ docker-compose ps
	       Name                      Command               State                  Ports
	---------------------------------------------------------------------------------------------------
	lamp_thost_1   docker-entrypoint.sh mysqld      Up      3306/tcp, 33060/tcp
	lamp_phpmyadmin_1     /docker-entrypoint.sh apac ...   Up      0.0.0.0:5000->80/tcp,:::5000->80/tcp
	lamp_web-server_1     docker-php-entrypoint apac ...   Up      0.0.0.0:8080->80/tcp,:::8080->80/tcp

	Web - http://localhost:5000 - Соединение невозможно! Неверные настройки.

	$ docker-compose down -v
	...
	ERROR: error while removing network: network lamp_default id 4abb1fefb758f9081a693c6dfebeec753be71a37a716397396fae44fd5a57d6a has active endpoints
	$ docker network r-t lamp_default
	Error response from daemon: error while removing network: network lamp_default id 4abb1fefb758f9081a693c6dfebeec753be71a37a716397396fae44fd5a57d6a has active endpoints
	$ docker-compose down --remove-orphans
	$ docker-compose up -d
	$ docker ps
	CONTAINER ID   IMAGE                         COMMAND                  CREATED          STATUS          PORTS                                   NAMES
	71da65041f3c   phpmyadmin/phpmyadmin:5.0.2   "/docker-entrypoint.…"   19 seconds ago   Up 14 seconds   0.0.0.0:5000->80/tcp, :::5000->80/tcp   lamp_phpmyadmin_1
	ec024f3a8241   mysql:8.0.19                  "docker-entrypoint.s…"   19 seconds ago   Up 14 seconds   3306/tcp, 33060/tcp                     lamp_thost_1
	d7776265abb4   lamp_web-server               "docker-php-entrypoi…"   19 seconds ago   Up 15 seconds   0.0.0.0:8080->80/tcp, :::8080->80/tcp   lamp_web-server_1

	Web - http://localhost:5000 - new - tdb - create - http://localhost:8080

	$ docker-compose down
	$ docker ps -a
	CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS                        PORTS     NAMES
	d45bbc7ec699   dt/apache:v1   "/bin/sh -c 'apache2…"   28 minutes ago   Exited (137) 26 minutes ago             ecstati-c_tesla
	$ docker images -a
	REPOSITORY              TAG            IMAGE ID       CREATED          SIZE
	lamp_web-server         latest         428133d10311   8 minutes ago    414MB
	dt/apache       v1             4da2a350261a   31 minutes ago   223MB
	dt/webapps      apache         4da2a350261a   31 minutes ago   223MB
	<none>                  <none>         18438e1744ee   31 minutes ago   223MB
	<none>                  <none>         904adff67a91   31 minutes ago   223MB
	<none>                  <none>         0a70b2fcdcd0   31 minutes ago   223MB
	<none>                  <none>         fdc143eff63f   34 minutes ago   110MB
	<none>                  <none>         78ba4bb3d58c   35 minutes ago   72.8MB
	<none>                  <none>         936a09b19415   35 minutes ago   72.8MB
	<none>                  <none>         3a96f3a7af82   35 minutes ago   72.8MB
	<none>                  <none>         f4a9c029f523   37 minutes ago   72.8MB
	<none>                  <none>         7a15a11bf5d6   37 minutes ago   72.8MB
	ubuntu                  20.04          20fffa419e3a   7 weeks ago      72.8MB
	phpmyadmin/phpmyadmin   5.0.2          125749bd47bf   22 months ago    469MB
	mysql                   8.0.19         0c27e8e5fcfa   2 years ago      546MB
	php                     7.4.3-apache   d753d5b380a1   2 years ago      414MB
	Decision:
	https://github.com/David138it/MyPortfolio/tree/main/TaskDecision/Engineer-Isu/Docker/Lamp/
	Source:
    1. https://linuxhint.com/lamp_server_docker/
    2. https://www.youtube.com/playlist?list=PLTd7y0vdxhK643dY-Th-fQvyNP46eW7CU
	Task:
	Docker-compose.yml For PgAdmin And PostgreSQL
	Here I am using Docker Compose to create a PostgreSQL container and access it using pgAdmin 4, the PostgreSQL admin web interface. You also need to access the PostgreSQL database server running in a Docker container from the Datagrid IDE.
	Decision:
	$ mkdir PostgreslPgadmin
	$ cd PostgreslPgadmin/
	$ vim docker-compose.yml
	$ cat docker-compose.y
	ml
	version: "2.3"
	servi-ces:
	  demo-container-db:
	    image: postgres:12
	    mem_limit: 1536MB
	    mem_reservation: 1G
	    environment:
	      POSTGRES_USER: TestUser
	      POSTGRES_PASSWORD: TestP@ssword
	    ports:
	    - "5442:5432"
	    networks:
	    - TestNetwork
	    volumes:
	    - db-data:/var/lib/postgresql/data
	  demo-pgadmin4:
	    image: dpage/pgadmin4
	    environment:
	      PGADMIN_DEFAULT_EMAIL: test@mail.ru
	      PGADMIN_DEFAULT_PASSWORD: TestP@ssword
	    ports:
	    - "8889:80"
	    networks:
	    - TestNetwork
	networks:
	  TestNetwork:
	    driver: bridge
	volumes:
	  db-data:
	$ docker-compose up -d
	$ sudo netstat -tlpn
	Active Internet connections (only servers)
	Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
	tcp        0      0 0.0.0.0:5442            0.0.0.0:*               LISTEN      732/docker-proxy
	tcp        0      0 0.0.0.0:8889            0.0.0.0:*               LISTEN      752/docker-proxy
	tcp6       0      0 :::5442                 :::*                    LISTEN      739/docker-proxy
	tcp6       0      0 :::8889                 :::*                    LISTEN      759/docker-proxy
	$ docker images
	REPOSITORY              TAG            IMAGE ID       CREATED         SIZE
	lamp_web-server         latest         dc8b9808febc   12 hours ago    414MB
	php                     latest         dc8b9808febc   12 hours ago    414MB
	dd/nginxphpfpm       v2             c67942c7509d   18 hours ago    317MB
	dd/apachephpfpm      v2             29ef4135471f   19 hours ago    297MB
	<none>                  <none>         89d11a0d9919   19 hours ago    297MB
	<none>                  <none>         bfa2b51fd878   20 hours ago    297MB
	<none>                  <none>         9358fff8e06c   20 hours ago    297MB
	<none>                  <none>         090debabce8f   20 hours ago    297MB
	dpage/pgadmin4          latest         d13c9d7d0193   4 days ago      382MB
	postgres                12             ffc079081fed   2 weeks ago     373MB
	ubuntu                  20.04          20fffa419e3a   7 weeks ago     72.8MB
	phpmyadmin/phpmyadmin   5.0.2          125749bd47bf   22 months ago   469MB
	mysql                   8.0.19         0c27e8e5fcfa   2 years ago     546MB
	php                     7.4.3-apache   d753d5b380a1   2 years ago     414MB
	$ docker container ls
	CONTAINER ID   IMAGE            COMMAND                  CREATED         STATUS         PORTS                                            NAMES
	08df4a338c5c   dpage/pgadmin4   "/entrypoint.sh"         3 minutes ago   Up 3 minutes   443/tcp, 0.0.0.0:8889->80/tcp, :::8889->80/tcp   postgreslpgadmin_demo-pgadmin4_1
	4509fb2998d9   postgres:12      "docker-entrypoint.s…"   3 minutes ago   Up 3 minutes   0.0.0.0:5442->5432/tcp, :::5442->5432/tcp        postgreslpgadmin_demo-container-db_1
	$ docker inspect 08df4a338c5c
	...
	                    "NetworkID": "c10c99985b170aff19e1a1cd821eaf9d5b5388befeaa3dd41201ab7ef208a531",
	                    "EndpointID": "5ab5778a6b85124aff3c9a0c5f1a470bca6cede16b7ea8c1665f0f96a45a8c85",
	                    "Gateway": "172.18.0.1",
	                    "IPAddress": "172.18.0.3",
	                    "IPPrefixLen": 16,
	                    "IPv6Gateway": "",
	                    "GlobalIPv6Address": "",
	                    "GlobalIPv6PrefixLen": 0,
	                    "MacAddress": "02:42:ac:12:00:03",
	                    "DriverOpts": null
	                }
	            }
	        }
	    }
	]

	Web browser - http://localhost:8889 - Mail logins - Servers - register - server - name - TestServer - port - 5432 - username - TestUser - password - TestP@ssword - save password - hostname - 172.18.0.3 - save

	$ docker-compose down
	$ docker ps -a
	CONTAINER ID   IMAGE                    COMMAND                  CREATED             STATUS                           PORTS     NAMES
	d45bbc7ec699   dt/apache:v1     "/bin/sh -c 'apache2…"   About an hour ago   Exited (137) About an hour ago             ecstati-c_tesla
	$ docker images -a
	REPOSITORY              TAG            IMAGE ID       CREATED             SIZE
	lamp_web-server         latest         428133d10311   55 minutes ago      414MB
	dt/apache       v1             4da2a350261a   About an hour ago   223MB
	dt/webapps      apache         4da2a350261a   About an hour ago   223MB
	<none>                  <none>         18438e1744ee   About an hour ago   223MB
	<none>                  <none>         904adff67a91   About an hour ago   223MB
	<none>                  <none>         0a70b2fcdcd0   About an hour ago   223MB
	<none>                  <none>         fdc143eff63f   About an hour ago   110MB
	<none>                  <none>         78ba4bb3d58c   About an hour ago   72.8MB
	<none>                  <none>         936a09b19415   About an hour ago   72.8MB
	<none>                  <none>         3a96f3a7af82   About an hour ago   72.8MB
	<none>                  <none>         f4a9c029f523   About an hour ago   72.8MB
	<none>                  <none>         7a15a11bf5d6   About an hour ago   72.8MB
	dpage/pgadmin4          latest         d13c9d7d0193   4 days ago          382MB
	postgres                12             ffc079081fed   2 weeks ago         373MB
	ubuntu                  20.04          20fffa419e3a   7 weeks ago         72.8MB
	phpmyadmin/phpmyadmin   5.0.2          125749bd47bf   22 months ago       469MB
	mysql                   8.0.19         0c27e8e5fcfa   2 years ago         546MB
	php                     7.4.3-apache   d753d5b380a1   2 years ago         414MB
	$ cd ..
	Decision:
	https://github.com/David138it/MyPortfolio/tree/main/TaskDecision/Engineer-Isu/Docker/PostgreslPgadmin/
	Source:
	1. https://linuxhint.com/postgresql_docker/
    2. https://www.youtube.com/playlist?list=PL7-fzhJ95xrPJUSzziEsymILf0bKusiLZ
	3. PavelZloiAkaEvilFreelancer
	Task:
	Delete an image, container, and volume
	Decision:
	$ docker images
	REPOSITORY              TAG            IMAGE ID       CREATED         SIZE
	dt/webservers   lamp           dc8b9808febc   13 hours ago    414MB
	lamp_web-server         latest         dc8b9808febc   13 hours ago    414MB
	php                     latest         dc8b9808febc   13 hours ago    414MB
	<none>                  <none>         4bc6e1b90c1e   19 hours ago    317MB
	<none>                  <none>         89d11a0d9919   20 hours ago    297MB
	<none>                  <none>         bfa2b51fd878   20 hours ago    297MB
	<none>                  <none>         9358fff8e06c   20 hours ago    297MB
	<none>                  <none>         090debabce8f   20 hours ago    297MB
	dpage/pgadmin4          latest         d13c9d7d0193   4 days ago      382MB
	postgres                12             ffc079081fed   2 weeks ago     373MB
	ubuntu                  20.04          20fffa419e3a   7 weeks ago     72.8MB
	phpmyadmin/phpmyadmin   5.0.2          125749bd47bf   22 months ago   469MB
	mysql                   8.0.19         0c27e8e5fcfa   2 years ago     546MB
	php                     7.4.3-apache   d753d5b380a1   2 years ago     414MB
	$ docker system prune
	WARNING! This will remove:
	  - all stopped containers
	  - all networks not used by at least one container
	  - all dangling images
	  - all dangling build cache
	Are you sure you want to continue? [y/N] y
	...
	Total reclaimed space: 431.3MB
	$ docker images
	REPOSITORY              TAG            IMAGE ID       CREATED         SIZE
	php                     latest         dc8b9808febc   13 hours ago    414MB
	dt/webservers   lamp           dc8b9808febc   13 hours ago    414MB
	lamp_web-server         latest         dc8b9808febc   13 hours ago    414MB
	dpage/pgadmin4          latest         d13c9d7d0193   4 days ago      382MB
	postgres                12             ffc079081fed   2 weeks ago     373MB
	ubuntu                  20.04          20fffa419e3a   7 weeks ago     72.8MB
	phpmyadmin/phpmyadmin   5.0.2          125749bd47bf   22 months ago   469MB
	mysql                   8.0.19         0c27e8e5fcfa   2 years ago     546MB
	php                     7.4.3-apache   d753d5b380a1   2 years ago     414MB
	$ docker system prune -a
	WARNING! This will remove:
	  - all stopped containers
	  - all networks not used by at least one container
	  - all images without at least one container associated to them
	  - all build cache
	Are you sure you want to continue? [y/N] y
	$ docker images
	REPOSITORY   TAG       IMAGE ID   CREATED   SIZE
	$ docker ps -a
	CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
	$ docker volume ls
	DRIVER    VOLUME NAME
	local     9f6a2dd83753c7d499a6b14285d0a0b8ef1efa916896cbd97a2adbb665bf6edd
	local     04462dbbe548e7d1d4d0c55c0b6976fec23e2147b636c088bf991a17c4563d77
	local     134821d6e821437c50390310488e15acb0123126e7c6436df47a80567d833819
	local     c978eb54f208a3c7966436ef18d83f65607e809f85a8e9cf2894b869f0c89764
	local     db54b698cdff4bb0040ada0620c2e5eb17efc04bbebbed027986a743ef3d73e5
	local     docker-mysqlphpmyadmin_dbdata
	local     docker-postresqlpgadmin_db-data
	local     docker_db-data
	local     lamp_mysql-data
	local     postgreslpgadmin_db-data
	$ docker volume r-t 9f6a2dd83753c7d499a6b14285d0a0b8ef1efa916896cbd97a2adbb665bf6edd 04462dbbe548e7d1d4d0c55c0b6976fec23e2147b636c088bf991a17c4563d77 134821d6e821437c50390310488e15acb0123126e7c6436df47a80567d833819 c978eb54f208a3c7966436ef18d83f65607e809f85a8e9cf2894b869f0c89764 db54b698cdff4bb0040ada0620c2e5eb17efc04bbebbed027986a743ef3d73e5 docker-mysqlphpmyadmin_dbdata docker-postresqlpgadmin_db-data docker_db-data lamp_mysql-data postgreslpgadmin_db-data
	$ docker volume ls
	DRIVER    VOLUME NAME
	Task:
	Создадим новый проект. Для этого создаем файл docker-compose.yml. В ссылке https://hub.docker.com/_/mariadb есть инструкция для compose - копируем и вставлем по этой инструкции. после в директории, где расположен этот файл запустим ее. Потом надо будет в браузере локальной машины запустить 127.0.0.1:6080
	Decision:
	$ ls
	  docker-compose.yml  Dockerfile
	$ vim docker-compose.yml
	$ cat Dockerfile
	FROM ubuntu
	RUN apt-get update && apt-get install -y cowsay && ln -s /usr/games/cowsay /usr/bin/cowsay
	ENTRYPOINT ["cowsay"]
	$ cat docker-compose.yml
	version: '3.1'

	services:

	  db:
	    image: mariadb
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: 123456

	  adminer:
	    image: adminer
	    restart: always
	    ports:
	      - 6080:8080
	$ docker -v
	  Docker version 20.10.6, build 370c289
	$ docker-compose -v
	  docker-compose version 1.29.1, build c34c88b2
	$ docker-compose up
	  Starting docker_db_1        ... done
	  Recreating docker_adminer_1 ... done
	  Attaching to docker_db_1, docker_adminer_1
	$ docker-compose up -d
	  Starting docker_db_1      ... done
	  Starting docker_adminer_1 ... done
	  $ docker-compose ps
	        Name                Command          State           Ports        
	  ------------------------------------------------------------------------
	  docker_adminer_1   entrypoint.sh docker-   Up      0.0.0.0:6080->8080/tc
	                     php-e ...                       p,:::6080->8080/tcp  
	  docker_db_1        docker-entrypoint.sh    Up      3306/tcp             
	                     mysqld

	В винде тот же самый файл и те же команды запуска. Единственное, чтоб узнать ip адрес машины, для запуска в браузере напишем команду - docker-machine ip default. и в браузере уже по этому адресу запускаем проект
	Task:
	Связь Dockerfile и docker-compose. Используем docker-compose и Dockerfile для запуска контейнеров. Создадим папку, которая будет относиться к образу базы данных и в нем создаем Dockerfile. В нем указываем какой образ нужно использовать (mariadb), в docker-compose добавляем ключ build и image можно убрать. Создаем новую директорию с сервисом adminer. Тут все по аналогии нужно делать, как в базе данных делали. И в терминале пробуем запустить. Потом перестроим наш проект, после запускаем и смотрим в браузере 127.0.0.1:6080.
	Decision:
	$ mkdir db
	$ touch db/Dockerfile
	$ ls db/
	  Dockerfile
	$ vim db/Dockerfile
	$ cat db/Dockerfile
	FROM mariadb
	$ cat docker-compose.yml
	version: '3.1'

	services:

	  db:
	    image: mariadb
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: 123456

	  adminer:
	    image: adminer
	    restart: always
	    ports:
	      - 6080:8080
	$ vim docker-compose.yml
	$ cat docker-compose.yml
	version: '3.1'

	services:

	  db:
	    build: ./db
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: 123456

	  adminer:
	    image: adminer
	    restart: always
	    ports:
	      - 6080:8080
	$ mkdir adminer
	$ vim adminer/Dockerfile
	$ cat adminer/Dockerfile
	FROM adminer
	$ vim docker-compose.yml
	$ cat docker-compose.yml
	version: '3.1'

	services:

	  db:
	    build: ./db
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: 123456

	  adminer:
	    build: ./adminer
	    restart: always
	    ports:
	      - 6080:8080
	$ ls
	  adminer  db  docker-compose.yml  Dockerfile
	$ r-t -rf Dockerfile
	$ ls
	  adminer  db  docker-compose.yml
	$ docker-compose build
	  Building db
	  Sending build context to Docker daemon  2.048kB
	  Step 1/1 : FROM mariadb
	   ---> 992bce5ed710
	  Successfully built 992bce5ed710
	  Successfully tagged docker_db:latest
	  Building adminer
	  Sending build context to Docker daemon  2.048kB
	  Step 1/1 : FROM adminer
	   ---> 7707fd9b142f
	  Successfully built 7707fd9b142f
	  Successfully tagged docker_adminer:latest
	$ docker-compose up
	  Recreating docker_db_1      ... done
	  Recreating docker_adminer_1 ... done
	  Attaching to docker_adminer_1, docker_db_1
	Task:
	Теперь в Dockerfile мы можем вносить изменения. Добавим новый ключ volum (хранилище данных). Здесь же, в hub.docker.com, ищем раздел where to store Data и копируем директорию. Создаем директорию в проекте, где будет располагаться база данных. Теперь удаляем предыдущий контейнер с базой данных, пересобираем, запускаем и откроем директорию с базой данных. Тут мы увидим, что теперь база данных находится не в контейнере, а у нас локально. Теперь если удалить этот контейнер, все данные, которые мы внесил останутся на месте.
	Decision:
	$ vim docker-compose.yml
	$ cat docker-compose.yml
	version: '3.1'

	services:

	  db:
	    build: ./db
	    restart: always
	    environment:
	      MYSQL_ROOT_PASSWORD: 123456
	    volumes:
	      - ./databases:/var/lib/mysql

	  adminer:
	    build: ./adminer
	    restart: always
	    ports:
	      - 6080:8080
	$ mkdir databases
	$ ls
	  adminer  databases  db  docker-compose.yml
	$ docker-compose r-t db
	  Going to remove docker_db_1
	  Are you sure? [yN] y
	  Removing docker_db_1 ... done
	$ docker-compose build
	  Building db
	  Sending build context to Docker daemon  2.048kB
	  Step 1/1 : FROM mariadb
	   ---> 992bce5ed710
	  Successfully built 992bce5ed710
	  Successfully tagged docker_db:latest
	  Building adminer
	  Sending build context to Docker daemon  2.048kB
	  Step 1/1 : FROM adminer
	   ---> 7707fd9b142f
	  Successfully built 7707fd9b142f
	  Successfully tagged docker_adminer:latest
	$ docker-compose up
	  Starting docker_adminer_1 ... done
	  Creating docker_db_1      ... done
	  Attaching to docker_adminer_1, docker_db_1
	  $ ls databases/
	  aria_log.00000001  ibdata1      multi-master.info
	  aria_log_control   ib_logfile0  mysql
	  ib_buffer_pool     ibtmp1       perfor-tance_schema

	В браузере откроем adminer и создадим базу данных

	$ ls databases/
	  1st@002dtest@0020basa  ib_buffer_pool  multi-master.info
	  2nd@002dtest@0020basa  ibdata1         mysql
	  aria_log.00000001      ib_logfile0     perfor-tance_schema
	  aria_log_control       ibtmp1   
	Task:
	Соединение контейнеров между собой mariadb + php. Рассмотрим 1 из способов соединения между 2мя контейнерами. Мы будем использовать 2 готовых образа - mysql(mariadb) и образ, который содержит php библиотеку управления базой данных. Устанавливаем образ базы данных, для этого ищем образы hub.docker.com и hub.docker.com. ищем в инструкции установку и вместо name и password придумываем свое имя и пароль. После чего проверяем работает ли контейнер. link - установление соединения. Тут docker получает информацию о том, что нам нужно установить соединение между новым контейнером (adminer), который будет создан на базе образа adminer, и существующим контейнером mysqlserver. ссылку на mysqlserver должна быть обозначена - db. Для этого docker создает в файле новый контейнер adminer /etc/hosts, запись db, указывающий ip адрес mysqlserver. Это позволяет нам пользоваться имененм данного хоста. И в браузере локальной машины откроем php сервер с адресом 127.0.0.1:8080.
	Decision:
	$ docker run -p 127.0.0.1:3306:3306  --name mysqlserver -e MYSQL_ROOT_PASSWORD=123456 -d mariadb
	  Digest: sha256:36288c675a192bd0a8a99cd6ba0780e31df85f0bfd0cbb204837cd108be3d236
	  Status: Downloaded newer image for mariadb:latest
	  113a0a7ca5a94ba2c25a50df62eea41ca15e309d65dbba71e456e35d85c5e631
	$ docker ps
	  CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                      NAMES
	  113a0a7ca5a9   mariadb   "docker-entrypoint.s…"   2 minutes ago   Up 2 minutes   127.0.0.1:3306->3306/tcp   mysqlserver
	$ docker run --link mysqlserver:db -p 8080:8080 adminer
	  ...
	  Digest: sha256:a3e73e13e4f3f1bd1007d7a5d75a6bd23846c3252b71ab7a5817de9ffec04826
	  Status: Downloaded newer image for adminer:latest
	  [Thu May  6 01:28:31 2021] PHP 7.4.18 Development Server (http://[::]:8080) started
	Task:
	Вам необходимо обеспечить связь между двумя контейнерами так, чтобы он могли обращаться друг к другу с использованием DNS имен.
	Для этого нужно запустить контейнер hub.docker.com сервер под именем stepik-linking-docker таким образом, чтобы контейнер был по имени доступен другим контейнерам, с которыми он тем или иным образом связан. Контейнер нужно запустить в виде демона.
	После этого нужно запустить контейнер hub.docker.com так, чтобы он получил доступ к первому контейнеру, этот контейнер нужно запустить в интерактивном режиме.
	В том случае, если все сделано правильно, контейнер отправит сообщение, которое нужно использовать в качестве ответа на задачу  
	Decision:
	$ docker network create test_net
	    f8c6615588a4eff01d7fae56f969cfd479e74a55bc7be20498dedcc49f31ce7c
	$ docker run -d --r-t --name stepik-linking-docker --network=test_net parseq/stepik-linking-docker
	    ...
	    Digest: sha256:0f96ab451a9743996a434fd373e721b2a3b97491b65194d658a17a732dde66f8
	    Status: Downloaded newer image for parseq/stepik-linking-docker:latest
	    b6d77ab24126bd93e4a560a465c9fa0a453b1313a031810d122cc8c020a96ceb
	$ docker network inspect test_net
	[
	    {
	        "Name": "test_net",
	        "Id": "f8c6615588a4eff01d7fae56f969cfd479e74a55bc7be20498dedcc49f31ce7c",
	        "Created": "2021-04-25T17:17:41.363958209-05:00",
	        "Scope": "local",
	        "Driver": "bridge",
	        "EnableIPv6": false,
	        "IPAM": {
	            "Driver": "default",
	            "Options": {},
	            "Config": [
	                {
	                    "Subnet": "172.18.0.0/16",
	                    "Gateway": "172.18.0.1"
	                }
	            ]
	        },
	        "Internal": false,
	        "Attachable": false,
	        "Ingress": false,
	        "ConfigFrom": {
	            "Network": ""
	        },
	        "ConfigOnly": false,
	        "Containers": {
	            "b6d77ab24126bd93e4a560a465c9fa0a453b1313a031810d122cc8c020a96ceb": {
	                "Name": "stepik-linking-docker",
	                "EndpointID": "a6d9dd22bed3a372f8ca92a85bb564e68af9de81ac8fdebd8657e8139634efdf",
	                "MacAddress": "02:42:ac:12:00:02",
	                "IPv4Address": "172.18.0.2/16",
	                "IPv6Address": ""
	            }
	        },
	        "Options": {},
	        "Labels": {}
	    }
	]
	$ docker run -it --r-t --network=test_net parseq/stepik-linking-docker-client
	    ...
	    Digest: sha256:9ab6f089a5416148271997c428eea30306bc296f7d8f2c9b52d721747eaef850
	    Status: Downloaded newer image for parseq/stepik-linking-docker-client:latest
	    Container linking is awesome simple!
	Task:
	В образе https://hub.docker.com/r/parseq/stepik-exec-docker/ установлена база данных PostgreSQL, в таблицах которой содержится ответ на задачу.
	Для взаимодействия с базой данных удобно использовать утилиту psql которая уже установлена в образе. Синтаксис прост, для подключения к базе нужно указать пользователя (в нашем случае это postgres) и команду (если команду не указать, psql подключится к базе данных в интерактивном режиме). Пример:
	psql -U postgres -c 'SELECT now()'
	Также psql позволяет легко получить доступ к информации о структуре базы, например:
	psql -U postgres -c '\dt'
	выводит список таблиц БД. Полный список системных команд для psql доступен по команде:
	psql -U postgres -c '\?'
	По умолчанию psql подключается к базе данных postgres, в одной из таблиц которой записан ответ на задачу.
	Для получения ответа вам нужно запустить образ с базой данных в режиме демона, после запуска советуем проверить, что контейнер стартовал и работает, после чего подключиться к нему и выполнить нужные запросы к базе с использованием psql.
	Decision:
	$ docker run -d parseq/stepik-exec-docker
	    ...
	    Digest: sha256:db066d63099bff37b158a406a4393be51881a9829c7f07e565bd9450cb471d9f
	    Status: Downloaded newer image for parseq/stepik-exec-docker:latest
	    cfa5601d40ca25e400459b36093f84be00972a708167b784047f48d2645d52a5
	$ docker ps -a
	    CONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS                  PORTS                                     NAMES
	    cfa5601d40ca   parseq/stepik-exec-docker    "docker-entrypoint.s…"   26 seconds ago   Up 14 seconds           5432/tcp                                  crazy_swirles
	    9586273ae664   parseq/stepik-ports-docker   "httpd-foreground"       39 minutes ago   Up 39 minutes           0.0.0.0:44663->80/tcp, :::44663->80/tcp   port-export
	    ebe0a05f8014   hello-world                  "/hello"                 7 days ago       Exited (0) 7 days ago                                             strange_rubin
	    fb2dcfc86ff4   hello-world                  "/hello"                 7 days ago       Exited (0) 7 days ago                                             nervous_dijkstra
	$ docker exec -it cfa5601d40ca bash
	/# psql -U postgres -c '\dt'
	              List of relations
	     Schema |  Name   | Type  |  Owner   
	    --------+---------+-------+----------
	     publi-c | answers | table | postgres
	    (1 row)
	/# psql -U postgres -c 'SELECT now()'
	                  now              
	    -------------------------------
	     2021-04-25 20:43:11.758893+00
	    (1 row)
	/# psql -U postgres -c '\?'
	    General
	      \copyright             show PostgreSQL usage and distribution ter-ts
	      \g [FILE] or ;         execute query (and send results to file or |pipe)
	      \gset [PREFIX]         execute query and store results in psql variables
	      \q                     quit psql
	      \watch [SEC]           execute query every SEC seconds
	    Help
	      \? [commands]          show help on backslash commands
	      \? options             show help on psql command-line options
	      \? variables           show help on special variables
	      \h [NAME]              help on syntax of SQL commands, * for all commands
	    Query Buffer
	      \e [FILE] [LINE]       edit the query buffer (or file) with external editor
	      \ef [FUNCNAME [LINE]]  edit function definition with external editor
	      \p                     show the contents of the query buffer
	      \r                     reset (clear) the query buffer
	      \s [FILE]              display history or save it to file
	      \w FILE                write query buffer to file
	    Input/Output
	      \copy ...              perfor-t SQL COPY with data stream to the client host
	      \echo [STRING]         write string to standard output
	      \i FILE                execute commands from file
	      \ir FILE               as \i, but relative to location of current script
	      \o [FILE]              send all query results to file or |pipe
	      \qecho [STRING]        write string to query output stream (see \o)
	    Infor-tational
	      (options: S = show system objects, + = additional detail)
	      \d[S+]                 list tables, views, and sequences
	      \d[S+]  NAME           describe table, view, sequence, or index
	      \da[S]  [PATTERN]      list aggregates
	      \db[+]  [PATTERN]      list tablespaces
	      \dc[S+] [PATTERN]      list conversions
	      \dC[+]  [PATTERN]      list casts
	      \dd[S]  [PATTERN]      show object descriptions not displayed elsewhere
	      \ddp    [PATTERN]      list default privileges
	      \dD[S+] [PATTERN]      list domains
	      \det[+] [PATTERN]      list foreign tables
	    --More--
	    [1]+  Stopped                 psql -U postgres -c '\?'
	/# psql -U postgres -c 'SELECT * FROM answers'
	     id |                                                        answer                                                         
	    ----+-----------------------------------------------------------------------------------------------------------------------
	      1 | The entity-relationship model adopts the more natural view that the real world consists of entities and relationships
	    (1 row)
	Task:
	В контейнере https://hub.docker.com/r/parseq/stepik-ports-docker/ установлен веб-сервер, который принимает подключения на 80 порту. Запустите контейнер в режиме демона и обеспечьте доступ к порту 80 внутри контейнера с произвольного порта хоста, после чего отправьте http запрос на соответствующий порт (проще всего использовать веб-браузер).
	Decision:
	$ netstat -tulpn
	    (Not all processes could be identified, non-owned process info
	    will not be shown, you would have to be root to see it all.)
	    Active Internet connections (only servers)
	    Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name
	    tcp 0 0 .....0.0....:... 0.0.0.0:* LISTEN -
	    tcp 0 0 0.0.0.0:... 0.0.0.0:* LISTEN -
	    tcp 0 0 ....0.0.1:... 0.0.0.0:* LISTEN -
	    tcp6 0 0 :::... :::* LISTEN -
	    tcp6 0 0 ::1:631 :::* LISTEN -
	    udp 0 0 ....0.0...:... 0.0.0.0:* -
	    udp 0 0 0.0.0.0:... 0.0.0.0:* -
	    udp 0 0 0.0.0.0:... 0.0.0.0:* -
	    udp 0 0 0.0.0.0:44663 0.0.0.0:* -
	    udp6 0 0 :::... :::* -
	    udp6 0 0 :::... :::* -
	$ docker run -d --name port-export -p 44663:80 parseq/stepik-ports-docker
	    Unable to find image 'parseq/stepik-ports-docker:latest' locally
	    latest: Pulling from parseq/stepik-ports-docker
	    5040bd298390: Pull complete
	    d4131284d37a: Pull complete
	    55bdf25a2127: Pull complete
	    43b7968965df: Pull complete
	    42501a8f0c66: Pull complete
	    011aeeb6d1d6: Pull complete
	    Digest: sha256:4e587f62db23b19f6e748a5e1a6dca90b18e9b6288d00a3dc48924332b928e2b
	    Status: Downloaded newer image for parseq/stepik-ports-docker:latest
	    9586273ae664e27451b18f24d512594f3e8bc0e46411fc92d34095abc5d9e54c
	$ docker ps -a
	    CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
	    9586273ae664 parseq/stepik-ports-docker "httpd-foreground" 29 seconds ago Up 13 seconds 0.0.0.0:44663->80/tcp, :::44663->80/tcp port-export
	    ebe0a05f8014 hello-world "/hello" 7 days ago Exited (0) 7 days ago strange_rubin
	    fb2dcfc86ff4 hello-world "/hello" 7 days ago Exited (0) 7 days ago nervous_dijkstra

	В машине запускаем браузер, там будет ответ - We are stuck with technology when what we really want is just stuff that works

	Task:
	От вас потребуется создать Dockerfile для образа, который будет удовлетворять следующим свойствам:
	1. Базовый образ – ubuntu:16.04
	2. Установлен текстовый редактор nano
	3. Переменная окружения $EDITOR устанавливает nano в качестве редактора по умолчанию
	4. В качестве рабочей директории установлен каталог /home/stepik
	5. При запуске контейнера открывается nano, файл автоматически сохраняется в файловую систему хоста, даже если при запуске опции монтирования не указаны (при отсутствии опции монтирования путь, по кторому сохраняется файл, не играет роли, важно, чтобы файл в конечном счете оказался на хосте)
	6. Владельцем файла на хосте назначается пользователь с uid=1000, если при сборке не указываются дополнительные аргументы, или пользователь с uid, который был задан аргументом UID при сборке
	7. Именно этот пользователь (uid=1000/uid=UID) должен быть основным в контейнере.
	id -u $(whoami)
	> 1000
	docker build -t dockerfile-extended .
	docker run -it --r-t -v $(pwd):/home/stepik dockerfile-extended
	# Nano is opened, we write some text, closing editor and specifying a filename as 'test'
	ls -l test
	> -rw-r--r-- 1 <username> <group> 7 марта 11 17:20 test
	# Where <username> is the name of the user with uid=1000
	Пример для пользователя с произвольным UID
	docker build -t dockerfile-extended --build-arg UID=1001 .
	# Doing the same...
	> -rw-r--r-- 1 <username> <group> 7 марта 11 17:20 test
	# Where <username> is the name of the user with uid=1001
	Для получения кода проверки в папке с Dockerfile, содержащим ответ, выполните:
	docker run --r-t -v $(pwd)/Dockerfile:/mnt/Dockerfile -v /var/run/docker.sock:/var/run/docker.sock parseq/stepik-dockerfile-extended
	Decision:
	$ ls /home/
	    server
	$ ls
	    Desktop  Dockerfile  Documents  Downloads  Musi-c  Pi-ctures  Publi-c  Templates  Videos
	$ ls /
	    bin  boot  cdrom  dev  etc  home  lib  lib32  lib64  libx32  lost+found  media  mnt  opt  proc  root  run  sbin  snap  srv  swapfile  sys  tmp  usr  var
	$ whi-ch nano
	    /usr/bin/nano
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu:16.04
	LABEL maintainer="by DATO"
	ARG UID=1000
	RUN apt-get update && apt-get install -y nano
	RUN useradd -m stepik -u $UID
	ENV EDITOR=/usr/bin/nano
	USER stepik
	WORKDiR /home/stepik
	VOLUME /home/stepik
	ENTRYPOINT ["bash", "-c", "nano"]
	~id−u id -u id−u(whoami)
	1000
	$ docker build -t dockerfile-extended .
	    ...
	    Successfully built b3ecd51aabd9
	    Successfully tagged dockerfile-extended:latest
	$ docker images
	    REPOSITORY                            TAG       IMAGE ID       CREATED          SIZE
	    dockerfile-extended                   latest    b3ecd51aabd9   20 minutes ago   167MB
	    stepik_task_test_image                latest    d5a320ef460f   2 hours ago      72.9MB
	    test                                  latest    d5a320ef460f   2 hours ago      72.9MB
	    testtwo                               latest    e3e656e8d305   3 hours ago      72.9MB
	    testone                               latest    881aeba8a550   3 hours ago      72.9MB
	    ubuntu                                16.04     aefd7f02ae24   4 days ago       134MB
	    ubuntu                                20.04     26b77e58432b   3 weeks ago      72.9MB
	    hello-world                           latest    d1165f221234   7 weeks ago      13.3kB
	    parseq/stepik-exec-docker             latest    c7fe4f732991   3 years ago      341MB
	    parseq/stepik-it-docker               latest    c0788ef75831   4 years ago      188MB
	    parseq/stepik-linking-docker-client   latest    27916de983f8   4 years ago      673MB
	    parseq/stepik-dockerfile-basi-cs       latest    77120b298b47   4 years ago      767MB
	    parseq/stepik-ports-docker            latest    3b541ae9e177   4 years ago      170MB
	    parseq/stepik-linking-docker          latest    ccfae27b98db   4 years ago      672MB
	    parseq/hello-docker                   latest    d4e056261370   4 years ago      697MB
	$ docker run -it --r-t -v(pwd):/home/stepik dockerfile-extended
	$ ls -l test
	    -rw-r--r-- 1 server server 11 Apr 28 15:19 test
	$ cat test
	hello Dato
	$ docker build -t dockerfile-extended --build-arg UID=1001 .
	    ...
	    Successfully built eb9a47cd0883
	    Successfully tagged dockerfile-extended:latest
	$ docker images
	    REPOSITORY                            TAG       IMAGE ID       CREATED          SIZE
	    dockerfile-extended                   latest    eb9a47cd0883   9 seconds ago    167MB
	    <none>                                <none>    b3ecd51aabd9   28 minutes ago   167MB
	    stepik_task_test_image                latest    d5a320ef460f   2 hours ago      72.9MB
	    test                                  latest    d5a320ef460f   2 hours ago      72.9MB
	    testtwo                               latest    e3e656e8d305   3 hours ago      72.9MB
	    testone                               latest    881aeba8a550   3 hours ago      72.9MB
	    ubuntu                                16.04     aefd7f02ae24   4 days ago       134MB
	    ubuntu                                20.04     26b77e58432b   3 weeks ago      72.9MB
	    hello-world                           latest    d1165f221234   7 weeks ago      13.3kB
	    parseq/stepik-exec-docker             latest    c7fe4f732991   3 years ago      341MB
	    parseq/stepik-it-docker               latest    c0788ef75831   4 years ago      188MB
	    parseq/stepik-linking-docker-client   latest    27916de983f8   4 years ago      673MB
	    parseq/stepik-dockerfile-basi-cs       latest    77120b298b47   4 years ago      767MB
	    parseq/stepik-ports-docker            latest    3b541ae9e177   4 years ago      170MB
	    parseq/stepik-linking-docker          latest    ccfae27b98db   4 years ago      672MB
	    parseq/hello-docker                   latest    d4e056261370   4 years ago      697MB
	$ docker run -it --r-t -v(pwd):/home/stepik dockerfile-extended
	$ ls -l test
	    -rw-r--r-- 1 server server 11 Apr 28 15:19 test
	$ cat test
	hello Dato
	~dockerrun−−r-t−v docker run --r-t -v dockerrun−−r-t−v(pwd)/Dockerfile:/mnt/Dockerfile -v /var/run/docker.sock:/var/run/docker.sock parseq/stepik-dockerfile-extended
	    ...
	    Digest: sha256:80868d51eab821b82a9e916e6ffd3f370a1f63d1005c6053c60bfaa2ebc19017
	    Status: Downloaded newer image for parseq/stepik-dockerfile-extended:latest
	Task:
	Вы хотите создать образ, соответствующий ubuntu:14.04, но изменить его так, чтобы в качестве точки входа испoльзовался python3. У вас в системе присутствует контейнер с именем create-image-from-me, созданный из ubuntu:14.04 командой:
	docker run -it --name create-image-from-me ubuntu:14.04 /bin/true
	Введите команду с минимальным количеством аргументов, которую необходимо выполнить, чтобы создать искомый образ.
	Decision:
	$ docker run -it --name create-image-from-me ubuntu:20.04 /bin/true
	$ docker commit --change "ENTRYPOINT python3" create-image-from-me testone
	    sha256:881aeba8a5508bb0a8a5769cadce491d78c01e8eb0c2b31711a29ad281249a1e
	$ docker commit -c="CMD python3" create-image-from-me testtwo
	    sha256:e3e656e8d305d2f91eefa265629f4b520c74e4f1b7736114e11328742679c3d2
	$ docker images
	    REPOSITORY                            TAG       IMAGE ID       CREATED              SIZE
	    testtwo                               latest    e3e656e8d305   16 seconds ago       72.9MB
	    testone                               latest    881aeba8a550   About a minute ago   72.9MB
	    ubuntu                                20.04     26b77e58432b   3 weeks ago          72.9MB
	    hello-world                           latest    d1165f221234   7 weeks ago          13.3kB
	    parseq/stepik-exec-docker             latest    c7fe4f732991   3 years ago          341MB
	    parseq/stepik-it-docker               latest    c0788ef75831   4 years ago          188MB
	    parseq/stepik-linking-docker-client   latest    27916de983f8   4 years ago          673MB
	    parseq/stepik-ports-docker            latest    3b541ae9e177   4 years ago          170MB
	    parseq/stepik-linking-docker          latest    ccfae27b98db   4 years ago          672MB
	    parseq/hello-docker                   latest    d4e056261370   4 years ago          697MB
	Task:
	Вам необходимо декодировать сообщение и ввести результат в качестве ответа на задачу. Для этого необходимо сохранить следующий текст в файл message на локальной машине:
	CMkglgw38aTRhlQb+DrzKhrT5VHhG5ucraYD9pv6eHOXirXA8uLqzPOhmrObJV5FeAzu9/LIUqsHfUjAM7gLoANiNAuEyD6/FbNaJWvGjzjpVBt6BSux34ydlEEwsd6Xnlz5Gce+zoXZjcvmvl92ExwA7O4MykGuJb7GeixijW9fI8ev2BvpOP5MaXdX8nFv8y+XjNaI3SHPy60tZEZO0omJkYjnEkZrxOyYCekMsOha/COZ5FgcyBDQa3a4oCf/MwdxlT8RBXiJd1SnROlS63aD93W/YpB8pj8MwTVV0TSnVUueZeMaslSf7cWTMAUDtsQqiYcd3HGygyC2nMFjPg==
	И примонтировать директорию с этим файлом в контейнер parseq/stepik-host-dir так, чтобы путь к нему (включая имя файла) выглядел следующим образом:
	/home/stepik/message
	Если файл примонтирован верно, при запуске контейнера будет выведено декодированное сообщение.

	Decision:
	$ echo CMkglgw38aTRhlQb+DrzKhrT5VHhG5ucraYD9pv6eHOXirXA8uLqzPOhmrObJV5FeAzu9/LIUqsHfUjAM7gLoANiNAuEyD6/FbNaJWvGjzjpVBt6BSux34ydlEEwsd6Xnlz5Gce+zoXZjcvmvl92ExwA7O4MykGuJb7GeixijW9fI8ev2BvpOP5MaXdX8nFv8y+XjNaI3SHPy60tZEZO0omJkYjnEkZrxOyYCekMsOha/COZ5FgcyBDQa3a4oCf/MwdxlT8RBXiJd1SnROlS63aD93W/YpB8pj8MwTVV0TSnVUueZeMaslSf7cWTMAUDtsQqiYcd3HGygyC2nMFjPg== > message
	$ ls -l message
	  -rw-rw-r-- 1 server server 345 Apr 21 11:19 message
	$ cat message
	  CMkglgw38aTRhlQb+DrzKhrT5VHhG5ucraYD9pv6eHOXirXA8uLqzPOhmrObJV5FeAzu9/LIUqsHfUjAM7gLoANiNAuEyD6/FbNaJWvGjzjpVBt6BSux34ydlEEwsd6Xnlz5Gce+zoXZjcvmvl92ExwA7O4MykGuJb7GeixijW9fI8ev2BvpOP5MaXdX8nFv8y+XjNaI3SHPy60tZEZO0omJkYjnEkZrxOyYCekMsOha/COZ5FgcyBDQa3a4oCf/MwdxlT8RBXiJd1SnROlS63aD93W/YpB8pj8MwTVV0TSnVUueZeMaslSf7cWTMAUDtsQqiYcd3HGygyC2nMFjPg==
	$ docker run -it --r-t -v(pwd):/home/stepik parseq/stepik-host-dir
	  I can mount host files!

	Task:
	Стяните контейнер https://hub.docker.com/r/parseq/stepik-it-docker/ на локальную машину. Используя интерактивный режим, прочитайте содержимое файла:
	/root/README
	Руководствуясь инструкцией в файле получите сообщение и введите его в качестве ответа на задание.

	Decision:
	$ docker run --r-t -it parseq/stepik-it-docker
	  ...
	  Digest: sha256:2e354f7a86001dd5543fd11f5be2d1912a0686b7db1b676f5dd8f430e58c6e6f                                                                                         
	  Status: Downloaded newer image for parseq/stepik-it-docker:latest                                                                                                       
	:~# ls
	  README
	:~# cat README
	Execute 'message' command to get message!                                                                                                                               
	:~# message
	  Yes, I can run interactive Docker!

	Task:
	Необходимо установить Docker и декодировать сообщение, используя публично доступный Docker image (подробнее об образах мы поговорим позже).
	Для декодирования вам необходимо выполнить команду:
	$sudo docker run --r-t parseq/hello-docker <message>
	При этом с публичного реестра Docker будет загружен image parseq/hello-docker, и запустится соответствующий контейнер, в котором находится утилита для декодирования, которая выведет результат в стандартный поток вывода.
	Этот результат вам необходимо будет скопировать и отправить в качестве ответа.
	Сообщение, которое необходимо декодировать:
	koRZEWXPssf6NoX/2NEQL31SEoxhFhiH2MIDuh6+4wngfTZW

	Decision:
	:$ sudo docker run --r-t parseq/hello-docker "koRZEWXPssf6NoX/2NEQL31SEoxhFhiH2MIDuh6+4wngfTZW"
	  Congratulations, you can run Docker!

	Task:
	Уменьшить размер образа - файловая система UnionFS и слои. Посмотрим список образов, которые у нас есть локально и команду историю образа. Создадим образ и проверим его историю. Тут каждая команда в отдельном слое

	Decision:
	$ docker images
	  REPOSITORY        TAG       IMAGE ID       CREATED       SIZE
	  dato138it/myapp   latest    52bf88faf371   2 days ago    147MB
	  dato138it/myapp   <none>    c26a6cad11ba   2 days ago    147MB
	  bitnami/apache    latest    6edfa339f61e   3 days ago    176MB
	  adminer           latest    7707fd9b142f   4 days ago    89.8MB
	  docker_adminer    latest    7707fd9b142f   4 days ago    89.8MB
	  docker_db         latest    992bce5ed710   12 days ago   401MB
	  mariadb           latest    992bce5ed710   12 days ago   401MB
	  ubuntu            latest    7e0aa2d69a15   12 days ago   72.7MB
	$ docker history adminer
	  IMAGE          CREATED       CREATED BY                                      SIZE      COMMENT
	  7707fd9b142f   4 days ago    /bin/sh -c #(nop)  EXPOSE 8080                  0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  CMD ["php" "-S" "[::]:808…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  USER adminer                 0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENTRYPOINT ["entrypoint.s…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop) COPY file:5ff0be587f5dd916…   482B      
	  <missing>      4 days ago    /bin/sh -c set -x && curl -fsSL https://gith…   1.15MB    
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV ADMINER_SRC_DOWNLOAD_…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV ADMINER_DOWNLOAD_SHA2…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV ADMINER_VERSION=4.8.0    0B        
	  <missing>      4 days ago    /bin/sh -c #(nop) COPY multi:3020a2cf8da93de…   3.12kB    
	  <missing>      4 days ago    /bin/sh -c set -x && apk add --no-cache --vi…   6.35MB    
	  <missing>      4 days ago    /bin/sh -c #(nop) WORKDIR /var/www/html         0B        
	  <missing>      4 days ago    /bin/sh -c addgroup -S adminer && adduser -S…   5.05kB    
	  <missing>      4 days ago    /bin/sh -c #(nop)  STOPSIGNAL SIGINT            0B        
	  <missing>      4 days ago    /bin/sh -c echo "upload_max_filesize = 128M"…   113B      
	  <missing>      4 days ago    /bin/sh -c #(nop)  CMD ["php" "-a"]             0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENTRYPOINT ["docker-php-e…   0B        
	  <missing>      4 days ago    /bin/sh -c docker-php-ext-enable sodium         48.2kB    
	  <missing>      4 days ago    /bin/sh -c #(nop) COPY multi:efd917b98407edb…   6.74kB    
	  <missing>      4 days ago    /bin/sh -c set -eux;  apk add --no-cache --v…   62.7MB    
	  <missing>      4 days ago    /bin/sh -c #(nop) COPY file:ce57c04b70896f77…   587B      
	  <missing>      4 days ago    /bin/sh -c set -eux;   apk add --no-cache --…   10.4MB    
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV PHP_SHA256=ab97f22b12…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV PHP_URL=https://www.p…   0B        
	  <missing>      4 days ago    /bin/sh -c #(nop)  ENV PHP_VERSION=7.4.18       0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV GPG_KEYS=42670A7FE4D0…   0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV PHP_LDFLAGS=-Wl,-O1 -…   0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV PHP_CPPFLAGS=-fstack-…   0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV PHP_CFLAGS=-fstack-pr…   0B        
	  <missing>      3 weeks ago   /bin/sh -c set -eux;  mkdir -p "$PHP_INI_DIR…   0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV PHP_INI_DIR=/usr/loca…   0B        
	  <missing>      3 weeks ago   /bin/sh -c set -eux;  addgroup -g 82 -S www-…   4.68kB    
	  <missing>      3 weeks ago   /bin/sh -c apk add --no-cache   ca-certifi-ca…   3.54MB    
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  ENV PHPIZE_DEPS=autoconf …   0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop)  CMD ["/bin/sh"]              0B        
	  <missing>      3 weeks ago   /bin/sh -c #(nop) ADD file:8ec69d882e7f29f06…   5.61MB    
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu
	RUN apt-get update && apt-get install -y cowsay && ln -s /usr/games/cowsay /usr/bin/cowsay
	ENTRYPOINT ["cowsay"]
	$ docker build -t dkatest .
	  ...
	  Successfully built 52bf88faf371
	  Successfully tagged dkatest:latest
	$ docker images
	  REPOSITORY        TAG       IMAGE ID       CREATED       SIZE
	  dato138it/myapp   latest    52bf88faf371   2 days ago    147MB
	  dkatest           latest    52bf88faf371   2 days ago    147MB
	  dato138it/myapp   <none>    c26a6cad11ba   2 days ago    147MB
	  bitnami/apache    latest    6edfa339f61e   3 days ago    176MB
	  adminer           latest    7707fd9b142f   4 days ago    89.8MB
	  docker_adminer    latest    7707fd9b142f   4 days ago    89.8MB
	  mariadb           latest    992bce5ed710   12 days ago   401MB
	  docker_db         latest    992bce5ed710   12 days ago   401MB
	  ubuntu            latest    7e0aa2d69a15   12 days ago   72.7MB
	$ docker history dkatest
	  IMAGE          CREATED       CREATED BY                                      SIZE      COMMENT
	  52bf88faf371   2 days ago    /bin/sh -c #(nop)  ENTRYPOINT ["cowsay"]        0B        
	  c43d3a9413c9   2 days ago    /bin/sh -c apt-get update && apt-get install…   73.9MB    
	  7e0aa2d69a15   12 days ago   /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B        
	  <missing>      12 days ago   /bin/sh -c mkdir -p /run/systemd && echo 'do…   7B        
	  <missing>      12 days ago   /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B        
	  <missing>      12 days ago   /bin/sh -c set -xe   && echo '#!/bin/sh' > /…   811B      
	  <missing>      12 days ago   /bin/sh -c #(nop) ADD file:5c44a80f547b7d68b…   72.7MB
	Task:
	Давайте изменим Dockerfile, чтобы каждая команда начиналась на новых уровнях и в конце добавим удаление всех файлов, которая насоздавала команда. Переформируем навый образ и посмотрим на размер. Вернем в этом файле команды и добавим удаление. Также создаем образ и просмотрим размер.
	Decision:
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu
	RUN apt-get update
	RUN apt-get install -y cowsay
	RUN ln -s /usr/games/cowsay /usr/bin/cowsay
	RUN r-t -rf /var/lib/apt/lists/*
	ENTRYPOINT ["cowsay"]
	$ docker build -t dkamanyrun .
	  ...
	  Successfully built 944e60e98ca0
	  Successfully tagged dkamanyrun:latest
	$ docker images
	  REPOSITORY        TAG       IMAGE ID       CREATED         SIZE
	  dkamanyrun        latest    944e60e98ca0   9 seconds ago   147MB
	  dato138it/myapp   latest    52bf88faf371   2 days ago      147MB
	  dkatest           latest    52bf88faf371   2 days ago      147MB
	  dato138it/myapp   <none>    c26a6cad11ba   2 days ago      147MB
	  bitnami/apache    latest    6edfa339f61e   3 days ago      176MB
	  adminer           latest    7707fd9b142f   4 days ago      89.8MB
	  docker_adminer    latest    7707fd9b142f   4 days ago      89.8MB
	  mariadb           latest    992bce5ed710   12 days ago     401MB
	  docker_db         latest    992bce5ed710   12 days ago     401MB
	  ubuntu            latest    7e0aa2d69a15   12 days ago     72.7MB
	$ vim Dockerfile
	$ cat Dockerfile
	FROM ubuntu
	RUN apt-get update \
	        && apt-get install -y cowsay \
	        && ln -s /usr/games/cowsay /usr/bin/cowsay \
	        && r-t -rf /var/lib/apt/lists/*
	ENTRYPOINT ["cowsay"]
	$ docker build -t dkaonerun .
	  ...
	  Successfully built af8ceb4756cc
	  Successfully tagged dkaonerun:latest
	$ docker images
	  REPOSITORY        TAG       IMAGE ID       CREATED          SIZE
	  dkaonerun         latest    af8ceb4756cc   14 seconds ago   119MB
	  dkamanyrun        latest    944e60e98ca0   4 minutes ago    147MB
	  dato138it/myapp   latest    52bf88faf371   2 days ago       147MB
	  dkatest           latest    52bf88faf371   2 days ago       147MB
	  dato138it/myapp   <none>    c26a6cad11ba   2 days ago       147MB
	  bitnami/apache    latest    6edfa339f61e   3 days ago       176MB
	  adminer           latest    7707fd9b142f   4 days ago       89.8MB
	  docker_adminer    latest    7707fd9b142f   4 days ago       89.8MB
	  mariadb           latest    992bce5ed710   12 days ago      401MB
	  docker_db         latest    992bce5ed710   12 days ago      401MB
	  ubuntu            latest    7e0aa2d69a15   12 days ago      72.7MB
	Task:
	Образы занимают места на диске и если они вам уже не нужны, то сначала удаляем контейнеры, а затем все образы
	Decision:
	$ docker images
	  REPOSITORY        TAG       IMAGE ID       CREATED         SIZE
	  dkaonerun         latest    af8ceb4756cc   2 minutes ago   119MB
	  dkamanyrun        latest    944e60e98ca0   6 minutes ago   147MB
	  dato138it/myapp   latest    52bf88faf371   2 days ago      147MB
	  dkatest           latest    52bf88faf371   2 days ago      147MB
	  dato138it/myapp   <none>    c26a6cad11ba   2 days ago      147MB
	  bitnami/apache    latest    6edfa339f61e   3 days ago      176MB
	  docker_adminer    latest    7707fd9b142f   4 days ago      89.8MB
	  adminer           latest    7707fd9b142f   4 days ago      89.8MB
	  docker_db         latest    992bce5ed710   12 days ago     401MB
	  mariadb           latest    992bce5ed710   12 days ago     401MB
	  ubuntu            latest    7e0aa2d69a15   12 days ago     72.7MB
	$ docker r-t -v $(docker ps -aq -f status=exited)
	  f3258c7f0edb
	  4e4079f3befe
	  cce4d69b3f3f
	  d6562c779f1a
	  c596a54cccb4
	  386a76086449
	  eae5e2c135a5
	$ docker r-ti $(docker images -q) --force
	Source:
	1. https://stepik.org/course/1612/syllabus

Виртуализация
	Task:
	Установить Hyper-V в Windows Server 2012
	Decision:
	диспетчер серверов - локальный сервер - управление - добавить роли и компоненты - выбор сервера - роли сервера - hyper-v - компоненты - средства удаленного администрирования - средства администрирования ролей - средства управления hyper-v - виртуальные коммутаторы - ethernet - хранилище по умолчанию - виртальных жестких дисков - C:\Data\VHD - конфигурации виртуальной машины - C:\Data\VM - автоматический перезапуск -готово - пуск - диспетчер hyper-v - выбираем нашу машину - создать - виртуальная машина - имя и местонахождение - Alt - C:\Data\VM\Alt - выделить память - 2048 - настройка сети - подключаем к сети - параметры установок - файл изо образа - C:\Data\alt-server-10.0-x86_64.iso - готово - пуск
	Task:
	Установить Hyper-V в Windows Server 2012 c помощью Shell
	Decision:
	PS C:\Windows\system32> Get-Module -ListAvailable
	    Каталог: C:\Windows\system32\WindowsPowerShell\v1.0\Modules
	ModuleType Version    Name                                ExportedCo
	---------- -------    ----                                ----------
	...
	Binary     1.1        Hyper-V                             {Add-VMDvd
	...
	PS C:\Windows\system32> Import-Module -Name Hyper-V
	PS C:\Windows\system32> Get-VM

	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> $ram=1*1024*1024*1024
	PS C:\Windows\system32> $ram
	1073741824
	PS C:\Windows\system32> $hdd=10*1024*1024*1024
	PS C:\Windows\system32> $hdd
	10737418240
	PS C:\Windows\system32> NEW-VM -Name Alt2 -MemoryStartupBytes $ram -NewVHDPath 'C:\Data\VM\Alt2\Alt2.vhdx'-NewVHD
	SizeBytes $hdd -Path 'C:\Data\VM\Alt2'
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt2 Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> Get-VM
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	Alt2 Off   0           0                 00:00:00 Работает нормально
	PS C:\Windows\system32> Start-VM -name Alt2
	PS C:\Windows\system32> Get-VM
	Name State   CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- -----   ----------- ----------------- ------   ------
	Alt  Off     0           0                 00:00:00 Работает нормально
	Alt2 Running 0           1024              00:00:16 Работает нормально
	PS C:\Windows\system32> Stop-VM Alt2
	Подтверждение
	Hyper-V не удается завершить работу виртуальной машины Alt2, так как служба интеграции по завершению работы недоступна.
	 Во избежание потенциальной потери данных вы можете приостановить виртуальную машину или сохранить ее состояние. Другим
	 вариантом является отключение виртуальной машины, но при этом возможна потеря данных.
	[Y] Да - Y  [N] Нет - N  [S] Приостановить - S  [?] Справка (значением по умолчанию является "Y"): y
	PS C:\Windows\system32> Get-VM
	Name State CPUUsage(%) MemoryAssigned(M) Uptime   Status
	---- ----- ----------- ----------------- ------   ------
	Alt  Off   0           0                 00:00:00 Работает нормально
	Alt2 Off   0           0                 00:00:00 Работает нормально
	Task:
	Конвертация Hyper-V vhdx в KVM qcow2
	Decision:
	$ qemu-img check -r all Alt2.vhdx
	$ qemu-img convert -O qcow2 Alt2.vhdx Alt2.qcow2
	Task:
	Ошибка после установки REDOS в Virtualbox
	"Упс, что то пошло не так"
	Decision:
	# su
	# systemctl disable gdm
	# systemctl enable sddm
	    Created symlink /etc/systemd/system/display-manager.service → /usr/lib/systemd/system/sddm.service.
	# reboot
	Task: 
	Создать виртуальные машины под виндой, чтобы пользователи грузились с Linux рхе подключались к серверу виртуальных машин и работали с office essentials и 1с
	Decision:
	Установить oVirt и на домашней машине настроить из виртуальных машин HCI кластер
	Task:
	Настройка Virtualbox. Добавьте сетевой адаптер (Host-only) через гипервизор. Настройте статический ip адрес в виртуальной машине на этом интерфейсе, в качестве dns и gateway укажите адрес хоста.
	Decision:
	Virtualbox-Файл-Настройки-Сеть-Добавить новую Нат сеть-Ok-настроить-виртуальный адаптер хоста-Инструменты-Менеджер сетей хоста-Создать-DHCP-+Включен-Редос-Настроить-Сеть-Тип подключения-Виртуальный адаптер хоста-vboxnet0-ок-запустить
	Task:
	Настроим Centos 7 после его установки. Посмотрим подключение ssh. Если он active runnig, значит мы можем подключаться к системе через SSH.
	Decision:
	service sshd status
	Task:
	Теперь нужно выключить firewalld
	Decision:
	systemctl status firewalld
	systemctl stop firewalld
	Task:
	C автозагрузки уберем его
	Decision:
	systemctl disable firewalld
	Task:
	Проверим таблицы. Везде ACCEPT (Никаких правил), значит firewalld сейчас отключен
	Decision:
	iptables -L
	Task:
	отключим Selinux - вместо enforcing пишем disabled
	Decision:
	vim /etc/selinux/config
	cat /etc/selinux/config
	Task
	Сделаем обновления перед перезагрузкой системы. htop - для производительности
	Decision:
	yum update
	yum install epel-release
	yum install vim htop mc tree
	yum install net-tools
	reboot
	Task:
	проверим сеть
	Decision:
	ifconfig
	    пробуем подключиться к ssh
	Task:
	Настроим Ubuntu 18.04 после его установки.
	Decision:
	sudo -s
	service sshd status
	Task:
	Теперь нужно выключить firewalld. если он - active exited, значит firewalld запушен и с автозагрузки уберем его. в systemcd отключим Selinux. убедимся что selinux отключен (disabled).
	Decision:
	systemctl status ufw
	systemctl stop ufw
	systemctl disable ufw
	sestatus
	Task:
	увидим что не найдена команда. поэтому устанавливаем сначала обновления а потом утилиту policycoreutlis
	Decision:
	apt update
	apt upgrade
	apt install policycoreutlis
	sestatus
	apt install vim htop mc tree
	reboot
	Task:
	так как сеть мы в virtualbox подключили через nat, тогда нам нужно пробросить порт в virtualbox.
	Decision:
	VirtualBox -> дополнительно проброс портов -> + -> Если я подkлючaюсь на свою машину на любой адрес (если несколько адресов на моей машине), то допустим на порт хоста 2222 (надо чтобы этот порт был открыт физически на вашей реальной машине), назовем его ssh to ubuntu (имя), и я хочу пробрасывать это на любой адрес моей машины с убунту на порт гостя 22 (ssh слушает порт 22).
	запускаем putty -> я могу подключиться на адрес 127.0.0.1 в host name на порт 2222 -> подлючаемся

	ifconfig
	Task:
	установка VirtualBox в Ubuntu 18.04
	Decision:
	$ sudo apt install gcc make linux-headers-$(uname -r) dkms
	$ wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -
	$ wget -q https://www.virtualbox.org/download/oracle_vbox.asc -O- | sudo apt-key add —
	$ sudo sh -c ‘echo «deb http://download.virtualbox.org/virtualbox/debian $(lsb_release -sc) contrib» >> /etc/apt/sources.list.d/virtualbox.list’
	$ sudo apt update
	$ sudo apt purge virtualbox*
	$ sudo apt install virtualbox-6.0
	Task:
	Добавить плагин Extension pack
	https://www.virtualbox.org/wiki/Downloads - VirtualBox 6.1.32 Oracle VM VirtualBox Extension Pack - https://download.virtualbox.org/virtualbox/6.1.32/Oracle_VM_VirtualBox_Extension_Pack-6.1.32.vbox-extpack - Virtualbox -    Настройки - Плагины - Добавить новый плагин
	Task:
	Настройка гостевых дополнений Centos 8
	Decision:
	# dnf -y update
	# dnf -y install perl make bzip2
	# dnf -y install kernel-headers
	# dnf -y install gcc
	# dnf -y install elfutils-libelf-devel
	# dnf -y install kernel-devel-$(uname -r)
	# poweroff
	    https://www.virtualbox.org/wiki/Testbuilds-> https://www.virtualbox.org/download/testcase/VBoxGuestAdditions_6.1.29-148369.iso -> Virtualbox -> Centos 8 Server -> Настройки -> Носители -> Атрибуты -> Выбрать файл диска... -> VBoxGuestAdditions_6.1.29-148369.iso -> ok -> Запустить
	# cd /media
	# mkdir cdrom
	# mount /dev/cdrom /media/cdrom/
	    mount: /media/cdrom: WARNING: device write-protected, mounted read-only.
	# ls cdrom/
	    AUTORUN.INF  OS2                      VBoxDarwinAdditionsUninstall.tool  VBoxWindowsAdditions.exe
	    autorun.sh   runasroot.sh             VBoxLinuxAdditions.run             VBoxWindowsAdditions-x86.exe
	    cert         TRANS.TBL                VBoxSolarisAdditions.pkg           windows11-bypass.reg
	    NT3x         VBoxDarwinAdditions.pkg  VBoxWindowsAdditions-amd64.exe
	# cd cdrom/
	# sh VBoxLinuxAdditions.run
	    Verifying archive integrity... All good.
	    Uncompressing VirtualBox 6.1.29 Guest Additions for Linux........
	    VirtualBox Guest Additions installer
	    Copying additional installer modules ...
	    Installing additional modules ...
	    VirtualBox Guest Additions: Starting.
	    VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel
	    modules.  This may take a while.
	    VirtualBox Guest Additions: To build modules for other installed kernels, run
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup <version>
	    VirtualBox Guest Additions: or
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup all
	    VirtualBox Guest Additions: Kernel headers not found for target kernel
	    4.18.0-348.2.1.el8_5.x86_64. Please install them and execute
	      /sbin/rcvboxadd setup
	    modprobe vboxguest failed
	    The log file /var/log/vboxadd-setup.log may contain further information.
	# uname -r
	    4.18.0-348.2.1.el8_5.x86_64
	# ls /usr/src/kernels/
	    4.18.0-348.el8.x86_64
	# dnf -y install "kernel-devel-uname-r == $(uname -r)"
	# ls /usr/src/kernels
	    4.18.0-348.2.1.el8_5.x86_64  4.18.0-348.el8.x86_64
	# ls
	    AUTORUN.INF  OS2                      VBoxDarwinAdditionsUninstall.tool  VBoxWindowsAdditions.exe
	    autorun.sh   runasroot.sh             VBoxLinuxAdditions.run             VBoxWindowsAdditions-x86.exe
	    cert         TRANS.TBL                VBoxSolarisAdditions.pkg           windows11-bypass.reg
	    NT3x         VBoxDarwinAdditions.pkg  VBoxWindowsAdditions-amd64.exe
	# sh VBoxLinuxAdditions.run
	    Verifying archive integrity... All good.
	    Uncompressing VirtualBox 6.1.29 Guest Additions for Linux........
	    VirtualBox Guest Additions installer
	    Removing installed version 6.1.29 of VirtualBox Guest Additions...
	    Copying additional installer modules ...
	    Installing additional modules ...
	    VirtualBox Guest Additions: Starting.
	    VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel
	    modules.  This may take a while.
	    VirtualBox Guest Additions: To build modules for other installed kernels, run
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup <version>
	    VirtualBox Guest Additions: or
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup all
	    VirtualBox Guest Additions: Building the modules for kernel
	    4.18.0-348.2.1.el8_5.x86_64.
	    ValueError: File context for /opt/VBoxGuestAdditions-6.1.29/other/mount.vboxsf already defined
	# reboot
	Task:
	Настройка гостевых дополнений Ubuntu 20.04
	Decision:
	$ sudo apt-get update
	$ sudo apt install openssh-server
	$ systemctl ststus sshd
	$ ifconfig
	C:\Users\...>ssh ubuntu@2.8.0.6
	$ sudo apt-get update
	$ sudo apt-get install -y build-essential gcc make perl dkms
	$ sudo poweroff
	    https://www.virtualbox.org/wiki/Testbuilds -> https://www.virtualbox.org/download/testcase/VBoxGuestAdditions_6.1.29-148369.iso-> Virtualbox -> Ubuntu 20.04 -> Настройки -> Носители -> Атрибуты -> Выбрать файл диска... -> VBoxGuestAdditions_6.1.29-148369.iso -> ok -> Запустить
	C:\Users\...>ssh ubuntu@2.8.0.6
	$ cd /media
	$ mkdir cdrom
	$ sudo su
	# mount /dev/cdrom /media/cdrom
	    mount: /media/cdrom: ВНИМАНИЕ: устройство защищено от записи, смонтировано только для чтения.
	# cd cdrom/
	# ls
	    AUTORUN.INF  OS2                      VBoxDarwinAdditionsUninstall.tool  VBoxWindowsAdditions.exe
	    autorun.sh   runasroot.sh             VBoxLinuxAdditions.run             VBoxWindowsAdditions-x86.exe
	    cert         TRANS.TBL                VBoxSolarisAdditions.pkg           windows11-bypass.reg
	    NT3x         VBoxDarwinAdditions.pkg  VBoxWindowsAdditions-amd64.exe
	# sh VBoxLinuxAdditions.run
	    Verifying archive integrity... All good.
	    Uncompressing VirtualBox 6.1.29 Guest Additions for Linux........
	    VirtualBox Guest Additions installer
	    Copying additional installer modules ...
	    Installing additional modules ...
	    VirtualBox Guest Additions: Starting.
	    VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel
	    modules.  This may take a while.
	    VirtualBox Guest Additions: To build modules for other installed kernels, run
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup <version>
	    VirtualBox Guest Additions: or
	    VirtualBox Guest Additions:   /sbin/rcvboxadd quicksetup all
	    VirtualBox Guest Additions: Building the modules for kernel 5.11.0-40-generic.
	    update-initramfs: Generating /boot/initrd.img-5.11.0-40-generic
	    VirtualBox Guest Additions: Running kernel modules will not be replaced until
	    the system is restarted
	# reboot
	$ sudo usermod -aG vboxsf ubuntu
	$ sudo reboot
	$ cd /media/sf_Downloads/
	$ vim filetest.txt
	$ ls filetest.txt
	    filetest.txt
	Task:
	Настройка гостевых дополнений Kali linux
	Decision:
	    https://www.virtualbox.org/wiki/Testbuilds -> https://www.virtualbox.org/download/testcase/VBoxGuestAdditions_6.1.29-148369.iso-> Virtualbox -> Kali Linux -> Настройки -> Носители -> Атрибуты -> Выбрать файл диска... -> VBoxGuestAdditions_6.1.29-148369.iso -> ok -> Запустить
	$ ls /media
	cdrom  cdrom0  sf_Downloads
	$ sudo su
	# mount /dev/cdrom /media/cdrom
	    mount: /media/cdrom0: WARNING: source write-protected, mounted read-only.
	# cd /media/cdrom
	# ls
	    AUTORUN.INF  OS2                      VBoxDarwinAdditionsUninstall.tool  VBoxWindowsAdditions.exe
	    autorun.sh   runasroot.sh             VBoxLinuxAdditions.run             VBoxWindowsAdditions-x86.exe
	    cert         TRANS.TBL                VBoxSolarisAdditions.pkg           windows11-bypass.reg
	    NT3x         VBoxDarwinAdditions.pkg  VBoxWindowsAdditions-amd64.exe
	# sh VBoxLinuxAdditions.run
	$ sudo usermod -aG vboxsf server
	$ sudo adduser kali vboxsf                                      100 ⨯
	    Adding user `kali' to group `vboxsf' ...
	    Adding user kali to group vboxsf
	    Done.
	$ sudo reboot
	$ cd /media/sf_Downloads
	$ touch test.txt
	$ ls
	     AnyDesk.exe             'installer 1c'          SteamSetup.exe                          test.txt
	     ChromeSetup.exe         'installer qt C++'      sublime_text_build_4121_x64_setup.exe   tsetup-x64.3.2.5.exe
	     desktop.ini              Lessons               'System Volume Information'              virtualbox
	    'Firefox Installer.exe'   Skype-8.78.0.159.exe  'Telegram Desktop'                       Yandex.exe
	$ ls test.txt
	    test.txt
	Task:
	установка Centos 8
	Decision:
	    Virtualbox -> Создать -> Centos Server -> 2048Mb -> 50Gb -> Настройки ->
	    -> Процессор 2ЦП -> выключить PEA/NX -> Видеопамять: 128Mb -> Граф контроллер: VMSVGA -> Носители: Centos8.iso -> Сеть: Сетевой мост -> Общая папка -> Запустить
	Task:
	установка Ubuntu 20.04
	Decision:
	СОЗДАТЬ -> В открывшемся окне введите имя будущей машины -> выберите объем ОЗУ - 3 Гб -> поставьте переключатель в положение "Создавать новый виртуальный диск". -> Тип файла указываем VDI и формат хранения - динамический виртуальный жесткий диск -> Размер файла я лично указываю 30 Гб для Убунту, мне этого хватит -> Дальше откройте для нее контекстное меню -> выберите НАСТРОИТЬ -> СИСТЕМА -> В порядке загрузки убираем галочку на ГИБКИЙ ДИСК опускаем его вниз стрелочками -> Во вкладке процессора 2 ЦП поставлю -> ДИСПЛЕЙ -> объем видеопамяти равным 128 -> НОСИТЕЛИ  -> выбираем пустой диск  -> ставим образ операционной системы Убунту в оптический привод, который мыскачали в официальном сайте -> ОБЩАЯ ПАПКА  -> добавляем новую общую папку  -> указываем путь к папке (выбираем свой)  -> жмем галочку на ПОДКЛЮЧАТЬСЯ АВТОМАТИЧЕСКИ -> СЕТЬ  -> тип подключения  -> сетевой мост, и имя к нему -> Запускаем -> выбираем загрузочный диск -> дальше идет обычная установка Centos 8 
	Task:
	установка Windows 10
	Decision:
	СОЗДАТЬ -> В открывшемся окне введите имя будущей машины -> Выберите объем ОЗУ - 3 Гб -> поставьте переключатель в положение "Создавать новый виртуальный диск" -> Параметры диска можно оставить по умолчанию -> объем - 32 гигабайта (я оставил 100гб) -> Настроить -> Дисплей -> Cделайте объем видеопамяти равным 128 -> СИСТЕМА -> В порядке загрузки убираем галочку на ГИБКИЙ ДИСК -> опускаем его вниз стрелочками -> Вы также можете корректировать основную память (я поставил 2048мб) и процессор (2цп) -> НОСИТЕЛИ -> выбираем пустой диск -> ставим образ операционной системы Виндоус в оптический привод, который мы скачали в официальном сайте -> СЕТЬ -> тип подключения -> сетевой мост, и имя к нему -> ОБЩАЯ ПАПКА -> настраиваем путь к папке (выбираем свой) -> ставим галочку подключаться автоматически -> Запуск -> дальше идет обычная установка Виндоус.
	Source:
	1. https://www.youtube.com/user/itsemaev

Мини-проекты
	Task:
	Есть команда Data Science, которые занимаются анализом данных, обучают модели. В своей работе они используют источников данных, несколько БД, s3.
	Необходимо предоставить для команды рабочие инструменты (из стека Amazon или иные) для такого перечня задач:
	1. Управление доступом к данным (базам и s3) - разным членам доступны разные БД или s3
	2. Анонимизация данных - подготовка дампов, где сделана замена настоящих user_id/email/etc на анонимизированные
	3. Отслеживание утечек данных - выявление ситуаций, когда начали выкачивать пол базы
	4. Расчет математических моделей - использование CPU/GPU для запуска вычислительных модулей
	5. Хранение промежуточных и финальных результатов работы (моделей) - хранение "бинарников"
	6. Версионирование кода и моделей
	Попробуйте предложить решение для как можно большего числа пунктов (опишите словами).
	А для любого одного пункта, попробуйте предложить черновик манифеста/кода/etc для демонстрации решения.
	Ожидаемый результат
	В результате тестового задания, ожидается:
	Ссылка на Gitlab/Github/etc
	1. С текстом, который показывает как подойти к решению одного или более пунктов
	2. С кодом, который иллюстрирует ваш подход к решению.
	Ссылку на открытый репозиторий приложи в опросник
	Task:
	На https://github.com надо создать репозиторий, где положить описание для создания Docker контейнера в котором надо сделать установку с нуля Redmine http://www.redmine.org, потом можно усложнить и разбить установку на три контейнера - БД, сервер приложений (Redmine+puma), фронтенд (nginx) и запускать это все при помощи docker-compose, все результаты работы также положить на гитхаб, Напиши, как закончишь, наши инженеры посмотрят. Также, вместо редмайна можно взять любое веб-приложение или написать свое, тут есть простор для творчества.
	Необязательное задание со звездочкой: подключить Travis CI для сборки docker образа Redmine, его тестирования и публикации на docker hub.