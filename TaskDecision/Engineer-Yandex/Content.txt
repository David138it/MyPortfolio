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
Яндекс: Ваш доменный пароль 1s1BN9v48smdKPG Пожалуйста, ознакомьтесь с информацией по ссылке https://ya.cc/3XoSLN
Для авторизации в интранете Яндекса для вас сгенерирован одноразовый пароль (TOTP) Секрет: HIOWTXU4DB4VUUWDQYUH6HJAS4
Логин: david138it@gmail.com Пароль: _OHzPfME6l

Task:
Установить антируткит rkhunter
Decision:
# cd /tmp
# wget https://sourceforge.net/projects/rkhunter/files/rkhunter/1.4.6/rkhunter-1.4.6.tar.gz
# tar -xvf rkhunter-1.4.6.tar.gz
# cd rkhunter-1.4.6/
# ./installer.sh --layout default --install
# rkhunter --update
# rkhunter --propupd
# vim /etc/cron.daily/rkhunter.sh
# cat /etc/cron.daily/rkhunter.sh
#!/bin/sh
(
/usr/local/bin/rkhunter --versioncheck
/usr/local/bin/rkhunter --update
/usr/local/bin/rkhunter --cronjob --report-warnings-only
) | /bin/mail -s 'rkhunter Daily Run (I-X)' d-t@gmail.com
# chmod 755 /etc/cron.daily/rkhunter.sh
# rkhunter --list rootkits
# rkhunter --check
# cat /var/log/rkhunter.log
# cat /var/log/rkhunter.log | grep hidden
# lsof | grep /etc/net/scripts
# cat /var/log/rkhunter.log | grep -A5 "\[ Warning \]"
Task:
Конвертация Hyper-V vhdx в KVM qcow2
Decision:
$ qemu-img check -r all Alt2.vhdx
$ qemu-img convert -O qcow2 Alt2.vhdx Alt2.qcow2
Task:
Настройка веб сервера Lamp
Decision:
#https://losst.ru/ustanovka-lamp-v-ubuntu-20-04?ysclid=l7yoe8gjvn834722367#7_%D0%9D%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0_MySQL
$ sudo apt update
$ sudo apt -y upgrade
$ sudo apt -y install apache2
$ sudo apt -y install default-mysql-server
$ sudo apt -y install php7.4 libapache2-mod-php7.4 php7.4-mysql php-curl php-json php-cgi php-gd php-zip php-mbstring php-xml php-xmlrpc
$ sudo ufw allow in 80/tcp
$ sudo service apache2 restart
$ sudo service mysql restart
Conclusion:
http://localhost
Decision:
$ touch index.php
$ echo "<?php phpinfo(); ?>" > index.php
$ sudo mv index.php /var/www/html/index.php
$ sudo a2dismod mpm_event
$ sudo a2dismod mpm_worker
$ sudo a2enmod mpm_prefork
$ sudo a2enmod rewrite
$ sudo sed -i '/\/var\/www\//,/\/Directory/s/AllowOverride None/AllowOverride All/g' /etc/apache2/apache2.conf
$ sudo sed -i '/\/usr\/share\//,/\/Directory/s/AllowOverride None/AllowOverride All/g' /etc/apache2/apache2.conf
$ sudo service apache2 reload
$ sudo sed -i 's/short_open_tag = Off/short_open_tag = On/' /etc/php/7.4/apache2/php.ini
$ sudo sed -i 's/error_reporting = E_ALL & ~E_DEPRECATED & ~E_STRi-cT/error_reporting = E_ALL/' /etc/php/7.4/apache2/php.ini
$ sudo sed -i 's/display_errors = Off/display_errors = On/' /etc/php/7.4/apache2/php.ini
$ sudo service apache2 reload
Conclusion:
http://localhost/index.php
Decision:
$ sudo mysql_secure_installation
 ... Failed! Error: SET PASSWORD has no signifi-cance for user 'root'@'localhost' as the authenti-cation method used doesn't store authenti-cation data in the MySQL server. Please consider using ALTER USER instead if you want to change authenti-cation parameters.'
#https://exerror.com/failed-error-set-password-has-no-significance-for-user-rootlocalhost-as-the-authentication-method-used-doesnt-store-authentication-data-in-the-mysql-server/
$ sudo mysql
mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password by 'Ykj38';
mysql> exit
$ sudo mysql_secure_installation
Enter password for user root:
Press y|Y for Yes, any other key for No: No
Change the password for root ? ((Press y|Y for Yes, any other key for No) : No
Remove anonymous users? (Press y|Y for Yes, any other key for No) : Y
Disallow root login remotely? (Press y|Y for Yes, any other key for No) : Y
Remove test database and access to it? (Press y|Y for Yes, any other key for No) : Y
Reload privilege tables now? (Press y|Y for Yes, any other key for No) : Y
#https://losst.ru/ustanovka-phpmyadmin-ubuntu-18-04?ysclid=l7zr7tpmb9179137130
$ sudo mysql -u root -p
mysql> CREATE USER 'user1'@'localhost' IDENTIFIED BY 'Ykj38';
mysql> GRANT ALL PRIVILEGES ON *.* TO 'user1'@'localhost';
mysql> FLUSH PRIVILEGES;
mysql> exit
$ sudo apt -y install php-mbstring
$ sudo apt -y install phpmyadmin
$ sudo ln -s /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf
$ sudo sed -i ':a;N;$!ba;s/# limit/AllowOverride All\n    # limit/' /etc/apache2/conf-available/phpmyadmin.conf
$ sudo a2enconf phpmyadmin.conf
$ sudo service apache2 restart
$ touch .htaccess
$ echo 'AuthType Basic
AuthName "Restricted Files"
AuthUserFile /etc/phpmyadmin/.htpasswd
Require valid-user' > .htaccess
$ sudo mv .htaccess /usr/share/phpmyadmin/
$ sudo htpasswd -c /etc/phpmyadmin/.htpasswd administrator
$ sudo htpasswd /etc/phpmyadmin/.htpasswd user1
$ sudo service apache2 start
$ sudo service mysql start
Conclusion:
http://localhost/phpmyadmin - user1 - Ykj38




Языки и системы программирования
Task:
Массивы: одномерные, двумерные, многомерный. Размещение в оперативной памяти, сравнение со связанными списками. Вставка элементов, поиск, удаление (для одномерных массивов), оценка алгоритмической сложности
Decision:
массив — совокупный тип данных, который позволяет получить доступ ко всем переменным одного и того же типа данных через использование одного идентификатора.
Рассмотрим случай, когда нужно записать результаты тестов 30 студентов в классе. Без использования массива нам придется выделить 30 почти одинаковых переменных!
// Выделяем 30 целочисленных переменных (каждая с разным именем)
int testResultStudent1;
int testResultStudent2;
int testResultStudent3;
// ...
int testResultStudent30;
С использованием массива всё гораздо проще. Следующая строка эквивалентна коду, приведенному выше:
int testResult[30]; // выделяем 30 целочисленных переменных, используя фиксированный массив
В объявлении переменной массива мы используем квадратные скобки [], чтобы сообщить компилятору, что это переменная массива (а не обычная переменная), а в скобках — количество выделяемых элементов (это называется длиной или размером массива).
В примере, приведенном выше, мы объявили фиксированный массив с именем testResult и длиной 30. Фиксированный массив (или «массив фиксированной длины») представляет собой массив, размер которого известен во время компиляции. При создании testResult, компилятор выделит 30 целочисленных переменных.
http://cppstudio.com/post/389/
Многомерные массивы
Массив массивов называется многомерным массивом:
int array[2][4]; // 2-элементный массив из 4-элементных массивов
Поскольку у нас есть 2 индекса, то это двумерный массив.
В двумерном массиве первый (левый) индекс принято читать как количество строк, а второй (правый) как количество столбцов. Массив выше можно представить следующим образом:
[0][0]  [0][1]  [0][2]  [0][3]  // строка №0
[1][0]  [1][1]  [1][2]  [1][3]  // строка №1
Чтобы получить доступ к элементам двумерного массива, просто используйте два индекса:
array[1][3] = 7; // без приставки int (типа данных)
https://www.tstu.ru/book/elib3/mm/2016/evdokimov/site/page90.90.html
https://habr.com/ru/post/104219/
Task:
Списки: линейные, кольцевые, двусвязные. Размещение в оперативной памяти, сравнение с массивами.
Decision:
http://algmet.simulacrum.me/theory_a4m/spiski/index.htm
https://rsdn.org/article/cpp/ObjectsAndPointers2.xml
Task:
Очереди, стеки, деки. Операции вставки, поиска, удаления; оценка алгоритмической сложности
Decision:
Стек
Стеком (англ. stack) называется хранилище данных, в котором можно работать только с одним элементом: тем, который был добавлен в стек последним. Стек должен поддерживать следующие операции:
push
    Добавить (положить) в конец стека новый элемент
pop
    Извлечь из стека последний элемент
back
    Узнать значение последнего элемента (не удаляя его)
size
    Узнать количество элементов в стеке
clear
    Очистить стек (удалить из него все элементы) 
Хранить элементы стека мы будем в массиве. Для начала будем считать, что максимальное количество элементов в стеке не может превосходить константы MAX_SIZE, тогда для хранения элементов массива необходимо создать массив размера MAX_SIZE.
Объявим структуру данных типа stack.
     const int MAX_SIZE=1000;
     struct stack {
         int m_size;            // Количество элементов в стеке
         int m_elems[MAX_SIZE]; // Массив для хранения элементов
         stack();               // Конструктор
         ~stack();              // Деструктор
         void push(int d);      // Добавить в стек новый элемент
         int pop();             // Удалить из стека последний элемент
                                // и вернуть его значение
         int back();            // Вернуть значение последнего элемента
         int size();            // Вернуть количество элементов в стеке
         void clear();          // Очистить стек
     };
Объявленная здесь структура данных stack реализует стек целых чисел. Поле структуры m_size хранит количество элементов в стеке в настоящее время, сами элементы хранятся в элементах массива m_elems с индексами 0..m_size-1. Элементы, добавленные позже, получают большие номера.
Очередь
Очередью (aнгл. queue)) называется структура данных, в которой элементы кладутся в конец, а извлекаются из начала. Таким образом, первым из очереди будет извлечен тот элемент, который будет добавлен раньше других.
Элементы очереди будем также хранить в массиве. При этом из очереди удаляется первый элемент, и, чтобы не сдвигать все элементы очереди, будем в отдельном поле m_start хранить индекс элемента массива, с которого начинается очередь. При удалении элементов, очередь будет "ползти" дальше от начала массива. Чтобы при этом не происходил выход за границы массива, замкнем массив в кольцо: будем считать, что за последним элементом массива следует первый.
Описание структуры очередь:
     const int MAX_SIZE=1000;
     struct queue {
         int m_size;            // Количество элементов в очереди
         int m_start;           // Номер элемента, с которого начинается очередь
         int m_elems[MAX_SIZE]; // Массив для хранения элементов
     
         queue();               // Конструктор
         ~queue();              // Деструктор
         void push(int d);      // Добавить в очередь новый элемент
         int pop();             // Удалить из очереди первый элемент
                                // и вернуть его значение
         int front();           // Вернуть значение первого элемента
         int size();            // Вернуть количество элементов в очереди
         void clear();          // Очистить очередь
     };
Дек
Деком (англ. deque – аббревиатура от double-ended queue, двухсторонняя очередь) называется структура данных, в которую можно удалять и добавлять элементы как в начало, так и в конец. Дек хранится в памяти так же, как и очередь. Система команд дека:
push_front
    Добавить (положить) в начало дека новый элемент
push_back
    Добавить (положить) в конец дека новый элемент
pop_front
    Извлечь из дека первый элемент
pop_back
    Извлечь из дека последний элемент
front
    Узнать значение первого элемента (не удаляя его)
back
    Узнать значение последнего элемента (не удаляя его)
size
    Узнать количество элементов в деке
clear
    Очистить дек (удалить из него все элементы) 
https://habr.com/ru/post/104219/
Task:
Бинарное дерево. Сбалансированное бинарное дерево. Обходы дерева, алгоритм. Прошитые деревья. В-деревья: определение и сравнение с бинарными деревьями
Decision:
https://habr.com/ru/post/267855/
https://www.niisi.ru/iont/projects/rfbr/90308/90308_miphi4.php
https://ru.wikipedia.org/wiki/B-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE
Task:
Языки и средства программирования Internet приложений. Язык гипертекстовой разметки html. 
Decision:
1. Клиентские технологии
HTML
Язык разметки гипертекста (Hypertext Markup Language), или, как его чаще называют, HTML, — это компьютерный язык, лежащий в основе World Wide Web. Благодаря языку HTML любой текст можно разметить, преобразовав его в гипертекст с последующей публикацией в Web.
Язык HTML имеет собственный набор символов, с помощью которых Web-браузеры отображают страницу. Эти символы, называемые дескрипторами, включают в себя элементы, необходимые для создания гиперссылок.
Одной из отличительных особенностей HTML-документов является то, что сам документ содержит только текст, а все остальные объекты встраиваются в документ в момент его отображения Браузером с помощью специальных тэгов и хранятся отдельно. При сохранении HTML-файла в месте размещения документа создается папка, в которую помещаются сопутствующие ему графические элементы оформления.
JavaScript
Язык программирования JavaScript разработан фирмой Netscape для создания интерактивных HTML-документов. Это объектно-ориентированный язык разработки встраиваемых приложений, выполняющихся как на стороне клиента, так и на стороне сервера. Синтаксис языка очень похож на синтаксис Java – поэтому его называют – Java-подобным.
Основные области применения JavaScript делятся на следующие категории:
    динамическое создание документа с помощью сценария;
    оперативная проверка достоверности заполняемых пользователем полей форм HTML до передачи их на сервер;
    создание динамических HTML-страниц совместно с каскадными таблицами стилей и объектной моделью документа;
    взаимодействие с пользователем при решении "локальных" задач, решаемых приложением JavaScript, встроенном в HTML-страницу.
VBScript
Язык создания сценариев VBScript разработан фирмой Microsoft, является подмножеством достаточно распространенного в среде программистов языка Visual Basic разработки прикладных программ Windows-приложений. Как и его родитель, язык VBScript достаточно прост и лёгок в изучении.
Преимуществом его применения для создания сценариев является возможность использования, с небольшими корректировками, ранее написанных процедур на языках Visual Basic и Visual Basic for Application.
Функциональные возможности сценариев, написанных на VBScript, ничем не отличаются от возможностей сценариев JavaScript: динамические создание документа или его частей, перехват и обработка событий и так далее.
VBScript используется для написания сценариев клиента (в этом случае браузер должен иметь встроенный интерпретатор этого языка), а также для написания сценариев на сервере (в этом случае сервер должен поддерживать язык VBScript).
Для создания сценариев клиента используется набор объектов, аналогичный набору JavaScript. Объекты клиента и сервера отличаются друг от друга, но существует общая часть (ядро) объектов, используемых при разработке как сценариев клиент, так и сценариев сервера.
Приложения Macromedia Flash
Технология Flash основана на использовании векторной графики в формате Shockwave Flash (SWF) разработанная компанией Macromedia®. Основным преимуществом Flash технологии является его межплатформенность, то есть этот формат может использоваться на любой аппаратно-программной платформе. И еще одна весомая особенность Flash технологии: созданные на его основе изображения могут быть не только анимированы, но еще и дополнены интерактивными элементами и звуковым сопровождением.
Особенностью технологии Flash является тот факт, что она вполне может заменить обыкновенную страницу, написанную на html. Но здесь открываются существенные преимущества технологии Flash перед языком HTML.
Интересны возможности языка ActionScript, а также интеграция его и самого конечного продукта с другими языками программирования и базами данных. Конечный продукт - это так называемый flash-ролик, которые представляет собой элемент active-x, который существует как самостоятельное приложение, и который, встраивается в html-документ по средствами включения его в тело документа как объекта active-x. Разумеется, для проигрывания такого ролика от браузера потребуется наличие установленного plug-in'а, но это не является проблемой, т.к. почти всё браузеры уже имеют предустановленный flash-plug-in, а если такового всё же в системе не имеется, то произойдёт автоматическая его загрузка с сервера производителя. Для достижения более сложной интерактивности Flash может взаимодействовать с JavaScript или VBScript.
Организация ссылок внутри flash-ролика может происходить как обычно, т.е. пряма ссылка на какой либо документ/cgi-шлюз, либо ссылка может приводить к загрузке новых роликов, которые могут содержать данные, и которые будут являть аналогом документов, загружаемых по щелчку на обыкновенную html-ссылку. Ссылка на другие ролики происходит внутри ролика, текст ссылки не покидает пределов ролика, его нельзя скопировать, либо просмотреть в строке состояния. Также перемещение по таким ссылками не будет вызывать перезагрузку главной страницу, в которую вставлен flash-ролик.
Ajax
Ajax расшифровывается как Asynchronous Javascript And XML (Асинхронные Javascript и XML) и технологией в строгом смысле слова не является. Если в стандартном web-приложении обработкой всей информации занимается сервер, тогда как браузер отвечает только за взаимодействие с пользователем, передачу запросов и вывод поступившего HTML, то в Ajax-приложении между пользователем и сервером появляется еще один посредник - движок Ajax. Он определяет, какие запросы можно обработать "на месте", а за какими необходимо обращаться на сервер.
Поведение сервера тоже изменилось. Если раньше на каждый запрос сервер выдавал новую страницу, то теперь он отсылает лишь те данные, которые нужны клиенту, а HTML из них прямо в браузере формирует движок Ajax.
Асинхронность проявляется в том, что далеко не каждый клик пользователя доходит до сервера, причем обратное тоже справедливо - далеко не каждая реакция сервера обусловлена запросом пользователя. Большую часть запросов формирует движок Ajax, причем его можно написать так, что он будет загружать информацию, предугадывая действия пользователя.
Где стоит использовать Ajax:
    Формы. Они очень медленны. Если асинхронно передавать данные, страница не перезагружается.
    Навигация в виде "дерева".
    Голосования. Пользователю будет приятней оставить свой голос за несколько секунд, чем за 30-40.
    Фильтры. Часто на сайтах делают сортировку по дате, по имени. Ajax это будет значительно удобнее.
2. Серверные технологии
PHP
В первую очередь PHP используется для создания скриптов, работающих на стороне сервера, для этого его, собственно, и придумали. PHP способен решать те же задачи, что и любые другие CGI-скрипты, в том числе обрабатывать данные html-форм, динамически генерировать html страницы и тому подобное. Но есть и другие области, где может использоваться PHP.
Вторая область – это создание скриптов, выполняющихся в командной строке. То есть с помощью PHP можно создавать такие скрипты, которые будут исполняться, вне зависимости от web-сервера и браузера, на конкретной машине.
И последняя область – это создание GUI-приложений (графических интерфейсов), выполняющихся на стороне клиента.
Task:
Языки и средства программирования Internet приложений. Язык гипертекстовой разметки html. Язык XML. Схема XML-документа
Decision:
1. Клиентские технологии
HTML
Язык разметки гипертекста (Hypertext Markup Language), или, как его чаще называют, HTML, — это компьютерный язык, лежащий в основе World Wide Web. Благодаря языку HTML любой текст можно разметить, преобразовав его в гипертекст с последующей публикацией в Web.
Язык HTML имеет собственный набор символов, с помощью которых Web-браузеры отображают страницу. Эти символы, называемые дескрипторами, включают в себя элементы, необходимые для создания гиперссылок.
Одной из отличительных особенностей HTML-документов является то, что сам документ содержит только текст, а все остальные объекты встраиваются в документ в момент его отображения Браузером с помощью специальных тэгов и хранятся отдельно. При сохранении HTML-файла в месте размещения документа создается папка, в которую помещаются сопутствующие ему графические элементы оформления.
JavaScript
Язык программирования JavaScript разработан фирмой Netscape для создания интерактивных HTML-документов. Это объектно-ориентированный язык разработки встраиваемых приложений, выполняющихся как на стороне клиента, так и на стороне сервера. Синтаксис языка очень похож на синтаксис Java – поэтому его называют – Java-подобным.
Основные области применения JavaScript делятся на следующие категории:
    динамическое создание документа с помощью сценария;
    оперативная проверка достоверности заполняемых пользователем полей форм HTML до передачи их на сервер;
    создание динамических HTML-страниц совместно с каскадными таблицами стилей и объектной моделью документа;
    взаимодействие с пользователем при решении "локальных" задач, решаемых приложением JavaScript, встроенном в HTML-страницу.
VBScript
Язык создания сценариев VBScript разработан фирмой Microsoft, является подмножеством достаточно распространенного в среде программистов языка Visual Basic разработки прикладных программ Windows-приложений. Как и его родитель, язык VBScript достаточно прост и лёгок в изучении.
Преимуществом его применения для создания сценариев является возможность использования, с небольшими корректировками, ранее написанных процедур на языках Visual Basic и Visual Basic for Application.
Функциональные возможности сценариев, написанных на VBScript, ничем не отличаются от возможностей сценариев JavaScript: динамические создание документа или его частей, перехват и обработка событий и так далее.
VBScript используется для написания сценариев клиента (в этом случае браузер должен иметь встроенный интерпретатор этого языка), а также для написания сценариев на сервере (в этом случае сервер должен поддерживать язык VBScript).
Для создания сценариев клиента используется набор объектов, аналогичный набору JavaScript. Объекты клиента и сервера отличаются друг от друга, но существует общая часть (ядро) объектов, используемых при разработке как сценариев клиент, так и сценариев сервера.
Приложения Macromedia Flash
Технология Flash основана на использовании векторной графики в формате Shockwave Flash (SWF) разработанная компанией Macromedia®. Основным преимуществом Flash технологии является его межплатформенность, то есть этот формат может использоваться на любой аппаратно-программной платформе. И еще одна весомая особенность Flash технологии: созданные на его основе изображения могут быть не только анимированы, но еще и дополнены интерактивными элементами и звуковым сопровождением.
Особенностью технологии Flash является тот факт, что она вполне может заменить обыкновенную страницу, написанную на html. Но здесь открываются существенные преимущества технологии Flash перед языком HTML.
Интересны возможности языка ActionScript, а также интеграция его и самого конечного продукта с другими языками программирования и базами данных. Конечный продукт - это так называемый flash-ролик, которые представляет собой элемент active-x, который существует как самостоятельное приложение, и который, встраивается в html-документ по средствами включения его в тело документа как объекта active-x. Разумеется, для проигрывания такого ролика от браузера потребуется наличие установленного plug-in'а, но это не является проблемой, т.к. почти всё браузеры уже имеют предустановленный flash-plug-in, а если такового всё же в системе не имеется, то произойдёт автоматическая его загрузка с сервера производителя. Для достижения более сложной интерактивности Flash может взаимодействовать с JavaScript или VBScript.
Организация ссылок внутри flash-ролика может происходить как обычно, т.е. пряма ссылка на какой либо документ/cgi-шлюз, либо ссылка может приводить к загрузке новых роликов, которые могут содержать данные, и которые будут являть аналогом документов, загружаемых по щелчку на обыкновенную html-ссылку. Ссылка на другие ролики происходит внутри ролика, текст ссылки не покидает пределов ролика, его нельзя скопировать, либо просмотреть в строке состояния. Также перемещение по таким ссылками не будет вызывать перезагрузку главной страницу, в которую вставлен flash-ролик.
Ajax
Ajax расшифровывается как Asynchronous Javascript And XML (Асинхронные Javascript и XML) и технологией в строгом смысле слова не является. Если в стандартном web-приложении обработкой всей информации занимается сервер, тогда как браузер отвечает только за взаимодействие с пользователем, передачу запросов и вывод поступившего HTML, то в Ajax-приложении между пользователем и сервером появляется еще один посредник - движок Ajax. Он определяет, какие запросы можно обработать "на месте", а за какими необходимо обращаться на сервер.
Поведение сервера тоже изменилось. Если раньше на каждый запрос сервер выдавал новую страницу, то теперь он отсылает лишь те данные, которые нужны клиенту, а HTML из них прямо в браузере формирует движок Ajax.
Асинхронность проявляется в том, что далеко не каждый клик пользователя доходит до сервера, причем обратное тоже справедливо - далеко не каждая реакция сервера обусловлена запросом пользователя. Большую часть запросов формирует движок Ajax, причем его можно написать так, что он будет загружать информацию, предугадывая действия пользователя.
Где стоит использовать Ajax:
    Формы. Они очень медленны. Если асинхронно передавать данные, страница не перезагружается.
    Навигация в виде "дерева".
    Голосования. Пользователю будет приятней оставить свой голос за несколько секунд, чем за 30-40.
    Фильтры. Часто на сайтах делают сортировку по дате, по имени. Ajax это будет значительно удобнее.
2. Серверные технологии
PHP
В первую очередь PHP используется для создания скриптов, работающих на стороне сервера, для этого его, собственно, и придумали. PHP способен решать те же задачи, что и любые другие CGI-скрипты, в том числе обрабатывать данные html-форм, динамически генерировать html страницы и тому подобное. Но есть и другие области, где может использоваться PHP.
Вторая область – это создание скриптов, выполняющихся в командной строке. То есть с помощью PHP можно создавать такие скрипты, которые будут исполняться, вне зависимости от web-сервера и браузера, на конкретной машине.
И последняя область – это создание GUI-приложений (графических интерфейсов), выполняющихся на стороне клиента.
Task:
Веб-программирование. Веб-сервисы.
Decision:
https://drupal-coder.ru/sites/drupal-coder.ru/files/WebProgr-WS(drupal-coder.ru).pdf

Математические основы информатики
Task:
Основы теории множеств и бинарных отношений. Множества конечные и бесконечные. Операции над множествами. Декартово произведение.
Source:
https://mipt.ru/diht/students/courses/combinatorics_and_number_theory.pdf?ysclid=l89k75kjsb3712703
Task:
Свойства бинарных отношений. Отношения эквивалентности. Частично упорядочные бинарные отношения. Экстремальные характеристики упорядочных множеств.
Source:
https://mipt.ru/diht/students/courses/combinatorics_and_number_theory.pdf?ysclid=l89k75kjsb3712703
Task:
Математическая логика. Основные законы математической логики.
Decision:
Высказывания и логические операции
Логика высказываний формализует определённые представления о (реальных) высказываниях и логических операциях.
Определение 1.12. Высказыванием называется повествовательное предложение, для которого имеет смысл говорить о его истинности или ложности.
Пример 1.13. Предложение «Лиссабон — столица Испании» является высказыванием.
Определение 1.14. Существуют два истинностных значения — «истина» и «ложь». Мы будем обозначать их И и Л, соответственно; считаем 1 и 0 синонимами И и Л.
Некоторые сложные высказывания строятся из более простых с помощью логических операций, таких как отрицание «не», конъюнкция «и», дизъюнкция «или», импликация «если ... , то ...».
пределение 1.15. Логическая операция — это такой способ построения сложного высказывания из данных высказываний, при котором истинностное значение сложного высказывания полностью определяется истинностными значениями исходных высказываний.
Пример 1.16. Отрицание является логической операцией. Предложение «Неверно, что Лиссабон — столица Испании» построено из высказывания «Лиссабон — столица Испании» с помощью отрицания.
Замечание 1.17. Употребляемую в естественном языке импликацию «если A, то B» нельзя в полной мере считать логической операцией, поскольку она, среди прочего, указывает и на причинно-следственную связь между высказываниями A и B, то есть не выражается только лишь через истинностные значения высказываний A и B. Более того, высказывание «если A, то B» полисемично, то есть может пониматься по-разному в разных контекстах.
В математическом языке используется материальная импликация, которая является логической связкой. При этом высказывание «если A, то B» считается ложным в том и только том случае, если A истинно и B ложно
Source:
https://docs.yandex.ru/docs/view?tm=1663641717&tld=ru&lang=ru&name=kurs_bekl.pdf&text=%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F%20%D0%BB%D0%BE%D0%B3%D0%B8%D0%BA%D0%B0&url=https%3A%2F%2Fwww.hse.ru%2Fdata%2F2011%2F11%2F24%2F1271317411%2Fkurs_bekl.pdf&lr=63&mime=pdf&l10n=ru&sign=69ebe5200299288ad9ede3eee898d206&keyno=0&serpParams=tm%3D1663641717%26tld%3Dru%26lang%3Dru%26name%3Dkurs_bekl.pdf%26text%3D%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F%2B%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D0%25BA%25D0%25B0%26url%3Dhttps%253A%2F%2Fwww.hse.ru%2Fdata%2F2011%2F11%2F24%2F1271317411%2Fkurs_bekl.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D69ebe5200299288ad9ede3eee898d206%26keyno%3D0
https://lfirmal.com/matematicheskaya-logika/?ysclid=l89lq7t622642497715#%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D0%B5_%D0%B7%D0%B0%D0%BA%D0%BE%D0%BD%D1%8B_%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B9_%D0%BB%D0%BE%D0%B3%D0%B8%D0%BA%D0%B8
Task:
Булева алгебра. Логика высказываний. Булевы функции, канонические формы задания булевых функций. Понятие полной системы. Критерий полноты Поста. Минимизации булевых функций в классах нормальных форм.
Source:
https://docs.yandex.ru/docs/view?tm=1663641717&tld=ru&lang=ru&name=kurs_bekl.pdf&text=%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F%20%D0%BB%D0%BE%D0%B3%D0%B8%D0%BA%D0%B0&url=https%3A%2F%2Fwww.hse.ru%2Fdata%2F2011%2F11%2F24%2F1271317411%2Fkurs_bekl.pdf&lr=63&mime=pdf&l10n=ru&sign=69ebe5200299288ad9ede3eee898d206&keyno=0&serpParams=tm%3D1663641717%26tld%3Dru%26lang%3Dru%26name%3Dkurs_bekl.pdf%26text%3D%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F%2B%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D0%25BA%25D0%25B0%26url%3Dhttps%253A%2F%2Fwww.hse.ru%2Fdata%2F2011%2F11%2F24%2F1271317411%2Fkurs_bekl.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D69ebe5200299288ad9ede3eee898d206%26keyno%3D0
https://docs.yandex.ru/docs/view?tm=1663643761&tld=ru&lang=ru&name=k4_n3.pdf&text=%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8%20%D0%B1%D1%83%D0%BB%D0%B5%D0%B2%D1%8B%D1%85%20%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B9%20%D0%B2%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B0%D1%85%20%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%20%D1%84%D0%BE%D1%80%D0%BC&url=http%3A%2F%2Fmoiseevs.ru%2Fdo%2Fk4_n3.pdf&lr=63&mime=pdf&l10n=ru&sign=e7aac9c8ce8a790c0804a74c25af43d5&keyno=0&nosw=1&serpParams=tm%3D1663643761%26tld%3Dru%26lang%3Dru%26name%3Dk4_n3.pdf%26text%3D%25D0%259C%25D0%25B8%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D0%25B8%2B%25D0%25B1%25D1%2583%25D0%25BB%25D0%25B5%25D0%25B2%25D1%258B%25D1%2585%2B%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B9%2B%25D0%25B2%2B%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B0%25D1%2585%2B%25D0%25BD%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D1%2585%2B%25D1%2584%25D0%25BE%25D1%2580%25D0%25BC%26url%3Dhttp%253A%2F%2Fmoiseevs.ru%2Fdo%2Fk4_n3.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3De7aac9c8ce8a790c0804a74c25af43d5%26keyno%3D0%26nosw%3D1
Task:
Исчисление предикатов первого порядка. Понятие интерпретации. Выполнимость и общезначимость формулы первого порядка.
Source:
https://docs.yandex.ru/docs/view?tm=1663644183&tld=ru&lang=ru&name=nntu28.pdf&text=%D0%98%D1%81%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%80%D0%B5%D0%B4%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D0%B2%20%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B3%D0%BE%20%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B0&url=http%3A%2F%2Fwindow.edu.ru%2Fresource%2F201%2F46201%2Ffiles%2Fnntu28.pdf&lr=63&mime=pdf&l10n=ru&sign=5375ac90699cf83554836ba7c01e975f&keyno=0&nosw=1&serpParams=tm%3D1663644183%26tld%3Dru%26lang%3Dru%26name%3Dnntu28.pdf%26text%3D%25D0%2598%25D1%2581%25D1%2587%25D0%25B8%25D1%2581%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%2B%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2582%25D0%25BE%25D0%25B2%2B%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B2%25D0%25BE%25D0%25B3%25D0%25BE%2B%25D0%25BF%25D0%25BE%25D1%2580%25D1%258F%25D0%25B4%25D0%25BA%25D0%25B0%26url%3Dhttp%253A%2F%2Fwindow.edu.ru%2Fresource%2F201%2F46201%2Ffiles%2Fnntu28.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D5375ac90699cf83554836ba7c01e975f%26keyno%3D0%26nosw%3D1
Task:
Отношения и функции.  Отношение эквивалентности и разбиения. Фактор множества. Отношения частичного порядка.
Source:
https://helpiks.org/5-63706.html?ysclid=l89qjgbd28449882870
https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%BD%D0%BE%D1%88%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B0
Task:
Основы комбинаторного анализа. Метод производящих функций, метод включений и исключений. Примеры применения.
Source:
Яблонский, Введение в дискретную математику, стр 171
Task:
Основы теории графов: определение графа, цели, циклы, пути, контуры. Матрица смежности графа. Матрица инциденций друг и ребер графов. Способы представления графов. Деревья. Связные и сильно связные графы.
Source:
https://habr.com/ru/company/otus/blog/568026/?ysclid=l89rakp2dt240583787
Task:
Пути эйлера и циклы. Алгортим построения циклов Эйлера. Гамильтоновы пути и циклы.
Source:
https://intuit.ru/studies/courses/101/101/lecture/2957
Task:
Понятие алгортима и его уточнения: машина Тьюринга, нормальные алгоритмы Маркова, рекурсивные функции. Эквивалентность данных формальных моделей алгоритмов
Source:
Яблонский, Введение в дискретную математику, стр 113
Тихомирова, Теория алгоритмов, стр 7, 126
Task:
Понятие об алгоритмической неразрешимости. примеры алгоритмически неразрешимых проблем
Source:
Тихомирова, Теория алгоритмов, стр 34
Task:
Понятие сложности алгоритмов. Классы P и NP. Полиномиальная сводимость задач. Примеры NP-полных задач, подходы к их решению. Точные и приближенные комбинаторные алгоритмы.
Source:
Тихомирова, Теория алгоритмов, стр 159
https://docs.yandex.ru/docs/view?tm=1664242710&tld=ru&lang=ru&name=Dfvo-l4-selezn.pdf&text=%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D1%8B%20p%20%D0%B8%20np.%20%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B%20np-%D0%BF%D0%BE%D0%BB%D0%BD%D1%8B%D1%85%20%D0%B7%D0%B0%D0%B4%D0%B0%D1%87&url=https%3A%2F%2Fmk.cs.msu.ru%2Fimages%2Fd%2Fdf%2FDfvo-l4-selezn.pdf&lr=63&mime=pdf&l10n=ru&sign=d9c3a329da763bbf2bf404e415b1942e&keyno=0&nosw=1&serpParams=tm%3D1664242710%26tld%3Dru%26lang%3Dru%26name%3DDfvo-l4-selezn.pdf%26text%3D%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D1%258B%2Bp%2B%25D0%25B8%2Bnp.%2B%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BC%25D0%25B5%25D1%2580%25D1%258B%2Bnp-%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BD%25D1%258B%25D1%2585%2B%25D0%25B7%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%26url%3Dhttps%253A%2F%2Fmk.cs.msu.ru%2Fimages%2Fd%2Fdf%2FDfvo-l4-selezn.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Dd9c3a329da763bbf2bf404e415b1942e%26keyno%3D0%26nosw%3D1
Task:
Примеры эффективных (полиномиальных) алгоритмов: быстрые алгоритмы поиска и сортировки; Полиномиальные алгоритмы для задач на графах и сетях (поиск в глубину и ширину, о минимальном остове, о кратчайшем пути, о назначениях)
Decision:
Поиск — обработка некоторого множества данных с целью выявления подмножества данных, соответствующего критериям поиска.
Все алгоритмы поиска делятся на
    поиск в неупорядоченном множестве данных;
    поиск в упорядоченном множестве данных.
Упорядоченность – наличие отсортированного ключевого поля.
Сортировка — упорядочение (перестановка) элементов в подмножестве данных по какому-либо критерию. Чаще всего в качестве критерия используется некоторое числовое поле, называемое ключевым. Упорядочение элементов по ключевому полю предполагает, что ключевое поле каждого следующего элемента не больше предыдущего (сортировка по убыванию).  Если ключевое поле каждого последующего элемента не меньше предыдущего, то говорят о сортировке по возрастанию.
Цель сортировки — облегчить последующий поиск элементов в отсортированном множестве при обработке данных.
Все алгоритмы сортировки делятся на
    алгоритмы внутренней сортировки (сортировка массивов);
    алгоритмы внешней сортировки (сортировка файлов).
Сортировка массивов
Массивы обычно располагаются в оперативной памяти, для которой характерен быстрый произвольный доступ. Основным критерием, предъявляемым к алгоритмам сортировки массивов, является минимизация используемой оперативной памяти. Перестановки элементов нужно выполнять на том же месте оперативной памяти, где они находятся, и методы, которые пересылают элементы из массива A в массив B, не представляют интереса.
Методы сортировки массивов можно разбить на три класса:
    сортировка включениями;
    сортировка выбором;
    сортировка обменом.
Сортировка файлов
Файлы хранятся в более медленной, но более вместительной внешней памяти, на дисках. Однако алгоритмы сортировки массивов чаще всего неприменимы, если сортируемые данные расположены в структуре с последовательным доступом, которая характеризуется тем, что в каждый момент имеется непосредственный доступ к одному и только одному компоненту.
Source:
https://prog-cpp.ru/algorithm-sort/?ysclid=l8jjhtxdua333388173
https://docs.yandex.ru/docs/view?tm=1664243504&tld=ru&lang=ru&name=NP.pdf&text=%D0%9F%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%B4%D0%BB%D1%8F%20%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%20%D0%BD%D0%B0%20%D0%B3%D1%80%D0%B0%D1%84%D0%B0%D1%85%20%D0%B8%20%D1%81%D0%B5%D1%82%D1%8F%D1%85&url=http%3A%2F%2Fintsys.msu.ru%2Fstaff%2Fvnosov%2FNP.pdf&lr=63&mime=pdf&l10n=ru&sign=cbfaee004afbcac9cef832a040e92262&keyno=0&nosw=1&serpParams=tm%3D1664243504%26tld%3Dru%26lang%3Dru%26name%3DNP.pdf%26text%3D%25D0%259F%25D0%25BE%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25BE%25D0%25BC%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B5%2B%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC%25D1%258B%2B%25D0%25B4%25D0%25BB%25D1%258F%2B%25D0%25B7%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%2B%25D0%25BD%25D0%25B0%2B%25D0%25B3%25D1%2580%25D0%25B0%25D1%2584%25D0%25B0%25D1%2585%2B%25D0%25B8%2B%25D1%2581%25D0%25B5%25D1%2582%25D1%258F%25D1%2585%26url%3Dhttp%253A%2F%2Fintsys.msu.ru%2Fstaff%2Fvnosov%2FNP.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Dcbfaee004afbcac9cef832a040e92262%26keyno%3D0%26nosw%3D1
Task:
Подходы к проектированию алгоритмов: "разделяй и влавствуй", динамическое программирование, жадная стратегия.
Source:
Ахо Структуры данных и алгоритмы, с 276
Task:
Аксиоматическое определение теории вероятности. Понятие вероятностного пространства и случайной величины. Проверка статистических гипотез. Анализ статистических взаимосвязей. Основы многомерного статистического анализа.
Source:
https://studopedia.net/5_69012_aksiomaticheskoe-opredelenie-veroyatnosti.html?ysclid=l8jqpwg170255218932
https://docs.yandex.ru/docs/view?tm=1664255533&tld=ru&lang=ru&name=TV17.pdf&text=%D0%9F%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D0%B5%20%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D0%BE%D0%B3%D0%BE%20%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%82%D0%B2%D0%B0%20%D0%B8%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D0%BE%D0%B9%20%D0%B2%D0%B5%D0%BB%D0%B8%D1%87%D0%B8%D0%BD%D1%8B&url=https%3A%2F%2Fmath.hse.ru%2Fdata%2F2017%2F09%2F26%2F1172864474%2FTV17.pdf&lr=63&mime=pdf&l10n=ru&sign=3288e93be8a789f7543b383e913240d2&keyno=0&nosw=1&serpParams=tm%3D1664255533%26tld%3Dru%26lang%3Dru%26name%3DTV17.pdf%26text%3D%25D0%259F%25D0%25BE%25D0%25BD%25D1%258F%25D1%2582%25D0%25B8%25D0%25B5%2B%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%2B%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D1%2582%25D0%25B2%25D0%25B0%2B%25D0%25B8%2B%25D1%2581%25D0%25BB%25D1%2583%25D1%2587%25D0%25B0%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B9%2B%25D0%25B2%25D0%25B5%25D0%25BB%25D0%25B8%25D1%2587%25D0%25B8%25D0%25BD%25D1%258B%26url%3Dhttps%253A%2F%2Fmath.hse.ru%2Fdata%2F2017%2F09%2F26%2F1172864474%2FTV17.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D3288e93be8a789f7543b383e913240d2%26keyno%3D0%26nosw%3D1
https://docs.yandex.ru/docs/view?tm=1664255617&tld=ru&lang=ru&name=%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-6-%D0%9F%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7.pdf&text=%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0%20%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85%20%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7&url=https%3A%2F%2Fmse.msu.ru%2Fwp-content%2Fuploads%2F2020%2F03%2F%25D0%259B%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F-6-%25D0%259F%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BA%25D0%25B0-%25D0%25B3%25D0%25B8%25D0%25BF%25D0%25BE%25D1%2582%25D0%25B5%25D0%25B7.pdf&lr=63&mime=pdf&l10n=ru&sign=f8bcd848897c5701c16072f41207e4da&keyno=0&nosw=1&serpParams=tm%3D1664255617%26tld%3Dru%26lang%3Dru%26name%3D%25D0%259B%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F-6-%25D0%259F%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BA%25D0%25B0-%25D0%25B3%25D0%25B8%25D0%25BF%25D0%25BE%25D1%2582%25D0%25B5%25D0%25B7.pdf%26text%3D%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BA%25D0%25B0%2B%25D1%2581%25D1%2582%25D0%25B0%25D1%2582%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D1%2585%2B%25D0%25B3%25D0%25B8%25D0%25BF%25D0%25BE%25D1%2582%25D0%25B5%25D0%25B7%26url%3Dhttps%253A%2F%2Fmse.msu.ru%2Fwp-content%2Fuploads%2F2020%2F03%2F%2525D0%25259B%2525D0%2525B5%2525D0%2525BA%2525D1%252586%2525D0%2525B8%2525D1%25258F-6-%2525D0%25259F%2525D1%252580%2525D0%2525BE%2525D0%2525B2%2525D0%2525B5%2525D1%252580%2525D0%2525BA%2525D0%2525B0-%2525D0%2525B3%2525D0%2525B8%2525D0%2525BF%2525D0%2525BE%2525D1%252582%2525D0%2525B5%2525D0%2525B7.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Df8bcd848897c5701c16072f41207e4da%26keyno%3D0%26nosw%3D1
https://stat.yartel.ru/index.php/osnovnye-metody-statisticheskogo-analiza/14-analiz-vzaimosvyazej
https://docs.yandex.ru/docs/view?tm=1664255971&tld=ru&lang=ru&name=UchebPosobie.pdf&text=%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B3%D0%BE%20%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B3%D0%BE%20%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0&url=https%3A%2F%2Fportal.tpu.ru%2FSHARED%2Fp%2FPOGADAEVA%2FUcheba%2FDopGlavMat%2FTab%2FUchebPosobie.pdf&lr=63&mime=pdf&l10n=ru&sign=3b4f5c0f56e87f9d5af323edb488af07&keyno=0&nosw=1&serpParams=tm%3D1664255971%26tld%3Dru%26lang%3Dru%26name%3DUchebPosobie.pdf%26text%3D%25D0%259E%25D1%2581%25D0%25BD%25D0%25BE%25D0%25B2%25D1%258B%2B%25D0%25BC%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%2B%25D1%2581%25D1%2582%25D0%25B0%25D1%2582%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B3%25D0%25BE%2B%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%25D0%25B0%26url%3Dhttps%253A%2F%2Fportal.tpu.ru%2FSHARED%2Fp%2FPOGADAEVA%2FUcheba%2FDopGlavMat%2FTab%2FUchebPosobie.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D3b4f5c0f56e87f9d5af323edb488af07%26keyno%3D0%26nosw%3D1
Task:
Статистическое описание и примеры случайных временных рядов. Стационарные временные ряды. Чисто разрывные случайные процессы.
Source:
https://russianblogs.com/article/5635779326/
https://lib.tsu.ru/mminfo/2016/Dombrovski/book/chapter-5/chapter-5-1.htm
https://spravochnick.ru/ekonomika/stacionarnye_vremennye_ryady/?ysclid=l8jsd7z9yj808238025
https://scask.ru/b_book_mp.php?id=3&ysclid=l8jsf8zaon563142089
Task:
Классические методы оптимизации, нелинейное программирование. Условная и безусловная оптимизация. Одномерный поиск. Многомерные задачи нелинейного программирования.
Source:
https://docs.yandex.ru/docs/view?tm=1664258815&tld=ru&lang=ru&name=20150201.pdf&text=%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8%2C%20%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B5%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5&url=http%3A%2F%2Fwww.lib.uniyar.ac.ru%2Fedocs%2Fiuni%2F20150201.pdf&lr=63&mime=pdf&l10n=ru&sign=a2cc7358ee2c81abef07b35b31cf9d9f&keyno=0&nosw=1&serpParams=tm%3D1664258815%26tld%3Dru%26lang%3Dru%26name%3D20150201.pdf%26text%3D%25D0%259A%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B5%2B%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4%25D1%258B%2B%25D0%25BE%25D0%25BF%25D1%2582%25D0%25B8%25D0%25BC%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D0%25B8%252C%2B%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B5%2B%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5%26url%3Dhttp%253A%2F%2Fwww.lib.uniyar.ac.ru%2Fedocs%2Fiuni%2F20150201.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Da2cc7358ee2c81abef07b35b31cf9d9f%26keyno%3D0%26nosw%3D1
http://bigor.bmstu.ru/?cnt/?doc=MO/ch0207.mod/?cou=MO/base.cou
https://docs.yandex.ru/docs/view?tm=1664259115&tld=ru&lang=ru&name=7%20%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B3%D0%BE%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8.pdf&text=%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BC%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8%20%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B3%D0%BE%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F&url=https%3A%2F%2Fportal.tpu.ru%2FSHARED%2Fm%2FMOE%2FUcheba%2FTab5%2F7%2520%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4%25D1%258B%2520%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8.pdf&lr=63&mime=pdf&l10n=ru&sign=f232726493ab3bf420544418936ef6a9&keyno=0&nosw=1&serpParams=tm%3D1664259115%26tld%3Dru%26lang%3Dru%26name%3D7%2520%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4%25D1%258B%2520%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8.pdf%26text%3D%25D0%259C%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2580%25D0%25BD%25D1%258B%25D0%25B5%2B%25D0%25B7%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%25D0%25B8%2B%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%2B%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F%26url%3Dhttps%253A%2F%2Fportal.tpu.ru%2FSHARED%2Fm%2FMOE%2FUcheba%2FTab5%2F7%252520%2525D0%25259C%2525D0%2525B5%2525D1%252582%2525D0%2525BE%2525D0%2525B4%2525D1%25258B%252520%2525D0%2525BD%2525D0%2525B5%2525D0%2525BB%2525D0%2525B8%2525D0%2525BD%2525D0%2525B5%2525D0%2525B9%2525D0%2525BD%2525D0%2525BE%2525D0%2525B3%2525D0%2525BE%252520%2525D0%2525BF%2525D1%252580%2525D0%2525BE%2525D0%2525B3%2525D1%252580%2525D0%2525B0%2525D0%2525BC%2525D0%2525BC%2525D0%2525B8.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Df232726493ab3bf420544418936ef6a9%26keyno%3D0%26nosw%3D1
Task:
Динамическое программирование. Потоки в сетях, многокритериальные задачи оптимизации. Транспортная задача.
Source:
https://docs.yandex.ru/docs/view?tm=1664259365&tld=ru&lang=ru&name=VMO.pdf&text=%D0%9F%D0%BE%D1%82%D0%BE%D0%BA%D0%B8%20%D0%B2%20%D1%81%D0%B5%D1%82%D1%8F%D1%85%2C%20%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8&url=http%3A%2F%2Fwww.unn.ru%2Fbooks%2Fmet_files%2FVMO.pdf&lr=63&mime=pdf&l10n=ru&sign=18e83949325e09c97014b0ec71d03de0&keyno=0&nosw=1&serpParams=tm%3D1664259365%26tld%3Dru%26lang%3Dru%26name%3DVMO.pdf%26text%3D%25D0%259F%25D0%25BE%25D1%2582%25D0%25BE%25D0%25BA%25D0%25B8%2B%25D0%25B2%2B%25D1%2581%25D0%25B5%25D1%2582%25D1%258F%25D1%2585%252C%2B%25D0%25BC%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D0%25BA%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5%25D1%2580%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B5%2B%25D0%25B7%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%25D0%25B8%2B%25D0%25BE%25D0%25BF%25D1%2582%25D0%25B8%25D0%25BC%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D0%25B8%26url%3Dhttp%253A%2F%2Fwww.unn.ru%2Fbooks%2Fmet_files%2FVMO.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D18e83949325e09c97014b0ec71d03de0%26keyno%3D0%26nosw%3D1
https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D0%BF%D0%BE%D1%80%D1%82%D0%BD%D0%B0%D1%8F_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0

Компьютерные технологии обработки информации
Task:
Основные виды программного обеспечения. Программные продукты и сервисы. Архитектура программных систем.
Decision:
Виды ПО:
    Системное - цинтральная часть ПО - операционная система (Винда, Линукс)
    Прикладное - общего назначения и специализированные - те, что помогут выполнить нужную нам работу (Текстовый редактор, электронные таблицы, браузер)
    Инструментальное - Создают ОС и программы для пользователей
Программные продукты (ИТ) - это любое цифровое решение, система, сервис, служба или программное обеспечение которые помогают предпринимателям и предприятиям создавать, обрабатывать, обмениваться, хранить, использовать и защищать электронно-цифровые данные.
Чтобы претендовать на включение в категорию корпоративных информационных систем, программная система или сервис должны:
    Хранить, искать и обрабатывать информацию;
    Реализовывать обще-инфраструктурные информационные функции;
    Обеспечивать внутренний и внешний доступ к информационным ресурсам, имеемым во владении.
Source:
https://docs.yandex.ru/docs/view?tm=1664845923&tld=ru&lang=ru&name=%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC.pdf&text=%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC&url=https%3A%2F%2Fwww.rea.ru%2Fru%2Fpublications%2FAttachmentsLibrary%2F%25D0%2590%25D1%2580%25D1%2585%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0%2520%25D0%25B8%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25BD%25D1%258B%25D1%2585%2520%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC.pdf&lr=63&mime=pdf&l10n=ru&sign=6670869c4216ecd91fdf86447dae780e&keyno=0&nosw=1&serpParams=tm%3D1664845923%26tld%3Dru%26lang%3Dru%26name%3D%25D0%2590%25D1%2580%25D1%2585%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0%2520%25D0%25B8%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5%2520%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25BD%25D1%258B%25D1%2585%2520%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC.pdf%26text%3D%25D0%2590%25D1%2580%25D1%2585%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0%2B%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25BD%25D1%258B%25D1%2585%2B%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%26url%3Dhttps%253A%2F%2Fwww.rea.ru%2Fru%2Fpublications%2FAttachmentsLibrary%2F%2525D0%252590%2525D1%252580%2525D1%252585%2525D0%2525B8%2525D1%252582%2525D0%2525B5%2525D0%2525BA%2525D1%252582%2525D1%252583%2525D1%252580%2525D0%2525B0%252520%2525D0%2525B8%252520%2525D0%2525BF%2525D1%252580%2525D0%2525BE%2525D0%2525B5%2525D0%2525BA%2525D1%252582%2525D0%2525B8%2525D1%252580%2525D0%2525BE%2525D0%2525B2%2525D0%2525B0%2525D0%2525BD%2525D0%2525B8%2525D0%2525B5%252520%2525D0%2525BF%2525D1%252580%2525D0%2525BE%2525D0%2525B3%2525D1%252580%2525D0%2525B0%2525D0%2525BC%2525D0%2525BC%2525D0%2525BD%2525D1%25258B%2525D1%252585%252520%2525D1%252581%2525D0%2525B8%2525D1%252581%2525D1%252582%2525D0%2525B5%2525D0%2525BC.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D6670869c4216ecd91fdf86447dae780e%26keyno%3D0%26nosw%3D1
Task:
Технологии проектирования программных систем
Source:
https://megaobuchalka.ru/3/11647.html
Task:
Принципы разработки человеко-машинного интерфейса
Decision:
Интерфейс – это набор стандартных приемов взаимодействия с техникой. Человеко-машинный интерфейс (ЧМИ) - это методы и средства обеспечения непосредственного взаимодействия между оператором и технической системой, представляющих возможности оператору управлять этой системой и контролировать ее работу. Рассмотрим основные принципы его разработки.
    Отдельная разработка интерфейса. При разработке различных частей интерфейса могут привлекаться специалисты различных специальностей, например: художники, дизайнеры, психологи, медики, пользователи, программисты.
    Учет возможностей аппаратных и программных средств разработчика и пользователя.
    Стандартизация, унификация и последовательность разработки. Использование общепринятых методов, приемов для всех элементов ПС. Необходимо применять стандарты ISO, IEC, NIST, IEEE, ГОСТ, Windows, Office и др.
    Учет особенностей и профессиональных навыков пользователя. Например, выдаваемая на экран информация не должна требовать перекодировки или дополнительной интерпретации пользователем. Пользователь должен запоминать как можно меньшее количество информации. Комфортность работы пользователя.
    Привлечение пользователя к разработке интерфейса. Использование знаний пользователя в предметной области, согласование принимаемых решений.
    Следует предусмотреть средства адаптации к пользователю. Адаптация – это способность устанавливать соответствие с уровнем подготовки пользователя. Существуют три типа адаптации: косметическая– использование клавиш прямого вызова; исключение повторных запросов; использование синонимов, опережающих ответов, умолчаний, использование макросов; многоуровневая помощь; фиксированная– пользователь явно выбирает уровень диалоговой поддержки; автоматическая – система строит модель поведения пользователя, изменяясь в процессе работы с пользователем, распознавая его характеристики (время ответа, ошибки, обращение к помощи). Можно предоставлять пользователям возможность самостоятельно распоряжаться некоторыми частями интерфейса. Это позволит в определенных пределах повысить производительность пользователей.
    Гибкость при анализе ответов пользователя. Способы достижения гибкости: сравнение со списком вариантов ответов, совпадение сокращений, частичное совпадение, алгоритм сокращения слов, использование синонимов.
    Интеллектуализация интерфейсов. Достижима путем преобразования входных сообщений в соответствии с контекстом отображаемой предметной области. Основными средствами интерфейса являются голосовой ввод информации и способность распознавания.
Организация и планирование процессов разработки программных средств. Формы организации разработки, виды планов и формы их записи.
Комплекс формально организованных мероприятий по созданию сложной системы с заданными характеристиками качества при ограниченных ресурсах получил название Проект.
Управление проектом – это вид деятельности, включающей в себя постановку задач, подготовку решений, планирование, организацию и стимулирование специалистов, контроль за ходом выполнения работ и использованием ресурсов при создании сложных систем.
Цель управления проектом – рациональное использование ресурсов путем сбалансированного распределения их по частным работам на протяжении всего цикла разработки.
Базой эффективного управления проектом является ПЛАН, в котором задачи исполнителей частных работ согласованы с выделяемыми для них ресурсами, а также между собой по результатам и срокам их достижения. План проекта отражает рациональное сочетание целей, стратегий действий, конкретных процедур, доступных ресурсов и других компонент, необходимых для достижения поставленной основной цели с заданным качеством. Планирование проектов должно обеспечивать компромисс между характеристиками создаваемой системы и ресурсами, необходимыми на ее разработку и применение.
Стадии планирования:
    первичное прогнозирование возможных характеристик проекта на базе обобщения данных подобных прототипов ранее реализованных проектов или нормативов. Существует три метода оценки стоимости разработки ПС: 1 - использование прототипов: фактическая стоимость разработанного проекта аналогичного по параметрам с разрабатываемым принимается за исходную для нового проекта. Далее эта сумма может уточняться с учетом особенностей нового проекта. 2 – нормативный. Рассмотрим этот метод на примере типовых норм времени по программированию задач для ЭВМ, утвержденных Постановлением Госкомитета СССР по труду и социальным вопросам № 454/22-70 от 27.07.1987 г. Все задачи разделены на десять классов (например бухгалтерские, плановые, статистические и др.). Для каждого класса в строках и колонках таблиц указывается число входных и выходных документов, соответственно, а в ячейках трудозатраты на программирование в человеко-днях. Существуют поправочные коэффициенты, учитывающие новизну и сложность задачи (А – применение принципиально новых методов разработки, Б – разработка типовых проектных решений, В - использование измененных типовых проектных решений, Г – привязка к готовым типовым проектным решениям); сложность документа и уровень автоматизации программирования для языков программирования. Вводятся коэффициенты и формулы расчета трудоемкости для других видов работ (проектирование, обследование, внедрение и др.) в которых участвуют рассчитанные затраты на программирование и другие показатели (количество файлов, НСИ, объемы документов в стороках и др.). Далее составляется смета затрат по видам работ в чел-днях и стоимостном выражении (чел-дни умножаются на стоимость 1 чел-дня). В настоящее время это методику вполне можно использовать после практической переоценки значений используемых параметров в расчетах с учетом новых программных средств проектирования и разработки ПС; 3 – экспертный метод: группа экспертов оценивает разработку ПС экспертным методом, который далее усредняются.
    подготовка рабочего плана (возможно сетевого графика) выполнения этапов и частных работ с учетом затрат ресурсов на их реализацию;
    управление реализацией плана, его оперативной корректировки и перераспределения ресурсов в соответствии с особенностями реализации завершенной части проекта;
    обобщение результатов планирования и управления проектом для использования этих данных в качестве прототипа при разработке последующих проектов.
Task:
Тестирование программного обеспечения
Source:
https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D1%8F
Task:
Модели представления данных, архиктектура и основные функции СУБД
Source:
https://docs.yandex.ru/docs/view?tm=1664846868&tld=ru&lang=ru&name=modeli-predstavleniya.pdf&text=%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85&url=https%3A%2F%2Fipi1.ru%2Fimages%2FPDF%2F2017%2F98%2Fmodeli-predstavleniya.pdf&lr=63&mime=pdf&l10n=ru&sign=b3d8ab7b79e9cd8787f9867ae0191866&keyno=0&nosw=1&serpParams=tm%3D1664846868%26tld%3Dru%26lang%3Dru%26name%3Dmodeli-predstavleniya.pdf%26text%3D%25D0%259C%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B8%2B%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F%2B%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585%26url%3Dhttps%253A%2F%2Fipi1.ru%2Fimages%2FPDF%2F2017%2F98%2Fmodeli-predstavleniya.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3Db3d8ab7b79e9cd8787f9867ae0191866%26keyno%3D0%26nosw%3D1
https://docs.yandex.ru/docs/view?tm=1664846958&tld=ru&lang=ru&name=677.pdf&text=%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D0%B8%20%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D0%B5%20%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8%20%D0%A1%D0%A3%D0%91%D0%94&url=https%3A%2F%2Fbooks.ifmo.ru%2Ffile%2Fpdf%2F677.pdf&lr=63&mime=pdf&l10n=ru&sign=82987176265ca4aaefaa67e6e1945c13&keyno=0&nosw=1&serpParams=tm%3D1664846958%26tld%3Dru%26lang%3Dru%26name%3D677.pdf%26text%3D%25D0%25B0%25D1%2580%25D1%2585%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0%2B%25D0%25B8%2B%25D0%25BE%25D1%2581%25D0%25BD%25D0%25BE%25D0%25B2%25D0%25BD%25D1%258B%25D0%25B5%2B%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B8%2B%25D0%25A1%25D0%25A3%25D0%2591%25D0%2594%26url%3Dhttps%253A%2F%2Fbooks.ifmo.ru%2Ffile%2Fpdf%2F677.pdf%26lr%3D63%26mime%3Dpdf%26l10n%3Dru%26sign%3D82987176265ca4aaefaa67e6e1945c13%26keyno%3D0%26nosw%3D1
Task:
Реляционный подход к организации БД. Базисные средства манипулирования реляционными данными. Методы проектирования реляционных баз данных
Decision:
Базисные средства манипулирования реляционными данными
Типовая организация СУБД
В современных СУБД логически можно выделить следующие основные компоненты:
    ядро СУБД;
    компилятор языка БД;
    подсистема поддержки времени выполнения;
    набор утилит.
Ядро СУБД отвечает за управление данными во внешней памяти, управление буферами оперативной памяти, управление транзакциями и журнализацию. Соответственно можно выделить такие компоненты ядра как менеджер данных, менеджер буферов, менеджер транзакций, менеджер журнала.
Функции всех компонентов ядра взаимосвязаны и для обеспечения корректной работы СУБД все эти компоненты должны взаимодействовать по тщательно продуманным и проверенным протоколам. Ядро СУБД обладает собственным интерфейсом, недоступным пользователям напрямую и используемым в программах, созданных средствами SQL, а также в утилитах БД. При использовании архитектуры клиент-сервер ядро является основным составляющим серверной части системы. Основной функцией компилятора языка БД является компиляция операторов языка БД в некоторую управляемую программу. Основной проблемой реляционной СУБД является то, что языки этих систем являются непроцедурными, поэтому компилятор должен решить, каким образом выполнить оператор языка, прежде чем воспроизвести программу. Результатом компиляции является выполняемая программа, представленная в некоторых системах в машинных кодах, но более часто в выполненном внутреннем машинонезависимом коде. В последнем случае реальное выполнение оператора производится с привлечением подсистемы поддержки времени выполнения, которая представляет собой интерпретатор внутреннего языка СУБД. В отдельные утилиты БД обычно выделяют такие процедуры, которые слишком накладно выполнять с использованием языка баз данных. Например, загрузка и выгрузка баз данных, сбор статистики, глобальная проверка целостности БД и т.д. Утилиты программируются с использованием интерфейса ядра СУБД, а в некоторых случаях с проникновением внутрь ядра.
Выделяют два базовых механизма манипулирования реляционными данными: реляционная алгебра и реляционное исчисление. Они обладают одним важным свойством: замкнуты относительно понятия «отношение». Это означает, что выражения «реляционная алгебра» и «формулы реляционного исчисления» определяются над отношениями баз данных и результатом вычислений также является отношение. В результате любое выражение или формула может интерпретироваться как отношение, что позволяет использовать их в других выражениях и формулах. Реляционная алгебра и реляционное исчисление обладают большой выразительной мощностью. Сложные запросы в базах данных могут быть выражены с помощью одного выражения реляционной алгебры или одной формулы реляционного исчисления. По этой причине эти механизмы включены в реляционную модель данных. Язык манипулирования реляционными данными называется реляционным, если любой запрос, представленный с помощью одного выражения реляционной алгебры или одной формулы реляционного исчисления, может быть выражен с помощью одного оператора этого языка. Механизмы реляционной алгебры и реляционного исчисления эквивалентны, то есть для любого допустимого выражения в реляционной алгебре можно построить эквивалентную, то есть производящую такой же результат, формулу реляционного исчисления и наоборот.
Присутствие в реляционной модели данных обоих механизмов обусловлено различным уровнем процедурности реляционной алгебры и реляционного исчисления. Выражения реляционной алгебры строятся на основе алгебраических операций, и подобно тому, как интерпретируются арифметические и логические выражения, выражения реляционной алгебры также имеют процедурную интерпретацию. Другими словами, запрос представленный на языке реляционной алгебры, может быть вычислен на основе элементарных алгебраических операций с учетом их старшинства и возможного наличия скобок. Для формулы реляционного исчисления однозначная интерпретация отсутствует. Формула только ставит условия, которым должны удовлетворять кортежи результирующего отношения, поэтому языки реляционного исчисления являются непроцедурными (декларативными).
Source:
https://studopedia.ru/3_89240_obshchie-ponyatiya-relyatsionnogo-podhoda-k-organizatsii-bd.html?ysclid=l8tiyaucop674004709
https://dit.isuct.ru/IVT/BOOKS/DBMS/DBMS7/dbms/1995/04/05.htm
Task:
Языки программирования в СУБД, их классификация и особенности. Стандартный язык баз данных SQL
Decision:
Классификация языков программирования в субд
Все популярные языки можно поделить на универсальные и специализированные. Универсальные языки используются для решения разных задач. Специализированные языки предназначены для решения задач одного, максимум нескольких, видов задач.(например, работы с базами данных, web-программирования или написание скриптов для администрирования операционных систем).
Языки для работы с базами данных:
    Языки, входящие в состав промышленных клиент-серверных систем управления базами данных.(СУБД) (PL-SQL в СУБД Oracle, Transact-SQL в Microsoft SQL Server)
    Языки являющиеся частью других видов СУБД (Visual FoxPro, Microsoft Access, Paradox и т.п.)
Source:
https://intuit.ru/studies/courses/508/364/lecture/8643?page=2
Task:
Основные сетевые концепции. Глобальные, территориальные и локальные сети. Проблемы стандартизации. Сетевая модель OSI. Модели взаимодействия компьютеров в сети.
Decision:
Основные понятия
Сеть – это соединение между двумя и более компьютерами, позволяющее им разделять ресурсы.
Классификация сетей по масштабу
Локальная сеть (Local Area Network) представляет собой набор соединенных в сеть компьютеров, расположенных в пределах небольшого физического региона, например, одного здания.
Это набор компьютеров и других подключенных устройств, которые укладываются в зону действия одной физической сети. Локальные сети представляют собой базовые блоки для построения объединенных и глобальных сетей.
Глобальные сети (Wide Area Network) могут соединять сети по всему миру; для межсетевых соединений обычно используются сторонние средства коммуникаций.
Соединения в глобальных сетях могут быть очень дорогими, так как стоимость связи растет с ростом ширины полосы пропускания. Таким образом, лишь небольшое число соединений в глобальных сетях поддерживают ту же полосу пропускания, что и обычные локальные сети.
Региональные сети (Metropolitan Area Network) используют технологии глобальных сетей для объединения локальных сетей в конкретном географическом регионе, например, городе.
Классификация сетей по наличию сервера
Одноранговые сети 
Компьютеры в одноранговых сетях могут выступать как в роли клиентов, так и в роли серверов. Так как все компьютеры в этом типе сетей равноправны, то одноранговые сети не имеют централизованного управления разделением ресурсов. Любой из компьютеров в этой сети может разделять свои ресурсы с любым компьютером из этой же сети. Одноранговын взаимоотношения также означают, что ни один компьютер не имеет ни высшего приоритета на доступ, ни повышенной ответственности за предоставление ресурсов в совместное использование .
Преимущества одноранговых сетей:
    они легки в установке и настройке;
    отдельные машины не зависят от выделенного сервера;
    пользователи в состоянии контролировать свои собственные ресурсы;
    недорогой тип сетей в приобретении и эксплуатации;
    не нужно никакого дополнительного оборудования или программного обеспечения, кроме операционной системы;
    нет необходимости нанимать администратора сети;
    хорошо подходит с количеством пользователей, не превышающих 10.
Недостатки одноранговых сетей:
    применение сетевой безопасности одновременно только к одному ресурсу;
    пользователи должны помнить столько паролей , сколько имеется разделенных ресурсов;
    необходимо производить резервное копирование отдельно на каждом компьютере, чтобы защитить все совместные данные;
    при получении доступа к ресурса, на компьютере, на котором этот ресурс расположен, ощущается падение производительности;
    не существует централизованной организационной схемы для поиска и управления доступом к данным.
Сети с выделенным сервером 
Компания Microsoft предпочитает термин Server-based. Сервер представляет собой машину (компьютер), чьей основной задачей является реакция на клиентские запросы. Серверы редко управляются кем-то непосредственно – только чтобы установить, настроить или обслуживать.
Достоинства сетей с выделенным сервером:
    они обеспечивают централизованное управление учетными записями пользователей, безопасностью и доступом, что упрощает сетевое администрирование;
    более мощное оборудование означает и более эффективный доступ к ресурсам сети;
    пользователям для входа в сеть нужно помнить только один пароль, что позволяет им получать доступ ко всем ресурсам, у которым имеет право;
    такие сети лучше масштабируются (растут) с ростом числа клиентов.
Недостатки сетей с выделенным сервером:
    неисправность сервера может сделать сеть неработоспособной, в лучшем случае – потеря сетевых ресурсов;
    такие сети требуют квалифицированного персонала для сопровождения сложного специализированного программного обеспечения;
    стоимость сети увеличивается, благодаря потребности в специализированном оборудовании и программном обеспечении.
Выбор сети
Выбор сети зависит от ряда обстоятельств:
    количество компьютеров в сети (до 10 – одноранговые сети);
    финансовые причины;
    наличие централизованного управления, безопасность;
    доступ к специализированным серверам;
    большая нагрузка на сетевые ресурсы;
    доступ к глобальной сети.
Сетевая модель OSI (The Open Systems Interconnection model) — сетевая модель стека (магазина) сетевых протоколов OSI/ISO. Посредством данной модели различные сетевые устройства могут взаимодействовать друг с другом. Модель определяет различные уровни взаимодействия систем. Каждый уровень выполняет определённые функции при таком взаимодействии. 
какие уровни модели OSI знаешь?
    прикладной (данные) - взаимодействие с конечным пользователем (http, smtp, snmp, ftp, telnet, ssh, scp, smb, nfs, rtsp, bgp, frp, dhcp, imap)
    представление (данные) - представление, кодирование и шифрование данных (xdr, afp, tls, ssl, ascii)
    сеансовый (данные) - управление сеансом связи (rpc, netbios, pptp, l2tp, asp, pap)
    транспортный (сегмент/дейтаграмма) - надежная свзяь между двумя конечными устройствами поверх ненадежной сети (rcp, udp, sctp, spx, rtp, atp, dccp, gre)
    сетевой (пакет) - логическая адрессация и определение маршрута (ip, icmp, igmp, clnp, ospf, rip, ipx, ddp, arp, rarp, nat, ipsec)
    канальный (кадр, фрейм) - канальная адрессация и обнаружение ошибок физ уровня (ppp, pppoe, ethernet, 802.1, arp, hdlc, frame relay, token ring, isdn, atm, mpls)
    физический(бит) - определяет характеристики сигналов и среды передачи данных (ethernet, usb, bluetooth, электрические провода, радиосвязь, волоконо-оптические провода, wifi)
Task:
Принципы функционирования Internet. Типовые информационные объекты и ресурсы. Ключевые аспекты www-технологии
Decision:
Основные компоненты технологии World Wide Web
Технология WWW состоит из четырех основных компонентов:
    язык гипертекстовой разметки документов HTML (HyperText Markup Language);
    универсальный способ адресации ресурсов в сети URL (Universal Resource Locator);
    протокол обмена гипертекстовой информацией HTTP (HyperText Transfer Protocol);
    универсальный интерфейс шлюзов CGI (Common Gateway Interface).
НТМL (Hyper Text Markup Language) — язык разметки гипертекста возник на стыке нескольких направлений исследований и разработок. Язык НТМL использует команды — теги, вводимые в текстовые документы, которые указывают, каким образом информация должна на выводиться на экран.
HTML-документ представляет собой текстовый файл, который может быть создан с помощью любого текстового редактора, но, в отличие от обычных текстовых файлов, он имеет расширение .htm (или .html).
Первые концепции и разработки, посвященные гипертексту, принадлежали Ванневару Бушу (научному советнику американского президента Ф.Д.Рузвельта), описавшему в своих трудах в сороковые годы двадцатого века гипертекст, а также браузер — диалоговую машину для просмотра обширной тексто-графической системы и пополнения ее записями, а также Дагласу Энгельбарту и Теодору Нельсону, работавшим над развитием гипертекстовой технологии в шестидесятые годы двадцатого века.
Сам термин "гипертекст" предложил Тед Нельсон в 1965 году. Вот как звучит определение гипертекста, которое дал Нельсон : "форма письма, которое ветвится или осуществляется по запросу". Иначе говоря, HTML — это "нелинейное письмо", которое "больше, чем текст" (hypertext).
Одним из важных фактором, определившим специфику HTML, было его применение в Интернете. В 1989 году Тим Бернерс-Ли предложил глобальную гипертекстовую систему, позволившую соединять связями не только текст, но и графику, звуки, видео. Глобальность этой системы предполагала, что данные будут распределяться по всему миру, а ее основой станет Интернет.
Через год Бернерс-Ли написал первое клиент-серверное программное обеспечение (гипертекстовую систему Enquire), а затем гипретекстовый протокол передачи данных HTTP (Hypertext Transfer Protokol).
Простота языка разметки гипертекста HTML, возможность готовить с его помощью Web-документы с использованием простейших текстовых редакторов, ориентация на HTML более «изысканных» механизмов оформления Web-документов сделали его на сегодняшний день ядром всех Web-технологий.
Source:
https://vuzlit.com/987074/printsipy_funktsionirovaniya_internet?ysclid=l8tkj5h518434071054
Task:
Адресация в сети Internet. Методы и средства поиска информации в Internet, информационно-поисковые системы
Decision:
Адресация в сети Internet — это название устройства, которое присваивается ему в интернете, то есть, это его IP-адрес, сформированный из некоторой совокупности символов и цифр. 
Основные методы поиска информации в Интернете
    Непосредственный поиск с использованием гипертекстовых ссылок (поиск информации производится путем последовательного просмотра связанных страниц с помощью браузера);
    Использование поисковых машин (использовании ключевых слов, которые передаются поисковым серверам в качестве аргументов поиска: что искать);
    Поиск с применением специальных средств (применении специализированных программ - спайдеров, которые в автоматическом режиме просматривают Web-страницы, отыскивая на них искомую информацию);
    Анализ новых ресурсов (необходим при проведении повторных циклов поиска, поиска наиболее свежей информации или для анализа тенденций развития объекта исследования в динамике).
Большинство поисковых машин обновляет свои индексы со значительной задержкой, вызванной гигантскими объемами обрабатываемых данных, и эта задержка обычно тем больше, чем менее популярна интересующая тема.
Большинство поисковых систем ищут информацию на сайтах Всемирной паутины, но существуют также системы, способные искать файлы на ftp-серверах, товары в интернет-магазинах, а также информацию в группах новостей Usenet.
Основные критерии качества работы поисковой машины:
    релевантность (степень соответствия запроса и найденного, то есть уместность результата);
    полнота базы;
    учёт морфологии языка.
Средства поиска информации:
    Тематические каталоги ресурсов;
    Поисковые системы;
    Метапоисковые системы.
Тематические каталоги.
В каталогах информация о сайтах упорядочена в соответствии с рубрикатором. В отличие от простых подборок ссылок имеется механизм поиска. Работа поисковых механизмов ограничивается поиском в кратких аннотациях сайтов.
Кроме основных разделов многие каталоги имеют дополнительные разделы, в которых сайты классифицированы по другому основанию:
    региону, стране;
    алфавиту;
    популярности.
Информация о сайтах в каталоги может вноситься вручную как авторами сайта через механизм регистрации, так и модераторами - ведущими рубрик каталога. Преимущество каталога - создание списка "отборных" сайтов. Недостатки: неоднозначность структуры, требуется время для поиска нужной категории, количество сайтов в каталоге недостаточно большое.
Особенность каталогов в том, что они более эффективны при поиске подборок информации на определенную общую тему, например, "развитие образования", "состояние промышленности", но не при поиске ответа на конкретный вопрос!
Каталоги могут быть:
    специализированными,
    универсальными.
Source:
https://otherreferats.allbest.ru/programming/00936930_0.html?ysclid=l8tkytkbd546444975











Специалист поддержки в Yandex Cloud
Task:
Что такое облачные технологии
Decision:
Облачные платформы позволяют арендовать IT-ресурсы: серверы, базы данных, IP-адреса, сетевую инфраструктуру или даже нейросети.
Основные преимущества облачных платформ
1. Разделение ответственности
Облачная платформа похожа на каршеринг: вы арендуете машину, а за техосмотр, мойку и обслуживание отвечает компания-арендодатель. В облаках точно так же: вы как пользователь не беспокоитесь об издержках владения техникой и её обслуживания. Сервер вышел из строя, жёсткий диск перестал работать, сисадмин на больничном? Всё это не ваша головная боль. Вы можете сосредоточиться на своём продукте или бизнесе.
2. Масштабируемость
Представьте, что вы владеете интернет-магазином подарков. Перед Новым годом сайт магазина посещает в десять раз больше посетителей, чем обычно. Если сервер с базой данных магазина не справится с наплывом покупателей — они наверняка перейдут к конкурентам. Докупить и настроить серверы? На это нужны время, деньги и специалисты. К тому же после праздников, когда посетителей мало, оборудование будет простаивать.
На облачной платформе достаточно оплатить аренду дополнительных серверов. Когда нагрузка спадёт, вы вручную или автоматически вернёте ненужные мощности.
3. Экономия
Во-первых, при аналогичном для бизнеса результате вы экономите время, деньги и усилия, которые пришлось бы потратить на техническую поддержку оборудования.
Во-вторых, вы платите только за то, чем пользуетесь. Например, если вы остановили свою виртуальную машину, то оплачивать её вычислительные ресурсы не нужно.
IaaS, PaaS и SaaS
Облачная платформа предоставляет вам IT-инфраструктуру и сервисы в аренду. Вы пользуетесь ими, когда они нужны, и освобождаете, когда в них больше нет необходимости. Такой принцип аренды называется as a Service — как сервис.
Infrastructure as a Service (IaaS) — это базовый уровень. В него входит аренда виртуальных серверов, виртуальных сетей и всего, что с ними связано. Команда Yandex Cloud подготовила и протестировала несколько популярных образов ВМ с наборами программного обеспечения для конкретных задач. С помощью IaaS вы можете отправить в облако сайт, бэкэнд мобильного приложения, систему Continuous Integration для разработчиков или 1C для бухгалтерии. Кроме того, вы можете загружать и использовать собственные образы.
Поверх IaaS строится следующий уровень — Platform as a Service (PaaS). PaaS позволяет разворачивать в облаке современные веб-приложения, не задумываясь об инфраструктурных элементах: виртуальных машинах и сетях. Пример PaaS — управляемые базы данных (БД). Вам необязательно устанавливать и администрировать БД на виртуальной машине: купите БД нужного размера с автоматическим резервным копированием и другими полезными возможностями и сразу же начните пользоваться ей.
Ещё выше находится уровень Software as a Service (SaaS). Здесь даже не надо настраивать БД: вы платите за готовый софт, который развёрнут для вас в облаке с помощью инфраструктурных и платформенных сервисов. Пример SaaS — GitLab, система для совместного управления кодом в командах разработки. Вы получаете готовое решение и не думаете о том, как оно устроено.
Task:
Вы вручную увеличили количество серверов в облаке, чтобы справиться с возросшей нагрузкой на сайт. Что произойдёт, когда нагрузка спадёт?
Decision:
Ресурсы выделены, их работа гарантирована, за это надо будет заплатить вне зависимости от нагрузки
Task:
Выберите правильные утверждения
Decision:
-Аренда виртуальных машин относится к PaaS:
-IaaS не позволяет загружать и использовать собственные образы
+Управляемые базы данных относятся к PaaS
-Управляемые базы данных относятся к SaaS
Task:
Облачные сервисы
Decision:
Универсальность курса
Yandex Cloud — не единственное в мире публичное облако. Существуют и другие сильные и функциональные облачные платформы зарубежных и российских провайдеров (Amazon, Google, VK). Однако все эти компании используют примерно один и тот же основной стек технологий: виртуальные машины, Docker, Kubernetes, Apache Hadoop, PostgreSQL, MariaDB, NoSQL. Пройдя этот курс, вы без труда разберётесь с любой платформой.
Состав платформы
Сервисы Yandex Cloud делятся на девять групп:
    Инфраструктура и сеть — инфраструктурные сервисы для обработки данных, безопасного доступа к ним и обмена трафиком.
    Платформа данных — управление базами данных и кластерами, масштабируемое хранение данных, сбор и визуализация метрик и данных.
    Контейнерная разработка — управление кластерами Kubernetes и Docker-образами.
    Инструменты разработчика — сервисы для оптимизации разработки и тестирования приложений.
    Бессерверные вычисления — сервисы для хранения данных и разработки приложений без создания виртуальных машин.
    Безопасность — управление ключами шифрования и TLS-сертификатами, защита от DDoS-атак.
    Ресурсы и управление — идентификация и контроль доступа к облачным ресурсам, управление ресурсами в каталогах и облаках.
    Машинное обучение — речевые технологии, анализ изображений и машинный перевод.
    Бизнес-инструменты — визуализация и анализ данных, хранение базы знаний, трекер задач для организации работы команды.
Базовая часть платформы — инфраструктурные сервисы для обработки данных и управления трафиком.
Сервисы этой группы позволяют:
    разворачивать виртуальные серверы и сеть между ними (Yandex Compute Cloud, Yandex Virtual Private Cloud, Yandex Cloud Interconnect);
    адаптировать конфигурацию виртуальных машин под изменяющиеся нагрузки, строить отказоустойчивые решения (Yandex Network Load Balancer и Yandex Application Load Balancer);
    хранить данные в объектном хранилище (Yandex Object Storage);
    повышать безопасность решений (Yandex DDoS Protection);
    создавать API-шлюзы (Yandex API Gateway).
Сервисы этой группы мы будем изучать в курсе «Виртуальные машины».
Платформа данных — группа сервисов для надёжного хранения, обработки и визуализации данных.
Сервисы Платформы данных позволяют:
    разворачивать кластеры баз данных (Managed Service for PostgreSQL, ClickHouse, MongoDB, MySQL, Redis, SQL Server, Apache Kafka, Elasticsearch, Greenplum, YDB);
    визуализировать и анализировать данные (Yandex DataLens);
    управлять кластерами Apache Hadoop (Yandex Data Proc);
    переносить базы данных с помощью Yandex Data Transfer;
    создавать очереди для обмена сообщениями между компонентами распределённых приложений и микросервисов (Yandex Message Queue).
    Подробнее об этих сервисах вы узнаете в курсах «Хранение и анализ данных» и «Serverless».
Сервисы группы Контейнерная разработка предназначены для управления кластерами Kubernetes и Docker-образами, а также для запуска контейнеризированных приложений без Kubernetes.
Сервисы группы Инструменты разработчика позволяют оптимизировать разработку и выполнять тестирование приложений.
Сервисы группы Бессерверные вычисления позволяют:
    запускать код в бессерверной архитектуре приложений (Yandex Cloud Functions);
    работать с устройствами интернета вещей (Yandex IoT Core);
    обмениваться данными между приложениями через сервис очередей (Yandex Message Queue);
    работать с базой данных с тарификацией по количеству запросов (Yandex Managed Service for YDB);
    применять сервис для управления API-шлюзами (Yandex API Gateway).
Подробнее об этих сервисах вы узнаете в курсе «Serverless».
Группа Безопасность представлена инструментами для управления безопасностью вашей облачной инфраструктуры.
Сервисы этой группы позволяют:
    управлять ключами шифрования (Yandex Key Management Service);
    защищать ресурсы от DDoS-атак (Yandex DDoS Protection);
    создавать и хранить логины и пароли, ключи сертификатов серверов, ключи сервисного аккаунта в облаке и других конфиденциальных данных (Yandex Lockbox);
    управлять TLS-сертификатами (Yandex Certificate Manager).
В группе Ресурсы и управление собраны инструменты мониторинга и управления ресурсами облака.
С их помощью вы можете:
    идентифицировать и контролировать доступ к облачным ресурсам (Yandex Identity and Access Management);
    собирать и визуализировать метрики (Yandex Monitoring);
    управлять ресурсами в каталогах и облаках (Yandex Resource Manager);
    управлять сервисами организации (Yandex Cloud Organization);
    читать и записывать логи сервисов и пользовательских приложений (Yandex Cloud Logging).
С помощью инструментов группы Машинное обучение вы можете строить современные решения на базе технологий искусственного интеллекта.
Сервисы Yandex SpeechKit и Yandex Translate позволяют работать с голосом и текстом, Yandex Vision — анализировать изображения, а Yandex DataSphere — создавать модели машинного обучения.
Наконец, группа Бизнес-инструменты объединяет инструменты для организации работы команды (Yandex Tracker), создания базы знаний (Yandex Wiki), настройки форм (Yandex Forms), визуализации и анализа данных (Yandex DataLens), а также сервис удаленных рабочих мест в облаке (Yandex Cloud Desktop).
Task:
Какие задачи можно решать с помощью Yandex Cloud?
Decision:
+Хранить бэкапы
-Создавать голосовых роботов
+Строить отказоустойчивые веб-приложения
Task:
Внутри облака
Decision:
Физическая инфраструктура Yandex Cloud
Любые облака всегда начинаются с серверов. А точнее, с центров обработки данных, или дата-центров (ДЦ), где эти серверы стоят. Yandex Cloud — не исключение.
Инфраструктура Yandex Cloud состоит из трёх дата-центров. Они расположены в Подмосковье, Владимирской и Рязанской областях и связаны между собой собственными оптоволоконными сетями.
С дата-центрами связано важное понятие — зона доступности, которая представляет собой независимый сегмент облачной инфраструктуры. В Yandex Cloud есть три зоны доступности: ru-central1-a, ru-central1-b и ru-central1-c. Каждая зона — это отдельный дата-центр. ДЦ полностью независимы друг от друга — сбой в одном из них не сказывается на работе остальных. Поэтому развернув своё приложение сразу в нескольких зонах доступности, вы заметно повысите его отказоустойчивость и снизите вероятность потери данных.
Сотрудники Яндекса сами спроектировали и обслуживают ДЦ. Например, стандарты стоек серверов разработаны специально для ДЦ и учитывают особенности его энергоснабжения и охлаждения. Поставщики производят стойки только в соответствии с этим стандартом.
Это же касается и серверов. Расположение узлов, коммуникации и корпуса спроектированы в соответствии с особенностями питания, охлаждения и эксплуатации ДЦ.
Yandex Cloud гарантирует, что виртуальные машины в облаке доступны как минимум 99,95% времени, и обязуется компенсировать потери, если нарушит эти гарантии.
Task:
Доступ к ресурсам, платёжный аккаунт, пробный период
Decision:
Прежде чем вы начнёте изучать технологии и сервисы Yandex Cloud, разберёмся с принципами использования платформы.
Иерархия ресурсов
Ресурсы в Yandex Cloud — это все сущности, которые вы можете создать (виртуальные машины, диски, виртуальные сети). Ресурсы объединены в группы — каталоги. Каталоги объединены в облако. Облако — самая крупная логическая единица в Yandex Cloud. Она изолирована: ресурсы в облаке не могут взаимодействовать с ресурсами в другом облаке.
Иерархия поможет вам не запутаться, если ресурсов много.
Поскольку в одном облаке может быть много каталогов, в общем случае достаточно простой иерархии: один проект — один каталог внутри общего облака на компанию. Но если проектов и направлений много, иерархию можно построить иначе. Например, в вашей компании два больших направления: разработка игр и разработка систем интернета вещей. Для каждого направления создайте облако, для каждого проекта — каталог, а в каталог поместите ресурсы, которые касаются только этого проекта.
Роли
Сервис Yandex Resource Manager определяет ресурсную модель Yandex Cloud и позволяет структурировать ресурсы с помощью каталогов. С его помощью вы также можете назначить пользователю роли, определяющие, какие действия этот пользователь может выполнять с доступными ресурсами.
Роли бывают двух типов:
    Примитивные роли — содержат разрешения, действующие для всех типов ресурсов Yandex Cloud. Это роли admin, editor и viewer.
    Сервисные роли — содержат разрешения только для определенного типа ресурсов в указанном сервисе.
Примитивных ролей три. Они позволяют:
Роль    Просматривать информацию о ресурсах Управлять ресурсами (создавать, изменять, удалять)  Настраивать доступ к ресурсам для других пользователей
viewer  +   –   –
editor  +   +   –
admin   +   +   +
Сервисных ролей много, так как они сильно зависят от логики работы сервисов. Например, у Yandex Resource Manager есть две сервисные роли:
    resource-manager.clouds.owner — назначается только на облако, даёт полный доступ к облаку и ресурсам в нём;
    resource-manager.clouds.member — роль необходима для доступа к ресурсам в облаке другим пользователям без административных прав.
Вернёмся к иерархии Облако → Каталог → Ресурс. Роли в ней наследуются:
    resource-manager.clouds.owner может выполнять любые действия с любым ресурсом в любом каталоге;
    пользователь с ролями resource-manager.clouds.member на облако и editor на облако может управлять любыми ресурсами в любом каталоге;
    editor каталога может управлять ресурсами только в своём каталоге.
У каждого облака обязательно должен быть хотя бы один владелец (owner). У пользователя может быть роль owner в нескольких облаках.
Пользователи
Пользователем называется человек или программа, которые взаимодействуют с ресурсами. Пользователи идентифицируются с помощью аккаунтов. В Yandex Cloud есть три типа аккаунтов, с помощью которых можно авторизоваться для доступа к ресурсам:
    Яндекс ID — единый аккаунт человека во всех сервисах Яндекса;
    Федеративный аккаунт — аккаунт человека во внешних по отношению к Яндексу системах, которые поддерживают технологии аутентификации. Например, аккаунт в Microsoft Active Directory — популярной системе хранения информации о корпоративных пользователях;
    Сервисный аккаунт — аккаунт, от имени которого программы могут управлять ресурсами.
Сервис Yandex Identity and Access Management (IAM) проверяет права пользователей на действия с ресурсами. Проверка прав называется авторизацией.
Чтобы пользователь подтвердил личность, он должен пройти аутентификацию. Её способы зависят от типа аккаунта. Например, с Яндекс ID аутентификация происходит автоматически. Сервисные аккаунты аутентифицируются с помощью IAM-токена, API-ключей или статических ключей доступа.
Для примера посмотрим на последовательность работы Yandex Cloud при создании ВМ.
Платёжный аккаунт
Платёжный аккаунт используется для идентификации пользователя, оплачивающего ресурсы. Он хранит информацию о плательщике и может быть не связан напрямую с конкретным человеком. С точки зрения Resource Manager платежный аккаунт — это ресурс. К нему можно давать доступ обычным аккаунтам, используя роль billing.accounts.member.
Платёжный аккаунт может оплачивать ресурсы нескольких облаков. Облако может быть привязано только к одному платёжному аккаунту.
Платёжный аккаунт хранит следующую информацию:
    Лицевой счёт — уникальный идентификатор. Используется для оплаты ресурсов во время действия договора. Содержит средства, зачисленные с помощью банковской карты или банковского перевода;
    Способ оплаты — способ, которым вы можете зачислять средства на лицевой счёт.
Лицевой счёт создается автоматически, а способ оплаты зависит от типа платёжного аккаунта:
Для кого предназначен   Способ оплаты   Документы об оплате, которые вы получите на email
Личный аккаунт  Физические лица — резиденты России или Казахстана*  Банковская карта    Чек
Бизнес-аккаунт  Юридические лица: резиденты и нерезиденты России    Перевод с расчётного счёта или корпоративной банковской карты   Акт и счёт
    Yandex Cloud не работает с физическими лицами — нерезидентами России и Казахстана.
Если вы никогда не пользовались услугами Yandex Cloud и создали платёжный аккаунт впервые — вы можете активировать пробный период и получить стартовый грант 4 000 ₽ для знакомства с облачными сервисами.
Пробный период заканчивается через 60 дней или когда стартовый грант израсходован. Когда это произойдёт:
    ваши ВМ и кластеры баз данных будут остановлены;
    вы не сможете прочитать или скачать сохранённые данные;
    остатки стартового гранта сгорят.
Если вы не перейдёте на платную версию, то через 30 дней ваши ресурсы будут удалены. Чтобы перейти на платную версию, в консоли управления на странице Биллинг откройте страницу платёжного аккаунта и нажмите Перейти на платную версию.
Мы рекомендуем останавливать работающие ВМ в перерывах между обучением, а ненужные — удалять, чтобы гранта хватило для прохождения всех курсов профессии. Лучше делать это в конце каждой темы, поскольку внутри тем есть сквозные практические занятия, использующие одну и ту же созданную ранее ВМ.
Остановленные ВМ тарифицируются, поскольку загрузочный диск занимает пространство в хранилище. Однако остановленная ВМ будет обходиться гораздо дешевле работающей. Инструкцию по остановке и удалению ВМ вы найдёте в одном из ближайших практических занятий.
Теперь вы знаете достаточно, чтобы начать работать с Yandex Cloud.
Task:
Как называется сервис, отвечающий за иерархию ресурсов?
Decision:
+Yandex Resource Manager
-Yandex Identity and Access Management
-Yandex Instance Groups
-Yandex Compute Cloud
Task:
Какой сервис отвечает за предоставление доступа к ресурсам?
Decision:
-Yandex Resource Manager
+Yandex Identity and Access Management
-Yandex Instance Groups
-Yandex Compute Cloud
Task:
Как устроены ресурсы в Yandex.Cloud, от более крупных к более мелким?
Decision:
-Облако -> Ресурс -> Каталог
-Каталог -> Облако -> Ресурс
+Облако -> Каталог -> Ресурс
-Ресурс -> Каталог -> Облако
Task:
Какой тип аккаунта не используется для авторизации при доступе к ресурсам?
Decision:
-Аккаунт на Яндексе
-Федеративный аккаунт
-Сервисный аккаунт
+Платёжный аккаунт
Task:
В чём различия бизнес-аккаунта и личного аккаунта?
Decision:
-Ни в чём
+Бизнес-аккаунт — для юридических лиц, личный — для частных лиц
-Разными уровнями техподдержки
-Разными правами доступа
Task:
Как происходит переход на платную версию?
Decision:
-Автоматически
-Через обращение в поддержку
+Нажатием специальной кнопки в консоли
-Через обращение к менеджеру


Task:
Практическая работа. Создание аккаунта
Decision:
В Профессии "Инженер облачных сервисов" вас ожидают не только теоретические уроки на платформе Яндекс Практикум, но и практические занятия в Yandex Cloud. Чтобы их выполнить, вам понадобится своё облако. Давайте его создадим!
Для того, чтобы работать в облаке, нужен платёжный аккаунт. Если это ваш первый аккаунт в Yandex Cloud, то после его создания вы сможете активировать 60-дневный пробный период и получить стартовый грант. Это позволит вам выполнять практические работы, не тратя собственных средств.
Размер гранта для резидентов России составляет 4 000 ₽. Этого должно хватить на прохождение всей программы. А в случае её успешного завершения обучения и соблюдения некоторых условий вы получите еще один грант, который можно будет использовать, чтобы применить полученные знания и навыки для решения ваших задач в облаке.
Если у вас нет личного платёжного аккаунта:
Откройте в браузере сайт cloud.yandex.ru. Если вы не авторизованы, в правом верхнем углу или на баннере нажмите кнопку Подключиться. - Войдите в свой аккаунт на Яндексе. - Если у вас его нет, то вам будет предложено создать новый Яндекс ID. При регистрации в Яндекс ID заполните все поля, в том числе номер телефона: он потребуется, чтобы создать платёжный аккаунт в Yandex Cloud. - Примите условия использования Yandex Cloud. - После аутентификации при первом входе в консоль управления вам будет предложено создать облако. Укажите название облака и нажмите кнопку Создать. - Вам будет назначена роль владельца — owner. - На этом этапе у вас есть облако, но нет платёжного аккаунта. Если вы перейдете в раздел Биллинг (в левом верхнем углу выберите меню Все сервисы -> Биллинг), то увидите, что в списке аккаунтов пока пусто. - Чтобы создать платёжный аккаунт, нажмите кнопку Создать аккаунт в разделе Биллинг или на стартовой странице. При создании аккаунта заполните все данные (выберите тип плательщика — Физическое лицо) и добавьте банковскую карту. Для проверки система спишет с неё небольшую сумму денег, а затем сразу вернёт на счёт. - Важно! Обязательно выберите опцию Включить пробный период. Если этого не сделать, то ваш платежный аккаунт будет сразу переведен в режим платного потребления. - Пробный период позволяет использовать ресурсы Yandex Cloud в ограниченном режиме в течение 60 дней. Потреблённые ресурсы оплачиваются из стартового гранта. После завершения пробного периода ваши ресурсы в облаке будут остановлены, а чтобы возобновить работу в полном объёме потребуется перейти на платную версию. - Важно! Не отвязывайте банковскую карту во время пробного периода: в этом случае облако заблокируется. Если вы не перейдёте на платную версию, средства с карты списываться не будут.
Task:
Основная информация о виртуальных машинах
Decision:
Виртуальная машина (ВМ) — это аналог физического сервера в облачной инфраструктуре. По сути, компьютер внутри компьютера, изолированный от операционной системы, в которой запущен.
Физически ВМ может находиться на сервере в любой из трёх зон доступности Yandex Cloud. У каждой зоны независимая инфраструктура, поэтому сбой в одной зоне не влияет на ВМ в других.
Внутри ВМ работает операционная система с прикладным программным обеспечением (ПО), например с базой данных. Для функционирования приложений выделяются ресурсы: ядра процессора, оперативная память и диски. Чтобы установить, настроить или обновить приложения, вы можете подключиться к ВМ удалённо.
Главное преимущество ВМ по сравнению с физическими серверами — ВМ проще управлять. Например, чтобы создать интернет-магазин в инфраструктуре Compute Cloud, достаточно создать ВМ с предустановленной системой OpenCart. Вам останется лишь зайти в административную панель OpenCart, настроить внешний вид магазина и эквайринг, добавить товары, а затем в административной панели регистратора доменных имён настроить перенаправление на ВМ.
При этом ВМ будет гарантированно выделяться указанное вами количество ресурсов. А если ресурсов не хватит, вы сможете быстро увеличить их или использовать автоматическое масштабирование. Тогда количество ВМ, обслуживающих магазин, автоматически подстроится под наплывы посетителей, чтобы вы не потеряли клиентов из-за медленной реакции и отказов сервера.
Чтобы создать резервные копии данных на дисках ВМ, достаточно нажать несколько кнопок в понятном веб-интерфейсе. Если вы неудачно накатили обновление веб-приложения, можно быстро вернуться к предыдущей версии.
Кроме того, ВМ предпочтительнее, если:
    вам нужен отказоустойчивый кластер. Поместите серверы в разных зонах доступности Yandex Cloud, настройте быстрый перенос данных и создание резервных копий.
    вы периодически запускаете ресурсоёмкие задачи. Например, преобразование форматов файлов, обработку и распознавание изображений, решение вычислительных задач. Не надо покупать оборудование — арендуйте ресурсы в Yandex Cloud, а когда надобность в них исчезнет, просто остановите их.
    вы занимаетесь тестированием и прототипированием. На ВМ удобно проверять тестовые версии продуктов. Проводите нагрузочное, функциональное и регрессионное тестирование компонентов перед публикацией. Yandex Compute Cloud позволяет быстро создавать прототипы решений и проверять идеи на ранних стадиях разработки.
Task:
Практическая работа. Создание виртуальной машины и подключение к ней
Decision:
Создание ВМ
На стартовой странице консоли управления выберите каталог для ВМ. По умолчанию при регистрации в Yandex Cloud создаётся каталог с именем default. 
В списке сервисов выберите Compute Cloud.
Нажмите кнопку Создать ВМ.
В открывшемся окне нужно указать параметры ВМ. Подробно мы разберем их на следующем уроке, а пока предлагаем использовать наши рекомендации и значения по умолчанию. В блоке Базовые параметры укажите имя ВМ из строчных латинских букв и цифр. Поле Описание необязательное — его обычно заполняют, чтобы не запутаться, если ВМ несколько. Выберите из списка зону доступности ru-central1-a.
В блоке Выбор образа/загрузочного диска на вкладке Операционные системы выберите систему, которая будет установлена в ВМ — Ubuntu 20.04.
В блоке Диски оставьте значения по умолчанию: тип — HDD, размер — 15 ГБ.
В блоке Вычислительные ресурсы оставьте значения по умолчанию: платформу Intel Ice Lake, гарантированную долю vCPU 100% и объём оперативной памяти (RAM) 2 ГБ.
В блоке Сетевые настройки оставьте используемую по умолчанию подсеть. Публичный адрес и Внутренний адрес — выберите Автоматически.
В блоке Доступ заполните поле Логин. Не указывайте идентификатор root или другие имена, зарезервированные операционной системой. Для операций, требующих прав суперпользователя, нужно будет использовать команду sudo.
Чтобы иметь возможность подключиться к создаваемой ВМ по протоколу SSH понадобится пара SSH-ключей. Открытый ключ хранится на ВМ, а закрытый — у пользователя. В публичных образах Linux, предоставляемых Yandex Cloud, возможность подключения по SSH с использованием логина и пароля по умолчанию отключена.
Как создать SSH-ключи? (В Linux/macOS или Windows 10/11)
Откройте терминал в Linux/macOS или утилиту cmd.exe в Windows 10/11 и создайте пару ключей с помощью команды ssh-keygen:
 ssh-keygen -t rsa -b 2048
После выполнения команды укажите имена файлов, куда сохранятся ключи, и введите пароль для закрытого ключа. По умолчанию используется имя id_rsa, ключи создаются в папке ~/.ssh текущего пользователя.
Открытый ключ сохранится в файле <имя_ключа>.pub. Содержимое этого файла вставляется в поле SSH-ключ виртуальной машины.
Как создать SSH-ключи? (В Windows 7/8)
Если на вашем компьютере установлена Windows 7 или 8, то создать SSH-ключи можно с помощью приложения PuTTY. Как это сделать, рассказывается в документации.
В поле SSH-ключ вставьте содержимое созданного открытого ключа. Убедитесь, что вставляете ключ без переносов строки.
Например:
ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAoIT+oFLFEHwNlGO71wZiamqHkzduK7V/B8ITxgLnddm725QZJbaO1JAUfaOryGckWqEHr0NQxZ+CfozLjtYwcYhnPfNs1vw7Ii5gnL4ne+Vu5Kl4f8rb+tOXAv6GAZIO1+05kB8K3nINfBkKFD1J0VmOr5P2MWy7aqdbyIqVJCH+YeU4SW5RGFPJbl5zGhlwSavVU0bgTYQmqWAOnR95bQVx1vRf4SyB003C8MYl8ccZ+emixM12eQPJ74fJyy1kKLRmU/IAlxyEiYESQglAaNQKN2ivnbfMaSVBnxMlYipxyeMDyCs8RD7zVUndTOJQg8PV7QVWqfAQjlY4uYlk8Q== rsa-key-20210429 
Изменяя параметры ВМ, в блоке Тарифы и цены (справа) вы увидите, как меняется её месячная стоимость.
Нажмите кнопку Создать ВМ.
Развёртывание ВМ занимает несколько минут. Вы можете отслеживать этот процесс по смене статуса.
Статус ВМ влияет на то, какие операции вы можете с ней выполнять. Например, статус Stopped означает, что машина остановлена и к ней невозможно подключиться. При запуске статус меняется на Provisioning (Yandex Cloud выделяет ВМ ресурсы), а после загрузки операционной системы — на Running.
Теперь, когда ВМ создана и запущена, к ней можно подключиться. Для этого понадобятся логин пользователя и публичный IP-адрес ВМ, который можно скопировать из строки с информацией о ней на странице Виртуальные машины.
Удалённое подключение к ВМ
Чтобы подключиться к запущенной ВМ (статус RUNNING) по протоколу SSH, используют утилиту ssh в Linux/macOS или Windows 10/11, программу PuTTY в Windows 7/8 или любой другой клиент SSH.
Подключение по SSH в Linux/macOS или Windows 10/11
Откройте терминал в Linux/macOS или запустите PowerShell в Windows 10/11 и выполните команду:
ssh <имя_пользователя>@<публичный_IP-адрес_ВМ> 
Если у вас несколько закрытых ключей, укажите нужный:
ssh -i <путь_к_ключу/имя_файла_ключа> <имя_пользователя>@<публичный_IP-адрес_ВМ> 
При первом подключении может появиться предупреждение о неизвестном хосте:
The authenticity of host '130.193.40.101 (130.193.40.101)' can't be established.
ECDSA key fingerprint is SHA256:PoaSwqxRc8g6iOXtiH7ayGHpSN0MXwUfWHkGgpLELJ8.
Are you sure you want to continue connecting (yes/no)? 
Введите в терминале yes, нажмите Enter и вы окажетесь в консоли созданной ВМ.
Установите обновления. Для этого запустите в консоли команды:
sudo apt-get update
sudo apt-get upgrade 
ВМ готова к работе. Если вы не собираетесь делать большой перерыв в обучении, то не удаляйте её — она понадобится нам на следующем практическом занятии.
Decision:
$ ssh-keygen -t rsa -b 2048
$ cat ubcloud.pub
$ ssh -i ubcloud test138@178.154.223.142
test138@linux-test:~$ sudo apt-get update
test138@linux-test:~$ sudo apt-get upgrade
Task:
Изучаем страницу создания виртуальной машины
Decision:
Итак, вы создали свою первую ВМ с настройками по умолчанию. Теперь давайте рассмотрим её ключевые характеристики.
Виртуальная машина как ресурс Yandex Cloud
ВМ создаётся в одном из каталогов в вашем облаке и наследует права доступа этого каталога. При необходимости вы можете переносить ВМ между каталогами внутри одного облака.
У каждой ВМ есть идентификатор и имя. Идентификатор, уникальный в пределах Yandex Cloud, генерируется автоматически при создании машины. Имя ВМ вы придумываете сами, оно должно быть уникальным в пределах каталога.
Базовые параметры
Имя. Требования к нему:
    длина — от 3 до 63 символов;
    состав — строчные буквы латинского алфавита, цифры и дефисы;
    первый символ — буква, последний символ — не дефис.
Описание — необязательное поле. Заполните его, чтобы лучше различать ВМ.
Зона доступности определяет, в каком дата-центре будут размещены данные. Изменить зону доступности уже созданной ВМ невозможно. Если требуется — создайте копию машины в другой зоне доступности с помощью снимков или образов.
Выбор образа / загрузочного диска
Выберите операционную систему с предустановленными приложениями или без них.
На вкладке Операционные системы доступны только OC с предустановленными базовыми сервисами. На вкладке Cloud Marketplace представлены партнёрские решения, позволяющие развернуть готовое лицензированное решение (например, систему 1С:Предприятие).
Диски
При создании ВМ нужно подключить загрузочный диск с операционной системой. Он обязателен, без него ВМ не будет работать.
Определите параметры загрузочного диска:
    Выберите тип диска: HDD или SSD.
    Передвигайте ползунок, чтобы установить размер диска в гигабайтах. Справа вы увидите сведения о производительности диска: максимальное количество операций чтения и записи, выполняемых в секунду (Макс. IOPS), и максимальную пропускную способность (Макс. bandwidth). Производительность диска зависит от размера.
При создании ВМ сервис создаёт только загрузочный диск операционной системы. Но вы можете подключить и дополнительные диски: пустые, либо восстановленные из снимка или образа. Для этого нажмите Добавить диск. В открывшемся окне задайте параметры и нажмите кнопку Добавить. Дополнительные диски не используются в качестве загрузочных.
Вычислительные ресурсы
При создании ВМ укажите, сколько ей требуется вычислительных ресурсов: количество и производительность ядер процессора (vCPU, т. е. виртуальный процессор), количество памяти (RAM). Если нагрузка изменится, вы можете остановить ВМ и изменить вычислительные ресурсы.
Платформа определяет тип физического процессора и набор допустимых конфигураций vCPU и RAM.  Наведите указатель мыши на значок вопроса, чтобы увидеть соответствие платформ и процессоров.
Укажите гарантированную долю vCPU, которая будет выделена ВМ, количество vCPU и объём RAM. ВМ с гарантированной долей меньше 100% обеспечивают указанный уровень производительности с вероятностью временного повышения до 100%. Такие машины подходят для задач, которые не требуют постоянной гарантии 100% производительности vCPU (подробнее об уровнях производительности).
Дополнительно вы можете сделать ВМ прерываемой, т. е. ее работа может быть принудительно остановлена Yandex Cloud для высвобождения ресурсов под обычные (не прерываемые) ВМ. Такие машины предоставляются с большой скидкой. Подробнее об этом вы можете узнать в дальнейших материалах курса.
Сетевые настройки
При создании ВМ заполните настройки сетевого интерфейса. Вы также можете выбрать группу безопасности для ограничения доступа к ВМ. Подробнее о настройке виртуальной облачной сети и групп безопасности вы узнаете в теме «Виртуальная сеть».
В поле Подсеть доступны для выбора подсети той зоны доступности, которую вы выбрали ранее в блоке Базовые параметры. Сетевой интерфейс можно изменять, если остановить ВМ.
Публичный адрес позволяет получить доступ к ВМ из интернета. В поле Публичный адрес можно выбрать способ назначения IP-адреса:
    Автоматически — в этом случае ВМ получит случайный публичный IP-адрес из пула адресов Yandex Cloud. При перезагрузке ВМ выданный адрес сохраняется, при остановке и повторном запуске выдается новый адрес.
    Список — позволяет выбрать публичный IP-адрес из списка ранее зарезервированных статических адресов с DDoS-защитой или без неё.
    Без адреса — ВМ будет выдан только внутренний IP-адрес из пула адресов выбранной подсети.
Внутренний адрес позволяет ВМ взаимодействовать с другими сервисами во внутренней сети облака. В поле Внутренний адрес можно выбрать способ назначения IP-адреса:
    Автоматически — в этом случае ВМ получит случайный внутренний IP-адрес из пула доступных адресов подсети.
    Вручную — позволяет назначить пользовательский IP-адрес.
Внутренний IP-адрес, назначенный ВМ внутри подсети, нельзя изменить.
Доступ
Установите параметры безопасности:
Создайте сервисный аккаунт для ВМ: он позволяет настраивать права доступа к ресурсам Yandex Cloud для приложений, выполняемых на машине.
Например, вы установили на ВМ программу, которая отслеживает статусы других виртуальных машин в облаке. Для этого ей достаточно иметь права на просмотр. Но программа работает от вашего имени, а у вас есть права на удаление. Чтобы защититься от случайного удаления ВМ вашей программой, вы можете создать сервисный аккаунт и дать ему доступ только на просмотр.
Задайте параметры учётной записи:
    Для машин с ОС на основе Linux вы можете сразу создать учётную запись администратора системы с авторизацией по SSH-ключу. Для этого укажите логин и введите открытый SSH-ключ.
    Для машин с ОС Windows в поле Пароль введите пароль для пользователя Administrator в соответствии с требованиями к сложности паролей этой операционной системы.
В поле Дополнительно вы можете разрешить доступ к серийной консоли ВМ. Серийная консоль — аналог консольного порта оборудования, через который пользователь получает доступ к ВМ вне зависимости от состояния сети или операционной системы. С помощью консоли можно устранять неисправности ПО.
Task:
Последовательный порт и серийная консоль
Decision:
Каждая запущенная ВМ создаёт последовательный порт ввода-вывода COM1, к которому можно подключиться.
Последовательный порт
Предположим, в ВМ произошёл сбой. Чтобы выяснить причину, нужно посмотреть журнал системных сообщений. Если у вас нет входа в машину по SSH или демон sshd (для Linux) не функционирует — подключитесь к последовательному порту COM1 ВМ и посмотрите информацию, которая туда направляется. Для этого в консоли управления перейдите на страницу ВМ и в меню слева выберите Последовательный порт.
Для поиска по всему выводу системы включите опцию Исходные данные. После этого используйте комбинацию клавиш Ctrl + F (Command + F для macOS) и воспользуйтесь поиском по журналу.
Серийная консоль
Вы можете подключиться к порту COM1 ВМ Linux или порту COM2 ВМ Windows, чтобы вводить данные и администрировать ВМ. Эта функция называется серийной консолью. По умолчанию при создании ВМ она отключена. Чтобы включить консоль, остановите ВМ и перейдите на страницу изменения параметров. В блоке Доступ выберите опцию Разрешить доступ к серийной консоли и сохраните изменения.
Включите ВМ, зайдите на эту машину по SSH (в случае ВМ с Linux) и задайте пароль входа для пользователя, которого вы указали при её создании. О том, как это сделать, вы узнаете в следующей практической работе.
После этого в консоли управления перейдите на страницу ВМ. В меню слева выберите Серийная консоль. Войдите в систему и проведите диагностику.
Включение серийной консоли даёт возможность попробовать диагностировать ВМ, если на неё невозможно зайти по SSH (или по RDP в случае Windows).
Риски для безопасности
Помните, что все пользователи с правом доступа к серийной консоли могут увидеть сессию, просмотреть действия других пользователей и подключиться к незавершённой сессии.
Чтобы обезопасить себя:
    включайте серийную консоль только при крайней необходимости;
    давайте доступ узкому кругу пользователей, которым доверяете;
    придумывайте стойкие пароли для доступа к ВМ;
    после работы с серийной консолью отключайте доступ к ней в ВМ.
Task:
Практическая работа. Получаем доступ к серийной консоли
Decision:
Работа серийной консоли зависит от настроек операционной системы. Yandex Compute Cloud обеспечивает канал связи между пользователем и COM-портом ВМ, но не гарантирует стабильность работы консоли со стороны операционной системы ВМ.
Доступ к серийной консоли ВМ с ОС на базе Linux возможен следующими способами:
    по протоколу SSH с другого компьютера;
    через консоль управления Yandex Cloud;
    с помощью интерфейса командной строки Yandex Cloud CLI. Подробнее работу с CLI мы рассмотрим в одной из следующих практических работ.
Вход по протоколу SSH
    Подключитесь к ВМ по протоколу SSH. Установите пароль текущему пользователю с помощью утилиты passwd в привилегированном режиме:
sudo passwd <имя_пользователя> 
После ввода команды дважды наберите одинаковый пароль.
    Для доступа к серийной консоли ВМ необходимо знать её идентификатор (ID). В консоли управления перейдите в раздел Compute Cloud. По умолчанию откроется страница со списком ВМ. В столбце справа указан идентификатор каждой ВМ.
    Используйте для входа идентификатор ВМ и имя (логин) созданного в ней пользователя. Вот шаблон команды подключения для Linux:
ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/<имя закрытого ключа> <ID виртуальной машины>.<имя пользователя>@serialssh.cloud.yandex.net 
    Вот так вы подключитесь к консоли, если в ВМ с ID fhm0b28lgfp4tkoa3jl6 есть пользователь yc-user:
ssh -t -p 9600 -o IdentitiesOnly=yes -i ~/.ssh/id_rsa fhm0b28lgfp4tkoa3jl6.yc-user@serialssh.cloud.yandex.net 
    Введите установленный ранее пароль.
    Чтобы отключиться от серийной консоли, нажмите клавишу Enter, а затем введите символы ~. (тильда и точка). В терминалах Linux для отключения также можно использовать комбинацию клавиш Ctrl + D.
Вход через консоль управления
В консоли управления откройте страницу ВМ и через меню слева перейдите на страницу Серийная консоль. При авторизации используйте логин, указанный при создании ВМ, и пароль, который вы установили после подключения к ней.
Decision:
$ ssh -i ubcloud test138@51.250.93.58
test138@linux-test:~$ sudo passwd test138
test138@linux-test:~$ exit
$ ssh -t -p 9600 -o IdentitiesOnly=yes -i ubcloud fhmppfei6vr0fgrqnd53.test138@serialssh.cloud.yandex.net
Task:
Прерываемые машины и уровни производительности
Decision:
Один из параметров, который задаётся при создании ВМ, — это уровень производительности vCPU. Он определяет доступную ВМ долю вычислительного времени физических ядер.
    ВМ с уровнем производительности 100% имеют непрерывный (100% времени) доступ к вычислительной мощности физических ядер. Такие ВМ предназначены для запуска приложений, требующих высокой производительности на протяжении всего времени работы.
    ВМ с уровнем производительности меньше 100% имеют доступ к вычислительной мощности физических ядер как минимум на протяжении указанного процента от единицы времени. Такие ВМ предназначены для запуска приложений, не требующих высокой производительности и не чувствительных к задержкам. Они обойдутся дешевле.
Для многих задач не требуется, чтобы виртуальная машина (ВМ) была доступна постоянно.
Например, вы настроили Jenkins для тестирования сборок (непрерывной интеграции) приложения. Изменения интегрируются несколько раз в день. Соответственно, ВМ использует ресурсы в полной мере лишь эти несколько раз в день, а в остальное время — простаивает. Получается, вы платите за постоянно доступную ВМ, хотя она вам не нужна.
В этом случае разумно создать прерываемую ВМ. Вкратце её суть такова: вы получаете вычислительные ресурсы за меньшую цену, но они могут быть отозваны в любое время. Yandex Cloud остановит прерываемую ВМ, если:
    обычным ВМ в той же зоне доступности не хватает ресурсов. Такое происходит, когда в этой зоне доступности быстро создаётся много обычных ВМ. Например, если появляется пользователь с обширными потребностями или пользователи массово масштабируются.
    с запуска прерываемой ВМ прошло больше 24 часов. Чтобы не перезапускать ВМ вручную каждый день, доверьтесь автоматике (например, сервису групп ВМ, о котором мы расскажем позже).
После остановки ВМ не удаляется, все её данные сохраняются.
Соглашение об уровне обслуживания (Service Level Agreement, или SLA) не распространяется на прерываемые ВМ. Вы можете запустить их вновь, только если в зоне доступности достаточно ресурсов. Поэтому прерываемые ВМ не подходят для решений, где требуется постоянная работа и отказоустойчивость.
Виртуальную машину можно сделать прерываемой при её создании. Второй способ — изменить тип обычной ВМ, предварительно её остановив.
Вот ещё несколько задач, для которых подходят прерываемые ВМ:
    Пакетная обработка данных. Обычно такие задания обрабатываются параллельно, процессом управляет оркестратор. Если прерываемая ВМ выключится, задание получит следующий исполнитель.
    Повышение производительности веб-сервисов при пиковых нагрузках. Когда посещаемость сайта резко возрастает, вам нужно на время подхватить нагрузку дополнительными ресурсами. Прерываемые ВМ отлично справятся с этой задачей. Важно включить в состав кластера и обычные ВМ, которые станут работать при стандартных нагрузках.
    Проекты на Kubernetes. Оркестратор Kubernetes позволяет автоматизировать развёртывание и масштабирование контейнеризированных приложений и управление ими. Базовая единица управления Kubernetes — под. Под отвечает за запуск одного или нескольких контейнеров на узле. Планировщик Kubernetes подбирает и назначает узел для каждого пода. Если узел с запущенным подом выйдет из строя, планировщик автоматически перенесёт под на работающий узел. Поэтому некоторые узлы можно размещать на прерываемых ВМ.
Task:
Практическая работа. Создаем ВМ с 5% vCPU и учимся использовать мониторинг
Decision:
Давайте создадим ВМ с гарантированной долей vCPU, равной 5%, и проверим её производительность.
В консоли управления перейдите в раздел Compute Cloud и нажмите кнопку Создать ВМ. Заполните имя и описание, выберите операционную систему CentOS 7.
В блоке Вычислительные ресурсы выберите платформу Intel Cascade Lake и укажите гарантированную долю vCPU 5%. Другие параметры оставьте по умолчанию.
После создания и запуска ВМ в списке машин нажмите её название. Вы перейдёте на страницу ВМ. Затем на левой боковой панели выберите Мониторинг. Откроется страница, где в динамике показывается информация о загрузке процессора, операциях с диском и сетевой активности. По умолчанию видны данные за одни сутки (1d — 1 day).
Переключитесь на один час: вверху слева нажмите 1h (1 hour).
На графике видно, что при запуске использование процессорных ресурсов было высоким, а позже снизилось до приемлемого. Чтобы посмотреть точные значения в определённый момент, поместите указатель над линией графика. Вы увидите всплывающее окно с показателями для этой точки времени.
Теперь удалите ВМ. Для этого сначала остановите её — вернитесь в список ВМ, отметьте нужную ВМ и на появившейся внизу контекстной панели нажмите Остановить.
Во всплывающем окне подтвердите действие и нажмите кнопку Остановить. Дождитесь смены статуса на Stopped.
Чтобы удалить ВМ, в списке ВМ справа напротив машины нажмите ... и в раскрывшемся меню выберите Удалить. Подтвердите действие. Через некоторое время ВМ будет удалена.
Task:
Сервис метаданных и cloud-init
Decision:
С каждой ВМ в Compute Cloud связаны метаданные. Метаданные  — это идентификатор, название и описание ВМ, список подключённых к ней дисков и сетевых интерфейсов, привязанные к ВМ сервисные аккаунты. Кроме того, вы можете определять дополнительные метаданные и указывать их, когда создаёте или изменяете ВМ.
Получайте метаданные изнутри ВМ с помощью сервиса метаданных (он доступен из любой ВМ по адресу http://169.254.169.254) и используйте их для настройки машины или софта на ней. Например, укажите через метаданные, в каком режиме запуститься приложению на сервере: в отладочном или в боевом. Меняйте режим, не заходя на сервер, просто меняя значение в сервисе метаданных.
Сервис возвращает метаданные в двух форматах: Google Compute Engine или Amazon EC2.
Помните, что в Yandex Cloud поддерживаются не все поля этих форматов.
В ВМ на базе Linux для работы с метаданными, как правило, используется агент cloud-init, в машинах с Windows — Cloudbase-Init. Но вы можете отправить запрос в сервис метаданных и самостоятельно — с помощью любого HTTP-клиента.
Вы можете передать метаданные при создании и изменении ВМ. Чаще всего это делается с помощью консольной утилиты Yandex Cloud CLI, о которой мы подробно расскажем позже.
Указывайте метаданные в CLI в одном из трёх параметров:
    --metadata принимает список пар «ключ=значение», разделённых запятой, например:  --metadata foo1=bar,foo2=baz.
    -metadata-from-file читает метаданные из файла, например:  -metadata-from-file key=path/to/file. Этот метод удобен, чтобы передавать длинные метаданные.
    -ssh-key — специальный тип метаданных для хранения публичного SSH-ключа. Доступен только для ВМ на Linux, где его читает агент cloud-init.
Чтобы получить метаданные ВМ от Yandex Cloud, вы можете использовать интерфейс командной строки Yandex Cloud (CLI) или API. Пошаговое руководство по использованию CLI для решения этой задачи приведено в документации. С кратким введением в использование CLI для работы с ВМ вы познакомитесь в одной из следующих тем этого курса.
Task:
Как можно передавать данные службе метаданных, обслуживающей виртуальные машины?
Decision:
+Из веб-консоли при создании и изменении виртуальной машины
+С помощью Yandex Cloud CLI
-С помощью cURL
Task:
Диски. Зависимость производительности от объёма
Decision:
Диски виртуальных машин физически размещаются на дисках, подключённых к сети через Ethernet. Внутри зоны доступности данные хранятся с избыточностью. Поэтому, если физический диск выходит из строя, данные пользователя остаются доступными. Кроме того, для каждого снимка и образа создаётся реплика (полная копия). Это называется репликацией. Если диск, на котором хранятся снимки и образы, выходит из строя, — всегда остаются копии.
HDD и SSD
Для ВМ вы можете использовать сетевые диски HDD и SSD.
    У HDD (hard disk drive, накопитель на жёстких магнитных дисках) самая низкая стоимость гигабайта, но при этом скорость чтения и записи данных в разы ниже, чем у SSD. Выбирайте HDD в качестве загрузочных дисков для веб-приложений, где скорость запуска не критична. 
    SSD (solid state disk, твердотельный накопитель) позволяют выполнять больше операций чтения и записи за единицу времени, они оптимальны для быстрой загрузки тяжёлых приложений и постоянной работы с файлами. На SSD можно хранить, например, базу данных.
Объём и тип диска влияют на производительность. Ключевое понятие здесь — блок размещения, т. е. единица выделения дискового пространства. Чем больше блоков размещения в вашем диске, тем более производительный диск в итоге вы получаете. У сетевых SSD размер блока размещения — 32 ГБ, у сетевых HDD он равен 256 ГБ. Поэтому при одинаковом объёме у SSD больше блоков размещения, а значит, выше IOPS (количество операций чтения и записи, выполняемых диском в секунду) и пропускная способность. Таким образом производительность диска будет расти ступенчато с увеличением количества блоков размещений в нем. Блоки размещения в разных типах дисков отличаются размером и лимитами на производительность, которые указаны здесь.
Загрузочные и дополнительные диски
Данные ВМ в Compute Cloud хранятся на загрузочном и дополнительных дисках. На загрузочном диске находится операционная система и приложения, поэтому его нельзя отключить от ВМ. На дополнительном диске вы можете хранить любые данные, его можно отключить от одной ВМ и подключить к другой.
Например, ваш веб-сервис хранит пользовательскую информацию в базе данных и может создавать выгрузки в удобном пользователю формате. В таком случае на загрузочном диске будет только само веб-приложение поверх операционной системы, а база данных и файлы выгрузки — на дополнительных дисках.
Task:
Вы хотите использовать ВМ для запуска веб-сервера и размещения базы данных (БД). Как вам лучше сконфигурировать дисковую подсистему с точки зрения быстродействия и стоимости?
Decision:
-Можно всё разместить на HDD: так дешевле, а система запускается один раз и работает до перезагрузки.
-Всё разместить на SSD: пусть дороже, но быстрее.
+Разместить веб-сервер на HDD, потому что он грузится в оперативную память при запуске ОС, а базу данных — на SSD, потому что к ней идёт постоянное обращение.
Task:
Что такое снимок и зачем он нужен
Decision:
Предположим, вы разместили на диске ВМ данные, с которыми работает приложение. Чтобы не потерять данные, создавайте резервные копии. Для этого отлично подходят снимки дисков.
Снимок можно назвать слепком или поблочной копией дискового устройства на определённый момент времени. Вы сами решаете, сколько хранить снимки, и удаляете их вручную.
Например, перед выкатыванием обновления вы сделали снимок диска ВМ, где работает бэкенд мобильного приложения. В ходе тестирования вы не нашли ошибки, сервис функционирует стабильно. К выкатыванию следующего обновления можно сделать снимок с новой версией, а старый — удалить.
Важно! Останавливайте операции с диском, перед тем как создать снимок (например, в ОС Linux вы можете остановить все операции записи на диск в приложениях, или просто остановить ВМ с любой ОС). Если работающее в ВМ приложение создаёт или меняет файлы на диске — в снимок могут попасть неполные, повреждённые файлы. При восстановлении из снимка может нарушиться целостность данных.
Когда снимок готов, проверьте его целостность: создайте ВМ из снимка и проверьте, как она работает. Подробнее об этом мы расскажем в следующем практическом задании.
Вы можете создавать сколько угодно снимков. Их хранение оплачивается дополнительно. Чтобы вы проще различали снимки, по умолчанию в их название включается имя диска. Например, disk2-1614952835886 снят с диска disk2, а 1614952835886 — это отметка времени Unix, когда был создан снимок.
Между репликацией дисков и репликацией снимков есть важное отличие. Диск находится в одной зоне доступности и реплицируется только внутри нее (кроме нереплицируемых дисков). Снимки же реплицируются во все зоны доступности. Поэтому чтобы перенести ВМ в другую зону, достаточно создать в этой зоне копию ВМ из снимка загрузочного диска.
Task:
Снимки дисков делаются, чтобы...
Decision:
+Сохранять состояние дисков с данными
+Сохранять настройки операционной системы
+Восстанавливать состояние дисков с данными
-Восстанавливать соединение с сервером, когда оно теряется
Task:
Практическая работа. Создаем снимок диска ВМ
Decision:
Допустим, вы планируете обновить ПО на виртуальной машине ВМ. Вы знаете, что взаимодействие приложений и системных сервисов после обновления может нарушиться, а данные могут быть повреждены. Поэтому хотите иметь резервную копию полностью работоспособной системы на случай неудачи.
Давайте на практике разберём, как сделать снимок и восстановить из него ВМ при повреждении системы. Для этого используем ВМ, на которой развернута система на основе Ubuntu или CentOS.
Целостность данных
В первую очередь обеспечьте целостность данных. Для этого подключитесь к ВМ по SSH. Чтобы записать кеш операционной системы на диск, выполните команду sync (иначе изменения файлов, хранящиеся в оперативной памяти, будут потеряны). Диски в Linux монтируются в ОС в виде файлов. Чтобы узнать нужный файл устройства диска, выполните команду df -h для вывода полного списка устройств и соответствующих точек монтирования. Затем, чтобы заморозить файловую систему, выполните команду fsfreeze -f <точка монтирования>.
Создание снимка
    В консоли управления откройте раздел Compute Cloud и перейдите на вкладку Диски. Справа от диска нажмите ... и выберите Создать снимок.
    В открывшемся окне вы увидите автоматически сформированное имя снимка диска. Оно состоит из идентификатора диска и идентификатора снимка. Вы можете переименовать снимок и заполнить его описание. После этого нажмите кнопку Создать.
    Откройте Снимки дисков. Как только снимок будет создан, статус операции сменится с Creating на Ready.
    Разморозьте файловую систему. Для этого в командной строке с интерфейсом подключения к ВМ по SSH выполните команду fsfreeze --unfreeze <точка монтирования>.
Намеренное повреждение системы
    Теперь протестируйте обновление системы: в командной строке с интерфейсом подключения к ВМ по SSH последовательно выполните команды apt-get update и apt-get dist-upgrade. Дождитесь, пока обновление завершится. Важно! Перед выполнением следующей команды убедитесь в том, что находитесь в консоли именно тестовой ВМ.
    Сымитируйте повреждение системы: выполните команду sudo rm -rf --no-preserve-root /. Вы увидите предупреждение, что все данные на диске будут удалены. Подтвердите своё намерение.
Восстановление из снимка
    Поскольку снимок создан с загрузочного диска, который всегда подключён к ВМ, для восстановления создайте новую ВМ вместо старой. При создании загрузочного диска машины выберите готовый снимок диска. Для этого в блоке Выбор образа/загрузочного диска перейдите на вкладку Пользовательские и нажмите кнопку Выбрать. В открывшемся окне перейдите на вкладку Снимок, выберите нужный снимок и нажмите кнопку Применить.
    Дождитесь завершения создания и запуска новой ВМ. Теперь старую ВМ можно остановить и удалить.
Будьте внимательны при создании новых виртуальных машин: в облаке действуют квоты и лимиты на используемые ресурсы.
Decision:
$ ssh -i ubcloud test138@51.250.93.58
test138@linux-test:~$ sync
test138@linux-test:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           198M  1.1M  197M   1% /run
/dev/vda2        15G  4.2G  9.9G  30% /
tmpfs           988M     0  988M   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           198M  4.0K  198M   1% /run/user/1000
test138@linux-test:~$ sudo fsfreeze -f /
test138@linux-test:~$ sudo fsfreeze --unfreeze /
test138@linux-test:~$ sudo apt-get update
test138@linux-test:~$ sudo apt-get dist-upgrade
test138@linux-test:~$ sudo rm -rf --no-preserve-root /
Task:
Что такое образы и публичные образы
Decision:
Предположим, вы разработали первую версию веб-сервиса и готовитесь сделать его публично доступным. В ВМ уже отлажено рабочее окружение для приложения, выверены настройки служб. Осталось её реплицировать, чтобы сервис оставался доступен, даже если возникнут неполадки.
Вы можете развернуть ВМ из снимка диска или из образа. Образы и снимки, созданные в одной зоне доступности, доступны и в других.
Образы оптимальны для распространения программного обеспечения, например дистрибутивов операционных систем (ОС) или дисков с установленными программами. В основном образы используются, чтобы быстро создать загрузочный диск ВМ. Ваш сервис — это как раз дистрибутив ОС с предустановленными и настроенными приложениями.
Создание ВМ из образа происходит быстрее, чем из снимка диска. Выбирайте образы, когда важна скорость. Например, если пользовательская база стремительно растёт — для распределения нагрузки надо добавлять ВМ как можно быстрее, чтобы приложение работало бесперебойно.
Использование готовых образов
Если вы только начинаете разработку — в качестве отправной точки для создания веб-сервиса в Compute Cloud можете использовать ВМ на базе готового образа ОС (Ubuntu, Fedora, CentOS и др.). Вы получите полнофункциональную систему и настроите её так, как хотите. Список образов ОС вы увидите при создании ВМ в блоке Выбор образа/загрузочного диска на вкладке Операционные системы.
Кроме того, в Cloud Marketplace доступны образы с предустановленными приложениями. Например, если сервис будет хранить данные в базе данных Postgres — создайте ВМ с предустановленной Postgres Pro Enterprise Database, работающей поверх CentOS 7.
Перенос локальной ВМ в Compute Cloud
Если вы разрабатываете сервис на рабочей станции в локальной ВМ, то можете перенести машину в Compute Cloud. Для этого подготовьте файл образа ВМ (поддерживаются образы форматов Qcow2, VMDK или VHD) и загрузите его в бакет Yandex Object Storage, после чего он станет доступен при создании ВМ.
Создание публичных образов
Чтобы пользователи Yandex Cloud могли создавать ВМ и диски с помощью вашего образа, нужно открыть к нему доступ и он станет публичным. Это делается не из консоли управления, а из интерфейса командной строки Yandex Cloud CLI.
Task:
С CLI вы познакомитесь в одной из следующих практических работ этого курса. Но если вам интересно, как с его помощью создать публичный образ, — загляните под кат.
Decision:
    Установите и настройте Yandex Cloud CLI.
    В командной строке выполните команду:
yc resource-manager folders list 
Результат:
+----------------------+---------+--------+--------+
|          ID          |  NAME   | LABELS | STATUS |
+----------------------+---------+--------+--------+
| b1gdf1scqef5bpqrpo5j | default |        | ACTIVE |
+----------------------+---------+--------+-------- 
Скопируйте название каталога, где хранится образ. В примере это default.
    Назначьте системной группе allAuthenticatedUsers роль compute.images.user:
yc resource-manager folder add-access-binding default \
  --role compute.images.user \
  --subject system:allAuthenticatedUsers 
Готово. Вы открыли всем аутентифицированным пользователям Yandex Cloud доступ к папке с образом.
Чтобы просмотреть список публичных образов, введите команду:
yc compute image list --folder-id standard-images 
Decision:
$ ssh -i ubcloud test@51.250.88.153
test@ubuntu-test:~$ curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
Task:
Вы разрабатываете веб-приложение на ВМ на ноутбуке и хотите перенести ВМ в облако. Для этого нужно:
Decision:
-Выложить файл образа на Яндекс Диск, связаться со службой техподдержки и попросить опубликовать его.
-Загрузить образ в Compute Cloud.
+Загрузить образ в бакет Yandex Object Storage.
Task:
Виртуальные сети, подсети, IP-адресация
Decision:
Сети и подсети
Физические серверы в дата-центрах соединяются друг с другом с помощью сети. Виртуальным серверам тоже нужно общаться друг с другом, поэтому для них поверх физической сети построена своя, виртуальная сеть. Она гарантирует, что нужные виртуальные машины смогут передавать данные друг другу, выходить в интернет и подключаться к базам данных, при этом владельцы «соседних» виртуальных машин не смогут увидеть этот трафик или повлиять на него.
Чтобы соединить несколько виртуальных машин, нужно создать облачную сеть. Ресурсы типа виртуальных машин и баз данных, находящиеся в одной облачной сети, по умолчанию «видят» друг друга, а находящиеся в разных сетях — нет. Кроме облачной сети, надо создать ещё и подсети — подмножество сети в конкретной зоне доступности. По умолчанию создаётся по одной подсети для каждой зоны, но вы можете этим управлять, если захотите.
IP-адреса
При создании подсети вы можете выбрать, какие IP-адреса будут выдаваться устройствам в этой подсети. Для этого можно выбрать любой диапазон адресов, вложенный в один из следующих: 10.0.0.0/8, 172.16.0.0/12 и 192.168.0.0/16. Это не случайные диапазоны: они зафиксированы в стандарте RFC1918 как немаршрутизируемые в интернете и используются только в локальных сетях.
Стоит учесть, что:
    Допустимая длина префикса варьируется от /16 до /28. Подсеть 10.0.0.0/17 создать можно, а 10.0.0.0/15 или 10.0.0.0/29 — нет.
    Первые два адреса подсети выделяются под шлюз (x.x.x.1 для маски сети /24) и DNS-сервер (x.x.x.2 для маски сети /24). Использовать их для виртуальных машин или других ресурсов не получится.
    Внутри одной облачной сети диапазоны IP-адресов всех подсетей не должны пересекаться. В то же время подсети разных облачных сетей могут пересекаться по IP-адресам, ведь две разные сети изолированы друг от друга.
    В Yandex Cloud пока используются только IPv4-адреса. Поддержка IPv6 планируется в будущем.
Внутренние IP-адреса не меняются в течение всего времени существования облачного ресурса. При создании виртуальной машины или другого ресурса их можно задать вручную, или они будут выбраны автоматически в выбранной подсети.
Кроме внутреннего адреса, вы можете выдать виртуальной машине или базе данных также и публичный IP-адрес. Он будет уже принадлежать маршрутизируемому диапазону (например 130.193.32.0/19), и благодаря этому адресу облачные ресурсы могут обмениваться данными с интернетом и с ресурсами из других облачных сетей. Публичные адреса сопоставляются с внутренними адресами ресурсов с помощью так называемого one-to-one NAT, т. е. одному внешнему адресу соответствует один ресурс в конкретной облачной сети. Подробнее о публичных адресах вы узнаете на одном из следующих уроков.
Task:
Практическая работа. Создание новой сети с подсетями и ВМ
Decision:
Облачные сети (Virtual Private Cloud, VPC) являются частью публичного облака, которая связывает пользовательские, инфраструктурные, платформенные и прочие ресурсы воедино, где бы они ни находились — в нашем облаке или за его пределами. При этом VPC позволяет не публиковать без необходимости эти ресурсы в интернете, они остаются в пределах вашей изолированной сети.
Когда вы создаёте облако, в нём автоматически появляется сеть и подсети в каждой зоне доступности. Но иногда их бывает недостаточно. На этой практической работе вы научитесь вручную создавать сеть и добавлять подсети.
Рассмотрим пример, как настроить облачную сеть, чтобы организовать работу сервера с доступом из публичной сети. Сначала создадим единую для всех ресурсов облака изолированную сеть с ВМ и другими объектами инфраструктуры.
    В консоли управления откройте раздел Virtual Private Cloud и нажмите кнопку Создать сеть. Заполните имя (пусть сеть называется yc) и описание. Оставьте выбранной опцию Создать подсети и нажмите кнопку Создать сеть. В результате появятся три подсети: yc-ru-central1-a, yc-ru-central1-b, yc-ru-central1-c.
    Для сервера создадим ещё одну подсеть с маской /28.
    В разделе Virtual Private Cloud перейдите на страницу сети yc и нажмите кнопку Добавить подсеть. Введите параметры: имя — yc-public, зона — ru-central-1a, CIDR — 192.168.0.0/28. Нажмите кнопку Создать подсеть. Доступом пользователей облака к сетевым ресурсам управляют с помощью назначения ролей.
    Теперь создайте ВМ с именем server. Убедитесь, что в блоке Базовые параметры выбрана зона доступности ru-central-1a. В качестве образа выберите Ubuntu 20.04, в блоке Сетевые настройки выберите подсеть yc-public. В блоке Доступ введите логин user и вставьте открытый SSH-ключ в соответствующее поле .
Чтобы ВМ полноценно заработала, организуйте доступ в интернет. Есть три способа:
    Назначить машине публичный IP-адрес.
    Включить NAT для подсети.
    Установить NAT-сервер и создать соответствующий маршрут. С точки зрения безопасности лучше выбрать способ 2 или 3, чтобы на сервер не было прямого доступа из интернета. Но в таком случае придётся установить бастион-хост: отдельный сервер, задача которого — противостоять атакам извне. В нашем примере для простоты присвоим серверу публичный адрес. Для этого при создании ВМ выберите автоматический способ назначения IP-адреса.
После создания ВМ, проверьте доступность сервера, чтобы убедиться, корректно ли настроена сетевая конфигурация. Для этого на странице с информацией о ВМ в блоке Сеть найдите публичный IP-адрес сервера:
Откройте интерфейс командной строки и введите команду:
    ping 51.250.78.143 
Если конфигурация корректна, в результаты выполнения команды ping вы увидите:
    Ping statistics for 51.250.78.143:
    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
    Approximate round trip times in milli-seconds:
    Minimum = 8ms, Maximum = 9ms, Average = 8ms 
Такой пример конфигурации подходит для небольшого веб-сервера. Если вы собираетесь строить озеро данных или обрабатывать математические вычисления, не рекомендуется давать к ресурсам прямой доступ из интернета — разместите их за NAT.
Task:
Публичные IP-адреса
Decision:
Если по внутреннему IP-адресу ВМ доступна только внутри облачной сети, то по публичным IP (они же белые или внешние) она видна и внешнему миру.
Публичный IP-адрес:
    Присваивается по умолчанию при создании облачного ресурса с публичным адресом, если выставлены соответствующие настройки.
    По умолчанию динамический (каждый раз новый при запуске ресурса), но его можно сделать статическим.
Динамические IP-адреса освобождаются при остановке ресурса и сохраняются при перезагрузке. Статические IP сохраняются при остановке ресурса. Их можно зарезервировать и использовать позже, даже если они не привязаны к ресурсу.
Публичные IP-адреса с ресурсами, к которым они привязаны, перечислены в консоли управления в разделе Virtual Private Cloud на вкладке IP-адреса. Эта вкладка доступна в двух случаях:
    У вас есть ВМ с публичными адресами.
    У вас есть статические публичные IP-адреса.
Если вы остановите и снова запустите ВМ, вы увидите, что её публичный IP-адрес изменился.
    Чтобы на ВМ работал сервис, постоянно доступный по одному и тому же публичному IP-адресу, поменяйте динамический IP на статический:
    В списке публичных IP-адресов найдите адрес нужной ВМ.
    Справа нажмите ... и в раскрывшемся списке выберите Сделать статическим.
    Остановите и снова запустите ВМ. Вы увидите, что публичный IP-адрес остался прежним.
Task:
Публичные адреса также называют
Decision:
+белыми
+внешними
-серыми
-чёрными
-внутренними
Статические IP-адреса:
+сохраняются при остановке ресурса
+можно зарезервировать без привязки к ресурсу
-нельзя зарезервировать без привязки к ресурсу
-меняются при остановке ресурса
Task:
Статическая маршрутизация
Decision:
С помощью статической маршрутизации вы можете направлять трафик из подсети на заданные диапазоны IP-адресов через ВМ, указанные в качестве шлюза (next hop). Для этого используются таблицы маршрутизации. Они содержат статические маршруты, состоящие из префикса целевой подсети в нотации CIDR и внутреннего IP-адреса шлюза.
Чтобы создать таблицу маршрутизации со статическим маршрутом, в консоли управления в разделе Virtual Private Cloud перейдите на страницу облачной сети, слева выберите вкладку Таблицы маршрутизации и нажмите кнопку Создать таблицу маршрутизации.
Укажите название таблицы, добавьте статический маршрут и нажмите кнопку Создать таблицу маршрутизации.
Таблица маршрутизации привязывается к подсети и не может содержать повторяющихся префиксов. Трафик из подсети с привязанной таблицей будет направляться к указанным в маршрутах префиксам через соответствующий адрес шлюза.
Префикс 0.0.0.0/0 в маршруте означает, что весь трафик, если он не направлен по другим маршрутам, будет направлен через указанный для этого префикса шлюз.
Например, к подсети с CIDR 10.1.0.0/24 привязана таблица маршрутизации с такими маршрутами:
Имя Префикс Шлюз
another-network 192.168.0.0/16  10.1.0.5
internet    0.0.0.0/0   10.1.0.10
В этом случае весь трафик в подсеть 192.168.0.0/16, которая находится в другой виртуальной сети, будет направляться через ВМ с адресом 10.1.0.5 — при условии, что у ВМ есть интерфейс в другой виртуальной сети. Весь остальной трафик — через ВМ 10.1.0.10. При этом переопределение маршрута для префикса 0.0.0.0/0 может повлиять на внешнюю доступность ВМ из подсети с таблицей, где есть такой маршрут.
В Yandex Cloud поддерживаются только префиксы назначения вне виртуальной сети (например, префиксы подсетей другой сети Yandex Cloud или вашей локальной сети).
При создании маршрута в качестве шлюза можно указать свободный внутренний IP-адрес, который не привязан ни к одной ВМ. В этом случае маршрут заработает, когда будет запущена ВМ с соответствующим IP-адресом.
Для чего используются статические маршруты
Есть две типичные схемы использования статических маршрутов в Yandex Cloud:
    Сетевой маршрут строится до нужного префикса через одну ВМ. В качестве шлюза используется внутренний IP-адрес NAT INSTANCE 1.
    Отказоустойчивая схема с маршрутами в нескольких зонах доступности. Создайте ВМ в разных зонах доступности и проложите через них маршруты до одной подсети назначения. Если ВМ в одной зоне выйдет из строя — у ВМ из других зон сохранится связность с подсетью назначения.
Изменение маршрутов трафика в интернет
Если в префиксе назначения у маршрута из таблицы маршрутизации указан префикс адресов из интернета, то доступ к таким адресам и с таких адресов станет невозможным через публичные IP-адреса ВМ из подсетей, к которым привязана эта таблица.
Допустим, есть машина vm-1 с публичным IP-адресом, подключенная к подсети my-subnet. Если к подсети my-subnet привязать таблицу my-route-table с маршрутом для префикса 0.0.0.0/0 (все адреса) через шлюз 10.0.0.5, то доступ через публичный адрес к vm-1 пропадёт. Это произойдёт потому, что весь трафик в подсеть my-subnet и из неё теперь будет направляться через адрес шлюза (см. первую схему).
Чтобы сохранить входящую связность с облачными ресурсами через публичный адрес, вы можете:
    вынести ресурсы с публичными адресами в отдельную подсеть;
    вместо настройки маршрута в интернет включить для подсети доступ в интернет через NAT (функция находится на стадии Preview и включается по запросу в техподдержку).
Task:
Группы безопасности
Decision:
Группы безопасности выполняют функцию межсетевого экрана и позволяют контролировать входящий и исходящий трафик ВМ.
В консоли управления этот инструмент находится в разделе Virtual Private Cloud. Чтобы переключиться на него, нужно нажать кнопку Группы безопасности (значок щита) в панели слева.
Если в вашем облаке групп безопасности ещё нет, эта кнопка может не отображаться в интерфейсе. В этом случае для создания группы перейдите по ссылке https://console.cloud.yandex.ru/link/vpc/security-groups. После того как первая группа безопасности будет создана, кнопка Группы безопасности появится на боковой панели каждой облачной сети.
Группа безопасности назначается сетевому интерфейсу при создании или изменении ВМ и содержит правила получения и отправки трафика.
Главный принцип: запрещено всё, что не разрешено явно.
Поэтому, если назначить сетевому интерфейсу ВМ группу безопасности без правил, ВМ не сможет передавать и принимать трафик и у вас не получится зайти на неё по SSH.
Правила
Нажмите кнопку Создать группу и добавьте для неё правила: определите протоколы и IP-адреса для приёма и отправки трафика.
Если сетевому интерфейсу ВМ назначены несколько групп безопасности, то учитываются правила из всех групп. В этом случае на ВМ поступит трафик, который подпадает хотя бы под одно из правил в группах.
Правила также хранят состояния сессий. Группы безопасности отслеживают состояние соединений и сопоставляют трафик ответа с уже открытой сессией, чтобы разрешить его приём.
Например, правило позволяет ВМ создать исходящую сессию на 80-й порт какого-либо IP-адреса. Ответы от 80-го порта на порт источника, откуда отправлялся запрос, будут автоматически разрешены.
Виды правил
    Для входящего трафика. Определяют диапазоны адресов и портов или другие группы безопасности, откуда ВМ могут принимать трафик.
    Для исходящего трафика. Определяют диапазоны адресов и портов или другие группы безопасности, куда ВМ могут отправлять трафик.
Если в группе безопасности есть правила только для исходящего трафика, но нет для входящего — ответный трафик будет поступать на ВМ. А если есть правила только для входящего трафика, ВМ сможет лишь отвечать на запросы, но не инициировать их.
Если две ВМ находятся в одной группе безопасности без правил, они не смогут обмениваться трафиком. Выберите любое решение:
    Используйте правило Self для всей группы. Оно разрешает любой трафик между ресурсами, которые относятся к одной и той же группе безопасности.
    Точно укажите адреса и порты ресурсов в правилах.
IP-адреса и диапазоны адресов
В правилах вы можете разрешать прием и отправку трафика на IP-адреса или диапазоны адресов. Указывайте конкретный IP-адрес в правилах с помощью СIDR с маской /32.
Чтобы разрешить передачу трафика на любые адреса по любым протоколам, укажите CIDR 0.0.0.0 с маской /0 и в поле выбора протокола выберите Любой.
Группы безопасности не блокируют отправку трафика на адреса сервисов, необходимых для работы ВМ и виртуальной сети. Это:
    Адрес сервера метаданных — 169.254.169.254.
    Адрес DNS-сервера — второй по порядку внутренний IP-адрес (обычно x.x.x.2) в каждой подсети.
Чтобы сетевой балансировщик мог проверять состояние подключённых к нему ресурсов, разрешите передачу трафика между диапазонами адресов 198.18.235.0/24 и 198.18.248.0/24 и целевыми ресурсами.
Параметры по умолчанию
Если не указано иное, группа безопасности автоматически создаётся в новой сети и назначается ВМ при подключении к подсетям новой сети, если у них нет ни одной группы безопасности.
При этом группа безопасности автоматически создаётся с рядом правил. Разрешён любой исходящий трафик. Для входящего трафика разрешены:
    любой входящий трафик от членов той же группы безопасности;
    SSH-соединения на порт 22 (протокол TCP) с любых адресов (0.0.0.0/0);
    RDP-соединения на порт 3389 (протокол TCP) с любых адресов (0.0.0.0/0);
    любой входящий трафик по протоколу ICMP с любых адресов (0.0.0.0/0).
Группу безопасности, созданную по умолчанию, невозможно удалить.
Task:
Вы создали группу безопасности с открытым портом 80. Как зайти на ВМ по SSH?
Decision:
-Включить серийную консоль
+Открыть порт 22
Task:
Практическая работа. Создаем группу безопасности и открываем доступ к серверу
Decision:
Давайте попробуем создать группу безопасности и сделать доступными страницы, предоставляемые веб-сервером NGINX.
    Перейдите по ссылке https://console.cloud.yandex.ru/link/vpc/security-groups и нажмите кнопку Создать группу.
    Введите имя группы yc-security и выберите сеть yc (вы создали её на одном из предыдущих практических занятий).
    В блоке Правила добавьте правила для исходящего трафика. Опишите правило, укажите диапазон портов 80 (HTTP) и протокол TCP, выберите назначение "CIDR" 0.0.0.0/0 . Создайте аналогичные исходящие правила для портов 443 (HTTPS) и 22 (SSH). Для входящего трафика добавьте правила для 80 и 22 портов, чтобы подключаться к веб-серверу и управлять ВМ извне. Вы увидите созданную группу в списке групп безопасности. Если инициировано соединение по определенному порту и протоколу с ВМ и есть исходящее правило, то значит и на входящий трафик будет разрешена передача данных в эту же сеть, на этот же протокол и порт. Если назначить сетевому интерфейсу ВМ группу безопасности без правил, ВМ не сможет передавать и принимать трафик.
    Создайте виртуальную машину на базе Ubuntu 20.04, выберите сеть yc и вновь созданную группу безопасности yc-security в сетевых настройках создаваемой ВМ, дождитесь запуска машины, подключитесь к ВМ по SSH и установите веб-сервер NGINX (по умолчанию он отсутствует). Для этого выполните команду:
sudo apt-get install nginx 
    Установка возможна, поскольку вы открыли порт 80: команда apt-get использует его для получения пакетов с ПО.
    После установки сервер автоматически запустится и будет доступен извне благодаря открытому порту 80. Проверьте это, зайдя через браузер на публичный IP-адрес ВМ, который найдёте на странице параметров машины в консоли управления.
Decision:
$ ssh -i ubcloud testuser@51.250.66.47
testuser@ubuntu-test2:~$ sudo apt-get install nginx
Conclusion:
51.250.66.47
Task:
Балансировка нагрузки
Decision:
Вы разрабатываете сайт интернет-магазина и хотите, чтобы он был готов к неожиданному наплыву посетителей и никогда не падал. Абсолютной стабильности, конечно, не добиться. Но в ваших силах сделать сайт устойчивым ко многим проблемам.
Пережить одновременный визит множества пользователей поможет развёртывание копий сайта на нескольких ВМ. В таком случае нагрузка равномерно распределится между ними. Если машин пять — то каждой достанется 20% запросов. Такой подход называется сетевой балансировкой.
Работает это так. Перед ВМ с сайтом ставят балансировщик — приложение, которое принимает запросы от пользователей и распределяет их по ВМ, а затем получает от ВМ ответы и передаёт (проксирует) их пользователям. При этом сервису, передающему трафик, не нужно знать адреса и названия ВМ: процедура разрешения имён делегируется балансировщику. Это называется абстрактностью имён.
Балансировка не только помогает распределить нагрузку между серверами, но и защищает веб-приложение от выхода ВМ из строя. Предположим, на одном из серверов возникли неполадки и он не может обрабатывать запросы. В этом случае балансировщик перераспределит нагрузку между другими серверами, и с этого момента недоступность сервиса для конечных пользователей прекратится.
Кроме того, балансировщик позволяет незаметно для пользователей обновлять код сайта или веб-приложения на серверах: вы просто поочередно убираете ВМ из-под балансировки, обновляете софт, после чего возвращаете их под балансировку.
Добиться максимальной доступности сайта или приложения можно, разместив ВМ в разных зонах доступности. Если в одной зоне произойдёт авария и все ВМ выйдут из строя — балансировщик начнёт распределять трафик по ВМ в других зонах, пока работа отключённой зоны не восстановится.
Task:
Преимущества балансировки — это
Decision:
+Абстрактность имён
+Устойчивость к отказам
+Повышение производительности
Task:
Yandex Network Load Balancer
Decision:
Чтобы настроить сетевую балансировку в Yandex Network Load Balancer, разберёмся с двумя базовыми понятиями.
Первое — это целевая группа, т. е. набор серверов или других облачных ресурсов, по которым распределяются запросы пользователей. Целевая группа выглядит как список внутренних IP-адресов и подсетей, к которым эти IP-адреса относятся.
Допустим, вам нужно распределить трафик по пяти виртуальным машинам (ВМ). В этом случае целевая группа балансировщика может выглядеть так:
    10.10.10.15, e9b7a3k9rqq3j0j36m9u
    10.10.10.20, e9b7a3k9rqq3j0j36m9u
    10.10.20.31, e2lgvksek5io187a48q5
    10.10.20.10, e2lgvksek5io187a48q5
    10.10.30.20, b0cnsvg8jfoe938ktqp4 
Здесь перечислены пять внутренних IP-адресов, причём для каждого адреса указан идентификатор его подсети. Все адреса целевых ресурсов должны принадлежать одной облачной сети.
Чтобы посмотреть список подсетей и их идентификаторов, откройте в консоли управления раздел Virtual Private Cloud и перейдите на вкладку Облачные сети.
Второе базовое понятие — обработчик. Это приложение принимает соединения от пользователей, распределяет их между IP-адресами целевой группы, а затем передаёт обратный трафик клиентам.
Адресация трафика строится по принципу 5-tuple: учитывается адрес и порт отправителя, адрес и порт целевого (принимающего) облачного ресурса, а также протокол передачи информации. Для приёма трафика обработчик использует порты от 1 до 32767.
При создании сетевого балансировщика необязательно сразу настраивать обработчик. Если хотите, добавьте его позднее.
Кроме того, вы можете задать несколько обработчиков. Это пригодится, если запущенный на ВМ сервис предполагает использование нескольких портов сразу. К примеру, вы используете надстройку над Git наподобие GitLab. Значит, одновременно должны быть доступны и веб-интерфейс, и сервер Git, работающие на разных портах.
Целевую группу можно подключить к нескольким балансировщикам — например, чтобы балансировщики на портах 80 и 443 смогли обрабатывать и HTTP-, и HTTPS-запросы. Однако в этом случае вам придётся использовать разные целевые порты. Если группа подключена к одному балансировщику на порту 8080, то к другому балансировщику вам придётся подключить её на порту 8081.
После подключения целевой группы балансировщик начнёт проверять состояние целевых ресурсов и сможет распределять нагрузку между ними.
Task:
Какие задачи решает Yandex Network Load Balancer?
Decision:
+Обеспечивает отказоустойчивость
+Распределяет веб-трафик
-Контролирует объём пересылаемого трафика
Task:
Возможно ли создать балансировщик без обработчика?
Decision:
+Да
-Нет
-Зависит от настроек целевой группы
Task:
Проверка состояния
Decision:
Вы создали сетевой балансировщик, настроили обработчики и указали целевую группу из пяти ВМ, на каждой из которых работает копия веб-сайта. Одна ВМ вышла из строя. Чтобы балансировщик узнал о неполадке и перестал проксировать трафик на проблемную ВМ, настройте проверку состояния: специальный запрос от балансировщика по протоколу TCP или HTTP.
Например, балансировщик раз в 10 секунд запрашивает у каждой ВМ страницу по HTTP. Если все ВМ за отведённое время отдают код 200, их состояние — Healthy (англ. здорова). Значит, ВМ готовы принимать трафик. Но если ВМ не успевает ответить, её состояние меняется на Unhealthy (англ. нездорова). Балансировщик обрабатывает результат проверки и затем перестаёт отправлять трафик на ВМ.
Когда работа ВМ восстанавливается и ВМ успевает за отведённое время отдать код 200 — её статус меняется на Healthy.
Подробнее о статусах ресурсов читайте в документации.
Task:
Задача балансировщика — узнать:
Decision:
+о состоянии ресурсов целевой группы
-о количестве ресурсов в целевой группе
-о состоянии подсетей целевой группы
Task:
ВМ пройдёт проверку, если...
Decision:
-ВМ отдала код 200 через 20 секунд при опросе раз в 10 секунд
+ВМ отдала код 200 через 8 секунд при опросе раз в 10 секунд
-ВМ отдала код 503 через 2 секунды при опросе раз в 10 секунд
Task:
Практическая работа. Знакомство с Yandex Cloud CLI
Decision:
На этом практическом занятии вы научитесь быстрее создавать веб-серверы, чтобы затем помещать их за сетевой балансировщик.
В качестве веб-серверов выступят две ВМ, на которых доступна информационная страница запущенного веб-сервера NGINX. Для реальных проектов с высокой нагрузкой вам, скорее всего, придётся создавать гораздо больше ВМ, поэтому вам пригодится умение делать это автоматически: с помощью консольного интерфейса Yandex Cloud CLI. Подробнее о работе с CLI поговорим в рамках другого курса. Здесь же остановимся на простом примере применения этого инструмента.
    Установите и настройте Yandex Cloud CLI, следуя инструкциям в документации.
    Создайте файл startup.sh , который будет запускаться на ВМ после её создания, со следующим содержимым.
#!/bin/bash
apt-get update
apt-get install -y nginx
service nginx start
sed -i -- "s/nginx/Yandex Cloud - ${HOSTNAME}/" /var/www/html/index.nginx-debian.html
EOF 
Скрипт получает список актуальных пакетов с софтом, устанавливает и запускает NGINX, а затем меняет информационную страницу работающего сервера.
    Создайте первую ВМ с помощью команды yc compute instance create. Чтобы указать, какую именно ВМ мы хотим создать, в команде используются параметры. В данном случае — имя ВМ (--name demo-1), откуда взять метаданные (из файла скрипта startup.sh, созданного ранее), из какого образа создать загрузочный диск (ubuntu-2004-lts), в какой зоне создать машину (--zone ru-central1-a) и в какую подсеть подключить сетевой интерфейс с IPv4-адресом (--network-interface ...).
yc compute instance create \
--name demo-1 \
--metadata-from-file user-data=startup.sh \
--create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
--zone ru-central1-a \
--network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 
    Самостоятельно измените и запустите еще раз эту команду, чтобы создать такую же ВМ с именем demo-2.
    Теперь проверим, что обе ВМ успешно созданы. Это можно сделать, заглянув в консоль управления:
    А можно и из командной строки с помощью команды yc compute instance list. Попробуйте этот способ. У вас должен получиться примерно такой результат:
+----------------------+--------+---------------+---------+----------------+-------------+
|          ID          |  NAME  |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
+----------------------+--------+---------------+---------+----------------+-------------+
| fhmnc7nireh12e25ctk0 | demo-1 | ru-central1-a | RUNNING | 84.201.174.97  | 10.130.0.18 |
| fhmrbmchmu3ganplskp2 | demo-2 | ru-central1-a | RUNNING | 84.252.129.231 | 10.130.0.31 |
+----------------------+--------+---------------+---------+----------------+-------------+ 
    Убедитесь, что статус машин сменился на Running, а скрипт выполнился (иногда нужно подождать до одной минуты, пока обновится список пакетов и установится NGINX).
    Введите публичные IP-адреса ВМ в браузере и проверьте, что главные страницы веб-серверов доступны:
Decision:
$ curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
$ yc init
...
Please enter OAuth token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
You have one cloud available: 'cloud-test138it' (id = b1gg01f1vt0rkid9qsuk). It is going to be used by default.
Please choose folder to use:
 [1] default (id = b1geto6411pvmr7j3pd2)
 [2] Create a new folder
Please enter your numeric choice: 1
Your current folder has been set to 'default' (id = b1geto6411pvmr7j3pd2).
Do you want to configure a default Compute zone? [Y/n] y
Which zone do you want to use as a profile default?
 [1] ru-central1-a
 [2] ru-central1-b
 [3] ru-central1-c
 [4] Don't set default zone
Please enter your numeric choice: 1
Your profile default Compute zone has been set to 'ru-central1-a'.
$ yc config list
token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
cloud-id: b1gg01f1vt0rkid9qsuk
folder-id: b1geto6411pvmr7j3pd2
compute-default-zone: ru-central1-a
$ vim startup.sh
$ cat startup.sh
#!/bin/bash
apt-get update
apt-get install -y nginx
service nginx start
sed -i -- "s/nginx/Yandex Cloud - ${HOSTNAME}/" /var/www/html/index.nginx-debian.html
EOF
$ yc compute instance create \
> --name demo-1 \
> --metadata-from-file user-data=startup.sh \
> --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
> --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
$ yc compute instance create \
> --name demo-2 \
> --metadata-from-file user-data=startup.sh \
> --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts \
> --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4
$ yc compute instance list
Conclusion:
62.84.114.54
http://51.250.79.17/
Task:
Практическая работа. Создание балансировщика
Decision:
Итак, у вас есть виртуальные машины. Можно сразу создать и балансировщик, и целевую группу, но мы поступим иначе: сначала создадим целевую группу, затем подключим её к балансировщику.
В консоли управления откройте раздел Network Load Balancer, на вкладке Целевые группы нажмите кнопку Создать целевую группу. На открывшейся странице введите имя целевой группы (например demo-web), выберите обе ВМ, созданные на предыдущем уроке, и нажмите кнопку Создать.
Остаётся создать балансировщик. Для этого сначала создайте обработчик и настройте проверку состояния ресурсов в целевой группе:
    На вкладке Балансировщики нажмите кнопку Создать сетевой балансировщик.
    Заполните имя балансировщика (например lb-demo-web) и нажмите кнопку Добавить обработчик.
    В открывшемся окне введите имя обработчика (например demo-web-listener). В качестве портов укажите 80и нажмите кнопку Добавить.
    После создания обработчика нажмите кнопку Добавить целевую группу. Укажите имя проверки состояния (например hc-demo-web), тип проверки (HTTP), порт (80), интервал отправки проверок состояния в секундах, порог работоспособности и порог неработоспособности. Оставьте указанный по умолчанию путь для проверок, используйте значения по умолчанию и для других параметров. Нажмите кнопку Применить, а затем кнопку Создать.
    После создания балансировщика проверьте состояние ресурсов: в консоли управления откройте страницу балансировщика и убедитесь, что его статус — Active. Значит, балансировщик готов передавать трафик целевым ресурсам.
    Перейдите на страницу балансировщика и посмотрите на блок Целевые группы. У запущенных ВМ, готовых принимать трафик, будет статус Healthy.
    Введите внешний IP-адрес балансировщика в адресную строку браузера — и балансировщик перенаправит вас на одну из машин целевой группы. Обратите внимание на имя ВМ, указанное во второй строке веб-страницы.
    Чтобы протестировать отказоустойчивость, в консоли управления перейдите в раздел Compute Cloud и остановите одну из ВМ целевой группы.
    Вернитесь на страницу балансировщика и убедитесь, что статус остановленной ВМ изменился на Unhealthy. Это означает, что целевой ресурс группы не прошёл проверку состояния и не готов принимать трафик.
    Обновите страницу с IP-адресом балансировщика, и вы увидите, что трафик перенаправлен на другую ВМ (изменилось имя ВМ, указанное во второй строке веб-страницы).
После завершения работы не забудьте удалить использованные ресурсы: две ВМ и балансировщик.
Task:
Как правильно использовать балансировщики
Decision:
Чтобы построить эффективную инфраструктуру с высокой отказоустойчивостью:
    Создавайте ресурсы в разных зонах доступности. Размещайте копии виртуальных машин (ВМ) в нескольких зонах доступности. Так приложения останутся доступны, даже если одна из зон выйдет из строя. В каждой зоне доступности разместите одинаковое количество облачных ресурсов. Если в ru-central1-a находится три ВМ — поместите по три ВМ и в ru-central1-b, и в ru-central1-c.
    Создавайте облачные ресурсы с запасом. Если одна ВМ в зоне доступности выйдет из строя, трафик продолжит поступать в зону в том же объёме, а нагрузка на оставшиеся машины увеличится. Чтобы все ВМ не вышли из строя — помимо ресурсов, необходимых для обслуживания расчётной нагрузки, добавьте в каждой зоне дополнительные вычислительные ресурсы (vCPU, RAM).
    Используйте разные балансировщики для разных приложений. Если вы разворачиваете в Yandex Compute Cloud несколько приложений — настройте для их обслуживания отдельные балансировщики. Это поможет эффективнее управлять нагрузкой. 
    Организуйте два уровня балансировщиков. Балансировщики Yandex Cloud работают с протоколами TCP и UDP — так называемыми транспортными протоколами или протоколами четвёртого уровня сетевой модели OSI. Они называются так потому, что предназначены для обеспечения надёжной передачи данных от отправителя к получателю. Кроме того, бывают балансировщики протоколов седьмого уровня — на этом уровне работает, например, протокол HTTP.
Поскольку балансировщики седьмого уровня, например веб-сервер NGINX, выполняют более сложную работу с IP-пакетами (сборка, анализ, журналирование), они выиграют от предварительного распределения нагрузки на четвертом уровне, особенно при DDoS-атаках.
Организуйте двухуровневую архитектуру с балансировщиками на четвертом (транспортном) уровне OSI и седьмом уровне приложения (например HTTP). Балансировщик четвертого уровня будет принимать трафик и передавать его целевой группе балансировщиков седьмого уровня, а те распределят трафик по ВМ с приложениями. В качестве балансировщиков седьмого уровня вы можете использовать ВМ, на которые установили ПО для балансировки (например NGINX).
Task:
Зачем нужны группы виртуальных машин
Decision:
Управлять большим количеством ВМ вручную непросто. Всегда есть риск, что вы не отследите программный сбой или пиковую нагрузку, из-за чего сервис — к неудовольствию пользователей — ляжет.
Чтобы избежать таких неприятностей, настройте управление группами ВМ (или Instance Groups). Сгруппируйте однотипные ВМ, которые могут находиться в разных зонах доступности, а затем определите, по каким правилам система работает с группами.
Предположим, о вашем стартапе написали в издании Motherboard. На сайт хлынули посетители, нагрузка резко выросла — и ВМ перестали с ней справляться. В этом случае правильно настроенная система сама создаст достаточно копий ВМ с приложением. Когда пик интереса пройдёт, система увидит снижение нагрузки и постепенно удалит ненужные копии.
Ещё пример: в одной из ВМ приложение перестало реагировать на запросы. Система сама определит сбой по заданным правилам и перезапустит эту ВМ или создаст заново.
Все ВМ в группе автоматически создаются по шаблону. Вы заполните его параметры при формировании группы. Шаблон описывает конфигурацию машины: какие ей нужны системные ресурсы, как создать дополнительный диск, какие сетевые параметры применить, создавать ли пользователей в системе автоматически и т.д.
Создание, обновление и удаление ВМ в группах выполняется от имени так называемого сервисного аккаунта. Это учетная запись со специфичным набором привилегий (например, административным). Группе ВМ можно присвоить только один сервисный аккаунт, созданный в том же самом каталоге.
Вы также можете использовать сервисный аккаунт для работы с другими API Yandex Cloud (например для интеграции групп ВМ с сетевым балансировщиком).
Task:
Группы ВМ применяются, чтобы...
+перезапускать машины, которые перестали откликаться на запросы
+автоматически увеличивать число машин при пиковых нагрузках
Task:
Практическая работа. Создание группы виртуальных машин
Decision:
Иногда вам требуется не автоматическое масштабирование, а автоматическое восстановление ВМ. Например, если вы отлаживаете работу веб-сервиса, который периодически падает. Для этого подойдут группы ВМ фиксированного размера. Давайте создадим и настроим такую группу.
В консоли управления откройте раздел Compute Cloud, перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу.
Откроется страница Создание группы виртуальных машин.
В блоке Базовые параметры введите имя и описание группы ВМ. Создайте новый сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
ВМ группы могут находиться в разных зонах и регионах. В блоке Распределение выберите две зоны доступности, чтобы обеспечить доступность сервиса, если в одной из них случится сбой.
В блоке Шаблон виртуальной машины нажмите кнопку Задать.
Шаблон создается так же, как и сама ВМ. В блоке Базовые параметры введите описание шаблона конфигурации, затем в блоке Выбор образа/загрузочного диска на вкладке Операционные системы выберите Ubuntu.
В блоках Диски и Вычислительные ресурсы для загрузочного диска оставьте значения по умолчанию. В блоке Сетевые настройки выберите существующую сеть и подсеть или создайте новые. В блоке Доступ выберите существующий или создайте новый сервисный аккаунт, укажите логин, вставьте в поле SSH-ключ содержимое файла с публичным ключом, доступ к серийной консоли не разрешайте.
Сохраните параметры и вы вернётесь на страницу Создание группы виртуальных машин.
В блоке В процессе создания и обновления разрешено установите политику развертывания:
    Добавлять выше целевого значения (на сколько ВМ можно превышать размер группы) — 2.
    Уменьшать относительно целевого значения (на сколько ВМ можно уменьшать размер группы) — 1.
    Одновременно создавать (сколько ВМ можно сразу создавать в группе) — 2.
    Время запуска (сколько времени должно пройти, прежде чем будут пройдены все проверки состояния и ВМ начнет получать нагрузку) — 2 минуты.
    Одновременно останавливать (сколько ВМ можно сразу удалять) — 1.
    Останавливать машины по стратегии — Принудительная. При принудительной стратегии Instance Groups самостоятельно выбирает, какие ВМ остановить.
В блоке Масштабирование выберите фиксированный тип, Размер (количество ВМ) — 3.
В блоке Интеграция с Load Balancer оставьте опцию Создать целевую группу выключенной. Не включайте пока проверку состояний, которая позволяет Instance Groups получать сведения о состоянии ВМ.
Нажмите кнопку Создать и вернитесь на страницу Группы виртуальных машин. В правом нижнем углу появится сообщение «Группа виртуальных машин создаётся». Одновременно можно создавать не более двух ВМ. Поэтому сначала будут созданы две ВМ, потом — третья.
После того как вы создали группу, протестируйте включение и выключение всех машин сразу. Обратите внимание: в соответствии с настройками сервис инициирует запуск не более двух машин одновременно. Третья ВМ будет оставаться остановленной. Как только первая будет запущена, один слот на запуск освободится, поэтому сразу будет инициирован запуск третьей и последней ВМ.
Task:
Автоматическое восстановление
Decision:
Ни одно приложение не работает идеально. Например, если сервис из-за программного сбоя начнёт создавать множество временных файлов, на диске рано или поздно закончится свободное место. Работа сервиса прекратится. Пользователи, чьи запросы обслуживает ВМ, будут видеть сообщение об ошибке.
Чтобы ВМ простаивала как можно меньше, Instance Groups регулярно проверяет состояние ВМ или отзывчивость приложения. Обнаружив неполадки, сервис действует по выбранному вами сценарию: перезапускает ВМ или создаёт новую.
Способ автоматического восстановления при сбое зависит от того, как вы настроили политику развёртывания:
    Если вы разрешили превышать целевой размер группы (поле Добавлять выше целевого значения), Instance Groups будет создавать ВМ вместо не прошедших проверку.
    Если вы разрешили уменьшать целевой размер группы (поле Уменьшать относительно целевого значения), Instance Groups перезагрузит ВМ. Иногда для устранения проблемы этого достаточно. Если проблема из примера выше в том, что в папке /tmp скопилось много файлов, при перезапуске системы папка автоматически очистится.
Если вы не знаете заранее, достаточно ли перезагрузки ВМ, комбинируйте оба способа восстановления: используйте сразу два параметра.
Допустим, вы разрешили и превышать, и уменьшать целевой размер группы на одну машину. Когда одна из ВМ не пройдет проверку, Instance Groups начнет одновременно перезапускать эту машину и создавать новую. ВМ, которая первая пройдет все проверки, начнет работать, а вторая будет удалена.
Старые машины не удаляются до тех пор, пока не созданы новые. А если в процессе создания новой ВМ все машины в группе станут работоспособны, то сервис отменит её создание.
Автоматическое восстановление прерываемых ВМ начнётся только тогда, когда в зоне доступности будет достаточно вычислительных ресурсов. Иногда это занимает немало времени.
Task:
Чтобы максимально защитить группу ВМ фиксированного размера от сбоев, нужно разрешить:
-превышение целевого размера группы
-уменьшение целевого размера группы
+и уменьшение, и превышение целевого размера группы
Task:
Автоматическое масштабирование
Вы разработали и запустили веб-сервис, дали к нему ранний доступ парочке популярных блогеров и со дня на день ждёте наплыва посетителей. Теперь надо сделать так, чтобы сервис продолжил работать при пиковой посещаемости как ни в чём не бывало.
Для этого настройте автоматическое масштабирование группы ВМ. Система сама будет отслеживать потребность в ВМ и добавлять их, а при снижении нагрузки — убирать лишние, чтобы экономить ресурсы и деньги.
Вот как это работает:
    Создайте группу ВМ.
    Укажите, какие метрики системе отслеживать, чтобы вовремя добавлять или убирать ВМ. Обычно это нагрузка CPU: при загруженном на 100% процессоре сервис попросту перестаёт отзываться на действия посетителей. Вы можете использовать и свои метрики (например время ответа сервиса).
    Укажите целевое значение метрики. Например загрузка CPU — в среднем не больше 50%.
Однако нагрузка на ресурсы бывает неравномерной. Например, ваш сервис мониторинга репутации в соцсетях хранит копии публикаций в микроблогах, на форумах, сайтах с отзывами и т. д. Пользователи часто будут делать выгрузки для отчётности, что подразумевает запросы по очень большому диапазону записей в очень большой базе данных. Поэтому среднее значение метрики может резко меняться.
Но если после каждого всплеска и спада нагрузки создавать и удалять ВМ — это тоже приведёт к расходу ресурсов. Поэтому количество ВМ регулируется при помощи нескольких переменных:
    Период стабилизации. После увеличения количества машин в группе ВМ не удаляются сразу и сохраняются заданное время, даже если среднее значение метрики стало заметно ниже её целевого значения. В этом случае при повторном всплеске нагрузки уже будет доступна хотя бы одна дополнительная ВМ, которая перехватит часть запросов.
    Период прогрева. Запуск ВМ иногда приводит к аномально высокой нагрузке. Период прогрева позволяет игнорировать ее в течение заданного времени.
    Период усреднения. Instance Groups использует усредненные значения всех метрик и игнорирует резкие скачки нагрузки CPU за некоторую единицу времени. Вне периодов стабилизации и прогрева Instance Groups несколько раз в минуту измеряет нагрузку CPU на каждой ВМ и усредняет значения за время, указанное как период усреднения. Затем выполняет дополнительное усреднение по зонам доступности. Если по результатам расчёта нужно создать ещё одну ВМ — Instance Groups запустит этот процесс и начнёт рассчитывать среднюю нагрузку заново.
Вы также можете установить в Yandex Monitoring пользовательские метрики. Например, среднее время ответа сервиса. Укажите имя метрики и ее целевое значение. Если оно будет превышено, Instance Groups создаст дополнительные машины для распределения нагрузки.
Task:
Чтобы сервис не создавал ВМ на каждый всплеск активности, нужно правильно:
Decision:
-настроить период прогрева
+настроить период усреднения
+задать целевое значение метрики
Task:
Практическая работа. Автоматическое масштабирование под нагрузкой
Decision:
Давайте разберёмся, как обеспечить доступность сервиса под высокой нагрузкой. Вы уже научились создавать группы ВМ. Теперь создадим автоматически масштабируемую группу ВМ.
В консоли управления откройте раздел Compute Cloud. Перейдите на вкладку Группы виртуальных машин и нажмите кнопку Создать группу. Задайте имя группе ВМ.
Создайте сервисный аккаунт. Чтобы иметь возможность создавать, обновлять и удалять ВМ в группе, назначьте сервисному аккаунту роль editor. По умолчанию все операции в группе ВМ выполняются от имени сервисного аккаунта.
В блоке Распределение выберите только одну зону доступности.
В блоке Шаблон виртуальной машины нажмите кнопку Задать. В открывшемся окне выберите:
    ОС: Ubuntu 20.04.
    Размер загрузочного диска: 50 ГБ.
    Тип загрузочного диска: SSD.
    Остальные параметры — по умолчанию. Не забудьте добавить публичный SSH-ключ. Он понадобится нам на следующем практическом занятии.
В блоке В процессе создания и обновления разрешено оставьте параметры по умолчанию.
Перейдите к блоку Масштабирование и выберите тип Автоматический.
Задайте параметры масштабирования:
    Тип автомасштабирования — зональное. При зональном автомасштабировании количество ВМ регулируется отдельно в каждой зоне доступности, указанной в настройках группы.
    Минимальное количество ВМ в зоне — 2. Сервис Instance Groups не будет удалять ВМ в зоне доступности, если их там всего две.
    Максимальный размер группы — 4. Instance Groups не будет создавать ВМ, если их уже четыре. В этот раз размер загрузочного диска ВМ — 50 ГБ, поэтому с учётом квот на суммарный объём SSD-дисков в одном облаке смогут запуститься четыре ВМ.
    Промежуток измерения загрузки (это период усреднения: время, за которое следует усреднять замеры нагрузки для каждой ВМ в группе) — 60 секунд.
    Время на разогрев ВМ — 3 минуты. В течение этого времени ВМ не учитывается в измерении средней нагрузки на группу. Фактически данное время мы можем определить, измерив, как быстро запускается ВМ.
    Период стабилизации — 5 минут. Отсчитывается с момента, когда Compute Cloud принял последнее решение о том, что количество ВМ в группе нужно увеличить.
    Начальный размер группы — 4. Это количество ВМ, которое следует создать вместе с группой.
В блоке Метрики укажите:
    Тип — CPU.
    Целевой уровень загрузки CPU, % — 80. Instance Groups будет управлять размером группы так, чтобы поддерживать указанную нагрузку CPU.
Нажмите кнопку Создать. Сервис начнет создавать ВМ. После создания статус группы изменится на Active. Обратите внимание, как меняются Состояния ВМ.
    Creating instance — ВМ создаётся и запускается.
    Awaiting warmup duration — ВМ начинает принимать сетевой трафик. В этом статусе ВМ находится в течение периода прогрева, указанного в настройках автоматического масштабирования. Значения метрик ВМ в этом статусе заменяются средними значениями ВМ из той же зоны доступности.
    Running actual — ВМ запущена, на неё подается сетевой трафик, пользовательские приложения работают.
Группа ВМ готова принимать рабочую нагрузку.
Task:
Практическая работа. Воссоздание виртуальных машин в группе
Decision:
Давайте сымитируем рост нагрузки на ВМ и посмотрим, как сервис на это отреагирует.
    В консоли управления откройте раздел Compute Cloud и перейдите на страницу группы ВМ, которую вы создали на прошлом практическом занятии. Откройте в двух вкладках браузера страницы Группы виртуальных машин и Мониторинг (открывается из раздела Группы виртуальных машин).
    После запуска группы зайдите по SSH на каждую из двух ВМ и установите приложение для стресс-тестирования Linux-систем. Для этого выполните команду:
sudo apt-get install stress 
    После этого для каждой ВМ запустите установленное приложение:
stress -c 2 
Аргумент -c означает, что при тестировании будет нагружаться процессор, а число после аргумента задаёт количество ядер процессора, которые будут нагружаться. Чтобы эксперимент удался — укажите количество ядер, которое вы выбрали в шаблоне ВМ.
    На вкладке со страницей мониторинга на графике Average CPU utilization in ru-central1-a следите за тем, как усреднённое значение нагрузки будет постепенно расти.
Как только усреднённое значение нагрузки превысит порог, сервис Instance Groups начнёт прогревать две дополнительные ВМ и вводить их в строй. Это будет видно на странице Группы виртуальных машин.
Поскольку стресс-тест не остановлен, сервис завершает запуск двух ВМ.
Через некоторое время усреднённое значение нагрузки процессоров в группе упадёт до 50%, поскольку первая половина ВМ загружена полностью, а вторая не загружена вовсе.
    Остановите работу стресс-теста на первой ВМ. В командной строке используйте сочетание клавиш Ctrl + C.
Через некоторое время усреднённое значение достигнет 25%, тогда Instance Groups удалит лишнюю ВМ:
    Остановите второй стресс-тест. Через некоторое время после того, как усредненное значение достигнет нуля, Instance Groups удалит вторую дополнительную ВМ.
    При минимальной нагрузке остаются работать две машины:
Вот так при растущей нагрузке группа ВМ автоматически масштабируется, чтобы обеспечить доступность ресурса.
Decision:
$ ssh -i ubcloud newu@178.154.207.23
$ sudo apt-get install stress 
$ ssh -i ubcloud newu@178.154.200.224
$ sudo apt-get install stress 
Task:
Какой вариант доступа к ВМ безопаснее, если в облаке несколько пользователей с одинаковыми правами?
Decision:
-Через серийную консоль
+По SSH
Task:
Прерываемыми называются виртуальные машины, которые…:
Decision:
+могут быть принудительно остановлены, если в той же зоне доступности не хватает ресурсов для запуска обычных ВМ.
-имеют уровень производительности vCPU менее 100%.
-принудительно останавливаются через 12 часов с момента запуска.
Task:
Прерываемые ВМ подойдут, чтобы…
Decision:
-развернуть интернет-магазин.
+решать задачи, связанные с пакетной обработкой данных.
+повысить производительность веб-сервисов при пиковых нагрузках.
-развернуть систему мониторинга серверов и сетевого оборудования.
Task:
Ваша ВМ, у которой есть только загрузочный диск, находится в зоне доступности ru-central1-a. Какими способами можно создать копию этой ВМ в зоне доступности ru-central1-b?
Decision:
+Сделать образ диска ВМ, он автоматически реплицируется в остальные зоны доступности, создать из этого образа ВМ в зоне ru-central1-b.
-Сделать снимок диска ВМ, создать из него копию ВМ в зоне доступности ru-central1-a, изменить в параметрах ВМ зону доступности на ru-central1-b.
-Сделать снимок диска ВМ, перенести его в зону доступности ru-central1-b, создать ВМ из этого снимка.
+Сделать снимок диска ВМ, он сам реплицируется в нужную зону доступности, создать ВМ в зоне ru-central1-b из этого снимка.
Task:
Ваша ВМ всегда должна быть доступна по одному и тому же публичному IP-адресу. Укажите все оптимальные варианты решения задачи.
Decision:
+Выбрать статический IP-адрес при создании ВМ.
+Сделать динамический IP-адрес статическим, если ВМ уже настроена и работает.
-Никогда не останавливать ВМ, а только перезапускать.
Task:
Какой главный принцип работы групп безопасности?
Decision:
Разрешено всё, что не запрещено напрямую.
+Запрещено всё, что не разрешено напрямую.
Task:
На всех копиях ВМ за балансировщиком работает несколько приложений, каждое из которых использует несколько портов, причём все порты уникальны. Какая конфигурация балансировщика оптимальна?
Decision:
-Отдельный балансировщик на каждый порт.
+Отдельный балансировщик на каждое приложение с обработчиками всех портов, используемых этим приложением.
Task:
На каждой машине в группе ВМ работает несколько «боевых» приложений. Как лучше настроить балансировщик?
Decision:
-Создать единый балансировщик и добавить обработчики на все используемые порты.
+Создать балансировщики на каждое приложение.
Task:
В работе одной из ВМ в группе произошёл сбой. Если при создании новой ВМ старая машина станет работоспособна, Instance Groups:
Decision:
+прекратит создавать ВМ.
-завершит создание ВМ и удалит ее.
Task:
Во время коротких всплесков активности на веб-сервисе Instance Groups автоматически создаёт ВМ. Когда нагрузка спадает — убирает их. Всплески происходят часто, и Instance Groups постоянно создаёт и удаляет ВМ. Какое решение обойдётся дешевле?
Decision:
-Увеличить количество постоянных ВМ в группе.
+Увеличить период стабилизации, чтобы ВМ не удалялись слишком быстро.
Task:
Хранение и анализ данных
Decision:
Введение
Чтобы принимать обоснованные решения, бизнесу требуется собирать, обрабатывать и анализировать данные. Для этого всё чаще применяются облачные технологии: доступ к ним происходит быстро, плюс облако позволяет экономить деньги, время и силы.
В Yandex Cloud для работы с данными используются сервисы управляемых баз данных (БД), таких как PostgreSQL, MySQL, MongoDB, ClickHouse и других, сервис управления кластерами Yandex Data Proc, сервис визуализации и анализа данных Yandex DataLens и масштабируемое хранилище данных Yandex Object Storage. Вместе они образуют группу сервисов «Платформа данных».
Виртуализация баз данных
Когда разрабатывают веб-приложения, для хранения структурированных данных чаще всего выбирают одну из популярных БД (например PostgreSQL или MySQL). Разработчикам и администраторам предстоит решить, на каких серверах её развернуть: на собственных, арендованных или в облаке. От выбора зависит, кто будет поддерживать и обслуживать серверы и БД.
Использование своих серверов обходится недёшево. Для их обслуживания понадобятся оборудование и квалифицированные специалисты. С примерным расчетом стоимости владения вы можете ознакомиться в этой статье.
Альтернативным вариантом всё чаще становится виртуализация. Виртуализация баз данных — это их размещение на виртуальных машинах в облаке. Этот вариант имеет ряд преимуществ:
    Более высокая доступность данных. Работа приложения не будет зависеть от «железа».
    Быстрое добавление ресурсов. Если БД под наплывом пользователей перестала справляться с нагрузкой — вам не придётся апгрейдить или покупать серверы.
    Интеграция с другими облачными сервисами.
    Экономия сил, времени и денег, которые вы бы потратили на администрирование сервера.
В Yandex Cloud вы можете развернуть БД двумя способами: поверх виртуальных машин или с помощью сервисов управляемых БД (managed services for databases).
Способ 1. Виртуальные машины
От вас потребуется:
    Создать виртуальную машину.
    Установить на ней БД и программы.
    Создать конфигурационные файлы.
    Настроить сеть.
Затем понадобится:
    Если виртуальных машин несколько — настроить репликацию.
    Организовать резервное копирование.
    Отслеживать работу виртуальной машины и БД.
    Устанавливать обновления и БД, и операционной системы.
    Если виртуальная машина или БД внезапно перестанут работать — быстро повторить пункты 1–4.
Плюсы: вы сами контролируете то, как всё работает, и экономите на услугах облачного провайдера.
Минусы: много самостоятельной работы.
Способ 2. Управляемые базы данных
При использовании этих сервисов вы будете настраивать БД через консоль управления в браузере, интерфейс командной строки Yandex Cloud CLI или Yandex Cloud API, а подключаться к БД из приложений (точно так же, как к обычным БД).
Сервисы управляемых БД:
    автоматически отслеживают производительность, отвечают за резервное копирование, восстановление, обновление;
    обеспечивают надёжную работу БД и операционной системы, безопасность данных и репликацию;
    позволяют за несколько минут получить готовый к работе кластер или добавить в него виртуальные машины.
Плюсы: вы можете сосредоточиться на приложении и не отвлекаться на администрирование.
Минусы: это немного дороже, чем виртуальные машины.
В конечном итоге, если у вас нет каких-то специфических ограничений, использовать управляемые БД выгоднее: вы сможете сосредоточиться на своей задаче, не тратя время на установку, настройку и администрирование серверов и ПО.
Task:
Какие базы данных представлены в сервисах управляемых баз данных Yandex Cloud?
Decision:
-Cassandra
+Redis
-Neo4j
+Greenplum
Task:
У сервисов управляемых баз данных есть преимущество перед базами данных поверх виртуальных машин. Какое?
Decision:
-Для них задействуются более производительные виртуальные машины
+Пользователю не придётся их администрировать
-Это более дешёвый вариант
Task:
Кластеры, масштабируемость, доступность и отказоустойчивость
Decision:
На ближайших уроках вы научитесь работать с БД в облаке: создавать, записывать, запрашивать и переносить данные между ними, создавать резервные копии и восстанавливаться из них.
Чтобы приступить к практике, стоит понимать несколько основных концепций, с которыми вы будете иметь дело на протяжении всего этого курса. Это кластеры, масштабирование, доступность и отказоустойчивость.
Кластеры
Использование управляемой БД в Yandex Cloud начинается с создания кластера. Кластер — это одна или несколько виртуальных машин (или хостов), где разворачивается БД.
Настройки кластера для БД мы подробно разберём на практических уроках. Здесь же остановимся на нескольких общих моментах.
При создании кластера вы увидите в интерфейсе настройки по умолчанию. Они позволяют быстро начать работу, не выставляя вручную каждый параметр. К тому же большинство настроек (например число ядер процессора, объём памяти и размер дискового пространства) можно изменить позже. Тогда сервис автоматически поочерёдно отключит хосты, применит изменения и включит хосты обратно. БД при этом будет недолго доступна только для чтения.
Значения некоторых настроек определяются размером кластера и зависят от числа ядер или объёма оперативной памяти. Например, лимит max_connection (максимальное число соединений с БД) в управляемой БД PostgreSQL — 200 соединений на один виртуальный процессор (vCPU).
Единственный параметр, который не стоит пропускать при создании кластера, — это настройка Окружение. Она глобально влияет на кластер, и ее нельзя будет изменить.
Окружение бывает PRESTABLE или PRODUCTION. Ключевое различие в том, как обновляется программное обеспечение, управляющее БД.
Мажорные и минорные обновления системы управления базой данных (СУБД), а также обновления, касающиеся работы сервиса управляемых БД, сначала попадают в окружение PRESTABLE. В нём обновления не гарантируют обратную совместимость и иногда содержат ошибки. Поэтому используйте PRESTABLE для сред разработки и тестирования, чтобы обкатывать новые приложения и функциональности.
В окружение PRODUCTION попадают только проверенные и стабильные обновления. Выбирайте его для той версии приложения, которая полноценно запущена и доступна пользователям.
Масштабируемость
Управляемые БД помогают оптимально использовать вычислительные и дисковые ресурсы виртуальных машин.
Допустим, вы планируете работу интернет-магазина подарков в декабре и январе. В предпраздничные дни поток посетителей вырастет в десятки раз, а в первые январские дни упадет. Если сервер с БД магазина не справится с наплывом покупателей — они, раздражённые подвисаниями или сбоями, наверняка перейдут к конкурентам. В то же время постоянно держать ресурсы про запас невыгодно — после праздников посетителей будет очень мало.
Управляемые БД решают эту проблему с помощью масштабирования, изменяя количество ресурсов с учётом реальной или ожидаемой нагрузки.
БД можно масштабировать двумя способами: вертикально и горизонтально.
При вертикальном масштабировании меняется количество ядер и объём жёсткого диска на хосте, где работает база данных. Например, вы можете поменять тип хоста с s2.medium (8 ядер и 32 ГБ оперативной памяти) на s2.x4large (40 ядер и 160 ГБ оперативной памяти) и увеличить объём диска на 50 ГБ. Такая операция занимает всего несколько минут. У этого способа, однако, есть предел: очередное увеличение станет или слишком дорогим, или технически невозможным.
При горизонтальном масштабировании меняется число хостов, между которыми распределена нагрузка на БД. Такой способ поддерживают не все СУБД, но если эта возможность есть, то с технической точки зрения уже неважно, сколько хостов вы добавляете.
Производительность и объём диска одного хоста бывают небольшими, но каждый хост обрабатывает лишь часть общей нагрузки и хранит часть общих данных. Такая система может быть эффективнее, чем один большой и мощный сервер.
Вернёмся к примеру с интернет-магазином. Используя управляемую БД, при резком наплыве посетителей вы добавите ресурсы — и система продолжит работать стабильно и быстро. А когда пиковые нагрузки пройдут, вы так же легко остановите или удалите ненужные хосты и уменьшите расходы.
Доступность и отказоустойчивость
Компания, владеющая интернет-магазином, заинтересована в том, чтобы сайт работал постоянно, даже если оборудование выходит из строя. Технически говоря, компанию интересуют высокая доступность и отказоустойчивость.
Доступность — способность работать беспрерывно — обычно измеряется в процентах и вычисляется как часть времени, когда система полноценно функционировала. Например, доступность 99,99% означает, что БД может не отвечать на запросы лишь 0,01% времени (за 30-дневный месяц это 4 минуты 19 секунд).
Сервисы управляемых БД помогут вам без особых усилий улучшить доступность приложения или сайта. Когда вы разворачиваете БД на одном из хостов кластера, сервис автоматически её реплицирует: создает копии (реплики) основной базы (мастера) на остальных хостах и синхронизирует данные между мастером и репликами.
Если хост выйдет из строя, система продолжит работать, и данные не потеряются. Вы заметите сбой только на мастере: до его восстановления БД будет обрабатывать лишь запросы на чтение.
Более того, вам не придется останавливать кластер, чтобы обновить операционную систему или СУБД. Сервис выполнит все обновления незаметно для вас, поочерёдно отключая хосты и распределяя нагрузку между ними.
Кластер можно настроить так, что любой хост автоматически станет мастером, если с первоначальным мастером возникнут проблемы. Такая группа хостов называется кластером высокой доступности (high-availability cluster).
Но что делать, если даже короткий сбой в работе приложения может привести к очень серьёзным последствиям (авариям, крупному финансовому ущербу)? В этом случае нужна 100%-я доступность, иными словами — отказоустойчивость.
Отказоустойчивая система продолжает работать, даже если любой компонент выходит из строя. Это достигается за счёт большой избыточности ресурсов. Чтобы сделать кластер отказоустойчивым, мы можем увеличить в нём число хостов и разнести их по разным зонам доступности. Это поможет избежать отказов на уровне дата-центров.
Какую систему строить: высокодоступную или отказоустойчивую? Это зависит от ваших приоритетов. В первом случае мы допускаем, что система сломается и недолгое время будет недоступна. Во втором — делаем всё возможное, чтобы система работала, даже если какой-то из её элементов выйдет из строя. В том числе добавляем избыточные ресурсы и тратим на них немалые деньги (каждый дополнительный хост стоит столько же, сколько хост-мастер). Решайте сами, за какие проценты доступности и сколько вы готовы платить.
Task:
В каком окружении рекомендуется развернуть БД, если вы начинаете разрабатывать приложение?
Decision:
-STABLE
+PRESTABLE
-PRODUCTION
Task:
Реляционные базы данных
Decision:
По оценкам аналитической компании Domo, в 2020 году в интернете каждый день генерировалось около 2,5 эксабайта (миллиарда гигабайт) данных. Но пока эти данные не собраны и не структурированы, они для нас бесполезны. БД позволяет сохранять данные, извлекать из них те, которые нужны, и передавать пользователю.
Поскольку и данные, и задачи пользователей различны, разработано довольно много систем управления базами данных (примерно 850). На этом и следующем уроках мы коротко расскажем, какие СУБД доступны в Yandex Cloud.
Обычно БД разделяют на реляционные и нереляционные. На этом уроке обсудим реляционные.
Реляционная БД — это набор таблиц и связей между ними. Если мы разрабатываем мессенджер, то нам понадобится хранить в БД сведения обо всех пользователях, чатах и сообщениях. Данные нужно чётко структурировать и свести в связанные между собой таблицы. В одной таблице будут данные о пользователях (ID, ник, ссылка на аватарку, номер телефона и город проживания), в другой — о сообщениях (номер, текст, кто, кому и когда написал сообщение, в каком чате, прочитано ли оно). Наша БД будет выглядеть так:
Для работы с реляционными БД используется SQL (Structured Query Language — структурированный язык запросов).
В сервисах Yandex Cloud представлены популярные реляционные СУБД MySQL и PostgreSQL. Эти СУБД организуют данные одинаково, но у каждой есть сильные и слабые стороны, которые могут проявляться в прикладных задачах.
MySQL
MySQL — самая популярная в мире СУБД с открытым исходным кодом.
MySQL проста. Ее синтаксис SQL не полностью соответствует последним версиям стандарта и соглашений этого языка, поскольку они усложняют написание запросов. Такой подход позволяет ускорить разработку, но ограничивает возможности тонко настраивать БД и оптимизировать запросы.
Эта СУБД входит в классический набор серверного программного обеспечения LAMP (Linux, Apache, MySQL, PHP). У LAMP есть вариации, в которых одни компоненты заменяются другими. Например, в LEMP вместо Apache используется веб-сервер NGINX.
MySQL популярна среди стартап-команд и PHP-разработчиков, широко используется в проектах разработки программного обеспечения с открытым исходным кодом.
PostgreSQL
PostgreSQL — еще одна СУБД с открытым исходным кодом, вторая по популярности после MySQL.
В отличие от MySQL, простота — точно не девиз PostgreSQL. Синтаксис её языка запросов наиболее полно соответствует последним стандартам SQL и где-то даже обгоняет их. От других СУБД PostgreSQL отличает множество настроек, продвинутая система репликации и поддержка большого числа типов данных.
Еще одно отличие от MySQL — механизм курсоров. Если MySQL отдаёт сразу все запрошенные данные, то PostgreSQL сохраняет ответы на запросы в памяти, а пользователь получает указатель (курсор) для перемещения по данным.
Всё это становится преимуществом при решении сложных задач, когда приходится оптимизировать запросы или тонко настраивать БД. Вместе с тем разнообразные настройки и сложный синтаксис языка запросов требуют более глубоких знаний и опыта.
PostgreSQL используют компании, которые работают над проектами со сложными операциями над множеством данных, избегают vendor lock-in и могут держать в штате специалиста по БД.
Реляционные БД не всегда являются лучшим инструментом, потому что:
    нестабильно работают при смешанной нагрузке, когда операции чтения и записи соотносятся примерно 1 : 1;
    плохо масштабируются;
    данные не всегда укладываются в жёсткую структуру таблицы (пример: медицинские карты пациентов поликлиники).
Из-за этого для решения многих задач используют нереляционные БД. О них мы расскажем на следующем уроке.
Task:
Нереляционные базы данных
Decision:
На этом уроке вы узнаете, что такое нереляционные БД и какие из них доступны на платформе Yandex Cloud.
Нереляционные (NoSQL) БД обеспечивают высокую доступность, быстро работают и хорошо масштабируются. Однако зачастую это достигается за счет ограничения транзакционных возможностей, которыми славятся реляционные базы.
В Yandex Cloud входят несколько популярных управляемых нереляционных СУБД.
MongoDB
MongoDB — это масштабируемая документоориентированная СУБД с открытым исходным кодом, самая популярная NoSQL БД в мире.
Данные в MongoDB не организуются в таблицы, как в реляционных БД, а хранятся в документах, которые группируются в коллекции. Это позволяет работать с данными с разными наборами полей: такие данные плохо укладываются в жёсткую структуру таблицы.
MongoDB позволяет размещать части БД на разных хостах (это называется шардированием), так что её легко масштабировать.
СУБД хранит данные в документах схожего с JSON формата, поэтому хорошо сочетается с JavaScript-фреймворками и широко применяется при разработке JavaScript-совместимых серверной и клиентской частей приложений. MongoDB входит в стек технологий MEAN (MongoDB, Express.js, Angular.js, Node.js).
MongoDB удобно использовать, например, для хранения каталога товаров или в системах управления контентом.
Redis
Redis (Remote Dictionary Server) — хранилище данных типа key-value (ключ-значение) с открытым исходным кодом.
В key-value хранилище хранятся пары ключей и значений. Ключ в паре — это идентификатор, а значение — любые данные: число, строка или сложный объект.
В Redis вместо SQL в запросах используются скрипты на языке Lua. В отличие от других подобных хранилищ, Redis поддерживает больше типов данных (строки, списки, хеш-таблицы, упорядоченные и неупорядоченные множества, битовые массивы и др.) и операций с ними.
Redis хранит данные в оперативной памяти и благодаря этому обеспечивает очень высокую производительность — миллионы операций в секунду. Это же порождает два принципиальных ограничения. Первое: объём хранимых данных не может превышать размера оперативной памяти. Второе: при сбое сервера всё, что ещё не сохранилось на диск, безвозвратно пропадёт. Так что не стоит использовать Redis в качестве основного хранилища данных.
Эти особенности и определяют задачи, подходящие для Redis: хранение данных сессий в веб-приложениях, кеширование, генерирование таблиц результатов в многопользовательских играх.
ClickHouse
ClickHouse — это столбцовая СУБД для онлайн-обработки аналитических запросов.
Данные в ClickHouse организованы в таблицах и жестко структурированы. Но в отличие от реляционных БД, которые хранят на диске рядом друг с другом значения всех свойств одного объекта (строку таблицы), в столбцовых СУБД рядом хранятся значения одного свойства всех объектов (столбец).
Благодаря такому способу хранения можно очень быстро выполнять запросы, в которых исследуются не все свойства объекта, а только некоторые из них. При поиске в строковых БД требуется просканировать всю таблицу, а строки читаются с диска или из памяти целиком, даже если из них нужно взять лишь одну ячейку. В столбцовых БД поиск идет по столбцам, извлекаются только нужные значения.
Выигрыш в скорости особенно заметен на больших объемах данных — СУБД ClickHouse способна читать сотни миллионов записей в секунду. БД горизонтально масштабируется, что позволяет работать с базой размером в несколько петабайт. Язык запросов в ней близок к стандартному SQL.
Ещё одна особенность ClickHouse: эта СУБД не поддерживает транзакции и точечное изменение или удаление записей (данные в базе изменяются и удаляются большими порциями).
Elasticsearch
Elasticsearch — это нереляционное хранилище данных с широким набором функций полнотекстового поиска.
Данные в Elasticsearch хранятся в виде документов в формате JSON. Они автоматически индексируются, размер индекса составляет от 20 до 30% размера БД. Благодаря этому достигается скорость чтения в сотни миллионов записей в секунду. Для взаимодействия с БД используются JSON-запросы в RESTful API, а в качестве языка запросов — Querydsl.
Elasticsearch хорошо масштабируется, что позволяет работать с большими объёмами данных. При этом она не обеспечивает транзакционную целостность, а запись данных в БД и их удаление не отличаются высокой производительностью.
Основное назначение Elasticsearch — полнотекстовый поиск в большом объёме документов. Эта СУБД широко применяется для аналитической обработки больших массивов данных, анализа логов и журналов работы приложений, функций автозавершения и умного ввода текста.
Yandex Database
Yandex Database (YDB) — это разработанная Яндексом распределённая СУБД. Она относится к типу NewSQL: основана на реляционной модели, но обеспечивает отказоустойчивость, доступность и масштабируемость на уровне NoSQL-решений.
В YDB данные организуются в таблицы, обеспечивается их транзакционное изменение и строгая консистентность, запросы пишутся на диалекте SQL.
При этом YDB изначально разрабатывалась для так называемых OLTP-сценариев (On-Line Transaction Processing). Это задачи, где СУБД должна поддерживать транзакции и консистентность данных, как это делают классические реляционные СУБД, но при этом надёжно и быстро работать с объёмными БД (т. е. иметь возможность масштабироваться) и с большим числом одновременных запросов на чтение и на запись.
YDB может использоваться в двух режимах: режиме бессерверных вычислений и режиме с выделенными виртуальными машинами. Первый режим подходит для небольших систем с незначительной нагрузкой, когда держать виртуальные машины с БД невыгодно с финансовой точки зрения.
Apache Kafka
Apache Kafka — разработка компании LinkedIn, сочетание распределённой БД и очереди сообщений. Формально Apache Kafka определяется как распределённая стриминговая платформа, т. е. система, управляющая потоками данных между сервисами.
Дело в том, что серверные приложения часто состоят из множества сервисов. Одни из них генерируют, а другие получают данные. Важно, чтобы даже сотни тысяч сообщений в секунду передавались надёжно.
Для таких задач и используются очереди сообщений. Одни сервисы генерируют сообщения и отправляют их Apache Kafka, а другие считывают сообщения тогда, когда удобно, и тем самым снижают нагрузку на систему. До доставки сообщения хранятся в БД, которая размещается на нескольких хостах.
Task:
База данных MongoDB входит в стек технологий:
Decision:
-LAMP
-JSON
+MEAN
Task:
Какую базу данных стоит предпочесть для онлайн-обработки аналитических запросов?
Decision:
ClickHouse
Task:
Выбор базы данных
Универсальной базы данных не существует. Выбор зависит от вашей задачи. Предположите, какую базу данных вы бы выбрали для приложения —  многопользовательской онлайн-стратегии в средневековом сеттинге, где можно развивать замки и захватывать территории противников?  Почему?
Decision:
Вопрос о том, какую БД выбрать для своего приложения, зачастую не имеет единственного правильного ответа. На этот выбор будут влиять и особенности стоящей перед вами задачи, и, в какой-то мере, ваши личные предпочтения.
Для ответа на данный вопрос стоит учитывать несколько факторов:
    возможность легко масштабировать систему;
    структура данных — если вы планируете активно развивать свое приложение, то она вряд ли будет жёсткой;
    необходимость реализации транзакций;
    отсутствие (или очень небольшое число) сложных аналитических запросов;
    работа под смешанной нагрузкой.
Task:
Object Storage и S3-совместимые хранилища
Decision:
Пару лет назад клиника купила охранную систему видеонаблюдения: несколько десятков видеокамер и сервер, где записи хранятся 28 дней. Однажды главный врач решил, что записи нужно хранить дольше: целый год. Реализовывать проект поручили вам — главному айтишнику клиники. Изучив варианты, вы задумались о хранении видеозаписей в облаке и узнали о такой вещи, как объектное хранилище. Давайте вместе разберёмся, что это такое.
Мы используем разные данные: видеозаписи, изображения, архивы, резервные копии для аварийного восстановления, цифровые библиотеки. Многие из них требуются не постоянно, а время от времени — их нужно куда-то сложить, чтобы легко находить и доставать.
Задумываясь о хранении, мы рассчитываем, что данные:
    никуда не пропадут;
    доступны (в идеале — отовсюду, где есть интернет);
    недоступны для тех, с кем мы не собирались делиться данными;
    находятся в системе хранения, которую просто расширить.
Кроме того, мы не хотим потратить много денег на хранение и много сил, чтобы поддерживать систему хранения в рабочем состоянии.
Всем этим требованиям отвечает объектное хранилище (англ. object storage) — облачная система хранения данных произвольного формата. В хранилище записываются объекты — по сути, привычные нам файлы. У каждого объекта есть уникальный идентификатор (аналог имени файла) и метаданные (дополнительные сведения). Идентификатор помогает различать объекты, а в метаданных хранятся, например, дата создания и MD5-хеш.
Идентификатор объекта — это строка. Ее часто записывают как путь в файловой системе, используя символ / как разделитель. Например, archive/2020/ivanov/x-ray.jpg. Благодаря этому файловые браузеры при работе с объектами имитируют привычное нам дерево папок. В консоли управления Yandex Cloud тоже есть такая возможность — части пути называются папками. Но важно понимать: на самом деле иерархии в объектном хранилище нет, оно плоское. За счет плоскости достигается большая скорость поиска по сравнению с файловой системой.
Внутри хранилища объекты группируются в бакеты. Это позволяет разделять данные разных проектов или пользователей. У каждого бакета в Yandex Cloud уникальное имя. Переименовать бакет нельзя, поэтому выбирайте имя для бакета с умом.
С практической точки зрения объектные хранилища имеют некоторые важные особенности:
    они рассчитаны, прежде всего, на взаимодействие с приложениями через HTTP API, а не напрямую с пользователем;
    объекты в них не редактируются. Чтобы изменить объект, придётся загрузить в хранилище новую версию. Это неудобно, если объекты большие или часто изменяются.
Класс хранилища
Размер оплаты за использование объектного хранилища зависит от количества данных, операций с ними и исходящего трафика. Первый гигабайт и первые десятки тысяч операций бесплатные.
Выбирайте класс хранилища — стандартное или холодное — в зависимости от количества данных и запросов к ним:
Сценарий работы с данными   Рекомендуемый класс хранилища   Тариф на хранение данных    Тариф на обращение к данным
Много данных, редкие запросы (например, резервные копии)    Холодное    Низкий  Высокий
Мало данных, частые запросы (например, аватарки пользователей приложения)   Стандартное Высокий Низкий
Класс хранилища выбирается при загрузке объекта в бакет. Если этого не сделать, то объект сохранится в хранилище того класса, который выбран для всего бакета.
Класс хранилища для бакета можно изменить в консоли управления. Если объект загружен, изменить его класс хранилища нельзя: можно либо загрузить объект заново, либо задать правило, управляющее жизненным циклом объекта. Настраивать жизненный цикл вы научитесь на практическом занятии.
S3-совместимость
Сервис Yandex Object Storage — S3-совместимое хранилище. Его API совместим с Simple Storage Service API (S3 API), который был создан компанией Amazon в 2006 году и де-факто стал стандартом облачного хранения данных. S3-совместимость позволяет использовать в Yandex Cloud популярные инструменты для работы с S3-протоколом: консольные клиенты S3cmd и AWS CLI, файловые браузеры Cyberduck и WinSCP, библиотеки (SDK) для Python и Java.
Возможности и преимущества объектного хранилища
Объектное хранилище — общепринятый способ хранения неструктурированных данных в облаке. Преимущества объектного хранилища:
    масштабируемость: размер системы архитектурно не ограничен, она может содержать любой объём данных;
    более высокая производительность при управлении большими объёмами данных по сравнению с иерархическими файловыми системами;
    отказоустойчивость: копии данных хранятся в нескольких географически распределённых дата-центрах;
    метаданные, позволяющие искать, сортировать и анализировать неструктурированные данные;
    невысокая стоимость хранения данных.
Используйте сервис Yandex Object Storage, чтобы разместить файлы любого формата для своего проекта (приложения или сайта) в закрытом или публичном доступе, хранить объёмные архивы (до 5 терабайт на файл), контролировать доступ к данным и организовать совместную работу с ними.
Task:
Вы используете Yandex Object Storage для хранения архива аудиозаписей. Какой класс хранилища стоит выбрать?
Decision:
+Холодное
-Стандартное
Task:
Имеет ли смысл положить в объектное хранилище файл с очень большой базой данных SQLite, если к ней идет много запросов на чтение и лишь немного на запись?
Decision:
-Да, если это решение дешевле сервиса управляемой базы данных
-Да, но только в стандартном хранилище
+Нет
Task:
Метаданные
Decision:
На прошлом уроке вы узнали, что у объекта в хранилище есть метаданные — сведения о его свойствах. Метаданные представляются в виде пары «имя: значение». Например, так записывается дата и время загрузки объекта в хранилище:
Date: Fri, 5 Mar 2021 14:00:00 GMT 
Метаданные бывают пользовательскими (их имена и значения вы добавляете и меняете сами) и системными (их устанавливает сервис). Например, дата создания или последнего изменения объекта, его размер, MD5-хеш — это системные метаданные. Их полный список вы найдете в документации.
Предположим, клиника хранит в облаке рентгеновские снимки пациентов. К каждому снимку (объекту) вы добавляете метаданные: Ф. И. О. пациента, номер медицинской карты, дату обследования, код диагноза, модель рентгенаппарата, Ф. И. О. проводивших обследование специалистов.
Задав много метаданных, вы сможете легко находить, сортировать и анализировать объекты. Например, врачи быстро узнают, какие пациенты получили избыточное облучение, если выяснится, что один из рентген-аппаратов барахлит, или проследят динамику заболеваемости воспалением легких у пожилых женщин с разбивкой по месяцам.
Пользовательские метаданные передаются в облачное хранилище при загрузке объекта в HTTP-заголовках запроса. Имена заголовков с пользовательскими метаданными при этом должны начинаться с префикса x-amz-meta-:
PUT /images/image0349.dat HTTP/1.1
Host: hospital.storage.yandexcloud.net
Content-Length: 1403256
Date: Sat, 20 Mar 2021 14:15:02 GMT
Authorization: *...*
x-amz-meta-Patient-Surname: Petrov
x-amz-meta-Patient-ID: 4536
*...* 
При запросе объекта сервис возвращает метаданные в виде таких же HTTP-заголовков с префиксом x-amz-meta-:
HTTP/1.1 200 OK
Server: nginx
Date: Tue, 23 Mar 2021 10:12:15 GMT
Content-Type: image/dat
Content-Length: 1403256
Connection: keep-alive
Keep-Alive: timeout=60
Accept-Ranges: bytes
Last-Modified: Sat, 20 Mar 2021 14:15:02 GMT
x-amz-meta-patient-surname: Petrov
x-amz-meta-patient-id: 4536
... 
Важно! До загрузки вы можете указывать имена метаданных объекта и прописными, и строчными буквами (хоть ABC, хоть abc). Однако после загрузки в хранилище они станут строчными (только abc). Таким образом, хранилище посчитает метаданные с именами Patient-ID и patient-id одинаковыми.
Пользовательские метаданные загруженных объектов тоже можно добавлять или изменять. Вы научитесь делать это на ближайшем практическом уроке.
Task:
Метаданные объекта в объектном хранилище нужны, чтобы:
Decision:
-фиксировать дату и время, когда был отправлен запрос на загрузку объекта в хранилище
+описывать свойства объекта
-корректно писать заголовки HTTP-запросов на загрузку и извлечение объекта
Task:
Управление доступом
Decision:
Чтобы система хранения рентгеновских снимков в облаке работала надёжно и с врачебной тайной не возникало проблем, необходимо ограничить доступ к снимкам. Уровень доступа у всех должен быть разным, чтобы врачи могли просматривать снимки, рентгенологи — и просматривать, и загружать, а сисадмин — настраивать объектное хранилище.
В объектном хранилище Yandex Cloud для контроля доступа используются три независимых механизма:
    сервис управления доступом (IAM, Identity and Access Management) работает на уровне всего облака;
    список управления доступом (ACL, Access Control List) работает на уровне бакетов и объектов;
    политика доступа (Bucket Policy) задаёт условия доступа к бакетам и объектам.
IAM
С основами работы IAM вы познакомились на предыдущем курсе. IAM проверяет все операции в Yandex Cloud и управляет доступом на основе ролей — набора разрешений, которые описывают допустимые операции. Если разрешения нет, сервис сообщит об ошибке.
Помните, что роли действуют и на вложенные ресурсы. Если вы дадите системной группе allUsers (все пользователи интернета) или allAuthenticatedUsers (все пользователи, прошедшие аутентификацию в Yandex Cloud) права доступа к облаку или каталогу с объектным хранилищем — пользователи получат доступ ко всем бакетам в нем.
На схеме ниже показано, какие роли дают разрешение на действия в объектном хранилище и как роли наследуют разрешения друг друга. Список разрешений, входящих в каждую роль, смотрите в документации.
Чтобы настроить права доступа к рентгеновским снимкам, врачам нужно выдать роль storage.viewer, рентгенологам — storage.uploader, а сотруднику, отвечающему за систему хранения, — storage.admin или storage.configurer в зависимости от ситуации.
ACL
ACL — это тоже список разрешений на действия. Но в отличие от IAM, он распространяется не на всё облако, а только на объектное хранилище. С помощью ACL можно разрешить:
    операции чтения для объектов и бакетов (READ);
    операции записи, перезаписи и удаления объектов (WRITE);
    полный доступ к объектам и бакетам (FULL_CONTROL).
Разрешения в ACL соответствуют ролям пользователей в сервисе IAM. Например, разрешение FULL_CONTROL соответствует роли admin. ACL используется в объектном хранилище для S3-совместимости, а также чтобы быстро предоставлять доступ к объектам или бакетам.
С помощью ACL можно выдать разрешения пользователям Yandex Cloud, сервисному аккаунту и системным группам allUsers и allAuthenticatedUsers. Чтобы выдать разрешение, надо знать идентификатор его получателя.
Подробности об ACL вы найдете в документации.
Политика доступа
С помощью политики доступа настраиваются дополнительные условия действий с бакетами и объектами. Вы можете, например, запретить доступ к бакету из конкретного диапазона IP-адресов.
Элементы политики доступа:
    ресурс — бакет, префикс (условный «путь» в идентификаторе объекта) или объект;
    действие — запрошенная операция над ресурсом;
    результат — запрет (Deny) или разрешение (Allow) действия;
    пользователь — тот, кто запрашивал выполнение операции;
    условие — случаи, когда политика действует;
    принцип выбора — включить или исключить пользователей.
Если для обеспечения безопасности и конфиденциальности данных вы разрешаете пользователям просматривать снимки только по защищённому соединению, то политика доступа будет выглядеть так:
{
  "Id": "epd4limdp3dgec7enpq5"
  "Version": "2021-01-15",
  "Statement": [
    {
      "Sid": "f1qqoehl1q53l06kqurs",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::hospital/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "true"
        }
      }
    }
  ]
} 
Подробности о том, как настраивать политики доступа, вы найдете в документации.
Как применяются механизмы контроля доступа
Разрешение на доступ к объекту проверяется на трех уровнях: в сервисе IAM, в политике доступа и в списке разрешений ACL. Система контроля доступа построена по принципу «запрещено всё, что не разрешено»: доступ даётся только тогда, когда есть хотя бы одно разрешающее правило и нет ни одного запрещающего.
Механизмы контроля доступа применяются в таком порядке:
Например, если запрос не прошел проверку IAM (нужная роль не назначена и разрешения на доступ нет), но доступ к объекту открыт с помощью ACL, то запрос будет выполнен, поскольку есть одно разрешение на доступ и нет явных запретов.
Какой механизм контроля доступа выбрать
Это зависит от ситуации.
    Проверка через IAM выполняется всегда. Назначьте пользователям вашего хранилища соответствующие роли. Для несложных сценариев (когда вы хотите знать, что может делать в хранилище конкретный пользователь) этого зачастую достаточно.
    Политика доступа управляет доступом по дополнительным критериям. Выбирайте её, если вам важно, при каких условиях разрешаются действия с бакетом или объектами.
    ACL позволяет дать доступ к объекту. Это удобно, когда нет смысла продумывать целую систему контроля доступа или тратить время на прописывание политик. Но если объектов много — лучше выбирать IAM или политику доступа.
Подписанные ссылки
Ещё одна возможность открыть доступ к объектному хранилищу для скачивания или загрузки объекта — механизм подписанных ссылок (pre-signed URL). С его помощью предоставляется временный (от нескольких секунд до семи дней) доступ к объекту или бакету.
Ссылка содержит:
    подпись, вычисленную на основе ключа доступа к каталогу и секретного ключа;
    описание действия, которое пользователь может выполнить в хранилище.
С помощью подписанных ссылок приложение может генерировать ссылки на объекты в хранилище (например, для пользователей, которые оплатили скачиваемый контент). Подробный алгоритм, как составить подписанные URL, вы найдете в документации.
Task:
Пользователи с какими ролями могут перезаписывать объекты в бакете?
Decision:
+storage.uploader
-storage.viewer
-storage.configurer
+storage.editor
Task:
Рентгеновские снимки вашей клиники хранятся в объектном хранилище в облаке. Подумайте, какому сотруднику следует дать роль storage.admin, а какому — storage.configurer.
Decision:
Чтобы это определить, посмотрите, какие действия доступны для этих ролей, в документации. Обратите внимание, что роль storage.configurer не даёт доступа к данным в бакете, что важно с точки зрения сохранения медицинской тайны. То есть роль storage.admin лучше давать врачу, отвечающему за систему хранения, а роль storage.configurer — например, системному администратору клиники.
Task:
Будет ли разрешён доступ к объекту, если запрос проходит проверку IAM, подпадает под правило Deny политики доступа и разрешается ACL?
Decision:
-да
+нет
Task:
Практическая работа. Создание бакетов и загрузка объектов
Decision:
Потренируемся работать с объектным хранилищем на практике. Представьте, что вы создаёте облачную систему хранения рентгеновских снимков для крупной клиники.
Рентгеновские снимки — это неструктурированные данные, которые нельзя изменять, нужно надежно хранить и легко находить. Загруженные файлы будут скачивать нечасто. Также важно предоставлять доступ к файлам другим клиникам (это пригодится, если пациента переводят или врачу надо посоветоваться с коллегами). Объектное хранилище — подходящее решение задачи.
Выберите на стартовой странице консоли управления сервис Object Storage.
Давайте создадим бакет для рентгеновских снимков.
Нажмите кнопку Создать бакет. Откроется окно с основными параметрами: 
Имя. Придумайте его с учетом правил. Обратите внимание, что дать бакету имя hospital не получится. Имена бакетов во всем Yandex Object Storage уникальны — назвать два бакета одинаково нельзя даже в разных облаках. Помните об этом, если будете создавать бакеты автоматически.
Макс. размер. У вас есть два варианта:
    Выбрать опцию Без ограничения. Размер бакета будет увеличиваться, сколько бы объектов в него ни помещали.
    Указать максимальный размер. Это убережёт вас от финансовых потерь, если что-то пойдёт не так и в бакет загрузится слишком много объектов.
Другие опции. Далее для всех типов операций оставьте ограниченный доступ (публичный позволяет выполнять операции всем пользователям интернета), выберите стандартный класс хранилища и нажмите кнопку Создать бакет.
На странице объектного хранилища появился пустой бакет. Мы приготовили два рентгеновских снимка: image01.dat и image02.dat. Файлы можно загрузить в бакет с помощью:
    консоли управления;
    приложений;
    S3-совместимого HTTP API;
    HTML-форм на сайте.
Разберём два способа: ручную загрузку через консоль управления и автоматическую с помощью утилиты S3cmd.
Для загрузки файла через консоль управления выберите созданный бакет и в открывшемся окне нажмите кнопку Загрузить объекты.
Выберем файл image01.dat. В появившейся форме нажмите кнопку Загрузить — и вы увидите, что файл оказался в хранилище.
Загрузите второй файл с помощью утилиты S3cmd — консольного клиента для Linux и MacOS, предназначенного для работы с S3-совместимым HTTP API. Для работы в Windows используйте один из вариантов:
    установите другой консольный клиент для объектных хранилищ, например AWS CLI;
    установите подсистему Linux на Windows с помощью утилиты WSL (Windows Subsystem for Linux) и работайте с S3cmd в ней;
    создайте в облаке виртуальную машину с Ubuntu и работайте с S3cmd в ней. Для загрузки файла-примера в виртуальную машину воспользуйтесь командой:
$ wget "https://disk.yandex.ru/i/2UlugGkurhcxWw" -O image02.dat 
Установите S3cmd (в Ubuntu, например, это делается с помощью команды sudo apt-get install s3cmd). Теперь настройте S3cmd для работы с Yandex Object Storage:
$ s3cmd --configure
Инструкции о настройке клиента вы найдете в документации.
После ввода параметров утилита попытается установить соединение с объектным хранилищем и в случае успеха покажет такое сообщение: Success. Your access key and secret key worked fine :-)
Загрузим в бакет второй файл (image02.dat) и затем получим список хранящихся в бакете объектов:
s3cmd put <путь к второму файлу>/image02.dat s3://<имя бакета>
s3cmd ls s3://<имя бакета>
Вернемся в консоль управления.
Мы видим, что класс хранилища у обоих объектов — стандартное. Напомним: стандартное хранилище подходит для данных, к которым обращаются часто, а тариф за размещение данных в нем примерно в два раза выше, чем в холодном хранилище.
Спустя несколько недель после того, как рентгеновский снимок сделан, к нему будут редко обращаться (если вообще будут), потому что пациент, скорее всего, выздоровеет.
Чтобы оптимизировать затраты на хранение данных, настроим жизненный цикл объектов в бакете. Создадим правило, согласно которому через 30 дней после загрузки объектов в бакет класс их хранилища будет автоматически меняться со стандартного на холодное.
Перейдите на вкладку Жизненный цикл и нажмите кнопку Настроить. Задайте произвольное описание. В поле Префикс укажите Все объекты. Выберите тип операции Transition. В качестве Условия срабатывания правила задайте Точную дату или Количество дней. В первом случае правило сработает в 00:00 установленной даты. Во втором — через указанное количество дней после загрузки объекта в бакет.
Если понадобится настроить автоматическое удаление объектов, выберите тип операции Expiration. Нажмите кнопку Сохранить.
Представим теперь, что объекты в хранилище — это оцифрованные рентгеновские снимки пациента Петрова. Первый из них (image01.dat) сделали несколько месяцев назад в ходе профосмотра, а второй (image02.dat) — вчера, после того как Петров обратился к врачу с жалобой на недомогание. В обоих случаях на снимках не увидели патологий.
Опишите с помощью пользовательских метаданных эти снимки, и позже вы быстро найдете их среди множества объектов в бакете.
С помощью утилиты S3cmd задайте для загруженных объектов метаданные с фамилией пациента (x-amz-meta-patient:petrov) и с результатами обследования (x-amz-meta-status:ok):
 s3cmd modify --add-header=x-amz-meta-patient:petrov --add-header=x-amz-meta-status:ok s3://hospital/image01.dat s3://hospital/image02.dat
Выведите на экран информацию об этих объектах, чтобы проверить, что получилось:
 s3cmd info s3://hospital/image01.dat s3://hospital/image02.dat
В результате вы должны увидеть информацию об объектах в бакете.
s3://hospital/image01.dat (object):
   File size: 33
   Last mod:  Thu, 04 Mar 2021 22:05:31 GMT
   MIME type: application/x-www-form-urlencoded
   Storage:   STANDARD
   MD5 sum:   6f6d5a1cb79839e523582ed8810a42fd
   SSE:       none
   Policy:    none
   CORS:      none
   x-amz-meta-patient: petrov
   x-amz-meta-status: ok
s3://hospital/image02.dat (object):
   File size: 16
   Last mod:  Thu, 04 Mar 2021 23:11:27 GMT
   MIME type: text/plain
   Storage:   STANDARD
   MD5 sum:   0366a1d19e584ce79d5c05ddedc69310
   SSE:       none
   Policy:    none
   CORS:      none
   x-amz-meta-patient: petrov
   x-amz-meta-status: ok 
Предположим, Петров чувствует себя хуже. Судя по анализам, он действительно болен. Лечащий врач решает проконсультироваться с более опытной коллегой Ивановой из профильной клиники. Объекты в бакете недоступны для внешних пользователей, поскольку при его создании мы ограничили доступ. Чтобы Иванова увидела рентгеновский снимок Петрова, отправим ей временную ссылку на объект image02.dat.
Для этого в консоли управления кликните на объект и в открывшемся окне информации об объекте нажмите кнопку Получить ссылку. Укажите время жизни ссылки в часах или днях.
Можно поделиться ссылкой или использовать ее в любом сервисе для доступа к файлу.
После консультации Иванова поставила Петрову правильный диагноз: вирусная пневмония (код J12 по Международной классификации болезней). Вам осталось исправить метаданные объекта image02.dat. Замените значение метаданных с результатами обследования с ok на J12 самостоятельно.
Decision:
$ sudo apt-get install s3cmd
$ s3cmd --configure
Decision:
%(bucket)s.storage.yandexcloud.net
Идентификатор ключа:
YCAJEuBFLYg8HHAHiZTgHJJv5
Ваш секретный ключ:
YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
Decision:
$ s3cmd --configure
Access Key: YCAJEuBFLYg8HHAHiZTgHJJv5
Secret Key: YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
Default Region [US]: ru-central1
S3 Endpoint [s3.amazonaws.com]: storage.yandexcloud.net
DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.storage.yandexcloud.net
Encryption password: 1992
Path to GPG program [/usr/bin/gpg]:
Use HTTPS protocol [Yes]:
HTTP Proxy server name:
New settings:
  Access Key: YCAJEuBFLYg8HHAHiZTgHJJv5
  Secret Key: YCPXwjbD3FPE2rzKydMVU8_vFVIDzp7mRntqSaac
  Default Region: ru-central1
  S3 Endpoint: storage.yandexcloud.net
  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.storage.yandexcloud.net
  Encryption password: 1992
  Path to GPG program: /usr/bin/gpg
  Use HTTPS protocol: True
  HTTP Proxy server name:
  HTTP Proxy server port: 0
Test access with supplied credentials? [Y/n] Y
Please wait, attempting to list all buckets...
Success. Your access key and secret key worked fine :-)
Now verifying that encryption works...
Success. Encryption and decryption worked fine :-)
Save settings? [y/N] Y
$ s3cmd put /home/administrator/image02.dat s3://bucket-testt
$ s3cmd ls s3://bucket-testt
$ s3cmd modify --add-header=x-amz-meta-patient:petrov --add-header=x-amz-meta-status:ok s3://hospital/image01.dat s3://hospital/image02.dat
$ s3cmd info s3://hospital/image01.dat s3://hospital/image02.dat
Task:
Резервное копирование
Decision:
При загрузке в объектное хранилище файлы копируются сразу в несколько географически распределенных дата-центров, что обеспечивает их надёжное хранение.
Тем не менее иногда быстрый доступ к данным или сами данные настолько важны, что стоит предусмотреть любые риски (например долгие перебои с доступом к интернету). В этом случае пригодится резервная копия файлов из объектного хранилища. Её можно хранить на своем сервере или в другом облаке.
В Yandex Object Storage нет собственных инструментов для резервного копирования, но есть поддерживаемые. Скачивайте объекты из хранилища самостоятельно или автоматизируйте этот процесс средствами операционной системы (планировщик заданий Windows, утилита cron в Linux).
Консольные клиенты
Команда sync консольных клиентов S3cmd и AWS CLI синхронизирует содержимое бакета с папкой на компьютере или двух бакетов между собой. В указанное место копируются файлы, которых там нет, и файлы, которые изменились после прошлой синхронизации. Для поиска изменившихся файлов по умолчанию используется подсчет хеш-суммы и оценка размера. Вы можете задать и свои правила синхронизации. Подробности в документации S3cmd.
Для S3cmd команда синхронизации в общем случае выглядит так:
s3cmd sync s3://<имя бакета> <путь к локальной папке на компьютере>/ 
Сделайте с помощью этой команды резервные копии размещённых в объектном хранилище файлов с рентгеновскими снимками из предыдущего урока на своем компьютере.
Файловые браузеры
Если вы не фанат консольных клиентов — используйте для резервного копирования файловые менеджеры с графическим интерфейсом, такие как Cyberduck или WinSCP.
WinSCP — популярный SFTP- и FTP-клиент для Windows  — работает с объектным хранилищем как с иерархической файловой системой. Ключи объектов, загружаемых в хранилище с помощью WinSCP, будут иметь вид пути к файлу (например prefix/subprefix/picture.jpg).
Для подключения к Yandex Object Storage с помощью WinSCP создайте соединение со следующими параметрами:
    протокол передачи: Amazon S3;
    имя хоста: storage.yandexcloud.net;
    идентификатор ключа доступа и секретный ключ доступа: вставьте значения, которые вы получили при генерации статического ключа.
Одна из функций WinSCP облегчает автоматизацию резервного копирования файлов из папок объектного хранилища на компьютер. Выберите в бакете объект для сохранения на компьютере. В открывшемся диалоговом окне вы увидите настройки передачи по умолчанию:
Измените настройки, если нужно, и нажмите кнопку OK. Программа сформирует код скрипта для резервного копирования объектов из хранилища и сохранит его в файл.
Откройте текстовый редактор (например Блокнот) и создайте простой пакетный файл backup.bat со следующим содержимым:
@echo off
winscp.exe /ini=nul /script=<имя файла со скриптом> 
Запускайте файл backup.bat (например при помощи планировщика заданий Windows), чтобы автоматически выполнять резервное копирование с нужной вам периодичностью.
Task:
Версионирование бакетов
Decision:
Мы можем потерять данные из-за ошибки пользователей, действий злоумышленников или сбоя в приложениях. Но некоторые данные критически важны. Их нельзя терять даже из-за форс-мажора. В примере с рентгеновскими снимками хранение всех версий электронных медицинских документов — это требование российского законодательства.
Благодаря версионированию — специальному инструменту хранилища — вы не потеряете объекты даже после удаления или перезаписи.
Версионирование сохраняет историю объекта и позволяет получать доступ к его предыдущим версиям. Версионирование включается на уровне бакета и применяется ко всем объектам внутри него. Взамен вы платите за место, ведь каждая версия объекта — это его полная копия, она занимает столько же места в вашем хранилище.
В новом бакете версионирование по умолчанию приостановлено. Загружаемым объектам присваивается идентификатор версии version_id равный null. В этом случае хранится только последняя версия каждого объекта.
Версионирование для бакета включается через консоль управления или с помощью API.
Когда версионирование включено, загружаемые объекты получают отличное от null значение параметра version_id, который позволяет работать с конкретной версией объекта. При перезаписи версии создается новый объект с тем же ключом (идентификатором объекта в бакете) и случайно сгенерированным значением version_id. Для обращения к предыдущим версиям объекта используются его ключ и version_id.
Отключить версионирование нельзя — можно лишь приостановить. Тогда все старые версии останутся доступными, а при перезаписи будет сохраняться только последняя из новых версий.
При удалении версия помечается delete-маркером и не занимает места в хранилище. Очищайте бакет от ненужных или удалённых версий вручную или настройте жизненный цикл объектов.
Task:
После приостановки версионирования бакета один из загруженных в него объектов перезаписан. Какое значение version_id будет у новой версии объекта?
Decision:
-Такое же, как и у перезаписанной версии, плюс один
+null
-Такое же, как и у перезаписанной версии
Task:
Практическая работа. Хранение статических веб-сайтов в Object Storage
Decision:
Представьте, что вам нужно выбрать оптимальный хостинг для сайта клиники. Главные критерии: отказоустойчивый, недорогой и простой в обслуживании. Один из вариантов решения такой задачи — использовать объектное хранилище. Вы можете, не настраивая никаких серверов, просто загрузить HTML-файлы, скрипты, стили и другие файлы в хранилище. Пользователи будут открывать в браузере ваш сайт, а по сути — скачивать файлы прямо из бакета.
Важно понимать, что этот вариант подойдет только для полностью статических сайтов. Иными словами, сайт должен быть сделан с помощью клиентских технологий (HTML, CSS и JavaScript) и не требовать запуска чего-либо на стороне веб-сервера.
Предположим, что сайт нашей клиники как раз такой — полностью статический. Опубликуем его с помощью объектного хранилища. Прежде всего создадим бакет:
Обратите внимание на несколько особенностей:
    Если вы планируете использовать собственный домен (например www.example.com), то присвойте бакету точно такое же имя.
    Откройте публичный доступ на чтение объектов. Это позволит пользователям интернета скачивать объекты из бакета и просматривать сайт в браузере.
Если публичный доступ открыт, то рядом с именем бакета в списке появится соответствующая надпись
Теперь загрузите в бакет файлы сайта (например, этот и этот) любым удобным способом.
Чтобы настроить хостинг, войдите в бакет в консоли управления. На левой панели выберите вкладку Веб-сайт и на открывшейся странице включите Хостинг.
Укажите файл с главной страницей сайта (как правило, это index.html), а поле со страницей ошибки можно не заполнять.
Сохраните настройки, и сайт станет доступен по адресам:
    http(s)://<имя_бакета>.website.yandexcloud.net
    http(s)://website.yandexcloud.net/<имя_бакета>
По умолчанию сайт будет доступен только по протоколу HTTP. Для поддержки HTTPS нужно загрузить в объектное хранилище собственный сертификат безопасности. Как это сделать, рассказано в документации.
Также о доступе к статическому сайту по протоколу HTTPS вы узнает из курса Безопасность.
Теперь пациенты точно вас найдут!
Если у вас есть собственный домен и вы хотите опубликовать сайт на нём, то настройте CNAME-запись у DNS-провайдера или на своем DNS-сервере. Например, для домена www.example.com CNAME-запись выглядела бы так:
www.example.com CNAME www.example.com.website.yandexcloud.net 
В этом случае можно использовать домены не ниже третьего уровня (то есть использовать домен example.com не получится, только www.example.com). Это связано с особенностями обработки CNAME-записей на DNS-хостингах.
Decision:
$ s3cmd put /home/administrator/index.html s3://www.aibolit38.ru
Conclusion:
http://www.aibolit38.ru.website.yandexcloud.net
Task:
Какой класс хранилища по умолчанию нужно выбрать для бакета, в котором вы размещаете файлы веб-сайта?
Decision:
+Стандартное
-Холодное
-Это неважно
Task:
Практическая работа. Создание кластера базы данных MySQL
Decision:
Object Storage — удобный и полезный инструмент для хранения данных в облаке. Но для решения практических задач важно не просто хранить данные, но и иметь возможность их изменять и выполнять с ними различные операции (сортировать, группировать, делать выборки и так далее). Для этого используются базы данных. В этой и следующих темах вы научитесь работать с несколькими управляемыми БД. И начнем мы с одной из самых популярных — MySQL.
На этом уроке вы создадите и настроите кластер управляемой БД MySQL, подключитесь к нему, перенесёте данные в облако, познакомитесь с возможностями резервного копирования и мониторинга. Эти навыки пригодятся вам и в других сервисах управляемых БД, поскольку принципы работы в них очень похожи.
Предположим, вы решили добавить в разрабатываемый вами мессенджер новую функциональность. Вы написали микросервис, который позволяет оценивать сообщения в групповых чатах и хранит оценки в БД MySQL. Давайте поместим эту БД в Yandex Cloud.
Прежде всего понадобится создать кластер: набор виртуальных машин (ВМ, или хостов), на которых будет развёрнута БД. Это обязательный первый шаг при использовании любого сервиса управляемых БД.
Войдите в консоль управления Yandex Cloud и выберите каталог для кластера. Вверху справа нажмите кнопку Создать ресурс и выберите из выпадающего списка Кластер MySQL.
Откроется страница с основными настройками кластера. Рассмотрим их подробнее.
Базовые параметры
Имя кластера может включать только цифры, прописные и строчные латинские буквы, дефисы.
Поле Описание заполнять необязательно. Оно полезно, если вам нужно создать несколько кластеров для разных целей, чтобы в них было проще ориентироваться.
О том, какое бывает Окружение кластера и чем различаются PRESTABLE и PRODUCTION, мы говорили на одном из предыдущих уроков. Поскольку микросервис только разрабатывается, выберите окружение PRESTABLE.
Версия. В качестве сервера MySQL в Yandex Cloud используется Percona Server версии 5.7 или 8.0. У этих реализаций сервера улучшенная производительность на многоядерных машинах. Если для вас критична стабильность работы микросервиса, выбирайте проверенную временем 5.7. Для нашей задачи подойдёт 8.0: в ней много новых функций, но она ещё не полностью обкатана.
Класс хостов
Следующий шаг — выбор класса хостов, или шаблона ВМ. Хосты кластера будут развёрнуты на базе ВМ Compute Cloud с использованием этого шаблона.
Платформа определяет тип физического процессора (Intel Broadwell или Intel Cascade Lake), а также конфигурации числа ядер виртуального процессора (vCPU) и размера оперативной памяти.
Если тип процессора для вас неважен, выбирайте более современную платформу Intel Cascade Lake. Она предоставляет широкий выбор конфигураций вычислительных ресурсов.
Также на конфигурации влияет тип ВМ, на которой будет развёрнута БД.
Standard — это обычные ВМ с 4 ГБ RAM на ядро vCPU. Это оптимальный баланс между количеством запущенных процессов, быстродействием и потребляемой оперативной памятью.
Memory-optimized — машины с вдвое увеличенным объёмом RAM на каждое ядро. Выбирайте их для высоконагруженных сервисов с повышенными требованиями к кешу.
Burstable — машины, для которых гарантируется использование лишь доли ядра vCPU (5, 20 или 50%) с вероятностью временного повышения вплоть до 100%. Они стоят дешевле и подходят для задач, где не нужен постоянный уровень производительности, т. е. для тестирования или разработки.
Выберем для микросервиса следующий класс хоста: платформа — Intel Cascade Lake; тип — standard; конфигурация вычислительных ресурсов — s2.micro (два ядра vCPU, 8 ГБ RAM).
Хранилище данных
Хранилище БД может быть сетевым или локальным. В первом случае данные находятся на виртуальных дисках в инфраструктуре Yandex Cloud. Локальное хранилище — это диски, которые физически размещаются в серверах хостов БД.
При создании кластера можно выбирать между следующими типами хранилища:
    Стандартное сетевое (network-hdd) — это наиболее экономичный вариант. Выбирайте его, если к скорости записи и чтения нет особых требований.
    Быстрое сетевое (network-ssd) стоит примерно в четыре раза дороже, но при размере хранилища от 100 ГБ работает быстрее стандартного в десять и более раз (чем больше размер, тем заметнее разница в скорости).
    Сетевое на нереплицируемых SSD-дисках (network-ssd-nonreplicated) — использует сетевые SSD-диски с повышенной производительностью, реализованной за счет устранения избыточности. Объём такого хранилища можно увеличивать только с шагом 93 ГБ.
    Быстрое локальное (local-ssd) — самое быстрое и дорогое. Если локальный диск откажет, все сохранённые на нём данные будут потеряны. Чтобы этого избежать, при выборе локального хранилища сервис автоматически создаст отказоустойчивый кластер минимум из трёх хостов.
При создании кластера внимательно выбирайте тип хранилища. Размер хранилища можно будет позже изменить, а тип — нет.
Выберите для кластера стандартное сетевое хранилище network-hdd размером 50 ГБ.
База данных
В этом разделе настроек задаются атрибуты базы: Имя БД, уникальное в рамках кластера, Имя пользователя (владельца БД) и Пароль пользователя.
Сеть
Здесь можно выбрать облачную сеть для кластера и группы безопасности для его сетевого трафика.
Оставьте сеть по умолчанию (default) или выберите сеть, которую создали на предыдущем курсе. Кластер будет доступен для всех ВМ, которые подключены к вашей облачной сети.
Параметры хостов
В этом блоке можно добавить количество хостов, которые будут созданы вместе с кластером, и изменить их параметры. Дополнительные хосты могут понадобиться, например, для репликации БД или снижения нагрузки на хост-мастер.
Для наших целей достаточно кластера из одного хоста. Нажмите значок редактирования параметров хоста и в открывшемся окне выберите опцию Публичный доступ. Это означает, что к хосту можно будет подключиться из интернета, а не только из облачной сети. Остальные параметры оставьте без изменений.
Дополнительные настройки
Здесь можно:
    указать время Начала резервного копирования и Окна обслуживания. Это пригодится, если вы хотите, чтобы резервное копирование и техобслуживание хостов кластера не совпадали с периодами пиковых нагрузок на БД;
    разрешить Доступ из DataLens, если вы планируете анализировать в DataLens данные из базы. Подробнее о DataLens вы узнаете на одном из следующих занятий;
    разрешить Доступ из консоли управления, чтобы выполнять SQL-запросы к БД из консоли управления Yandex Cloud. Отметьте этот пункт: доступ из консоли понадобится нам на следующих практических работах;
    разрешить Доступ из Data Transfer, чтобы разрешить доступ к кластеру из сервиса Yandex Data Transfer в Serverless-режиме;
    разрешить Сбор статистики, чтобы воспользоваться инструментом Диагностика производительности в кластере;
    установить Защиту от удаления, чтобы защитить кластер от непреднамеренного удаления пользователем.
В этом блоке также можно задать настройки БД (например используемую сервером MySQL кодировку при работе с данными и обмене информацией с клиентами). По умолчанию при создании кластера сервис выбирает оптимальные настройки. Изменяйте их, если уверены, что это необходимо.
Настройка завершена. Осталось только нажать кнопку Создать кластер.
Создание кластера займёт несколько минут. Когда он будет готов к работе, его статус на панели Managed Service for MySQL сменится с Creating на Running, а состояние — на Alive.
Статус показывает, что происходит с кластером:
    Creating — создаётся;
    Running — работает;
    Error — не отвечает, возникла проблема;
    Updating — обновляется;
    Stopped — остановлен;
    Unknown — статус неизвестен (так может быть, например, когда кластер не виден из интернета).
Состояние — это показатель доступности кластера:
    Alive — все хосты кластера работают;
    Degraded — часть хостов (один или больше) не работает;
    Dead — все хосты не работают.
Task:
Вы хотите поместить в облако БД, которая обслуживает работающий микросервис с постоянной, но не очень высокой нагрузкой. Какой тип ВМ стоит выбрать при создании кластера?
Decision:
+standard
-memory-optimized
-burstable
Task:
Лимиты, квоты и тарификация
Decision:
Не правда ли, создать кластер несложно. Но чтобы делать это легко и быстро — лучше заранее представлять, какой кластер вам нужен и получится ли его создать. На предыдущем уроке вы узнали о возможностях: какие классы хостов или типы хранилищ данных можно выбрать. На этом разберемся с ограничениями.
Лимиты и квоты
Ограничения делятся на лимиты и квоты.
Лимиты — технические ограничения, связанные с особенностями архитектуры Yandex.Cloud. Изменить их невозможно.
Квоты — организационные ограничения. Их можно увеличить, написав запрос в техническую поддержку.
У каждого сервиса управляемых БД собственные лимиты. Как правило, они связаны с тем:
    какие классы хостов можно использовать;
    сколько хостов может быть в кластере;
    какой максимальный размер хранилища данных.
Полный перечень лимитов каждого сервиса вы найдете в его документации в разделе Концепции → Квоты и лимиты (вот, к примеру, ограничения Yandex Managed Service for MySQL).
В кластере управляемой БД MySQL может быть не больше семи хостов. Размер хранилища данных для одного хоста не превышает 2048 ГБ для стандартного, 4096 ГБ для быстрого сетевого хранилища, 8184 ГБ для хранилища на нереплицируемых SSD-дисках и 1500 ГБ (для платформ Intel Broadwell и Intel Cascade Lake) или 2944 ГБ (для платформы Intel Ice Lake) для локального.
Квоты всех сервисов управляемых БД одинаковы. В одном облаке по умолчанию можно создать не более 16 кластеров разных БД. Максимальное число ядер процессора — 96, общий объем виртуальной памяти — 640 ГБ, объём хранилищ данных — 4096 ГБ.
Тарификация
Создавая кластер, вы наверняка обратили внимание на панель расчета стоимости сервиса. В зависимости от параметров кластера цена может быть и очень скромной, и весьма существенной.
Сравните стоимости кластера из одного хоста b2.nano cо стандартным сетевым хранилищем 10 ГБ и кластера из трех хостов s2.6xlarge c быстрым локальным хранилищем 1000 ГБ
Стоимость использования сервиса управляемых БД зависит от класса и количества хостов в кластере, типа и размера хранилища данных, объёма резервных копий и исходящего трафика.
Чем больше ядер процессора и оперативной памяти задействовано в кластере — тем он дороже. Выбирайте класс хоста разумно. Для учебных задач хватит минимального кластера с одним хостом типа burstable. Если понадобится, класс хоста можно изменить.
Оценивается каждый час работы хоста. Если кластер остановлен, оплата идет только за выделенный объём хранилища данных. Это выгодно, если вы используете кластер не постоянно, а время от времени: для учебных практических работ или тестирования. Также не забывайте удалять ненужные кластеры.
Важно! Если вы используете быстрое локальное хранилище данных (local-ssd), то при остановке кластера его вычислительные ресурсы не высвобождаются, и за него взимается полная оплата.
Стоимости стандартного сетевого, быстрого сетевого, быстрого локального и хранилища на нереплицируемых SSD-дисках различаются: стандартное сетевое — самое дешёвое, быстрое сетевое и быстрое локальное — самые дорогие. Также учитывайте, что для использования быстрого локального хранилища нужно создать кластер как минимум из трех хостов.
Хранение резервных копий оплачивается только в том случае, если размер БД и всех резервных копий больше выбранного размера хранилища.
При использовании сервиса оплачивается только исходящий трафик из Yandex Cloud в интернет сверх 10 ГБ в месяц. Передача трафика между сервисами Yandex Cloud и входящий трафик не тарифицируются.
Тарифы на использование каждого сервиса управляемых БД вы найдете в документации к нему в разделе Правила тарификации → Действующие правила (вот, к примеру, тарифы Managed Service for MySQL).
Чтобы посмотреть детализацию расходов, в консоли управления Yandex.Cloud откройте раздел Биллинг.
Для примера давайте посчитаем, во сколько обойдётся непрерывная работа кластера, который вы создали на предыдущем уроке (один хост класса s2.micro со стандартным сетевым хранилищем 50 ГБ). Кластер используется для учебы — значит, исходящий трафик в интернет незначительный, а в хранилище данных хватит места для бесплатных резервных копий.
Стоимость (в месяц) = стоимость работы хоста × число хостов + стоимость 1 ГБ хранилища × размер хранилища + стоимость хранения резервных копий сверх размера хранилища + стоимость исходящего трафика сверх 10 ГБ.
Стоимость (в месяц)=(4,9*24*30)*1+2,2881*50+0+0=3642,41 p
Допустим, вы учились работать с кластером 20 часов и запускали его только во время занятий. Тогда вы потратите:
Стоимость (в месяц)=(4,9*20)*1+2,2881*50+0+0=212,41 p
Task:
Можно ли для экономии использовать при создании кластера хосты с уровнем производительности vCPU меньше 5%?
Decision:
-Да
-Да, после запроса в техническую поддержку
+Нет
Task:
Практическая работа. Подключение к БД и добавление данных
Decision:
Доступ из консоли управления
Продолжим практическую работу. В кластере, который вы создали, уже есть БД. Она пока пустая. Поскольку при создании кластера вы выбрали в настройках пункт Доступ из консоли управления, в консоли управления Yandex Cloud появилась вкладка с интерфейсом для выполнения SQL-запросов к БД.
Давайте зайдём туда и создадим в БД таблицу для нашего микросервиса.
На странице Managed Service for MySQL выберите строку с созданным вами кластером. В панели консоли управления перейдите на вкладку SQL. Вам будет предложено выбрать БД для SQL-запросов и имя пользователя, а также ввести пароль. Все эти атрибуты вы задавали при создании  кластера.
Нажмите кнопку Подключиться. Откроется структура БД (сейчас там написано, что данных нет) и окно ввода для SQL-запросов.
Теперь создадим таблицу. Введите в окне ввода следующий запрос и нажмите кнопку Выполнить.
CREATE TABLE IF NOT EXISTS ratings (
    rating_id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    message_id INT NOT NULL,
    rating INT NOT NULL
) ENGINE=INNODB; 
Обратите внимание, что в качестве движка в сервисе управляемых БД MySQL используется только InnoDB.
В таблицу можно добавить данные с помощью команды INSERT.
INSERT INTO ratings (user_id,message_id,rating) VALUES (44,368,4); 
Чтобы отобразить обновлённую структуру БД, нажмите на имя БД и выберите таблицу ratings.
Наведите указатель на заголовок столбца, чтобы увидеть тип данных в нём.
SQL-запросы через консоль управления Yandex Cloud — нетипичный способ работы с БД. Используйте его для небольших, разовых задач, когда быстрее и проще открыть подключение в браузере. Этот способ не очень удобен: текст запроса и результат его выполнения доступны, только пока вы не закрыли или не перезагрузили страницу в браузере. Конечно, если запрос успешно запущен, то сервис обработает его независимо от состояния консоли управления.
В консоли выводятся только первые 1000 строк результата запроса, даже если данных больше. Чтобы увидеть строку, введите её номер в поле Номер первой строки.
Подключение к кластеру
В основном вы будете работать с БД из приложений или из командной строки. Однако для этого нужно подключиться к хосту, на котором развёрнута БД.
Есть два варианта подключения. Если публичный доступ к хосту открыт, подключитесь к нему через интернет с помощью защищённого SSL-соединения. Если публичного доступа нет, подключитесь к хосту с виртуальной машины, созданной в той же виртуальной сети. SSL-соединение можно не использовать, но тогда трафик между виртуальной машиной и БД шифроваться не будет.
Давайте подключимся к БД через интернет и создадим в ней ещё одну таблицу. Для выполнения этого задания вы можете использовать виртуальную машину с Ubuntu.
Для создания таблицы сделаем в текстовом редакторе файл createTables.sql с командами. Например, такой:
CREATE TABLE IF NOT EXISTS users (
    user_id INT AUTO_INCREMENT,
    nickname VARCHAR(128) NOT NULL,
    avatar VARCHAR(255),
    mail VARCHAR(255),
        PRIMARY KEY (user_id)
) ENGINE=INNODB; 
Чтобы выполнить этот запрос в БД, подключимся к хосту. Для этого понадобится SSL-сертификат. Команды для его получения в Ubuntu:
mkdir ~/.mysql
wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
chmod 0600 ~/.mysql/root.crt 
Чтобы получить команды для подключения к БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться. В результате их выполнения в директории /home/<домашняя_директория>/.mysql/ сохранится SSL-сертификат root.crt.
Установите утилиту mysql-client, если на вашем компьютере или виртуальной машине её нет.
sudo apt update
sudo apt install -y mysql-client 
Чтобы подключиться к БД, введите команду mysql. Для запуска нашего скрипта она выглядит следующим образом:
mysql --host=<адрес хоста> \
        --port=3306 \
        --ssl-ca=~/.mysql/root.crt \
        --ssl-mode=VERIFY_IDENTITY \
        --user=<имя пользователя> \
        --password \
    <имя_базы_данных> < createTables.sql 
Сервис помогает заполнить параметры в команде. Чтобы посмотреть пример команды с адресом хоста, именами пользователя и БД, в консоли управления перейдите на страницу кластера, на вкладке Обзор нажмите кнопку Подключиться.
После запуска команды введите пароль к БД, после чего в ней будет создана таблица users.
Если при создании кластера вы не включили публичный доступ, то к БД можно подключиться с виртуальной машины из той же облачной сети без использования шифрования. \\Следовательно, в этом случае в команде для подключения опускается параметр --ssl-ca, а --ssl-mode передаётся со значением DISABLED:
mysql --host=адрес_хоста \
      --port=3306 \
      --ssl-mode=DISABLED \
      --user=<имя пользователя> \
      --password \
      <имя_базы_данных> < createTables.sql 
Естественно, подключаться к БД можно не только из командной оболочки, но и из приложений. Нажмите уже знакомую вам кнопку Подключиться и посмотрите примеры кода для Python, PHP, Java, Node.js, Go, Ruby или настроек для драйвера ODBC.
Если вы хотите перенести БД в облако, то понадобится создать дамп и восстановить его в нужном кластере. Дамп — это копия БД или её части, представляющая собой текстовый файл с командами SQL (например, CREATE TABLE или INSERT). Его создают с помощью утилиты mysqldump.
Давайте попробуем перенести данные в кластер с помощью дампа. Для этого воспользуемся тестовой БД с данными о сотрудниках компании (имя, дата рождения, дата найма, место работы, зарплата и т. д.). Размер БД — около 167 Мб.
Скачайте из репозитория и сохраните на компьютере файлы с расширениями .sql и .dump. В файле employees.sql содержатся SQL команды, необходимые для создания таблиц и добавления в них данных из dump-файлов. Для переноса тестовой БД в облако понадобится запустить этот файл. Но, прежде чем приступить к переносу БД, откройте этот файл и удалите или закомментируйте (допишите в начало строки --) в нём строку 110. В этой строке расположена команда FLUSH LOGS, которая закрывает и снова открывает файлы журналов, а они в этой тестовой БД отсутствуют.
Создайте базу данных employees через консоль управления. Для этого на странице кластера перейдите на вкладку Базы данных и нажмите кнопку Добавить.
Добавьте пользователю, например user1, разрешение на доступ к БД employees. Для этого на странице кластера перейдите на вкладку Пользователи, напротив пользователя user1 нажмите кнопку ··· и выберите Настроить. Во всплывающем окне нажмите Добавить базу данных, выберите employees, добавьте роль ALL_PRIVILEGES и нажмите Сохранить.
Затем в командной строке перейдите в папку сохраненными файлами .sql и .dump и восстановите данные из дампа с помощью команды:
mysql --host=<адрес хоста> \
        --port=3306 \
        --ssl-ca=~/.mysql/root.crt \
        --ssl-mode=VERIFY_IDENTITY \
        --user=<имя_пользователя> \
        --password \
    employees < ~/employees.sql 
После того как данные скопируются, ваш кластер и БД будут готовы к работе. Подключитесь к БД в консоли управления и убедитесь, что данные перенесены.
Decision:
$ ssh -i ubcloud test@178.154.220.177
$ vim createTables.sql
$ cat createTables.sql
CREATE TABLE IF NOT EXISTS users (
        user_id INT AUTO_INCREMENT,
        nickname VARCHAR(128) NOT NULL,
        avatar VARCHAR(255),
        mail VARCHAR(255),
        PRIMARY KEY (user_id)
) ENGINE=INNODB;
$ mkdir ~/.mysql
$ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.mysql/root.crt
$ chmod 0600 ~/.mysql/root.crt
$ sudo apt update
$ sudo apt install -y mysql-client 
$ mysql --host=rc1a-lv4y1m7nmsaa620f.mdb.yandexcloud.net \
>       --port=3306 \
>       --ssl-ca=~/.mysql/root.crt \
>       --ssl-mode=VERIFY_IDENTITY \
>       --user=user1 \
>       --password \
>       db1
mysql> Show tables;
mysql> select * from ratings;
Task:
Распределение ответственности между Yandex Cloud и пользователем
Decision:
Ответственность облачного провайдера
Как вы могли убедиться на практике, создание кластера управляемой БД и подключение к нему занимает лишь несколько минут. Почти всю работу — поднять и настроить виртуальные машины, установить ПО, сконфигурировать сеть — делает сервис.
Управляемые БД действительно экономят время. Давайте посмотрим, что требует внимания пользователя, для трёх вариантов развёртывания БД: на своём оборудовании, на виртуальной машине в облаке и с помощью сервиса управляемых БД.
    На своем оборудовании   БД в облачной виртуальной машине    Управляемая БД
Развитие продукта   +   +   +
Закупка оборудования    +   -   -
Установка сервера   +   -   -
Конфигурация сети   +   -   -
Установка ОС и ПО   +   +   -
Безопасность    +   -   -
Аудит   +   -   -
Настройка групп безопасности    +   +   -
Тестирование и проверка +   +   -
Доступность +   +   -
Резервное копирование   +   +   -
Мониторинг  +   +   -
Обновления  +   +   -
Настройка кластера  -   -   +
Помимо создания кластера, сервис управляемой БД выполняет важные функции, в том числе связанные с отказоустойчивостью и доступностью БД. Вот за что отвечает сервис:
    своевременное обновление ПО;
    мониторинг работы хостов и БД;
    автоматическое создание резервных копий БД;
    репликация данных между хостами;
    автоматическое переключение нагрузки на резервную реплику при сбое на мастере.
То есть сервис минимизирует время, необходимое для настройки и поддержки системы, и позволяет вам работать над своим продуктом, не отвлекаясь на рутинные задачи по поддержке инфраструктуры.
В качестве примера рассмотрим, как сервис управляемой БД MySQL обновляет ПО.
Сервис поддерживает версии MySQL 5.7 и 8. При выходе новых минорных версий обновление происходит автоматически. Владельцы кластеров получают оповещение о сроках работ и доступности БД в этот период.
Когда версия СУБД становится неподдерживаемой, спустя месяц сервис направляет владельцам кластеров оповещение по электронной почте. Кластеры автоматически обновляются до следующей поддерживаемой версии через семь дней после оповещения для минорных версий и через один месяц для мажорных. Вам не нужно отслеживать выход обновлений и устанавливать их.
Ещё одна важная часть ответственности сервиса — обеспечение безопасности данных. На самом деле ответственность здесь общая, а точнее — разделяемая (shared responsibility), но на долю облака её приходится гораздо больше.
И наконец, облако предоставляет техническую поддержку, куда вы можете обратиться, если появятся вопросы или проблемы.
Ответственность пользователя
Вы, пользователь сервиса, отвечаете за корректность работы приложения и связанной с ним БД: правильность схемы данных, оптимизацию запросов к базе и т. д.
Если нужно перенести данные в облако (например, когда вы переходите на использование сервисов управляемых БД) — то вам понадобится сделать это самостоятельно.
Сервис предоставляет инструменты для мониторинга работы хостов и БД, но отслеживать и оценивать показатели — ваша обязанность. Если БД разрастётся или к ней будет гораздо больше запросов, чем планировалось, — вам самим нужно будет увеличить размер хранилища или число и класс хостов.
Если говорить о безопасности данных, вы несёте часть ответственности и за неё. Любые меры облачного провайдера  могут оказаться бесполезны, если вы неправильно настроите доступ к кластерам и БД или установите пароли вроде 12345678, password и т. п.
Таким образом, сервисы управляемых БД значительно облегчают жизнь и экономят время. Но при этом помните, каким вещам следует уделять внимание, чтобы не возникало проблем.
Task:
Какие функции выполняют сервисы управляемых БД?
Decision:
+Обновление ПО
+Мониторинг хостов и БД
+Резервное копирование БД
-Перенос данных в облако
+Репликация данных
Task:
Логи и мониторинг кластера
Decision:
Мониторинг
Вам наверняка знакомо неприятное ощущение, когда стабильно и быстро работающая система начинает тормозить, подвисать или, того хуже, падать. Чтобы с вашим кластером и БД не случалось таких проблем, заботьтесь об их здоровье и отслеживайте метрики их работы.
Сервисы управляемых БД предоставляют вам инструменты для мониторинга работы хостов и БД, а также анализа логов.  Это позволяет вовремя распознавать и исправлять проблемы. Например, вы легко увидите, что кластеру не хватает вычислительных ресурсов или что запросы к БД выполняются слишком медленно.
Рассмотрим эти инструменты на примере управляемой БД MySQL. Войдите в консоль управления, перейдите на страницу кластера и выберите вкладку Мониторинг.
Мониторинг кластера
Вы увидите информационную панель (дашборд) с графиками.
Queries per second — общее количество запросов в секунду, для каждого хоста.
Average query time — среднее время исполнения запросов, для каждого хоста (в миллисекундах).
Slow queries per second — количество SQL-запросов в секунду, выполняющихся дольше, чем указано в параметре long_query_time, для каждого хоста.
Connections — количество подключений, для каждого хоста.
Threads running — количество запущенных потоков, для каждого хоста. При увеличении нагрузки на кластер это значение будет быстро расти.
Disk usage — занятое дисковое пространство (в байтах), для каждого хоста и для кластера в целом.
Is Primary — показывает, какой хост является мастером и как долго.
Is Alive — показывает доступность кластера в виде суммы состояний его хостов. Каждый хост в состоянии Alive увеличивает общую доступность на 1. При выходе из строя одного из хостов общая доступность уменьшается на 1. Для повышения доступности кластера вы можете добавить в него хосты.
Replication lag — отставание реплики от мастера (в секундах).
Мониторинг состояния хостов
В разделе Хосты консоли управления на вкладке Мониторинг можно отслеживать состояние хостов:
CPU usage — загрузка процессорных ядер. При повышении нагрузки значение Idle уменьшается.
Memory usage — использование оперативной памяти (RAM) в байтах. При высоких нагрузках значение параметра Free уменьшается, а Used и других — растёт.
Disk read/write bytes — средний объём данных, записанных в хранилище и прочитанных из него (в байтах).
Disk IOPS — среднее количество операций ввода-вывода в хранилище.
Network bytes — средний объём данных, отправленных в сеть и полученных из неё (в байтах).
Network packets — среднее количество пакетов, отправленных в сеть и полученных из неё.
Анализируйте графики на дашборде, чтобы оперативно определять просадку производительности хостов и следить за состоянием всего кластера.
За мониторинг ресурсов в Yandex Cloud отвечает сервис Yandex Monitoring. Вы подробно познакомитесь и научитесь работать с ним на курсе «DevOps и автоматизация».
В блоке MySQL overview отображаются расширенные сведения о состоянии БД на хосте.
Подробнее о мониторинге состояния кластера и хостов рассказывается в документации.
Логирование
Если в работе с БД появляются ошибки или производительность системы падает, изучите логи ошибок или медленных запросов. Сервис фиксирует такие события по умолчанию. Чтобы просмотреть записи, зайдите на вкладку Логи и выберите тип логирования и хост (или хосты), работу которого вы хотите проанализировать.
Типы логирования:
    MYSQL_GENERAL — общий журнал запросов к БД;
    MYSQL_ERROR — журнал ошибок MySQL;
    MYSQL_SLOW_QUERY — журнал медленных запросов MySQL.
    MYSQL_AUDIT — информация о подключениях к БД.
Task:
Что делать, если показатель загрузки CPU почти достиг максимального уровня?
Decision:
+Увеличить число хостов в кластере и распределить нагрузку между ними
+Проверить в логах, есть ли медленные запросы к БД, и оптимизировать их
-Увеличить размер хранилища данных
+Выбрать другой класс хоста
Task:
Резервное копирование и восстановление данных
Decision:
Резервное копирование
Чтобы не потерять данные, применяют резервное копирование. Благодаря ему вы восстановите БД, если данные повреждены. Сервисы управляемых БД выполняют резервное копирование автоматически, а также позволяют сделать его вручную.
В сервисе управляемой БД MySQL резервное копирование происходит раз в сутки. Все журналы транзакций кластера БД сохраняются. Это позволяет восстановить состояние кластера на любой момент в пределах периода хранения резервных копий, кроме последних 30 секунд. По умолчанию резервные копии хранятся семь дней.
Время начала резервного копирования задаётся при создании или изменении кластера в блоке Дополнительные настройки. Резервное копирование начнётся в течение получаса после указанного времени. По умолчанию начало резервного копирования устанавливается на 22:00 UTC. Во время окна резервного копирования кластеры остаются полностью доступными.
Чтобы изменить время резервного копирования и окно обслуживания, в разделе Обзор нажмите Изменить кластер.
Резервную копию можно сделать и вручную, например, если вы подозреваете, что ваш эксперимент с БД может плохо для неё закончиться. В консоли управления на странице кластера выберите вкладку Резервные копии и нажмите кнопку Создать резервную копию.
Восстановление из резервной копии
Если вы случайно удалили таблицу с данными, резервное копирование позволит откатить эти изменения. Восстановите кластер из резервной копии:
    В консоли управления на странице каталога выберите сервис Managed Service for MySQL.
    Выберите кластер и перейдите на вкладку Резервные копии.
    Напротив резервной копии нажмите … и выберите Восстановить кластер.
    Задайте настройки нового кластера. Выберите каталог для него в списке Каталог.
    Нажмите кнопку Восстановить кластер.
Сервис запустит операцию восстановления и создаст новый кластер с данными из резервной копии. Если для нового кластера в каталоге не хватает ресурсов, то восстановить резервную копию не получится.
Task:
Средняя скорость восстановления из резервной копии — 10 МБ/с на каждое ядро vCPU хоста БД. Вы восстанавливаете из резервной копии БД размером 1,2 ГБ в кластер с одним хостом класса s2.small. Сколько примерно времени займёт восстановление?
Decision:
+0,5 минуты
-1 минуту
-2 минуты
Task:
Практическая работа. Создание кластера базы данных PostgreSQL
Decision:
В этой практической работе вы создадите кластер еще одной управляемой БД, на этот раз PostgreSQL, подключитесь к ней и загрузите в нее данные. Вы сможете убедиться, что принципы организации сервисов управляемых БД в Yandex Cloud практически одинаковы. Хорошо изучив один из них, вы будете легко ориентироваться и в остальных.
Создание кластера
Создание кластера управляемой базы данных PostgreSQL аналогично созданию кластера базы данных MySQL.
Перейдите в сервис управляемых баз данных PostgreSQL и нажмите кнопку Создать кластер.
В появившемся окне настроек задайте необходимые параметры.
    Имя кластера и его описание. Выберите уникальное в облаке имя кластера. Описание опционально, поэтому можно оставить это поле пустым.
    В поле Окружение выберите PRODUCTION.
    Выберите версию PostgreSQL и класс хоста.
    Выберите размер и тип сетевого хранилища.
    Задайте атрибуты базы данных.
    Выберите из списка сеть, в которой будут находиться хосты кластера (для подключения потребуются публичные хосты).
    В блоке Хосты добавьте ещё два хоста в других зонах доступности для обеспечения отказоустойчивости кластера. База автоматически реплицируется.
    В блоке Дополнительные настройки задайте время начала резервного копирования и включите доступ из консоли управления.
    Нажмите кнопку Создать кластер.
Подключение
Как и в случае с MySQL, к хостам кластера Managed Service for PostgreSQL можно подключиться двумя способами.
Через интернет
Если вы настроили публичный доступ для нужного хоста, то подключиться к нему можно с помощью SSL-соединения.
С виртуальных машин Yandex Cloud
Они должны быть расположены в той же облачной сети. Если к хосту нет публичного доступа, для подключения с таких виртуальных машин SSL-соединение использовать необязательно. Обратите внимание, что если публичный доступ в вашем кластере настроен только для некоторых хостов, автоматическая смена мастера может привести к тому, что вы не сможете подключиться к мастеру из интернета.
Установите клиент для подключения к БД PostgreSQL. Команда установки в Ubuntu:
sudo apt update && sudo apt install -y postgresql-client 
Скачайте сертификат для подключения к БД PostgreSQL:
mkdir -p ~/.postgresql
wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt 
chmod 0600 ~/.postgresql/root.crt 
Пример команды для подключения можно посмотреть в консоли управления, нажав на кнопку Подключиться на странице кластера. Подключение с SSL происходит при помощи следующей команды:
psql "host=<FQDN_хоста> \ 
      port=6432 \
      sslmode=verify-full \
      dbname=<имя базы данных> \
      user=<имя пользователя базы данных> \
      target_session_attrs=read-write"
Загрузка данных в базу данных из CSV
Одним из способов добавления данных в базу является их загрузка из csv-файла.
Предположим, вы используете БД для организации работы транспортной службы интернет-магазина. Вам нужно добавить в базу таблицу, содержащую данные о расстояниях между складом и пунктами самовывоза, а также о стандартном времени доставки товаров со склада в эти пункты. Создадим csv-файл, например DTM.csv, который содержит такие данные (100 - код склада, 101-109 - коды пунктов, Time - стандартное время доставки в минутах, Distance - расстояние в километрах):
"depot","store","time","distance"
"100","101",31,12
"100","102",38,17
"100","103",56,33
"100","104",70,60
"100","105",41,25
"100","106",21,8
"100","107",33,14
"100","108",62,42
"100","109",45,29 
Важные моменты при миграции из CSV:
    Названия колонок в файле и в таблице необязательно совпадают.
    Файл содержит заголовок, который не нужно импортировать.
    Первые 2 колонки конвертируем из строк (string) в целые числа (int).
PostgreSQL позволяет импортировать данные из файла несколькими способами:
    Командой copy.
    Через функции pl/pgsql.
    Средствами другого языка, например Python.
Воспользуемся первым способом.
Сначала нам понадобится создать таблицу, в которую будет осуществлена миграция данных. Подключитесь к БД согласно инструкциям выше. Выполните следующую команду:
CREATE TABLE dtm (
    id serial PRIMARY KEY,
    depot int NOT NULL,
    store int  NOT NULL,
    time int NOT NULL,
       distance int  NOT NULL
);
Загрузите данные:
\copy dtm(depot,store,time,distance) from '/<путь к файлу>/DTM.csv' DELIMITERS ',' CSV HEADER;
В этой команде мы учли те моменты, о которых говорили вначале:
    dtm (depot, store, time, distance) маппинг колонок связывает колонки в файле с колонками в таблице, их имена могут не совпадать
    CSV HEADER показывает, что заголовок импортировать не нужно
    Колонки в таблице уже имеют правильные типы данных, конвертация будет выполнена автоматически.
В консоли управления на странице кластера перейдите на вкладку SQL. Введите пароль пользователя БД и нажмите кнопку Подключиться. Выберите таблицу dtm, чтобы убедиться, что добавление данных выполнено правильно.
Decision:
$ sudo apt install -y postgresql-client 
$ mkdir -p ~/.postgresql
$ wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O ~/.postgresql/root.crt
$ chmod 0600 ~/.postgresql/root.crt
$ vim DTM.csv
$ cat DTM.csv
"deport","store","time","distance"
"100","101",31,12
"100","102",38,17
"100","103",56,33
"100","104",70,60
"100","105",41,25
"100","106",21,8
"100","107",33,14
"100","108",62,42
"100","109",45,29
$ psql "host=rc1a-4ze6pmpwek1evn80.mdb.yandexcloud.net,rc1b-kg04sg8hlh6fcgvc.mdb.yandexcloud.net \
>       port=6432 \
>       sslmode=verify-full \
>       dbname=db2 \
>       user=user2 \
>       target_session_attrs=read-write"
db2=> CREATE TABLE dtm (
db2(>     id serial PRIMARY KEY,
db2(>     depot int NOT NULL,
db2(>     store int  NOT NULL,
db2(>     time int NOT NULL,
db2(>        distance int  NOT NULL
db2(> );
db2=> \copy dtm(depot,store,time,distance) from '/home/test/DTM.csv' DELIMITERS ',' CSV HEADER;
db2=> exit
Task:
Репликация
Decision:
Всё ломается не только в физическом мире — виртуальные машины тоже выходят из строя. Чтобы решить проблему сохранности и доступности БД, можно создать несколько хостов и поместить на каждый её копию. Это повысит отказоустойчивость системы и даже улучшит её производительность, поскольку запросы распределятся между хостами. Однако в этом случае возникает новая задача: синхронизировать данные.
Механизм синхронизации копий БД внутри кластера называют репликацией. В различных СУБД он реализуется по-разному (ниже мы кратко остановимся на основных его видах). Выбор решения зависит от задачи и должен учитывать размер системы, требования к скорости ее работы, критичность потери даже небольшого фрагмента данных.
Различают асинхронную и синхронную репликации. Асинхронный механизм допускает задержку между подтверждением транзакции (целостного выполнения связанных между собой операций) на одном хосте и её передачей на другие хосты. Это ускоряет работу, но данные при сбое могут быть потеряны. Такой подход реализован в БД ClickHouse.
В кластерах PostgreSQL используется синхронная репликация. В этой классической СУБД запись в БД происходит на хосте-мастере. Остальные хосты в кластере — реплики, для них доступны только операции чтения данных. Транзакция подтверждается лишь тогда, когда данные записаны на диск и на хосте-мастере, и на всех репликах. Это повышает надёжность системы, но может заметно (на 25−30%) снизить скорость ее работы по сравнению с асинхронным режимом.
Асинхронная и синхронная репликации — это виды физической репликации, при которой изменённые блоки данных побайтово копируются с диска одного хоста на диск другого.
В PostgreSQL также реализован метод логической репликации. Его основное отличие от физической заключается в том, что вместо последовательностей байтов копируются таблицы и строки.
При логической репликации используются публикации и подписки. Один экземпляр БД (подписчик) подписывается на изменения в другом экземпляре (публикующем узле) и получает от него данные. Изменения на стороне публикующего узла передаются подписчику в реальном времени, публикациям в рамках подписки гарантируется транзакционная целостность (т. е. результат транзакции записывается в БД только тогда, когда все операции транзакции успешно завершены).
Подписчик, в свою очередь, также может публиковать данные, что позволяет организовать каскадную репликацию. В этом случае хост-мастер синхронизируется только с одной репликой, которая затем используется для синхронизации с другими репликами группы. Такой подход позволяет заметно уменьшить транзакционную нагрузку на хост-мастер.
С помощью репликации можно перенести данные из вашей базы в управляемую БД PostgreSQL.
Механизм подписки, на котором построена логическая репликация, позволяет перенести данные на кластер с минимальным простоем.
Логическая репликация поддерживается начиная с версии PostgreSQL 10 и выше. Вы можете переносить данные и между одинаковыми версиями, и на более свежие версии. Для этого настройте репликацию с сервера-источника на сервер-приёмник с более свежей версией СУБД.
В кластере Managed Service for PostgreSQL подписки может применять пользователь, созданный одновременно с кластером, и пользователи с ролью mdb.admin для этого кластера.
Task:
Синхронная репликация:
Decision:
-быстрее асинхронной
+медленнее асинхронной
+надёжнее для сохранения данных
-может приводить к потере данных при сбое
Task:
Миграция данных в облако репликацией
Decision:
Предположим, вы решили перенести БД в сервис управляемых БД PostgreSQL. Для этого придётся выполнить логическую репликацию:
    Настроить сервер с источником данных.
    Экспортировать схему БД из источника.
    Создать кластер Managed Service for PostgreSQL и восстановить схему БД.
    Создать публикации и подписки.
    Перенести PostgreSQL-sequences после репликации.
    Отключить репликацию и перенести нагрузки.
Как видите, процесс довольно трудоёмкий. Разберём его по шагам.
1. Настройка сервера с источником данных
Чтобы перенести данные с помощью репликации, настройте PostgreSQL на сервере-источнике. Измените настройки SSL и WAL (Write Ahead Log) в файле postgresql.conf (в Ubuntu путь к нему по умолчанию — /etc/postgresql/10/main/postgresql.conf).
Для переноса данных используйте SSL: это поможет не только зашифровать данные, но и сжать их. Подробнее читайте в документации PostgreSQL: разделы SSL Support и Database Connection Control Functions.
Включите использование SSL:
SSL=ON 
Измените уровень логирования для WAL, чтобы добавить в него информацию для логической репликации. В файле postgresql.conf найдите строку с настройкой wal_level, раскомментируйте её при необходимости и установите значение logical:
wal_level=logical 
Теперь настройте аутентификацию хостов в источнике: внесите хосты кластера в облаке в файл pg_hba.conf (в Debian и Ubuntu путь к нему по умолчанию — /etc/postgresql/10/main/pg_hba.conf). Добавьте в файл строки, которые разрешат входящие соединения к БД с указанных хостов.
Если вы используете SSL:
hostssl    all            all             <адрес хоста>      md5
hostssl    replication    all             <адрес хоста>      md5          
Если вы не используете SSL:
host    all            all             <адрес хоста>      md5
host    replication    all             <адрес хоста>      md5
Если на сервере-источнике работает фаервол, разрешите входящие соединения с хостов кластера Managed Service for PostgreSQL. Например, для Ubuntu 18:
sudo ufw allow from <адрес хоста> to any port 5432 
Перезагрузите сервер БД, чтобы применить настройки:
sudo systemctl restart postgresql 
После перезапуска проверьте статус PostgreSQL с помощью команды:
sudo systemctl status postgresql 
2. Экспорт схемы БД из источника
С помощью утилиты pg_dump создайте файл со схемой БД, которую нужно применить в кластере Managed Service for PostgreSQL.
pg_dump -h <адрес сервера СУБД> \
    -U <имя пользователя> \
    -p <порт> \
    --schema-only \
    --no-privileges \
    --no-subscriptions \
    -d <имя базы данных> -Fd -f /tmp/db_dump
В этой команде при экспорте исключаются все данные, которые связаны с привилегиями и ролями. Это необходимо, чтобы не возникало конфликтов с настройками БД в Yandex Cloud. Если базе нужны дополнительные пользователи, создайте их.
3. Создание кластера Managed Service for PostgreSQL и восстановление схемы БД
Если у вас ещё нет PostgreSQL-кластера в Yandex Cloud, создайте его. При создании кластера укажите то же имя БД, что и на сервере-источнике.
Восстановите схему в созданном кластере:
pg_restore -Fd -v --single-transaction -s --no-privileges \
          -h <адрес приемника> \
          -U <имя пользователя> \
          -p 6432 \
          -d <имя базы данных> /tmp/db_dump
4. Создание публикаций и подписки
Определите публикацию (группу логически реплицируемых таблиц) на сервере-источнике и подписку (описание соединения с другой базой) на сервере-приёмнике.
На сервере-источнике создайте публикацию для всех таблиц БД. Если переносите несколько баз — для каждой сделайте свою публикацию.
Чтобы создать публикации для всех таблиц, потребуются права суперпользователя. Чтобы перенести выбранные таблицы, права не нужны. Воспользуйтесь запросом:
CREATE PUBLICATION p_data_migration FOR ALL TABLES; 
На хосте кластера Managed Service for PostgreSQL создайте подписку со строкой подключения к публикации. Подробности о создании подписок смотрите в документации PostgreSQL (раздел Create subscription).
Запрос с включённым SSL:
    CREATE SUBSCRIPTION s_data_migration CONNECTION 'host=<адрес сервера-источника> port=<порт> user=<имя пользователя> sslmode=verify-full dbname=<имя базы данных>' PUBLICATION p_data_migration;
Без SSL:
    CREATE SUBSCRIPTION s_data_migration CONNECTION 'host=<адрес сервера-источника> port=<порт> user=<имя пользователя> sslmode=disable dbname=<имя базы данных>' PUBLICATION p_data_migration;
Следите за статусом репликации через каталоги pg_subscription_rel. Общий статус репликации на приёмнике можно получить через pg_stat_subscription, на источнике — через pg_stat_replication.
select * from pg_subscription_rel; 
Важно следить за статусом репликации на приёмнике по полю srsubstate. Значение r в поле srsubstate говорит о том, что синхронизация завершилась и базы готовы к репликации.
5. Перенос PostgreSQL-sequences после репликации
Чтобы завершить синхронизацию источника и приёмника, запретите запись данных на сервере-источнике и перенесите PostgreSQL-sequences в кластер Managed Service for PostgreSQL.
Экспортируйте PostgreSQL-sequences из источника:
pg_dump -h <адрес сервера СУБД> \
    -U <имя пользователя> \
    -p <порт> -d <имя базы данных> \
    --data-only -t '*.*_seq' > /tmp/seq-data.sql
Обратите внимание на паттерн: если в переносимой БД есть sequences, которые не соответствуют паттерну *.*_seq, то для их выгрузки укажите другой паттерн. Подробная информация о паттернах приводится в документации PostgreSQL.
Восстановите sequences на хосте Managed Service for PostgreSQL:
psql -h <адрес сервера СУБД> \
    -U <имя пользователя> -p 6432 \
    -d <имя базы данных> \
    < /tmp/seq
6. Отключение репликации и перенос нагрузки
После того как репликация завершилась и вы перенесли sequences, удалите подписку на сервере-приёмнике (в кластере Managed Service for PostgreSQL):
DROP SUBSCRIPTION s_data_migration; 
После этого можно переносить нагрузку на сервер-приёмник.
Task:
Data Transfer. Инструмент для миграции баз данных
Decision:
Как вы могли убедиться на предыдущем уроке, миграция данных бывает довольно трудоёмкой.
Сервис Yandex Data Transfer позволяет ускорить миграцию и минимизировать простой при переключении на новую БД. Чтобы перенести данные, вам не нужно устанавливать драйверы: вся настройка делается в консоли управления. При этом исходная и целевая БД должны быть одинаковы, а структуры схем, типы данных и коды — совместимы.
Сервис можно использовать не только для миграции данных, но и для других задач.
Тестирование Yandex Cloud
Если вы тестируете сервисы управляемых БД MySQL и PostgreSQL, то лучше делать это на реальных данных. С помощью Data Transfer легко перенести данные в эти сервисы.
Аварийное восстановление
Data Transfer позволяет организовать миграцию данных из Yandex Cloud в локальную БД. На вашем сервере всегда будет актуальная копия данных, пригодная для аварийного восстановления.
Организация разработки и разделение нагрузки
Если разработчики, тестировщики или аналитики вашего продукта используют инфраструктуру Yandex Cloud, то вы быстро организуете окружение для нового участника. За актуальную копию данных в окружении отвечает Data Transfer.
Схема работает и в обратном направлении, когда актуальная стабильная версия сервиса базируется в Yandex Cloud, а для организации рабочего процесса необходима реплика данных в локальных базах. При этом основная БД не будет испытывать лишнюю нагрузку.
Разделение и объединение БД
С помощью Data Transfer можно разделить БД и в каждую новую базу можно перенести разный набор таблиц исходной базы. Также с его помощью можно объединить несколько БД. Например, при миграции в Yandex Cloud вы соберёте данные в единую БД в одном из сервисов управляемых БД. Объединяются только однородные БД.
Основные понятия сервиса — эндпоинт и трансфер.
Эндпоинт — это конфигурация для подключения к БД. Эндпоинты доступны для следующих БД:
    Пользовательская БД BigQuery.
    БД ClickHouse — пользовательская или в составе сервиса Managed Service for ClickHouse.
    БД Greenplum — пользовательская или в составе сервиса Managed Service for Greenplum.
    БД MongoDB — пользовательская или в составе сервиса Managed Service for MongoDB.
    БД MySQL — пользовательская или в составе сервиса Managed Service for MySQL.
    Пользовательская БД Oracle.
    БД PostgreSQL — пользовательская или в составе сервиса Managed Service for PostgreSQL.
    БД Managed Service for YDB — в составе сервиса Managed Service for YDB.
Эндпоинты делятся на два типа:
    Источник описывает настройки БД, откуда передаётся информация.
    Приёмник описывает настройки БД, куда переносится информация.
Трансфер — это перенос данных из источника в приёмник. Трансфер должен находиться в одном каталоге с эндпоинтами, которые соединяет.
Сведения о совместимости источников и приемников при трансфере приведены в документации.
Трансферы бывают следующих типов:
Копирование — перенос снапшота (моментального снимка) БД.
Репликация — непрерывное получение изменений из БД источника и применение их к БД приёмника.
Копирование и репликация — перенос состояния базы источника в базу приёмника и поддержка этого состояния.
При копировании скорость достигает 15 МБ/с. База размером 100 ГБ скопируется за 2–3 часа. При репликации пропускная способность составляет 20–30 тысяч транзакций в секунду.
Task:
Какой тип трансфера выбрать, чтобы добавлять в БД приёмника только текущие изменения БД источника?
Decision:
-Копирование и репликация
-Резервное копирование
+Репликация
-Копирование
Task:
Введение. Несколько слов о NoSQL
Decision:
Вы уже знаете, что реляционные БД — это набор таблиц и связей между ними. Такие БД подходят не для всех задач. Например, данные без структуры невозможно уложить в таблицу. В итоге разработчики создали для нереляционных моделей данных NoSQL (not only SQL) БД.
Различия реляционных и нереляционных БД
    Реляционные Нереляционные
Способ хранения данных  В таблицах  По-разному: как документы, как граф из вершин и ребер, как пары «ключ-значение» и т. д.
Структура данных    Жесткая: у каждого объекта одни и те же поля    Жестких требований нет — у объектов могут быть разные поля
Добавление полей    Потребует изменить структуру таблицы и все объекты в ней    Не потребует ничего менять — можно добавить поля только к новым объектам
Требования ACID — это:
    Atomicity — атомарность: транзакция не выполняется, пока не выполнены все ее части;
    Consistency — целостность: когда транзакция завершилась, данные соответствуют схеме БД, а все реплики базы синхронизированы;
    Isolation — изолированность: параллельные транзакции выполняются отдельно друг от друга;
    Durability — надежность: способность восстанавливаться до последнего сохраненного состояния после сбоя.
На этом и следующих уроках вы познакомитесь с документо-ориентированной БД MongoDB и тем, как с ней работать в Yandex Cloud.
MongoDB — это популярная NoSQL БД, в которой данные хранятся не в строках таблиц, а в документах. Один объект — один документ. Структура каждого документа подобна структуре JSON (JavaScript Object Notation).
Так может выглядеть документ из базы данных поликлиники с медицинскими картами пациентов:
{
  "Имя":"Сергей",
  "Фамилия":"Шишкин",
  "Дата рождения":"02.12.1961",
  "Номер медицинской карты":23264,
  "Посещения врача":[
    {
      "Дата":"24.02.2021",
      "Врач":"Сидорова О.С.",
      "Анамнез":"...",
      "Назначенные обследования":"Общий анализ крови",
      "Диагноз":"ОРВИ",
      "Лечение":"Теплый чай с медом перорально до 12 раз в сутки"
    },
    {
      "Дата":"05.03.2021",
      "Врач":"Сидорова О.С.",
      "Анамнез":"...",
      "Назначенные обследования":"",
      "Диагноз":"Здоров"
    }
  ]
}
В отличие от строк в реляционных БД, документы:
    позволяют сохранять объекты со сложной структурой, которая может изменяться;
    могут отличаться друг от друга размером и полями.
Высокоуровнево документы состоят из пар «ключ - значение». В примере с медицинской картой имя — это ключ, а Сергей — его значение. Значениями могут быть числа, строки, аудиофайлы, изображения, массивы или другие объекты, даже сложные.
Однотипные документы объединяются в коллекции — аналог таблиц в реляционных БД. Например, в БД поликлиники могут входить коллекции «Медицинские карты пациентов», «Результаты лабораторной диагностики» и «Листы нетрудоспособности».
MongoDB подойдет, если необходимо:
    Управлять большим объемом данных с заранее неизвестной структурой (каталоги товаров, пользовательские профили, системы управления контентом).
    Пример: интернет-магазин, где продается множество товаров с разными характеристиками. Чтобы выставить товар на сайт, контент-менеджер выбирает характеристики и их значения из набора. Менеджер также может добавить или удалить любое поле и установленные для него значения.
    Горизонтально масштабироваться.
    Пример: создание национальной БД медицинских карт. Реляционная БД здесь — не лучшее решение: ее горизонтальное масштабирование сопряжено с трудностями и плохо автоматизируется.
Task:
Модель данных в MongoDB называется:
Decision:
-аспектно-ориентированной
+документо-ориентированной
-объектно-ориентированной
Task:
Практическая работа. Создание кластера MongoDB
Decision:
На этом уроке вы создадите кластер MongoDB, подключитесь к нему и загрузите в него данные. Раньше вы работали только с реляционными БД, но использование кластера MongoDB принципиально не отличается от работы с кластером MySQL или PostgreSQL, так что многое будет вам знакомо.
Создание кластера базы данных
Выберите в консоли управления Yandex Cloud каталог для кластера БД. На дашборде каталога откройте раздел Managed Service for MongoDB. В открывшемся окне нажмите кнопку Создать кластер.
Установите основные настройки кластера. Для этого урока создайте кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano, стандартное сетевое хранилище размером 10 ГБ. Откройте публичный доступ к хосту и задайте пароль пользователя БД. Остальные значения оставьте по умолчанию.
Подключение к базе данных
В сервисе управляемых БД MongoDB к хостам можно подключаться через интернет или с виртуальных машин в той же сети. Порт для подключения — 27018.
Для подключения через интернет хосты кластера должны находиться в публичном доступе. Подключаться можно только через зашифрованное соединение.
Обратите внимание: если публичный доступ настроен только для некоторых хостов в кластере, то при автоматической смене основной реплики она может оказаться недоступной из интернета.
Если к хосту нет публичного доступа и вы подключаетесь к нему с виртуальных машин Yandex Cloud, то зашифрованное соединение необязательно.
Подключитесь к созданной БД из интернета. Используйте SSL-сертификат, который вы подготовили на одной из предыдущих практических работ, или команду (для Ubuntu):
sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt 
Если всё пройдет успешно — вы получите сообщение операционной системы о том, что сертификат сохранён.
Установите утилиту Mongo Shell:
sudo apt install mongodb-clients 
Подключитесь к БД с помощью команды mongo. Чтобы получить строку подключения, на основной странице сервиса в консоли управления выберите кластер, на вкладке Обзор нажмите кнопку Подключиться.
Сервис сформирует пример строки подключения для кластера. Там же вы можете посмотреть примеры кода на Python, PHP, Java, Node.js, Go для подключения из приложений.
Подключитесь к кластеру из командной строки.
mongo --norc \
        --ssl \
        --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
        --host '<FQDN хоста MongoDB>:27018' \
        -u <имя пользователя БД> \
        -p <пароль пользователя БД> \
        <имя БД> 
При успешном подключении вы получите сообщение:
Создадим в БД коллекцию users. Предположим, в ней содержится информация о пользователях вашего приложения.
db.createCollection("users") 
Загрузим в коллекцию тестовые данные с помощью методов добавления одного документа db.insertOne(...) и сразу нескольких db.insertMany(...).
Сначала добавим один документ (данные одного пользователя).
db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"}); 
Ответ должен выглядеть примерно так:
Дополним коллекцию данными еще двух пользователей.
db.users.insertMany( [
      {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
      {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
   ] ); 
Обратите внимание, что документы в коллекции users содержат разный набор данных. С помощью MongoDB мы можем работать с данными, структура которых частично не совпадает.
Теперь посмотрим на содержимое коллекции с помощью команды db.users.find(). Результат показывает, что все данные успешно добавлены:
Проверим, есть ли среди пользователей те, кому больше 37 лет. Сделаем запрос к БД с помощью метода find.
db.users.find({age: {$gt: 37}}); 
Decision:
$ sudo mkdir -p /usr/local/share/ca-certificates/Yandex && \
> sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt
$ wget https://downloads.mongodb.com/linux/mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
$ tar -zxvf mongodb-linux-x86_64-enterprise-ubuntu2004-6.0.2.tgz
$ sudo ln -s  /path/to/the/mongodb-directory/bin/* /usr/local/bin/
$ sudo apt install mongodb-clients
$ mongo --norc \
        --ssl \
        --sslCAFile /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt \
        --host 'rs01/rc1b-b7xwau9lvu3hdt0w.mdb.yandexcloud.net:27018' \
        -u user1 \
        -p 2019Atdhfkz \
        db1
rs01:PRIMARY> db.createCollection("users")
{
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1665235984, 1),
                "signature" : {
                        "hash" : BinData(0,"JH1IX4da7oRrpZn9wNQuuIVg7Zw="),
                        "keyId" : NumberLong("7151968284189917188")
                }
        },
        "operationTime" : Timestamp(1665235984, 1)
}
rs01:PRIMARY> db.users.insertOne({firstName: "Adam", lastName: "Smith", age: 37, email: "adam.smith@test.com"});
{
        "acknowledged" : true,
        "insertedId" : ObjectId("63417c1dbd4e36e89a4d2c78")
}
rs01:PRIMARY> db.users.insertMany( [
...       {firstName: "Viktoria", lastName: "Holmes", age: 73, email: "viktoria.holmes@test.com", phone: "737772727"},
...       {firstName: "Tina", lastName: "Anders", age: 29, email: "tina.anders@test.com", children: [{firstName: "Sam", lastName: "Anders"},{firstName: "Anna", lastName: "Anders"}]}
...    ] );
{
        "acknowledged" : true,
        "insertedIds" : [
                ObjectId("63417c29bd4e36e89a4d2c79"),
                ObjectId("63417c29bd4e36e89a4d2c7a")
        ]
}
rs01:PRIMARY> db.users.find({age: {$gt: 37}});
{ "_id" : ObjectId("63417c29bd4e36e89a4d2c79"), "firstName" : "Viktoria", "lastName" : "Holmes", "age" : 73, "email" : "viktoria.holmes@test.com", "phone" : "737772727" }
Task:
Шардирование
Decision:
Шардирование — это горизонтальное масштабирование данных, при котором данные разбиваются на шарды (т. е. части) и размещаются на разных хостах кластера. Нагрузка на БД при этом распределяется по хостам, что позволяет добиться большей производительности системы, чем если бы она была расположена на одном мощном сервере. Это особенно важно, если данных или запросов к ним очень много.
Плюсы шардирования в том, что оно позволяет:
    Обойти технические ограничения. Если БД работает на пределе производительности — можно разбить её на части и распределить запросы на чтение между ними.
    Ускорить доступ к данным для пользователей из конкретного региона. Например, международные соцсети могут хранить контент на русском языке на серверах ближе к России.
    Выполнить юридические требования. Например, хранить конфиденциальные данные на шарде, публичный доступ к которому отключен.
    Повысить доступность. Если БД находится на одном нешардированном хосте, то его выход из строя приведет к потере всех данных. Если же БД шардирована, то при отказе одного шарда все данные на других остаются доступными.
    Для шардов можно дополнительно настроить репликацию. Так вы обойдетесь без потерь, если сервер выйдет из строя. А размещение реплик шарда в разных зонах доступности даст вам отказоустойчивую систему.
    Ускорить запросы. Они могут выполняться медленнее из-за того, что конкурируют за ресурсы сервера. Шардирование устраняет конкуренцию, исполняя запросы параллельно на разных серверах.
Каким бы замечательным ни было шардирование, у него есть и недостатки:
    Правильно шардировать данные, т. е. разбить их на части — непростая задача. Если вы сделаете это неверно, то мощности серверов будут использоваться нерационально. Например, потребуется много межсерверных запросов.
    Из-за несбалансированного распределения данных между шардами появляются горячие точки — разделы БД, к которым идет намного больше обращений по сравнению с остальными. Запросы к горячим точкам обрабатываются заметно медленнее.
Шардирование в MongoDB
В MongoDB шардирование коллекций поддерживается по умолчанию — части коллекций MongoDB размещаются на разных хостах кластера. На какой шард попадет фрагмент данных, определяет ключ шардирования.
От выбора ключа зависит удобство работы с коллекцией и производительность. Следует логично распределить данные коллекции по шардам. Данные на шардах не должны быть связаны между собой.
Шардировать данные имеет смысл, если:
    Данных много (коллекция больше 200 ГБ).
    Данные неоднородны и четко делятся на примерно одинаковые по объему данных категории.
    Требования к скорости чтения и записи данных высоки. Шардирование распределит нагрузку по хостам, чтобы обойти технические ограничения.
Шардирование доступно для кластеров MongoDB с версией не ниже 4.0. Оно происходит с автоматическим созданием служебных хостов mongos (для маршрутизации запросов пользователей) и mongocfg (для хранения конфигурации шардов), которые тарифицируются отдельно от основных хостов БД.
Сервис поддерживает две основные стратегии шардирования данных: - по хешу (ключ шардирования на базе хеша); - по диапазону значений (ranged sharding).
Отменить шардирование кластера невозможно. Чтобы воссоздать кластер до шардирования, придется сделать его резервную копию, а затем из копии создать новый кластер.
Чтобы повысить доступность, составляйте каждый шард из трёх или более хостов БД.
Task:
Шардирование позволяет изолировать:
Decision:
+отказы хостов
-отказы наборов реплик
-сбой в мастере
Task:
Шард — это:
Decision:
-реплика БД
+часть БД, хранящаяся на хосте
-распределённо хранящаяся коллекция
Task:
Выберите правильные утверждения. Ключ шардирования:
Decision:
+может быть причиной возникновения горячих точек
-нужен для доступа к шарду
+говорит БД о том, где найти или куда записать фрагмент данных
+влияет на скорость чтения и записи данных
Task:
Особенности сервиса управляемых баз данных MongoDB
Decision:
Ответственность сервиса
Как вы могли увидеть, в работе с различными сервисами управляемых БД в облаке есть много общего. При этом, каждый из сервисов имеет свои особенности, обусловленные как самой СУБД, так и спецификой её реализации в облаке.
Как и в случае реляционных БД, сервис управляемых баз данных MongoDB выполняет задачи, связанные с администрированием инфраструктуры:
    при создании кластера выделяет ресурсы, устанавливает СУБД и создает БД;
    обновляет программное обеспечение;
    автоматически создает резервные копии БД;
    предоставляет инструменты мониторинга хостов и БД;
    обеспечивает репликацию данных между хостами;
    при аварии автоматически переключает нагрузку на резервную реплику.
Вы можете выбирать любые клиенты для MongoDB. Yandex Managed Service for MongoDB гарантирует работу интерактивного интерфейса the mongo Shell, а также драйверов для Python и Java.
Обновление базы данных
Сервис поддерживает мажорные версии MongoDB 4.2, 4.4 и 5.0. Переходить на новую мажорную версию следует вручную. При выходе новых минорных версий MongoDB кластеры обновляется автоматически. Владельцы кластеров получают оповещение о сроках работ и доступности БД.
Когда разработчики перестают поддерживать версию MongoDB, создание хостов с ней становится невозможным, а кластеры автоматически обновляются до ближайшей поддерживаемой версии. Это происходит через семь дней после оповещения для минорных версий и через месяц для мажорных.
Резервное копирование
Сервис создает резервные копии БД автоматически, а также позволяет делать это вручную. Резервные копии сохраняются в специальном хранилище в сжатом виде. Вы не платите за их хранение, пока размер БД и всех резервных копий не превышает размера хранилища, который вы выбрали при создании кластера.
По умолчанию автоматическое резервное копирование выполняется раз в день с 01:00 по 05:00 по московскому времени, а резервные копии хранятся неделю. Время начала резервного копирования и срок хранения резервных копий (от 7 до 35 дней) можно установить при создании или изменении кластера.
Автоматически созданные копии удаляются, когда истекает срок хранения. Копии, созданные вручную, хранятся бессрочно.
Из резервной копии можно восстановить как существующий, так и удаленный кластер. Средняя скорость восстановления — 10 МБ/с.
Для версий MongoDB 4.2 и выше сервис поддерживает технологию point-in-time recovery (восстановление состояния кластера на заданный момент). Вы можете восстановить кластер в состояние на любой момент времени после создания самой старой полной резервной копии. Это достигается за счет дополнения данных выбранной резервной копии записями из архивируемого журнала операций.
Point-in-time recovery работает только для кластеров с выключенным шардированием.
Мониторинг и логи
Сервис отслеживает и выводит на дашборд следующие метрики: общий объём данных, размер журнала операций, место на диске, число подключений к базе, сессий и операций, лаг репликации и пр. Подробнее о мониторинге Yandex Managed Service for MongoDB вы можете узнать в документации.
Кроме того, сервис ведет запись логов событий.
Репликация и отказоустойчивость
Сервис управляемых БД MongoDB по умолчанию реплицирует данные, т. е. копирует их на несколько хостов. Если в кластере больше одного активного хоста, из них автоматически выбирается главный — первичная реплика. Этот хост обрабатывает все запросы на запись.
Первичная реплика выбирается автоматически, если в кластере работоспособно больше половины хостов. По этой причине кластер из двух хостов не обеспечивает отказоустойчивости. Если первый хост отказал, второй не сможет назначить сам себя первичной репликой и будет обрабатывать только операции чтения.
Кластер из трёх хостов продолжит обрабатывать операции записи при потере одного хоста. Кластер из четырёх хостов также может потерять только один хост: при потере второго оставшихся хостов не хватит, чтобы автоматически выбрать первичную реплику. Поэтому в MongoDB лучше разворачивать кластеры с нечетным числом хостов.
Лимиты и тарификация
Лимиты сервиса таковы.
В кластере БД MongoDB можно создать не более 10 шардов. Шард состоит не более чем из семи хостов. Таким образом, максимальное число хостов в одном кластере не может превышать 70.
Максимальное число одновременных подключений к одному хосту зависит от объёма его оперативной памяти: не более 2048 подключений на каждые 2 ГБ.
Максимальный объём данных на хосте — 605 ГБ при использовании сетевого хранилища или 600 ГБ при использовании локального хранилища.
MongoDB тарифицируется по тем же принципам, что и управляемые реляционные БД. Об особенностях тарификации этого сервиса вы можете узнать из документации.
Task:
Вы используете для своего приложения управляемую базу данных MongoDB. Кластер состоит из одного хоста s2.large (12 vCPU, 48 ГБ) с быстрым сетевым хранилищем объёмом 50 ГБ.
С учётом растущей нагрузки на кластер и далеко идущих планов по развитию приложения вы решаете разбить БД на два шарда, понизив при этом класс хостов до s2.medium (8 vCPU, 32 ГБ) и увеличив объём хранилища до 100 ГБ, а также сделать БД отказоустойчивой.
Сколько хостов в кластере вам для этого понадобится?
Decision:
-2 (два хоста для шардов)
-3 (два хоста для шардов и еще один для реплики)
-4 (два хоста для шардов и еще по одной реплике на каждый шард)
-5 (два хоста для шардов и еще три реплики, потому что хостов должно быть нечетное число)
+6 (два хоста для шардов и еще по две реплики на каждый шард)
Task:
Сколько примерно времени займет восстановление кластера размером 10 ГБ из резервной копии?
Decision:
-1,5 минуты
+17 минут
-34 минуты
Task:
Описание ClickHouse
Decision:
В этой теме вы узнаете о сервисе управляемых баз данных ClickHouse. Эта БД предназначена для задач, связанных с аналитической обработкой данных, и не подходит там, где основная часть операций — обработка транзакций. Чтобы разобраться в том, почему это так, давайте сначала разберёмся с различными сценариями работы с данными.
Базы данных помогают решать различные задачи. То, какие при этом делаются запросы, насколько их много и как соотносятся операции чтения и записи, называют сценарием работы с данными. Универсальной БД, которая подходит для любого сценария, не существует.
Сценарии можно разделить на две группы:
    Обработка транзакций, т. е. связанных между собой операций с данными. Классический пример — банковский перевод, при котором в БД одновременно изменяются записи о количестве денег на двух счетах.
    Обработка аналитических запросов (online analytical processing, OLAP). В этом случае требуется быстро извлечь из БД сведения.
Например, если у вас есть мобильное приложение, то вас интересуют его продуктовые метрики: количество уникальных пользователей, среднее время нахождения на экране, среднее время, необходимое, чтобы выполнить последовательность действий... Вы отслеживаете, как меняются метрики, и решаете, как развивать приложение. Если метрики хранятся в большой БД, а запросы к ней выполняются медленно, то анализ метрик тоже станет весьма небыстрым.
Как правило, при OLAP-сценариях:
    данные в базе не изменяются или изменяются редко (нет команд модификации существующих данных типа UPDATE или REPLACE);
    данные добавляются в базу крупными порциями (командой INSERT);
    большинство запросов — это операции чтения;
    данные читаются из большого количества строк и небольшого количества столбцов;
    на выходе данные фильтруют или агрегируют, поэтому результат выполнения запроса содержит гораздо меньше исходных данных;
    нет транзакций;
    нет строгих требований к консистентности данных.
Классические реляционные БД не всегда удобно использовать для задач, в которых идут частые и сложные запросы к большому массиву данных. Для решения таких задач разработаны столбцовые (колоночные) БД. О них мы кратко говорили раньше.
Строковые и столбцовые БД обрабатывают аналитические запросы по-разному. Предположим, для выполнения запроса нужны данные из трёх  столбцов БД. В строковой придётся полностью прочитать несколько десятков или сотен тысяч строк со всеми столбцами, а в столбцовой — только данные из этих трёх столбцов.
Посмотрите на различие в обработке запросов в строковых и столбцовых БД:
Более того, в столбцовой БД эти данные физически хранятся вместе, что ещё больше ускоряет ответ.
ClickHouse
ClickHouse — одна из популярных столбцовых БД с открытым исходным кодом. Яндекс создал ClickHouse, когда понадобилось быстро обрабатывать аналитические онлайн-запросы к Яндекс Метрике.
Метрика — это одна из крупнейших систем веб-аналитики. Она установлена более чем на миллионе сайтов и каждый день собирает больше 20 миллиардов событий (посещений сайтов, кликов, переходов со страницы на страницу и т. д.). Объём данных превышает 3,5 петабайта (13 триллионов записей) и постоянно увеличивается, к этим данным обращаются сотни тысяч раз в день. Чтобы такая система работала стабильно, в Яндексе создали ClickHouse — распределённую столбцовую СУБД, оптимизированную для быстрого выполнения большого числа аналитических запросов к огромному объёму данных.
ClickHouse работает на любой операционной системе Linux, FreeBSD или macOS, а также доступна в виде сервиса управляемой БД в Yandex Cloud.
ClickHouse позволяет создавать БД и таблицы, загружать в них данные из разных источников и выполнять к данным запросы. Эту СУБД можно интегрировать с Apache Kafka, а также с внешними источниками данных, включая БД MySQL и PostgreSQL.
Высокая скорость работы ClickHouse достигается за счёт:
    Шардирования. Вы можете разделить данные на шарды (т. е. части) и хранить их на одном или нескольких хостах-репликах. Кроме того, шардирование повышает доступность БД.
    Автоматического распараллеливания запросов на несколько процессорных ядер одного сервера и распределённых вычислений на шардированном кластере.
    Возможности приближенных вычислений. Система способна выполнять запросы на основе части данных (выборки). Можно агрегировать данные не по всем ключам, а по некоторым. Иногда это помогает получить довольно точный результат, задействовав меньше ресурсов.
    Других архитектурных особенностей: индекса (первичного ключа), физической сортировки данных по первичному ключу с помощью merge дерева, хранения данных в сжатом виде и т. д.
ClickHouse предназначен прежде всего для работы с аналитическими запросами. Если вам нужны транзакционная целостность и построчная выборка данных по ключу — применяйте другие БД, например MySQL или PostgreSQL.
Эту СУБД используют:
    чтобы анализировать логи (и мгновенно получать полную информацию об инцидентах в системе);
    чтобы отслеживать метрики поведения пользователей на сайтах (например, переходы на страницы, клики) или в онлайн-играх;
    как аналитический инструмент, когда данные в БД ClickHouse копируются из основной БД (например, PostgreSQL или Oracle), которая медленно обрабатывает аналитические запросы.
Если вам интересны подробности о том, как устроен ClickHouse и что у неё под капотом, посмотрите доклады разработчиков:
    Виктор Тарнавский. ClickHouse: как сделать самую быструю распределённую аналитическую СУБД.
    Как работает ClickHouse. Лекция в ШАД.
Task:
Какие особенности ClickHouse ускоряют работу с аналитическими запросами?
Decision:
+Поддержка приближенных вычислений
+Шардирование БД
+Распараллеливание выполнения запросов
-Хранение данных на быстрых жёстких дисках
Task:
Практическая работа. Создание кластера ClickHouse и подключение к нему
Decision:
Создание кластера
В этой практической работе вы создадите кластер ClickHouse. Вы уже знаете, как создавать кластеры и выставлять их основные настройки в сервисах платформы данных. Но у БД ClickHouse есть свои особенности.
Когда вы создадите кластер из двух или более хостов, сервис дополнительно создаст ещё один кластер из трёх хостов, где развернёт Apache ZooKeeper. Это служба для распределенных систем, которая управляет конфигурацией, репликацией и распределением запросов по хостам БД. Без неё кластер ClickHouse работать не будет. К ZooKeeper у пользователей доступа нет, однако его хосты учитываются при расчёте квоты ресурсов облака и стоимости сервиса.
ZooKeeper синхронизирует шарды (т. е. хосты) ClickHouse. В отличие от классических реляционных БД, у ClickHouse нет главного узла (мастера), через который добавляются данные. В ClickHouse данные можно и записывать, и читать с любого узла.
Давайте приступим к практике. Перейдите в каталог, где нужно создать кластер БД, выберите Managed Service for ClickHouse и нажмите кнопку Создать кластер.
Для практической работы нам понадобится кластер с минимальной конфигурацией: тип хоста burstable, класс b2.nano и стандартное сетевое хранилище размером 10 ГБ.
Задайте настройки: введите имена для кластера и БД, а также имя и пароль пользователя. Откройте публичный доступ к хосту.
Обратите внимание: в отличие от сервисов, которые мы уже рассматривали, здесь в разделе База данных можно включить опции управления пользователями и БД с помощью SQL-запросов.
Кроме того, в дополнительных настройках можно включить доступ к БД из консоли управления, сервисов DataLens, Яндекс Метрики и AppMetrica, а также возможность использовать бессерверные вычисления (подробно о них мы расскажем на курсе «Serverless»). С помощью DataLens, например, вы визуализируете результаты поисковых запросов в виде графиков, диаграмм и дашбордов, а подключение AppMetrica позволит импортировать данные из этого сервиса в кластер.
Отметьте пункт Доступ из DataLens: он понадобится вам на одном из следующих уроков.
Нажмите кнопку Создать кластер.
Подключение к базе данных
К хостам кластера ClickHouse можно подключаться через интернет или с виртуальных машин в той же виртуальной сети. Если к хостам БД открыт публичный доступ, то для подключения к ним используется шифрованное соединение.
Подключайтесь к кластеру с помощью HTTP-протокола или более низкоуровневого Native TCP-протокола. В большинстве случаев рекомендуется взаимодействовать с ClickHouse не напрямую, а с помощью инструмента или библиотеки. Официально поддерживаются консольный клиент, драйверы JDBC и ODBC, клиентская библиотека для C++. Также можно использовать библиотеки сторонних разработчиков для Python, PHP, Go, Ruby и т. д.
Примеры строк подключения приводятся в документации и консоли управления на вкладке Обзор страницы кластера.
С БД удобно работать в приложении с графическим интерфейсом. Один из вариантов — универсальный клиент DBeaver. Другие варианты вы найдёте в полном списке клиентов.
Подробная информация о настройке подключения приведена в документации. Чтобы создать подключение к ClickHouse в DBeaver, помимо обычных параметров (адреса хоста, порта, имени БД, логина и пароля) задайте на вкладке Свойства драйвера настройки свойств драйвера JDBC. Укажите следующие параметры: ssl = true; sslmode = strict; sslrootcert = <путь к SSL-сертификату>. Как получить SSL-сертификат, вы уже узнали на предыдущих уроках.
При подключении DBeaver покажет номер версии ClickHouse и пинг до хоста.
В двух следующих практических работах мы используем кластер для аналитической работы с датасетами и для создания БД ClickHouse.
Decision:
$ wget https://dbeaver.io/files/dbeaver-ce_latest_amd64.deb
$ sudo dpkg -i dbeaver-ce_latest_amd64.deb
$ dbeaver-ce &
$ mkdir -p ~/.clickhouse-client /usr/local/share/ca-certificates/Yandex
$ sudo wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" -O /usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt
$ sudo wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" -O ~/.clickhouse-client/config.xml
$ sudo apt install -y clickhouse-client
$ clickhouse-client --host rc1a-mg8yquor7pspcwkc.mdb.yandexcloud.net \
>                   --secure \
>                   --user user1 \
>                   --database db1 \
>                   --port 9440 \
>                   --ask-password

Не работает !!!!!!!!!!!!



Task:
Вы планируете поместить БД ClickHouse в облаке на четырех шардах и создаёте кластер с четырьмя хостами. Сколько хостов будет при этом создано? В поле ответа напишите число.
Decision:
7
Task:
Практическая работа. Работа с данными из объектного хранилища
Decision:
В интернете выложено множество датасетов —  структурированных наборов данных, связанных общей темой. Например в репозитории проекта Our World in Data находится около тысячи разнообразных датасетов: от численности населения государств до сведений об употреблении алкоголя в США с 1850 года.
Датасеты часто выкладывают в виде CSV- или TSV-файлов. В них значения разделены запятой (comma separated values, CSV) или табуляцией (tab separated values, TSV).
Сохраняйте датасеты в объектное хранилище и анализируйте данные с помощью ClickHouse. При этом не требуется создавать БД и копировать в неё данные из датасета. Отправляйте запросы к ClickHouse — а ClickHouse сходит за данными напрямую в объектное хранилище.
В качестве примера возьмем датасет с историей метеонаблюдений за 10 лет и попробуем развеять мифы о разнице погоды в Москве и Санкт-Петербурге. Датасет содержит примерно 50 тысяч записей, он выложен в объектном хранилище Yandex Cloud и доступен всем.
Воспользуемся кластером БД, который мы создали на предыдущем уроке. Откройте его в консоли управления. Запросы к датасету будем делать через SQL-консоль. На панели слева выберите вкладку SQL и введите пароль пользователя. В правом поле открывшейся консоли мы и станем вводить SQL-запросы.
Как вы думаете, где зарегистрирована самая низкая температура? Наверняка в Санкт-Петербурге! Давайте проверим.
Выполните запрос:
SELECT
    City,
    LocalDate,
    TempC
FROM s3(
        'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
        'TSV',
        'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
ORDER BY TempC ASC
LIMIT 1 
Всё-таки наши интуитивные представления не всегда верны и могут опровергаться данными.
А что насчет самой высокой температуры, скорости ветра и влажности? Проверьте сами, изменив поля в запросе (средняя скорость ветра за 10 минут — WindSpeed10MinAvg, относительная влажность — RelHumidity; сортировка по возрастанию — ASC, по убыванию — DESC). Увеличив количество выводимых данных, вы получите более точное представление (измените параметр LIMIT c 1 до 10).
Но это были крайние значения. Давайте проверим, насколько в этих городах отличается климат в целом. Узнаем, например, разницу среднегодовых температур.
SELECT
    Year,
    msk.t - spb.t
FROM
(
    SELECT
        toYear(LocalDate) AS Year,
        avg(TempC) AS t
    FROM s3(
        'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
        'TSV',
        'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
    WHERE City = 'Moscow'
    GROUP BY Year
    ORDER BY Year ASC
) AS msk
INNER JOIN
(
    SELECT
        toYear(LocalDate) AS Year,
        avg(TempC) AS t
    FROM s3(
        'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
        'TSV',
        'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
    WHERE City = 'Saint-Petersburg'
    GROUP BY Year
    ORDER BY Year ASC
) AS spb ON msk.Year = spb.Year 
Измените поля в запросе, чтобы проверить разницу относительной влажности.
Давайте теперь рассчитаем, где раньше начинается лето. Будем считать началом лета день, начиная с которого температура поднималась выше +15 °С хотя бы пять раз в течение 10-дневного периода (864 тысячи секунд).
SELECT
    City,
    toYear(LocalDate) AS year,
    MIN(LocalDate)
FROM
(
    SELECT
        City,
        LocalDate,
        windowFunnel(864000)(LocalDateTime, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15, TempC >= 15) AS warmdays
    FROM s3(
        'https://storage.yandexcloud.net/arhipov/weather_data.tsv',
        'TSV',
        'LocalDateTime DateTime, LocalDate Date, Month Int8, Day Int8, TempC Float32,Pressure Float32, RelHumidity Int32, WindSpeed10MinAvg Int32, VisibilityKm Float32, City String')
    GROUP BY
        City,
        LocalDate
)
WHERE warmdays = 5
GROUP BY
    year,
    City
ORDER BY
    year ASC,
    City ASC 
Практическая работа. Добавление данных
Decision:
Предположим, вы работаете в метеорологической службе и постоянно изучаете датасеты с погодными данными. Сбор данных о погоде автоматизирован: на территории области расположены несколько десятков пунктов наблюдения с датчиками. Информация о температуре, давлении, влажности и скорости ветра раз в полчаса передаётся с датчиков на центральный сервер. Приложение на сервере обрабатывает данные, переводит их в нужный формат и записывает в файл. Каждый файл содержит данные за три часа наблюдений. Для прогноза нужно учитывать всю историю наблюдений за последние несколько лет, то есть все файлы потребуется собрать в одну БД.
Давайте потренируемся добавлять данные из файлов в БД ClickHouse.
На предыдущих уроках мы создали кластер, на котором развёрнута БД, и научились подключаться к нему. Продолжим работать с этой БД, а в качестве добавляемого файла возьмем уже известный вам датасет с данными о погоде в Москве и Санкт-Петербурге.
Сохраните файл на компьютере.
Прежде чем добавлять файл в БД, создадим в ней таблицу, куда будут вставляться данные. Перейдите в SQL-консоль кластера и выполните команду:
CREATE TABLE <имя вашей БД>.Weather
(  LocalDateTime DateTime,
   LocalDate Date,
   Month Int8,
   Day Int8,
   TempC Float32,
   Pressure Float32,
   RelHumidity Int32,
   WindSpeed10MinAvg Int32,
   VisibilityKm Float32,
   City String
) ENGINE=MergeTree
ORDER BY LocalDateTime; 
В результате будет создана пустая таблица с полями и типами данных, соответствующими полям и типам данных в нашем файле (датасете).
Вставим данные в таблицу с помощью клиента командной строки clickhouse-client. Команды для его установки (для Ubuntu):
sudo apt update && sudo apt install --yes apt-transport-https ca-certificates dirmngr && \
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 && \
echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee \
/etc/apt/sources.list.d/clickhouse.list
sudo apt update && sudo apt install --yes clickhouse-client
mkdir --parents ~/.clickhouse-client && \
wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" \
--output-document ~/.clickhouse-client/config.xml 
Подробности о том, как установить клиент и работать с ним, вы найдёте в документации ClickHouse.
Подключитесь к кластеру. Пример строки подключения посмотрите в консоли управления.
Добавим файл с данными в БД с помощью команды:
cat weather_data.tsv | clickhouse-client \
--host <адрес вашей БД> \
--secure \
--user user1 \
--database db1 \
--port 9440 \
-q "INSERT INTO db1.Weather FORMAT TabSeparated" \
--ask-password 
Переключившись в SQL-консоль, вы увидите, что данные появились в таблице.
Данные в БД можно загружать и другими способами: из приложений или клиентов с графическим интерфейсом (например DBeaver). В этом случае подключение к БД и передача данных будут идти по HTTP-протоколу через порт 8443.
Теперь вы можете анализировать 10-летний срез данных о погоде в Москве и Санкт-Петербурге непосредственно в ClickHouse, без обращений к внешним источникам. Попробуйте, например, выяснить, какой день был самым ветреным в этих городах.
После практической работы остановите кластер, но не удаляйте его. Кластер ещё понадобится, когда мы будем рассматривать сервис визуализации и анализа данных Yandex DataLens.
Decision:
$ sudo apt update && sudo apt install --yes apt-transport-https ca-certificates dirmngr && \
> sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 && \
> echo "deb https://repo.clickhouse.com/deb/stable/ main/" | sudo tee \
> /etc/apt/sources.list.d/clickhouse.list
$ sudo apt update && sudo apt install --yes clickhouse-client
$ mkdir --parents ~/.clickhouse-client && \
> wget "https://storage.yandexcloud.net/mdb/clickhouse-client.conf.example" \
> --output-document ~/.clickhouse-client/config.xml
$ wget https://storage.yandexcloud.net/arhipov/weather_data.tsv
$ cat weather_data.tsv | clickhouse-client \
> --host rc1a-mg8yquor7pspcwkc.mdb.yandexcloud.net \
> --secure \
> --user user1 \
> --database db1 \
> --port 9440 \
> -q "INSERT INTO db1.Weather FORMAT TabSeparated" \
> --ask-password
Task:
Особенности сервиса управляемых баз данных ClickHouse
Decision:
На этом уроке мы рассмотрим особенности реализации СУБД ClickHouse в Yandex Cloud.
Типы хранилища
При создании кластера ClickHouse вы можете выбрать быстрое локальное (local-ssd), быстрое сетевое (network-ssd), стандартное сетевое (network-hdd) хранилище или хранилище на нереплицируемых SSD-дисках (network-ssd-nonreplicated).
Сетевые хранилища используют обычные сетевые диски: такие же, как в виртуальных машинах.
Стандартное сетевое хранилище заметно дешевле других вариантов. Но если скорость доступа к БД вас не устраивает, то выбирайте быстрое сетевое или быстрое локальное хранилище.
Нереплицируемые диски — это, по сути, быстрые сетевые хранилища на SSD-дисках, для которых не предусмотрена репликация на уровне облака. Производительность системы повышается за счет снижения надёжности хранения данных. Но для управляемых БД этот недостаток не критический: репликацию можно настроить на уровне сервиса.
Особенность локального хранилища заключается в том, что если локальный диск откажет, все сохранённые на нём данные будут потеряны. Поэтому, при выборе такого хранилища сервис автоматически создаст отказоустойчивый кластер минимум из двух хостов.
Также вы можете использовать Гибридное хранилище. Часто используемые, горячие данные хранятся на дисках сетевого хранилища, а редко используемые, холодные — в объектном хранилище Yandex Cloud.
Обновления СУБД
Релизы ClickHouse выходят довольно часто. Поэтому сервис управляемых БД ClickHouse использует небольшой набор версий СУБД и регулярно его актуализирует. Кластеры с устаревшей, т. е. уже не поддерживаемой версией ClickHouse автоматически обновляются.
Сервис использует два типа версий: с расширенным периодом поддержки (LTS — long term support) и промежуточные.
LTS-версии выходят раз в полгода. Поддерживаются только две таких версии: текущая и предыдущая. То есть поддержка LTS-версии длится один год. Неподдерживаемая LTS-версия обновляется сразу до текущей.
При выходе новой промежуточной версии прекращается поддержка самой старой из них. При этом одновременно поддерживается не более трёх промежуточных версий.
Резервное копирование
Сервис каждый день автоматически выполняет резервное копирование БД, а также позволяет создавать резервные копии вручную. Они записываются в хранилище данных. Стоимость хранения копий смотрите в правилах тарификации. Сейчас хранение не тарифицируется, пока суммарный размер БД и всех резервных копий не превышает выбранного размера хранилища. Любые копии (и автоматически, и вручную созданные) гарантированно хранятся семь дней.
Любые резервные копии делаются по инкрементальной схеме: если хотя бы в одной копии есть идентичные фрагменты данных и эти фрагменты не старше 30 дней, то они не дублируются.
Данные в резервной копии хранятся только для таблиц, использующих движки семейства MergeTree. Это наиболее функциональные движки таблиц ClickHouse. Когда в таблицу на таком движке вставляется большое количество данных, эти данные записываются частями, которые затем объединяются по определённым правилам в фоновом режиме. Для остальных движков в резервной копии хранятся лишь схемы таблиц.
Шардирование
Шардирование используется для горизонтального масштабирования кластера, при котором части одной БД ClickHouse размещаются на разных шардах. Шард — это один или несколько хостов-реплик. Запрос на запись или чтение в шард можно отправить на любую его реплику, выделенного мастера нет.
Чтобы распределить данные по шардам, нужно создать распределённую таблицу. Части данных фактически хранятся в нижележащих таблицах на хостах каждого шарда, а распределённая таблица маршрутизирует запросы к этим таблицам.
ClickHouse определяет, на какой шард поместить новые данные, с помощью ключа шардирования. Выбирайте ключ так, чтобы данные логично распределялись по шардам и данные разных шардов не были связаны между собой.
Кластеры управляемых БД ClickHouse изначально создаются с одним шардом. Чтобы воспользоваться преимуществами шардирования, вам понадобится добавить еще один или несколько шардов и создать распределённую таблицу.
Словари
Словарь — это хранилище данных типа «ключ-значение», которое полностью или частично находится в оперативной памяти сервера ClickHouse.
Основное преимущество словарей — высокая скорость работы по сравнению с операциями JOIN. Словари полезны, когда приходится часто обращаться к справочнику, чтобы получить набор значений по ключу.
В качестве источников данных словарей могут выступать встроенные словари ClickHouse или внешние источники: HTTP-ресурсы или другая БД (MySQL, ClickHouse, MongoDB, PostgreSQL).
Сервис также содержит встроенный словарь-геобазу и набор функций для работы с ним. Геобаза позволяет:
    получить имя региона по его идентификатору на нужном языке;
    получить идентификатор города, области, федерального округа, страны, континента по идентификатору региона;
    проверить, что один регион входит в другой;
    получить цепочку родительских регионов.
Подробнее о функциях для работы со встроенными словарями читайте в документации.
Если встроенная геобаза вам не подходит, подключите к ClickHouse собственную геобазу.
Машинное обучение
Сервис позволяет анализировать данные с помощью моделей машинного обучения CatBoost без использования дополнительных инструментов. Чтобы применить модель, подключите её к кластеру и вызовите в SQL-запросе с помощью встроенной функции modelEvaluate(). В результате выполнения запроса модель выдаст предсказания для каждой строки входных данных. Подробная информация о машинном обучении в ClickHouse приводится в документации.
Тарификация
При планировании расходов на кластер ClickHouse учитывайте то, что при создании кластера из двух и более хостов автоматически создаётся ещё три хоста ZooKeeper. Хосты ZooKeeper тарифицируются по тому же принципу, что и обычные: за время использования ядер процессора и оперативной памяти.
Task:
Вы планируете создать кластер ClickHouse. Вы хотели бы минимизировать затраты и полагаете, что для вашей задачи достаточно одного хоста. Для вас важна скорость выполнения запросов, а отказоустойчивость — не очень (при этом вы, естественно, не хотите потерять данные). Какой тип хранилища стоит выбрать при создании кластера?
Decision:
-стандартное локальное
-стандартное сетевое
-быстрое локальное
+быстрое сетевое
Task:
Краткий обзор Yandex Database
Decision:
На предыдущих уроках мы рассматривали и классические реляционные, и NoSQL базы данных. Каждый из этих типов БД имеет свои сильные стороны и области применения.
Классические реляционные БД хорошо подходят для сценариев работы, связанных с обработкой транзакций (online transaction processing, OLTP). Они обеспечивают выполнение транзакционных требований ACID, что особенно важно для систем, связанных с финансами или обработкой заказов. Сложности возникают, когда данных становится много, поскольку эти БД плохо масштабируются.
Базы данных NoSQL изначально создавались как распределённые системы. Поэтому они легко масштабируются и обеспечивают высокую доступность данных. Однако это имеет свою цену: требования ACID, в частности целостность и надёжность, в этих БД выполняются не полностью.
Стремление объединить преимущества классических реляционных и NoSQL баз данных привели к созданию нового типа СУБД, который назвали NewSQL или Distributed SQL. Это распределённые реляционные базы данных, которые одновременно обеспечивают строгое выполнение требований ACID и хорошее горизонтальное масштабирование.
К таким БД относится и Yandex Database (YDB), которая была разработана компанией Yandex для решения внутренних задач, а с 2019 года стала доступна для внешних пользователей. Она спроектирована с учетом высоких требований к производительности (миллионы запросов в секунду) и объёму данных (сотни петабайт). YDB используется для хранения данных сервисов Yandex Cloud, а также во многих сервисах Yandex (например, Яндекс Go, Яндекс Погода, Яндекс Маркет, Авто.ру, Кинопоиск и других).
YDB обеспечивает:
    строгую консистентность с возможностью ослабления для увеличения производительности;
    высокую доступность БД с автоматической обработкой отказов вычислительных узлов и дата-центров;
    автоматическую репликацию БД;
    автоматическое партиционирование данных при увеличении их объёма или росте нагрузки.
YDB поддерживает высокопроизводительные распределенные ACID-транзакции, которые могут затрагивать несколько записей из разных таблиц. При этом обеспечивается самый строгий уровень изоляции транзакций — serializable. Если возникает необходимость увеличить производительность БД или уменьшить задержки, можно ослабить уровень изоляции для операций чтения.
YDB основана на реляционной модели данных и оперирует таблицами с предопределённой схемой. Таблицы в YDB всегда имеют один или несколько столбцов, являющихся первичным ключом.
Для удобной организации хранения таблиц в YDB используется дерево папок (директорий), в которых находятся таблицы и другие сущности БД. По аналогии с обычными файловыми системами в одной директории могут быть несколько поддиректорий и несколько таблиц. Имена у сущностей внутри одной директории должны быть уникальны.
YDB может использоваться в двух режимах: dedicated и serverless.
В dedicated режиме для БД выделяются отдельные хосты. Вы самостоятельно задаете число и класс хостов, параметры хранилища данных и облачных сетей. Иными словами, в этом режиме вы работаете с YDB как с сервисом управляемой БД.
В этом режиме вам будет нужно самостоятельно отслеживать, насколько выделенные вычислительные ресурсы соответствуют нагрузке на БД. Если их будет недостаточно, то скорость обработки запросов будет снижаться вплоть до отказа в обслуживании. Чтобы избежать этого, понадобится самостоятельно наращивать мощности (увеличивать число и/или повышать класс хостов, увеличивать объём хранилища данных).
Тарификация в dedicated режиме основана на тех же принципах, что и в сервисах управляемых БД. Вы платите за:
    время работы вычислительных ресурсов (в зависимости от числа и класса используемых хостов);
    хранилище данных (с учетом типа и размера групп хранения);
    хранение резервных копий БД в объектном хранилище;
    трафик из Yandex Cloud в интернет.
В serverless, или бессерверном, режиме БД разворачивается без создания отдельных хостов. Все необходимые для работы БД ресурсы (вычислительные мощности, хранилище данных, сети) предоставляются сервисом автоматически и так же автоматически масштабируются в зависимости от нагрузки.
Этот режим подходит для небольших проектов, когда отдельный сервер не нужен, или для задач с неравномерной и плохо прогнозируемой нагрузкой. При его использовании вы платите за операции с БД (чтение и запись данных), а также за хранение данных и резервных копий.
С работой YDB в бессерверном режиме вы подробно познакомитесь в курсе «Serverless».
Для работы с данными в Yandex Database используется декларативный язык запросов YQL, являющийся диалектом SQL. В бессерверном режиме также доступен Document API — HTTP API, совместимый с Amazon DynamoDB. С его помощью можно выполнять операции над документными таблицами (на уровне приложений для работы с такими таблицами используется формат JSON).
YDB является альтернативой имеющимся решениям в следующих случаях:
    при использовании NoSQL систем, когда нужна строгая консистентность данных;
    при использовании NoSQL систем, когда требуется транзакционное изменение данных, хранящихся в разных строках одной или нескольких таблиц;
    в системах, где нужно обрабатывать и хранить большой объём данных с возможностью бесконечного масштабирования;
    в системах с незначительной нагрузкой, когда держать для БД отдельный хост нерационально с финансовой точки зрения (в этом случае YDB используется в бессерверном режиме);
    в системах с плохо предсказуемой или сезонно меняющейся нагрузкой;
    в системах, где важно распределять нагрузку между инстансами реляционной БД.
    Task:
    Модель и схема данных в Yandex Database, запросы и транзакции
    Decision:
    В этом уроке мы более подробно разберем, как в Yandex Database организованы данные, каких типов они могут быть, а также познакомимся с поддерживаемыми режимами транзакций.
Модель и схема данных
YDB основана на реляционной модели данных. Это означает, что данные организованы в виде связанных друг с другом таблиц, которые состоят из столбцов и строк.
Таблицы хранят записанную в БД информацию об объектах или сущностях. В столбцах таблицы записываются определённые типы данных, а в ячейках — значения. Каждая строка таблицы представляет собой набор связанных значений, относящихся к одному объекту или сущности.
Таблица YDB всегда имеет один или несколько столбцов, являющихся первичным ключом (primary key). Первичный ключ представляет собой уникальный идентификатор строки, то есть для одного значения ключа может быть не больше одной строки. Допускаются таблицы, состоящие только из ключевых столбцов.
Таблицы без первичного ключа создавать нельзя!
В YDB таблицы всегда упорядочены по ключу. Это приводит к тому, что точечное чтение по ключу и диапазонные запросы по ключу или префиксу ключа фактически выполняются с использованием индекса, то есть с высокой эффективностью.
Схема данных определяет имена (names) столбцов таблицы и то, какие типы данных (types) в них хранятся. Пример схемы данных приведен на рисунке.
image
Здесь изображена схема таблицы series, которая состоит из четырех столбцов с именами series_id, release_date, series_info и title. Первый столбец имеет тип данных Uint64, второй — Date, а последние два — Utf8. В качестве первичного ключа (PK) объявлен столбец series_id.
Партиционирование таблиц
Первичный ключ также используется в YDB для партиционирования таблиц, то есть разбиения их на несколько частей. Каждая часть — партиция — включает отдельный диапазон первичных ключей, то есть диапазоны ключей, обслуживаемых разными партициями, не пересекаются.
Партиции одной таблицы могут располагаться на разных, в том числе расположенных в разных локациях, хостах распределённой БД. Они могут перемещаться между хостами независимо друг от друга для перебалансировки данных и равномерного распределения нагрузки, а также для поддержания работоспособности партиции при отказах серверов или сетевых сбоях.
Если данных немного, таблица может состоять из одной партиции. YDB автоматически разобьёт партицию на две части, когда объём данных в ней или нагрузка увеличатся сверх определенного предела. Включение и выключение автоматического разделения можно настроить для каждой таблицы БД индивидуально.
Помимо автоматического разделения таблиц на партиции YDB дает возможность создавать пустые таблицы с предопределённым количеством партиций. Для этого нужно либо задать границы диапазонов ключей вручную, либо указать, что таблица разделяется на заданное число партиций равномерно. В последнем случае границы диапазонов будут созданы по первой компоненте первичного ключа, которая должна быть целым числом.
Типы данных
Тип данных определяет множество значений, которые эти данные могут принимать, и то, какие операции можно выполнять со значениями. Например, данные логического типа (Bool) могут принимать значения true и false, и с ними можно выполнять логические операции И, ИЛИ, НЕ, ИСКЛЮЧАЮЩЕЕ ИЛИ, а также операции сравнения.
Для работы с данными в YDB применяется декларативный язык запросов YQL (Yandex Query Language). Соответственно, в YDB можно использовать те типы данных, которые есть в YQL. Некоторые из типов данных YQL поддерживаются в YDB с ограничениями: они могут использоваться только в вычислениях, но не могут быть типом столбца или первичным ключом.
Элементарными или примитивными типами данных называют такие, для которых значения нельзя разделить на несколько значений другого типа. К таким типам данных относят, например:
    логический – Bool;
    числовой – IntXX, UintXX, Float, Double (целые знаковые и беззнаковые числа, где XX показывает разрядность в битах; числа с плавающей точкой; числа двойной точности);
    строковый – String, Utf8, Json, Uuid (произвольные бинарные данные, текст, форматы данных);
    временной – Date, Datetime, Timestamp, Interval (дата, дата и время с точностью до секунд, временная отметка и интервал с точностью до микросекунд).
Композитные типы данных, или контейнеры, состоят из данных других элементарных или композитных типов. Такие типы данных в YQL включают:
    список (List) – последовательность из 0 и более элементов одного типа. Количество элементов не фиксировано.
    кортеж (Tuple) – упорядоченный набор из 0 и более элементов произвольных типов. Количество элементов фиксировано.
    структуру (Struct) – именованный неупорядоченный набор из 0 или более элементов произвольных типов. Количество элементов фиксировано.
    словарь (Dict) – набор пар «ключ-значение». Все значения ключей уникальны. Все ключи имеют один и тот же тип, как и значения. Количество элементов не фиксировано.
    опциональный тип (Optional) – последовательность из 0 или 1 элементов некоторого типа. Случай, когда в последовательности 0 элементов, изображается как NULL.
    вариант (Variant) – подвид кортежа или структуры, в котором заполнен ровно один элемент.
Все столбцы, в том числе столбцы с первичным ключом, также могут содержать специальное значение NULL, которое используется, чтобы обозначить отсутствие значения.
С полным перечнем типов данных, а также возможных операций над ними вы можете познакомиться в документации.
Запросы и транзакции
Основным средством добавления, обновления и удаления строк данных, извлечения наборов данных, а также управления работой БД в YDB является декларативный язык запросов YQL. Для выполнения YQL запросов можно использовать консоль управления Yandex Cloud в браузере, консольный клиент для командной строки, а также SDK для различных языков программирования, которые позволяют встраивать запросы в приложения.
Запрос – это команда на выполнение операции чтения или записи данных в БД. Например, запрос на чтение всех данных из таблицы будет выглядеть так:
SELECT * FROM <имя таблицы> 
В одном из следующих практических уроков вы потренируетесь выполнять различные запросы к базе данных YDB.
Транзакция – это несколько связанных запросов к БД, то есть последовательность операций чтения и записи данных, представляющая собой единую логическую задачу. Например, если вы перечисляете деньги с одного счёта на другой, то с точки зрения работы с БД транзакция будет включать:
    чтение данных о состоянии первого счёта, чтобы проверить достаточно ли на нём средств;
    уменьшение значения баланса этого счёта на сумму перевода;
    чтение данных о состоянии второго счёта;
    увеличение значения баланса этого счёта на сумму перевода.
Транзакция будет выполнена только в том случае, если будут успешно завершены все операции, которые она включает.
Транзакции в YDB по умолчанию выполняются в режиме Serializable. Это самый строгий уровень изоляции транзакций. Иными словами, в этом режиме гарантируется, что каждая транзакция полностью независима от других.
Если требования к консистентности или свежести читаемых данных могут быть ослаблены (например для повышения производительности БД, увеличения пропускной способности или уменьшения задержек), то пользователь может использовать режимы с менее строгими уровнями изоляции транзакций:
    Online Read-Only – каждая из операций чтения в транзакции получает последние данные из имеющихся на момент выполнения. Консистентность полученных данных определяется настройкой allow_inconsistent_reads;
    Stale Read Only – операции чтения данных возвращают результаты с возможным отставанием от актуальных (отставание составляет доли секунды). Для каждой отдельной операции чтения данные консистентны, но для разных операций чтения консистентность не гарантируется.
Поскольку таблицы в YDB могут быть партицированы, а отдельные партиции – храниться на разных шардах, YDB поддерживает распределённые транзакции, то есть транзакции, которые затрагивают более одного шарда одной или нескольких таблиц.
Task:
Практическая работа. Создание базы данных
Decision:
В этой практической работе вы создадите БД YDB в dedicated режиме, научитесь подключаться к ней и добавлять данные из тестового приложения.
Создание базы данных
    На стартовой странице консоли управления перейдите в каталог, в котором будете создавать БД, выберите в списке сервисов База данных YDB и нажмите кнопку Создать ресурс.
    В открывшемся окне выберите тип БД dedicated. Появившийся интерфейс создания новой БД практически идентичен уже знакомым вам интерфейсам создания кластеров управляемых БД.
    Выберите для вашей БД имя, назначьте необходимые вычислительные ресурсы (для этой и следующих практических работ достаточно одного хоста конфигурации medium), тип и количество групп хранения (достаточно одной группы).
    Группа хранения – это массив независимых дисковых накопителей, объединённых по сети в единый логический элемент. В YDB такой массив состоит из 9 дисков, расположенных по три в каждой из трёх зон доступности. Такая конфигурация обеспечивает устойчивость при одновременном отказе одной из зон и отказе диска в другой зоне. Стандартный размер группы хранения — 100 ГБ.
    Выберите облачную сеть и подсети для работы с БД. Вы можете оставить сеть по умолчанию или выбрать ту, которую создали на предыдущем курсе. БД будет доступна для всех виртуальных машин, которые подключены к той же облачной сети.
Также выберите опцию присвоения публичного IP-адреса, чтобы иметь возможность подключаться к БД из интернета.
    Нажмите кнопку Создать базу данных.
    Создание БД занимает несколько минут. Когда статус БД изменится с Provisioning на Running, она готова к работе.
    Кликнув на созданную БД в консоли управления, вы перейдёте на вкладку Обзор.
В разделе Соединение на этой странице приведена информация, которая вам понадобится для подключения к БД:
    Эндпоинт — точка подключения с указанием протокола, представляющая собой в данном случае адрес, на который посылаются сообщения;
    Размещение базы данных — полный путь к БД.
    Примеры подключений из командной строки и приложений вы можете посмотреть, нажав на кнопку Подключиться.
Подключение к базе данных и запуск тестового приложения
В этой части практической работы вы подключитесь к БД и запустите тестовое приложение, чтобы создать в ней несколько таблиц с данными о популярных сериалах.
    Для того, чтобы выполнить эту задачу, вам понадобится сервисный аккаунт с ролями viewer и editor. Перейдите в дашборд каталога и выберите вкладку Сервисные аккаунты. Создайте сервисный аккаунт, назначив для него указанные роли. Сохраните идентификатор этого аккаунта.
    Вы можете запускать тестовое приложение со своего компьютера или с виртуальной машины в Yandex Cloud. В данном примере используется OC Ubuntu и приложение на Python.
Если при создании БД вы не присвоили ей публичный IP-адрес, то подключиться к ней вы сможете только с виртуальной машины, расположенной в той же облачной сети.
Для запуска приложения нужно склонировать на свою машину репозиторий YDB Python SDK, из которого оно будет вызываться, а также установить библиотеки ydb, iso8601 и yandexcloud. Воспользуйтесь для этого следующими командами:
git clone https://github.com/yandex-cloud/ydb-python-sdk.git
sudo pip3 install iso8601 ydb yandexcloud 
    Создайте авторизованный ключ для вашего сервисного аккаунта и сохраните его в файл с помощью интерфейса командной строки Yandex Cloud.
mkdir ~/.ydb
yc iam key create \
  --folder-id <идентификатор каталога> \
  --service-account-name <имя сервисного аккаунта> \
  --output ~/.ydb/sa_name.json 
    Получите SSL-сертификат:
wget "https://storage.yandexcloud.net/cloud-certs/CA.pem" \
  -O ~/.ydb/CA.pem 
Установите переменную окружения YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS и переменную окружения с SSL-сертификатом.
export YDB_SERVICE_ACCOUNT_KEY_FILE_CREDENTIALS=~/.ydb/sa_name.json
export YDB_SSL_ROOT_CERTIFICATES_FILE=~/.ydb/CA.pem 
    Запустите тестовое приложение basic_example_v1 из репозитория ydb-python-sdk, указав в качестве параметров подключения значения протокола, эндпоинта и полного пути к БД.
cd ./ydb-python-sdk/examples/basic_example_v1
python3 __main__.py \
-e <Эндпоинт> \
-d <Размещение базы данных> 
Результат выполнения приложения должен выглядеть так:
> describe table: series
column, name: series_id , Uint64
column, name: title , Utf8
column, name: series_info , Utf8
column, name: release_date , Uint64
> select_simple_transaction:
series, id:  1 , title:  IT Crowd , release date:  b'2006-02-03'
> bulk upsert: episodes
> select_prepared_transaction:
episode title: To Build a Better Beta , air date: b'2016-06-05'
> select_prepared_transaction:
episode title: Bachman's Earnings Over-Ride , air date: b'2016-06-12'
> explicit TCL call
> select_prepared_transaction:
episode title: TBD , air date: b'2022-08-24' 
    Вернитесь в консоль управления Yandex Cloud, чтобы посмотреть на результаты работы приложения. Переключитесь на вкладку Навигация.
В вашей БД созданы три таблицы: episodes, seasons и series с информацией о двух популярных сериалах IT Crowd и Silicon Valley. Кликнув по названию таблицы, вы увидите содержащиеся в ней данные. А если подвести к названию таблицы курсор и кликнуть на значок «информация» справа, то внизу появится дополнительное окно с вкладками Обзор, Схема и Партиции.
Кнопка Создать на панели Навигация служит для создания директорий и таблиц. С её помощью можно создать новую таблицу, не прибегая к командам YQL.
Task:
Практическая работа. YQL и работа с данными
Decision:
В этом уроке вы освоите базовый набор операций для работы с данными с использованием YQL и консоли управления Yandex.Cloud. Подробная информация о YQL приведена в разделе Справочник YQL в документации.
Чтобы начать, войдите в раздел Навигация консоли управления и откройте редактор SQL, нажав на кнопку SQL-запрос.
На прошлом уроке мы уже создали в нашей БД три таблицы, содержащие информацию о сериалах IT Crowd и Silicon Valley.
    Добавим в БД еще одну таблицу с рейтингами эпизодов сериала IT Crowd на IMDb.com.
YQL является диалектом SQL, поэтому многие инструкции в этих языках идентичны.
Для создания таблицы вам понадобится сделать запрос к БД, содержащий инструкцию CREATE TABLE. Например, если бы мы хотели создать таблицу seasons (она уже есть в вашей БД), то SQL запрос выглядел бы следующим образом:
CREATE TABLE seasons
(
    series_id Uint64, 
    season_id Uint64, 
    first_aired Date, 
    last_aired Date, 
    title Utf8, 
        PRIMARY KEY (series_id, season_id)
); 
Обратите внимание, что в пределах директории YDB имена таблиц должны быть уникальны. Первичный ключ (PRIMARY KEY) — это столбец или комбинация столбцов, однозначно идентифицирующих каждую строку в таблице. Он может содержать только неповторяющиеся значения. Для таблицы YDB указание первичного ключа обязательно, при этом он может быть только один.
Первичный ключ по сути является первичным индексом, который помогает СУБД быстрее обнаруживать отдельные записи в таблице и сокращает время выполнения запросов. Также в таблицу можно добавить один или несколько вторичных индексов. Они служат той же цели, но в отличие от первичного индекса могут содержать повторяющиеся значения. Добавить вторичные индексы можно в любой момент, когда возникнет необходимость, и это не вызовет деградацию производительности БД. Чтобы при создании таблицы добавить в нее вторичный индекс, используется такая конструкция:
INDEX <имя индекса> GLOBAL ON (<имя столбца1>, <имя столбца2>, ...) 
Вторичный индекс можно добавить и в уже существующую таблицу. Работа БД при этом не прерывается. В отличие от предыдущего случая в существующую таблицу можно добавлять только один вторичный индекс за раз. Делается это с помощью следующей команды:
ALTER TABLE <имя таблицы> ADD INDEX <имя индекса> GLOBAL ON (<имя столбца>); 
Задание 1: создайте таблицу ratings, в которой будут содержаться рейтинги всех эпизодов сериала IT Crowd, со столбцами season_id (Uint64), episodes_id (Uint64), title (Utf8), air_date (Date) и imdb_rating (Uint64) и вторичным индексом rating_index по полю imdb_rating.
Ваш запрос должен быть написан так:
CREATE TABLE ratings (
    season_id Uint64, 
    episodes_id Uint64, 
    title Utf8, 
    air_date Date, 
    imdb_rating Uint64, 
        PRIMARY KEY (season_id, episodes_id), 
        INDEX rating_index GLOBAL ON (imdb_rating)
); 
    Добавим в эту таблицу данные. Для вставки данных в YDB помимо обычной SQL инструкции INSERT также используются инструкции REPLACE и UPSERT.
При выполнении INSERT перед операцией записи выполняется операция чтения данных. Это позволяет убедиться, что уникальность первичного ключа будет соблюдена. При выполнении инструкций REPLACE и UPSERT осуществляется слепая запись.
Инструкции REPLACE и UPSERT используются для добавления новой или изменения существующей строки по заданному значению первичного ключа. При операциях записи и изменения данных использование этих инструкций эффективнее.
Если при выполнении этих инструкций строка с указанным значением первичного ключа не существует, то она будет создана. Если же такая строка существует, то значения ее столбцов будут заменены на новые. Отличие между REPLACE и UPSERT заключается в том, что первая из этих инструкций устанавливает значения столбцов, не участвующих в операции, в значения по умолчанию, а вторая такие значения не меняет.
Одним запросом REPLACE, UPSERT или INSERT можно вставить в таблицу несколько строк.
Например, если бы мы хотели добавить в таблицу series те данные, которые в ней сейчас содержатся, то SQL запрос выглядел бы так:
REPLACE INTO series (series_id, title, release_date, series_info) 
VALUES 
    ( 
        1, 
        "IT Crowd", 
        Date("2006-02-03"), 
        "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."), 
    ( 
        2, 
        "Silicon Valley", 
        Date("2014-04-06"), 
        "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley." 
    ); 
Задание 2: добавьте в таблицу ratings данные из этого файла.
Ваш запрос должен выглядеть таким образом:
REPLACE INTO ratings (season_id, episodes_id, title, air_date, imdb_rating) VALUES 
    (1, 1, "Yesterday's Jam", Date("2006-02-03"), 76),
    (1, 2, "Calamity Jen", Date("2006-02-03"), 82),
    (1, 3, "Fifty-Fifty", Date("2006-02-10"), 79),
    (1, 4, "The Red Door", Date("2006-02-17"), 80),
    (1, 5, "The Haunting of Bill Crouse", Date("2006-02-24"), 85),
    (1, 6, "Aunt Irma Visits", Date("2006-03-03"), 81),
    (2, 1, "The Work Outing", Date("2006-08-24"), 95),
    (2, 2, "Return of the Golden Child", Date("2007-08-31"), 82),
    (2, 3, "Moss and the German", Date("2007-09-07"), 82),
    (2, 4, "The Dinner Party", Date("2007-09-14"), 87),
    (2, 5, "Smoke and Mirrors", Date("2007-09-21"), 78),
    (2, 6, "Men Without Women", Date("2007-09-28"), 76),
    (3, 1, "From Hell", Date("2008-11-21"), 78),
    (3, 2, "Are We Not Men?", Date("2008-11-28"), 85),
    (3, 3, "Tramps Like Us", Date("2008-12-05"), 82),
    (3, 4, "The Speech", Date("2008-12-12"), 90),
    (3, 5, "Friendface", Date("2008-12-19"), 85),
    (3, 6, "Calendar Geeks", Date("2008-12-26"), 78),
    (4, 1, "Jen The Fredo", Date("2010-06-25"), 80),
    (4, 2, "The Final Countdown", Date("2010-07-02"), 84),
    (4, 3, "Something Happened", Date("2010-07-09"), 75),
    (4, 4, "Italian For Beginners", Date("2010-07-16"), 82),
    (4, 5, "Bad Boys", Date("2010-07-23"), 84),
    (4, 6, "Reynholm vs Reynholm", Date("2010-07-30"), 76); 
    C помощью SQL запросов можно добавлять и удалять не только строки таблицы, но и столбцы. Для этого используется команда ALTER TABLE и фразы ADD COLUMN и DROP COLUMN.
Например, если вы хотите добавить в таблицу ratings столбец viewed с данными о том, какие эпизоды сериала вы уже посмотрели, то это можно сделать с помощью следующей команды.
ALTER TABLE ratings ADD COLUMN viewed Bool;  
Задание 3: Вы решили, что столбец с датой выхода эпизодов в таблице ratings не нужен, поскольку эта информация уже содержится в другой таблице. Удалите столбец air_date из таблицы ratings.
Для этого понадобится выполнить такую команду:
ALTER TABLE ratings DROP COLUMN air_date;  
    Теперь потренируемся извлекать данные из БД. Для этого используется команда SELECT. В простейшем случае ее синтаксис выглядит так:
SELECT <имя столбца1>, <имя столбца2>, ...
FROM <имя таблицы>; 
Например, чтобы выбрать всю информацию из таблицы seasons, нужно сделать следующий запрос к БД.
SELECT * FROM seasons; 
Если нужно выбрать из таблицы только те строки, которые удовлетворяют определенному условию, в запросе используют секцию WHERE. В этой секции должно находиться выражение, возвращающее логический результат. Обычно оно состоит из логических операций and, or, not и операций сравнения.
Например, выбрать из таблицы episodes только первые эпизоды всех сезонов можно так:
SELECT * FROM episodes
WHERE episode_id = 1
;  
Запрос SELECT извлекает строки без определенного порядка. Чтобы отсортировать полученные данные нужным образом, в этот запрос включают секцию ORDER BY. В ней указывается список столбцов, которые будут определять порядок сортировки результатов запроса.
Задание 4: получите список самых популярных (с рейтингом не менее 85) эпизодов сериала IT Crowd. При поиске используйте созданный ранее вторичный индекс rating_index. Чтобы упорядочить результаты по убыванию рейтинга используйте конструкцию ORDER BY … DESC.
Используемый для этого запрос:
SELECT 
    season_id, 
    episodes_id, 
    title, 
    imdb_rating
FROM ratings VIEW rating_index 
WHERE 
    imdb_rating >= 85 
ORDER BY 
    imdb_rating DESC
;  
    Для получения обобщённых сведений о содержащихся в таблице данных — например, о числе строк в таблице или среднем значении какого-либо выражения — в запрос SELECT включают агрегатные функции и секцию GROUP BY. Эта секция используется для агрегации внутри каждого ключа. Ключом является значение одной или более колонок, указанных в GROUP BY.
Примеры агрегатных функций:
COUNT(*) — вычисляет число строк в таблице.
MAX(expr) — находит максимум выражения expr по всем строкам.
SUM(expr) — суммирует выражение expr по всем строкам. Тип выражения должен быть числовым.
AVG(expr) — находит среднее значение выражения expr по всем строкам. Тип выражения должен быть числовым или интервалом.
SOME(expr) — возвращает одно произвольное значение выражения по всем строкам.
Результаты выполнения агрегатной функции выводятся в отдельном столбце. Чтобы задать этому столбцу имя, используют оператор AS. Конструкция может выглядеть, например, так:
SELECT 
    <имя столбца1>, 
    MAX(<имя столбца2>) AS max_value
...
; 
Задание 5: Напишите SQL запрос к таблице episodes, который выводит данные о числе эпизодов каждого сериала.
Подсказка:
Вам понадобится вычислить число строк для каждого значения столбца series_id и сгруппировать результаты по series_id.
Ответ:
SELECT 
    series_id, 
    COUNT(*) AS total_episodes 
FROM episodes 
GROUP BY 
    series_id 
ORDER BY 
    series_id 
; 
Задание 6: Напишите SQL запрос, с помощью которого можно сравнить популярность сезонов сериала IT Crowd. 
Подсказка:
Вам понадобится вычислить средний рейтинг эпизодов для каждого сезона и сгруппировать результаты по столбцу season_id.
Ответ:
SELECT 
    season_id, 
    AVG (imdb_rating) AS avg_rating
FROM ratings 
GROUP BY season_id
ORDER BY avg_rating DESC; 
    В реляционной БД таблицы логически связаны друг с другом. С помощью объединений (JOIN) можно получить данные из нескольких связанных друг с другом таблиц и представить их в виде одной результирующей таблицы.
Столбцы, по которым выполняется объединение, можно указать одним из двух способов.
    После ключевого слова USING, например table1 AS a JOIN table2 AS b USING (foo). Это более короткий способ записи, удобный для простых случаев. Имена столбцов, по которым происходит объединение таблиц, должны быть одинаковы.
    После ключевого слова ON (например, a JOIN b ON a.foo = b.bar). Этот способ позволяет использовать разные имена столбцов и указывать дополнительные условия по аналогии с WHERE.
Поскольку такие запросы затрагивают столбцы разных таблиц, имена столбцов должны содержать и имя таблицы (то есть, например, не просто series_id, а seasons.series_id).
В YDB доступны следующие логические типы объединений:
INNER (используется по умолчанию) — строки попадают в результат, только если значение ключевых колонок присутствует в обеих таблицах;
FULL, LEFT и RIGHT — при отсутствии значения в обеих или в одной из таблиц включает строку в результат, но оставляет пустыми (NULL) колонки, соответствующие противоположной таблице.
LEFT/RIGHT SEMI — одна сторона выступает как белый список (whitelist) ключей, её значения недоступны. В результат включаются столбцы только из одной таблицы, декартового произведения не возникает;
LEFT/RIGHT ONLY — вычитание множеств по ключам (blacklist). Практически эквивалентно добавлению условия IS NULL на ключ противоположной стороны в обычном LEFT/RIGHT, но, как и в SEMI, нет доступа к значениям;
CROSS — декартово произведение двух таблиц целиком без указания ключевых колонок, секция с ON/USING явно не пишется;
EXCLUSION — обе стороны минус пересечение.
Простой пример запроса с объединением таблиц приведен ниже.
SELECT
    sa.title AS season_title,
    sr.title AS series_title,
    sr.series_id, sa.season_id 
FROM seasons AS sa
INNER JOIN series AS sr ON sa.series_id = sr.series_id 
WHERE sa.season_id = 1
ORDER BY sr.series_id; 
Этот запрос извлекает из таблиц series и seasons сведения о первых сезонах всех сериалов и выводит объединённые данные в результирующей таблице.
Задание 7: напишите запрос, который выводит таблицу, содержащую название сериала IT Crowd и названия всех его эпизодов (то есть, каждая строка итоговой таблице должна содержать название сериала и название отдельного эпизода).
Это запрос может выглядеть следующим образом:
SELECT 
    sr.title AS series_title, 
    ep.title AS episode_title, 
    ep.season_id,     
    ep.episode_id 
FROM 
    series AS sr 
INNER JOIN 
    episodes AS ep 
ON sr.series_id = ep.series_id 
WHERE sr.series_id = 1 
ORDER BY 
    ep.season_id,     
    ep.episode_id 
;  
План запроса
Decision:
Когда вы обращаетесь к БД, оптимизатор запросов YDB пытается составить наилучший, по его мнению, план выполнения запроса.
Чтобы оптимизировать свои запросы к БД с точки зрения скорости их выполнения (и/или стоимости, что актуально для бессерверного режима YDB), нужно получить и проанализировать этот план. Вы можете это сделать через консоль управления или с помощью YDB CLI.
Давайте разберём план запроса, который мы использовали на прошлом уроке в качестве примера объединения таблиц.
SELECT 
    sa.title AS season_title, 
    sr.title AS series_title, 
    sr.series_id, sa.season_id 
FROM seasons AS sa 
INNER JOIN series AS sr ON sa.series_id = sr.series_id 
WHERE sa.season_id = 1  
Войдите в редактор SQL вашей БД и вставьте в поле ввода текст запроса. Нажмите на стрелку справа от кнопки Выполнить и в выпадающем меню выберите опцию Explain.
В результате внизу отобразится поле, содержащее план запроса.
{
  "plan": {
    "meta": {
      "type": "script",
      "version": "0.2"
    },
    "queries": [
      {
        "tables": [
          {
            "name": "/ru-central1/b1glk1805em030s2ir60/etn9c1c7v3s2bc06lfm8/seasons",
            "reads": [
              {
                "columns": [
                  "season_id",
                  "series_id",
                  "title"
                ],
                "type": "FullScan",
                "scan_by": [
                  "series_id",
                  "season_id"
                ]
              }
            ]
          },
          {
            "name": "/ru-central1/b1glk1805em030s2ir60/etn9c1c7v3s2bc06lfm8/series",
            "reads": [
              {
                "columns": [
                  "series_id",
                  "title"
                ],
                "lookup_by": [
                  "series_id (expr)"
                ],
                "type": "MultiLookup"
              }
            ]
          }
        ]
      }
    ]
  }
} 
Основная секция (tables) плана запроса содержит информацию об обращениях к таблицам. Операция чтения описываются в разделе reads, а операции записи — в разделе writes (в этом плане запроса данный раздел отсутствует).
Ключевой характеристикой любого обращения к таблице является его тип.
Типы чтения:
    FullScan — полное сканирование таблицы, читаются все записи на всех шардах;
    Scan — читается определённый диапазон записей;
    Lookup — чтение по ключу или префиксу ключа;
    MultiLookup — множественные чтения по ключу или префиксу ключа (такой тип обращения возможен, например при выполнении инструкций JOIN).
Типы записи:
    Upsert — добавление одной записи;
    MultiUpsert — добавление нескольких записей;
    Erase — единичное удаление по ключу;
    MultiErase — множественные удаления.
Рассмотрим план запроса из нашего примера.
Параметр lookup_by показывает, по каким колонкам (ключу или префиксу ключа) выполняется чтение. Параметр scan_by показывает, по каким колонкам выполняется scan, то есть чтение всех записей в определённом диапазоне значений. В columns перечислены колонки, значения которых будут считываться из таблицы.
Из плана запроса следует, что для таблицы seasons будет выполнен FullScan, а для таблицы series — множественные чтения (тип обращения MultiLookup) по ключу series_id (lookup_by). Это говорит нам  о том, что данный запрос составлен не лучшим образом. Тип чтения FullScan означает, что для выполнения запроса потребуется полностью прочитать всю таблицу. Если таблица большая, то такой запрос приведет к избыточному росту нагрузки на БД и задержкам, а в режиме serverless — еще и к повышенным расходам.
Task:
Диагностика и мониторинг
Decision:
Когда вы работаете с БД, важно отслеживать, как она справляется с нагрузкой. YDB предоставляет пользователям необходимые для этого инструменты мониторинга и диагностики.
В разделе Мониторинг консоли управления вы найдете дашборды, на которых приведены графики изменения метрик, характеризующих потребление ресурсов (CPU, оперативной памяти и дискового пространства) и время задержки транзакций (например, задержки операций чтения и операций записи на сервере или на клиенте БД).
В разделе Диагностика предоставлена возможность доступа к «системным» (то есть, специальным служебным) таблицам, содержащим детализированную информацию о работе БД. Нужные сведения извлекаются из системных таблиц с помощью YQL запросов.
В этом разделе вы можете получить информацию о размерах партиций таблиц и нагрузке на них; выяснить, какие запросы выполняются дольше других, больше всего нагружают CPU или приводят к чтению наибольшего объема данных. Для наиболее характерных вопросов о работе БД нужные YQL запросы уже составлены. Вам остается только их запустить и проанализировать полученные результаты.
Давайте посмотрим, как это работает. Откройте раздел Диагностика в консоли управления и кликните на пункт «Топ таблиц по размеру».
Вы перейдете в раздел Навигация, где в SQL-редакторе увидите заполненное поле ввода с запросом к служебной таблице partition_stats, в которой хранится информация об отдельных партициях всех таблиц БД.
Нажмите кнопку Выполнить. YDB выведет информацию об имеющихся в БД таблицах, их размере, суммарном числе строк и количестве партиций, а также суммарном потреблении процессора.
Эти сведения могут помочь вам улучшить работу БД или предвосхитить возможные проблемы. Например, если основная нагрузка (потребление процессора) приходится на одну из таблиц, то впору задуматься о ее правильном партиционировании и/или оптимизации идущих к ней запросов.
Подробная информация о системных таблицах и о том, какие данные в них хранятся, приведена в документации.
Пользуйтесь инструментами диагностики и мониторинга, это поможет вашей БД работать быстро и надежно.
Task:
Создание резервных копий
Decision:
YDB создает резервные копии вашей БД автоматически. При этом регламент резервного копирования вы можете настраивать самостоятельно .
Зайдите в раздел Резервные копии в консоли управления и выберите вкладку Расписание. По умолчанию резервное копирование выполняется ежедневно в 17:00, а срок хранения резервных копий (TTL, time-to-live) составляет два дня.
Чтобы изменить эти параметры нажмите … и выберите Редактировать. В открывшемся окне вы можете выбрать периодичность создания резервных копий (ежедневно или еженедельно), время начала резервного копирования, а также срок хранения.
YDB также позволяет создавать резервные копии БД вручную как непосредственно в хранилище данных, так и в виде csv-файлов в файловой системе (например на вашем компьютере) или S3-совместимом объектном хранилище (например в Yandex Object Storage).
Создавать резервные копии в хранилище данных и восстанавливать их можно через консоль управления Yandex Cloud. Резервное копирование в файловую систему или объектное хранилище выполняется с помощью консольного клиента YDB. О том, как это сделать, подробно рассказано в документации.
Чтобы выполнить резервное копирование вашей БД вручную через консоль управления, зайдите в нее и выберите раздел Резервные копии. Нажмите на кнопку Создать резервную копию.
В появившемся окне задайте имя резервной копии и выберите время запуска — Немедленно. Чтобы создаваемая резервная копия была удалена автоматически, выберите соответствующую опцию и укажите время хранения в днях. Запустите резервное копирование, нажав на кнопку Создать.
Когда процесс завершится, резервная копия сменит статус с Creating на Ready. Статус создания и восстановления резервных копий также можно отслеживать в разделе Операции.
Восстанавливать резервную копию БД удобно в отдельную директорию. Создайте директорию restore для восстанавливаемых данных (это делается в разделе Навигация).
Нажмите … справа от созданной резервной копии и выберите Восстановить. Для восстановления данных укажите текущую БД и созданную директорию restore. Нажмите кнопку Восстановить.
Когда операция завершится, зайдите в директорию restore в разделе Навигация и убедитесь, что в ней появились восстановленные таблицы с данными.
Task:
Большие данные, Apache Hadoop и Apache Spark
Decision:
Изучая вопросы хранения, обработки и анализа данных, было бы неправильно обойти такую концепцию, как большие данные (Big Data). Давайте рассмотрим облачные технологии для работы с большими данными.
Big Data
Как понять, что данные по-настоящему большие? Обычно к таким относят данные, размер которых увеличивается хотя бы на 100 ГБ в день. Данные при этом могут быть и структурированными, и нет.
Источники больших данных в современном мире — это социальные сети, сайты и СМИ, информация, которую компании собирают для бизнеса, а также интернет вещей с миллионами датчиков. Чтобы оценить масштаб, приведём лишь несколько примеров: датчики беспилотного автомобиля генерируют примерно 6 ГБ данных за каждый километр пути, морской нефтяной платформы — до 1 ТБ за неделю, а пассажирского авиалайнера — до 20 ТБ за час полёта.
Большие данные всё активнее используются в государственном управлении и бизнесе. Например, в компьютерных играх с помощью больших данных анализируют поведение игроков и делают выводы о предпочтениях аудитории. Это позволяет лучше понимать, как развивать игру, чтобы она оставалась популярной и приносила прибыль.
Чтобы работать с такими объёмами данных в реальном времени, необходимы специальные технологии. В их основе лежат три принципа:
    Распределённое хранение и обработка. Данных много, а их обработка требует больших вычислительных ресурсов, поэтому необходимы распределённые системы из большого количества узлов. Создать такую систему и обеспечить её высокую производительность — непростая задача.
    Горизонтальная масштабируемость. Данные постоянно накапливаются, а задачи их обработки варьируются от относительно простых до очень сложных. Поэтому нужна возможность легко наращивать объёмы хранилища и гибко изменять количество узлов, на которых обрабатываются данные.
    Отказоустойчивость. Узлов в кластере много (сотни или даже тысячи), и вероятность того, что они будут выходить из строя, довольно высока. Следует построить распределённую систему так, чтобы сбой на одном или нескольких узлах не влиял на её работу в целом.
Hadoop
Первой широко распространённой технологией хранения и обработки больших данных стал фреймворк Apache Hadoop. Это платформа с открытым программным кодом, разработанная под эгидой фонда Apache Software Foundation. По сути, Hadoop — это набор утилит и библиотек для работы с данными, распределёнными между несколькими кластерами, каждый из которых состоит из сотен или даже тысяч узлов.
В основе экосистемы Hadoop лежит четыре модуля.
Hadoop Common — набор библиотек и утилит, которые используются в других решениях, в частности для создания инфраструктуры и управления распределёнными файлами.
HDFS — распределённая файловая система (Hadoop Distributed File System), отказоустойчивая и не требующая высокопроизводительного оборудования.
Файл в системе делится на блоки, которые распределяются между узлами вычислительного кластера (DataNode). Данные файловой системы и информация о распределении блоков и об узлах данных, содержащих эти блоки, хранится на центральном узле имён (NameNode). HDFS надёжно хранит крупные файлы за счёт дублирования и репликации блоков.
YARN (Yet Another Resource Negotiator) — система управления ресурсами, обеспечивающая безопасное планирование заданий и управление данными. Это набор программ, предоставляющих удобный интерфейс между аппаратными ресурсами кластера и приложениями, использующими эти ресурсы для вычислений и обработки данных.
MapReduce — система распределённых вычислений для обработки больших данных.
Вычисления происходят локально: обрабатывающая данные программа копируется на узлы с данными и выполняется там. В MapReduce входной набор данных разбивается на независимые блоки, а задание для их обработки — на подзадачи map и reduce.
Подзадачи map — это применение функции к блоку данных. Эти подзадачи выполняются одновременно над разными блоками, а результаты записываются на диски с исходными данными. Подзадачи reduce — это агрегация результатов выполнения map с разных узлов.
Пример задачи MapReduce — подсчёт числа вхождений каждого слова в тексте. Map определяют, какие слова и сколько раз входят в каждую строку. Reduce суммируют значения. Посмотрите в руководстве MapReduce, как решается эта задача.
Классическая конфигурация кластера Hadoop состоит из сервера имён (NameNode), узла-мастера MapReduce (JobTracker) и набора узлов, на каждом из которых развёрнуты сервер данных (DataNode) и так называемый воркер (TaskTracker).
Все операции в MapReduce подразумевают чтение с жёсткого диска и запись на него. Часто время, необходимое для этих операций, в разы превышает время самих вычислений. Поэтому технология MapReduce хорошо работает при распределённых вычислениях в пакетном режиме: когда данные подаются на обработку отдельными большими пакетами. А вот для обработки потоков данных в реальном времени она не подходит. Для таких задач разработаны фреймворки распределённой потоковой обработки данных, наиболее популярный из них — это Apache Spark.
Одно из основных отличий Spark от Hadoop заключается в том, что он хранит результаты промежуточных вычислений в памяти, не записывая их на диск. Это даёт большой прирост в производительности.
В основе Spark лежит движок, управляющий планированием и оптимизацией выполнения заданий, а также работой с источником данных (HDFS, объектным хранилищем или БД). Поверх ядра работают библиотеки. Вот основные из них:
    Spark SQL — чтобы запускать SQL-подобные команды в распределённых наборах данных;
    MLlib — для машинного обучения;
    Structured Streaming — для потоковой обработки данных в реальном времени;
    GraphX — для задач с графами.
Spark и Hadoop используются для разных задач и дополняют друг друга.
Hadoop подходит, чтобы:
    обрабатывать большие наборы данных, когда объём даже промежуточных результатов вычислений превышает доступную память;
    создавать инфраструктуру анализа данных при ограниченном бюджете (Spark требует много оперативной памяти, так что его использование более затратно);
    решать задачи, если времени на них достаточно и не требуется немедленно получить результаты;
    заниматься пакетной обработкой данных, выполнять много операций чтения с диска и записи на него (например анализ исторических и архивных данных).
Spark подходит, когда:
    нужно анализировать потоковые данные в реальном времени;
    скорость решения задачи принципиально важна;
    задачи включают много параллельных операций с использованием итерационных алгоритмов;
    задачи связаны с машинным обучением.
Task:
Заходя на сайт Yahoo, посетители видят персонализированную новостную ленту. Алгоритмы компании автоматически категоризируют новостные статьи по мере появления, а также предсказывают, какие новости интересны каждому посетителю. Какую технологию работы с большими данными использует Yahoo?
Decision:
-Hadoop
+Spark
Task:
Обзор Yandex Data Proc
Decision:
Создавать кластеры Hadoop и Spark вручную — непростое и небыстрое занятие. Нужно настроить виртуальные машины (ВМ), развернуть сервисы, изменить множество конфигурационных файлов...
Сервис Yandex Data Proc автоматически создаст кластеры Hadoop или Spark, настроит сеть, установит ПО и обновит его, когда выйдет новая версия. В сервисе есть интерфейсы запуска заданий и инструменты мониторинга. Data Proc интегрируется с другими сервисами Yandex Cloud и автоматически масштабирует ресурсы.
Компоненты Data Proc
В развёрнутый кластер, помимо самого Hadoop, будут включены следующие компоненты.
Tez — фреймворк для обработки больших данных, содержащий ряд улучшений технологии MapReduce.
Spark — фреймворк для распределённой потоковой обработки данных.
Hive — платформа для хранения больших данных в распределённом хранилище и для управления ими.
ZooKeeper — служба, координирующая работу приложений. Она хранит информацию о настройках системы, обеспечивает синхронизацию распределённого выполнения групповых задач, выявляет конфликтующие задачи и нерациональное использование ресурсов.
HBase — распределённая NoSQL база данных (БД), основанная на модели Google BigTable и использующая HDFS. Её основная задача — хранить очень большие таблицы (миллиарды строк и миллионы столбцов) на узлах кластера.
Sqoop — инструмент для передачи данных между Hadoop и реляционными БД. С его помощью можно импортировать данные из реляционных СУБД в Hadoop, преобразовать их с использованием MapReduce, а затем экспортировать обратно.
Oozie — инструмент для управления рабочим процессом и координации заданий MapReduce. Может объединить несколько задач в единое логическое задание.
Flume — распределённая служба, которая собирает, сортирует и перемещает большие объёмы данных журнала событий. Она может обрабатывать потоковые данные, позволяя создавать аналитические приложения для всей экосистемы Hadoop.
Livy — служба, которая через REST-интерфейс обеспечивает взаимодействие с кластером Spark, включая отправку заданий или частей кода Spark, а также синхронное или асинхронное получение результатов.
Zeppelin — многопользовательский инструмент для анализа и визуализации данных в браузере, а также совместной работы над данными с использованием Spark. Позволяет создавать запросы к данным в Hadoop на SQL, Scala или Python и отображать результаты в виде таблиц, графиков и диаграмм.
Сервис Yandex Data Proc интегрирован с Yandex DataSphere — облачной средой для разработки моделей машинного обучения. С её помощью можно пройти весь цикл создания моделей: от эксперимента и разработки до запуска готовой версии на вычислительных мощностях Yandex Cloud. DataSphere использует бессерверные вычисления, но если вам предстоит решать ресурсоёмкие задачи, то расчёты можно запустить и на вашем кластере Data Proc.
Кластер Data Proc
Основная сущность сервиса Data Proc — кластер. Он объединяет все ресурсы, доступные Hadoop: вычислительные мощности и хранилище.
Каждый кластер состоит из подкластеров. Подкластеры объединяют хосты, выполняющие идентичные функции:
    подкластер с управляющими хостами (например NameNode для HDFS или ResourceManager для YARN);
    подкластер для хранения данных (например DataNode для HDFS);
    подкластеры для обработки данных (например NodeManager для YARN).
Подкластеры каждого кластера должны находиться в одной облачной сети и одной зоне доступности.
Хранение данных
Чтобы вы не переплачивали за хранение большого объёма данных на вычислительных узлах, в сервисе реализована связь с объектным хранилищем. В него помещается основной объём данных, а на вычислительных узлах хранятся только горячие данные, к которым нужен быстрый доступ.
Вы можете запускать задания по SSH — без непосредственного доступа к кластеру Data Proc. Поэтому, чтобы вам было удобно, журнал выполнения заданий находится в отдельном бакете в объектном хранилище. Записи в журнал делаются от имени сервисного аккаунта, указанного при создании кластера.
Для кластера Data Proc рекомендуется использовать хотя бы два бакета в объектном хранилище. Один, где сервисный аккаунт имеет права только на чтение, — для исходных данных. Второй, с полным доступом сервисного аккаунта — для журналов и результатов операций. Два бакета помогут уменьшить риски непредвиденных изменений и удаления исходных данных.
Сеть
Все подкластеры должны находиться в одной сети, а все хосты каждого подкластера — в определённой подсети этой сети.
У хостов кластера нет публичного IP-адреса. Чтобы подключиться к кластеру Data Proc, используйте ВМ, расположенную в той же облачной сети, что и кластер. То есть вы создадите ВМ, к которой подключитесь по SSH, а с этой машины, в свою очередь, подключитесь к кластеру.
В Yandex Cloud хосты без публичных IP-адресов не имеют доступа к ресурсам за пределами виртуальной сети. Поэтому для корректной работы кластера включите NAT в интернет для нужной подсети: зайдите в раздел Virtual Private Cloud в каталоге с кластером Data Proc, выберите подсеть и включите для неё эту опцию.
Task:
Какие хосты входят в кластер Data Proc?
Decision:
+Master Node
-S3 Storage Node
+Compute Node
-Slave Node
+Data Node
Task:
Практическая работа. Создание кластера Hadoop
Decision:
На этом уроке вы создадите и настроите кластер Hadoop с помощью сервиса Yandex Data Proc. Hadoop предназначается для работы с большими данными, поэтому создание кластера потребует от вас больше усилий, чем на предыдущих практических работах (но гораздо меньше, чем если бы вы делали это самостоятельно).
Создание кластера
Для хранения зависимостей заданий нашего кластера и результатов их выполнения нужно предварительно создать бакет в объектном хранилище. О том, как это сделать, мы рассказывали на одном из предыдущих занятий.
Также создайте сервисный аккаунт для доступа к кластеру. Обратите внимание: можно использовать только аккаунт с ролью mdb.dataproc.agent. Для автоматического масштабирования кластера сервисному аккаунту также понадобятся роли editor и dataproc.agent.
Откройте каталог, где будете создавать кластер, и выберите сервис Data Proc.
В открывшемся окне нажмите кнопку Создать кластер.
Задайте для кластера имя и выберите версию образа — 1.4. В образ включена одна из версий Hadoop и дополнительные компоненты. Некоторые вы можете устанавливать по выбору. Кроме того, в каждую версию образа входит Conda (менеджер окружений для Python) и набор инструментов машинного обучения (scikit-learn, TensorFlow, CatBoost, LightGBM и XGBoost).
Обратите внимание на то, что некоторые из сервисов обязательны, чтобы использовать другие. На следующем уроке нам понадобится сервис HIVE. Выберите его, и рядом с MAPREDUCE и YARN вы увидите напоминания о том, что они нужны для HIVE.
Вставьте в поле публичный ключ публичную часть SSH-ключа. Как сгенерировать и использовать SSH-ключи, мы рассказывали в одной из практических работ о виртуальных машинах.
Выберите созданный сервисный аккаунт для доступа к кластеру.
Выберите зону доступности для кластера. Все подкластеры будут находиться в этой 
Если нужно, задайте свойства Hadoop и его компонентов. Доступные свойства перечислены в документации.
Выберите бакет в объектном хранилище, где будут храниться зависимости заданий и результаты их выполнения.
Выберите или создайте сеть для кластера. Включите опцию NAT в интернет для подсетей, в которых размещается кластер.
Если нужно, создайте группу безопасности. Правила для неё вы добавите позже в сервисе Virtual Private Cloud.
Включите опцию UI Proxy, чтобы получить доступ к веб-интерфейсам компонентов Data Proc. У некоторых компонентов (например Hadoop, Spark, YARN и Zeppelin) есть пользовательские веб-интерфейсы, доступные на мастер-узле кластера. С помощью этих интерфейсов вы можете:
    отслеживать ресурсы кластера и управлять ими (YARN Resource Manager, HDFS NameNode);
    просматривать статус и отлаживать задания (Spark History, JobHistory);
    проводить эксперименты, совместно работать или выполнять отдельные операции (Zeppelin).
Подробности об интерфейсах вы найдёте в документации.
Настройка подкластеров
В состав кластера входит один главный подкластер (Мастер) с управляющим хостом, а также подкластеры для хранения данных (Data) или вычислений (Compute).
В подкластерах Data можно разворачивать компоненты для хранения данных, а в подкластерах Compute — для обработки данных. Хранилище в подкластере Compute предназначено только для временного хранения обрабатываемых файлов.
Для каждого подкластера можно задать число и класс хостов, размер и тип хранилища, а также подсеть той сети, в которой расположен кластер. Кроме того, для подкластеров Compute можно настроить автоматическое масштабирование. Это позволит выполнять задания на обработку данных быстрее без дополнительных усилий с вашей стороны.
Создадим подкластер Compute с одним хостом.
В блоке Добавить подкластер нажмите кнопку Добавить.
В поле Роли выберите COMPUTENODE. В блоке Масштабирование включите опцию Автоматическое масштабирование.
Все открывшиеся настройки знакомы вам из практических работ по созданию виртуальных машин.
Автоматическое масштабирование подкластеров обработки данных поддерживается в кластерах Yandex Data Proc версии 1.2 и выше. Чтобы оно работало, в кластере с установленным Spark или Hive должен быть также установлен сервис YARN.
Yandex Data Proc автоматически масштабирует кластер, используя для этого системные метрики нагрузки на кластер. Когда их значение выходит из установленного диапазона, запускается масштабирование. Если значение метрики превысит порог, в подкластер добавятся хосты. Если опустится ниже порога, начнётся декомиссия (высвобождение ненужных ресурсов), а избыточные хосты удалятся.
По умолчанию для масштабирования используется внутренняя метрика YARN (yarn.cluster.containersPending). Она показывает, сколько единиц ресурсов нужно заданиям в очереди. Выбирайте эту опцию Масштабирование по умолчанию, если в кластере выполняется много относительно небольших заданий.
Другой вариант — масштабирование на основе метрики загрузки процессора (vCPU). Чтобы использовать его, отключите опцию Масштабирование по умолчанию и укажите целевой уровень загрузки vCPU.
Настроив подкластеры, нажмите кнопку Создать кластер.
Сервис запустит создание кластера. После того как статус кластера изменится на Running, вы сможете подключиться к любому активному подкластеру с помощью указанного в настройках SSH-ключа.
Завершив практическую работу, не удаляйте кластер: он понадобится вам на следующем уроке.
Task:
Практическая работа. Подключение к кластеру и работа с Hive
Decision:
На этом уроке вы научитесь подключаться к кластеру Hadoop и работать с ним на примере выполнения запросов с помощью Hive.
Подключение к кластеру
Подключимся к управляющему хосту главного подкластера. Поскольку хостам кластера Hadoop не назначается публичный IP-адрес, для подключения к ним нужна виртуальная машина, расположенная в той же сети Yandex Cloud.
Выберите машину, которую создавали раньше, или создайте новую. Подключитесь к ней по SSH. Вы уже делали это, когда изучали виртуальные машины.
Подключитесь с этой машины к хосту главного подкластера также с помощью SSH. Для этого на машине должна быть закрытая часть SSH-ключа, открытую часть которого вы указали при создании кластера Data Proc. Вы можете скопировать ключ на виртуальную машину или подключаться к ней с запущенным SSH-агентом.
Скопировать ключ можно с помощью утилиты nano. На виртуальной машине выполните команду:
sudo nano ~/.ssh/<имя ключа> 
В открывшийся редактор скопируйте содержимое закрытой части SSH-ключа с вашей локальной машины.
Запустите SSH-агент:
eval `ssh-agent -s` 
Добавьте ключ в список доступных агенту:
ssh-add ~/.ssh/<имя ключа> 
Узнайте внутренний FQDN хоста главного подкластера. Для этого в консоли управления на странице кластера перейдите на вкладку Хосты и выберите хост с ролью MASTERNODE.
Откройте SSH-соединение с хостом Data Proc для пользователя root, например:
ssh root@<FQDN хоста> 
Пошаговые инструкции по различным способам подключения к кластеру Data Proc приведены в документации.
Проверим, что команды Hadoop выполняются, например:
hadoop version 
Результат выполнения этой команды выглядит так:
Запуск заданий Apache Hive
Как мы уже говорили ранее, Hive — это платформа для хранения данных и управления ими в экосистеме Hadoop. Она используется для доступа к большим датасетам, сохранённым в распределённом хранилище.
Hive позволяет работать с данными различного формата (csv, tsv, Parquet, ORC, Avro и другими), подключаться к БД и взаимодействовать с ней с помощью SQL-подобного языка запросов. Hive используется преимущественно для работы с данными в HDFS, HBase, S3-совместимых хранилищах и реляционных СУБД.
Запрос на действия с данными в Hive называется заданием. Задания можно запускать на управляющем хосте с помощью командной оболочки CLI Hive, а также с помощью CLI Yandex Cloud.
Для запуска Hive CLI выполните команду hive на управляющем хосте.
Проверьте, всё ли работает: выполните, например, команду select 1; — корректный результат выглядит так:
Теперь создайте внешнюю таблицу (external table) в формате Parquet, содержащую открытые данные о списке перелётов между городами США в 2018 году. Для этого с помощью Hive CLI выполните запрос:
hive> CREATE EXTERNAL TABLE flights (Year bigint, Month bigint, FlightDate string, Flight_Number_Reporting_Airline bigint, OriginAirportID bigint, DestAirportID bigint) STORED AS PARQUET LOCATION 's3a://yc-mdb-examples/dataproc/example01/set01'; 
Проверим список таблиц, выполнив команду show tables. Результат должен выглядеть так:
Запросим число перелётов с разбивкой по месяцам:
hive> SELECT Month, COUNT(*) FROM flights GROUP BY Month; 
Пример результата такого запроса:
Безусловно, на одном примере сложно показать возможности сервиса Data Proc. Если вас интересует работа с большими данными в облаке, посмотрите доклады сотрудников Yandex Cloud об управлении кластерами Hadoop и заданиями в Data Proc на YouTube-канале Yandex Cloud.
Task:
Описание и особенности DataLens
Decision:
Собирать, структурировать и надёжно хранить информацию — это не самоцель. Данные — лишь сырьё для анализа, после которого принимаются решения.
Анализ информации, особенно если её много, требует усилий и времени. Этот процесс можно облегчить и ускорить с помощью визуализации данных. График или диаграмма позволят заметить тенденцию, сформулировать гипотезу о причинно-следственных связях или увидеть за цифрами важный факт.
Что такое DataLens
DataLens — сервис Yandex.Cloud для визуализации и анализа данных. Он позволяет создавать чарты — диаграммы и графики, а также строить дашборды — страницы с инфографикой. Всё, что для этого нужно, — взять источник данных, создать датасет и настроить его визуальное представление.
Пример дашборда торговой сети. На чартах находятся ключевые показатели продаж, динамика продаж по категориям товаров и по магазинам, число заявок по районам города и т. д. Фактически дашборд представляет собой аналитическую панель для быстрого управления торговой сетью
Для чего нужен DataLens
С помощью DataLens можно быстро проанализировать данные, взятые напрямую из источника. Например, бизнес-показатели — количество звонков и заказов — или данные из сервисов аналитики Яндекс Метрика и AppMetriсa. Анализ помогает сформулировать гипотезы для детальной проработки или увидеть, как развивается ситуация.
Настроив дашборд, вы получите инструмент для регулярного мониторинга работы сайта, приложения или всего продукта. Например, с помощью дашборда на изображении выше сотрудники торговой сети видят, успешны ли продажи, какие товары пользуются спросом, в каких районах дела идут нормально, а где сеть не особенно популярна. Хорошо построенный дашборд помогает быстрее и лучше понять, что происходит, и вовремя принять решения.
С помощью DataLens вы можете наглядно показать результаты исследования коллегам, руководству, бизнес-партнёрам, а если захотите — то и всему интернету. Посмотрите, например, как на публичном дашборде представлена информация о пандемии COVID-19. Вы можете легко понять, как развивается ситуация с коронавирусом в мире, в отдельных странах или регионах России.
Ключевые особенности DataLens
Работа с DataLens не требует умения писать код или продвинутых технических знаний для развёртывания и администрирования инфраструктуры. Чтобы создавать чарты и дашборды, не нужна специальная подготовка, и в этом сила DataLens.
Источником данных для визуализаций могут быть базы данных (облачные и локальные), плоские файлы (простые текстовые файлы с данными, например csv), Яндекс Метрика, AppMetrica и другие сервисы. DataLens может работать с источником данных напрямую или кешировать их в кластере ClickHouse, чтобы снять нагрузку с источника и обеспечить более быстрый отклик. На дашборде или графике можно комбинировать данные из разных источников.
DataLens позволяет настраивать права доступа к объектам, в том числе на уровне строк с данными. То есть, например, можно открыть дашборд для нескольких пользователей, но каждый из них увидит данные, доступные только для него. Например, сотрудники регионального отделения компании используют дашборд с бизнес-показателями, отражающими работу своего отделения, а центральный офис видит полную картину: и по каждому филиалу, и по компании в целом.
DataLens находится в экосистеме Яндекса и интегрирован с другими сервисами. Например, он из коробки работает с ClickHouse и имеет встроенные функции для работы с геоданными Яндекс Карт, что позволяет отображать данные прямо на карте.
В маркетплейсе DataLens есть дополнительные коннекторы для источников данных (например для подключения к 1С) и внешние датасеты (например данные Яндекс Погоды).
Ещё один немаловажный момент. В отличие от многих BI-инструментов (инструментов для бизнес-аналитики) DataLens полностью бесплатный.
Обсудить технические вопросы или поделиться опытом использования DataLens вы можете в Telegram-чате сообщества визуализаторов данных Yandex DataLens.
Task:
Какие источники данных подойдут для визуализации в DataLens?
Decision:
+Сервис AppMetrica
+Сервис Яндекс.Метрика
+Базы данных
+Плоские файлы
-Видеофайлы
Task:
Обзор DataLens, модель данных
Decision:
Работа с данными в DataLens происходит так: мы подключаем источник данных, создаём из него датасет, а на основе датасета делаем чарты, из которых составляем дашборд.
В качестве источника данных DataLens использует БД (ClickHouse, MySQL, PostgreSQL, SQL Server, Oracle Database, YDB, Greenplum), которые могут находиться и в облаке, и вне него, а также csv-файлы, сервисы Google Sheets, Яндекс Метрика и AppMetrica. Подойдут и другие источники, если в маркетплейсе для них есть коннекторы. Список поддерживаемых источников постоянно пополняется.
Для доступа к источнику данных понадобится подключение. Оно содержит информацию о параметрах доступа. Например, IP-адрес хоста БД или номер порта.
Из источника данных сервис создаёт датасет: набор данных и их описание.
Так выглядит датасет со статистикой заболеваемости коронавирусом в России
Датасет может работать в двух режимах. В режиме прямого доступа DataLens направляет запросы к источнику данных каждый раз, когда создаёт визуализацию. В режиме материализации DataLens один раз загружает все данные из источника в свою БД и потом на её основе строит чарты. Материализацию используют, если не хотят нагружать исходную БД.
На основе данных из одного или нескольких датасетов создают чарты: диаграммы, графики, картограммы и таблицы. Чарты позволяют быстро оценить данные или проверить гипотезы. Подробности обо всех типах чартов DataLens вы найдёте в документации.
Столбчатая диаграмма с общим числом случаев заболевания коронавирусом по федеральным округам
Чарты с данными можно добавить на дашборд. Как правило, дашборд состоит из чартов, заголовков, поясняющих надписей и селекторов, с помощью которых к данным применяют фильтры (например, выбирают диапазон дат или категорию товара).
На дашборде чарт помещается в отдельный блок — виджет. Вы можете передвигать виджет, изменять его размер, а также название чарта в виджете.
Вот так выглядит готовый дашборд.
Дашборд с информацией о ключевых показателях работы торговой сети
Откройте этот дашборд и посмотрите, как он позволяет анализировать данные. А на следующем уроке вы создадите дашборд сами.
Модель данных
DataLens берёт данные из одной или нескольких таблиц источника. Если таблиц несколько, то их связывают, указав одинаковые поля (столбцы или строки — в зависимости от источника). Сервис может связать таблицы и автоматически по первому совпадению имени и типа данных этих полей.
Выше показано, как DataLens загружает БД торговой сети, состоящую из пяти таблиц. Между таблицами установлены связи: таблица с данными о продажах товаров связывается с таблицей с данными о магазинах сети по полю ShopID.
Из данных источника DataLens формирует датасет. Датасет состоит из полей двух типов: измерение и показатель.
Поля-измерения содержат качественные характеристики (например название магазина, товара, дата покупки). В интерфейсе эти поля зелёные.
Поля-показатели содержат количество, т. е. это числовые значения (например стоимость заказа, цена товара). В интерфейсе эти поля синие.
На чартах и дашбордах по значениям измерений агрегируются значения показателей (например по дате суммируется стоимость заказов в этот день).
Для полей-показателей обязательно определяйте функцию агрегации (например, количество, сумма, среднее) Для полей-измерений функция агрегации не используется.
Остановимся на типах полей ещё раз: это очень важно! От того, как вы определите тип поля — измерение это или показатель, — зависит логика генерации запроса в источник данных и результат их визуализации.
Измерения (дата, название продукта, регион, тип доставки и т. д.) используются для группировки и формирования разрезов в данных и отчётах. Показатели — это агрегируемые значения, метрики (выручка, число сессий, число заказов и т. д.). Их анализируют в разрезе измерений.
В зависимости от того, что мы хотим отобразить на чарте, одно и то же поле может быть и измерением, и показателем. Например, если поле «Клиент»:
    измерение — то используем его для группировки показателя «Выручка» и получим отчёт с выручкой по клиентам;
    показатель — то применим к нему правило агрегации COUNTD (Count Distinсt, количество уникальных значений) и построим отчёт, группирующий клиентов по регионам, т. е. показывающий, сколько в каждом регионе уникальных клиентов.
Если вы планируете использовать поле в качестве и измерения, и показателя, то лучше продублировать его на уровне датасета и сделать два разных поля.
На графике элементы поля-измерения располагаются по оси X, а агрегированные значения поля-показателя — по оси Y.
DataLens также позволяет создавать вычисляемые поля. Это можно сделать на уровне датасета (тогда они будут доступны во всех чартах) или на уровне чарта (поля будут видны только в нём). Вычисляемое поле — это дополнительное поле данных, его значения рассчитываются по формуле. Формулы представляют собой выражения, в которых используются функции, имена полей датасета и константы.
В DataLens более 150 функций, позволяющих выполнять операции с данными: агрегатные, логические, строковые, преобразования типов данных, работы с временными рядами. Например, функция MONTH(datetime) возвращает номер месяца в году для указанной даты:
MONTH(#2019-01-23#) = 1 
Task:
Среди доступных в DataLens типов чартов есть географическая карта. Она позволяет создавать визуализации в виде точечной, фоновой и тепловой карты. На тепловой карте показаны:
Decision:
-закрашенные области на карте
-данные о температуре, привязанные к геоточкам
+географические точки с разной интенсивностью закрашивания
Task:
Выберите правильное утверждение. Вычисляемое поле может быть:
Decision:
-только полем-измерением
-только полем-показателем
+и полем-измерением, и полем-показателем
Task:
Выберите поля датасета, которые больше всего похожи на показатели и требуют указать правило агрегации:
Decision:
-Дата
-Название продукта
-Категория продукта
-Бренд продукта
-Продавец
-Покупатель
-Магазин
-Тип оплаты
-Тип доставки
+Продажа без скидки, р
+Продажа со скидкой, р
Task:
Представьте, что с помощью этого датасета вы хотите определить число потенциальных участников планирующейся рекламной акции «Разнообразное питание». Для этого нужно понять, сколько покупателей в прошлом месяце приобретали много разных продуктов. Дополнительный показатель из какого измерения понадобится определить?
Decision:
-Дата
+Название продукта
-Категория продукта
-Бренд продукта
-Продавец
-Покупатель
-Магазин
-Тип оплаты
-Тип доставки
-Продажа без скидки, р
-Продажа со скидкой, р
Task:
Практическая работа. Создание датасетов, чартов и дашбордов
Decision:
Приступим к практике. На этом уроке вы научитесь создавать чарты и дашборды. Мы пройдём по всей цепочке сущностей DataLens начиная с источника данных.
Изучая ClickHouse, мы анализировали данные о погоде с помощью SQL-запросов. Давайте посмотрим на примере того же самого набора данных, как с помощью DataLens быстро и наглядно показать отличия климата в Москве и Санкт-Петербурге.
Источник данных
ClickHouse и DataLens интегрированы друг с другом, поэтому подключение DataLens к ClickHouse можно настроить всего за пару кликов.
В консоли управления запустите кластер ClickHouse, в котором развёрнута БД с таблицей Weather, созданной вами ранее. Перейдите на страницу кластера, на панели слева выберите DataLens.
Подключение
Нажмите кнопку Создать подключение. В открывшемся диалоговом окне вы увидите, что кластер ClickHouse, из которого мы возьмём данные для анализа, имя хоста и имя пользователя БД уже указаны.
Вам осталось только дать имя подключению в пустом поле вверху, ввести пароль к БД, нажать кнопку Проверить подключение и убедиться, что всё в порядке, а потом — кнопку Создать.
Датасет
После того как подключение будет создано, DataLens выведет на панели слева таблицы из БД и предложит создать датасет. Наш датасет будет состоять из одной таблицы: db1.Weather. Перетащите её на центральную панель, и внизу откроется предпросмотр данных.
Нажмите кнопку Сохранить и задайте имя датасета.
Подготовим данные. Это важная часть аналитической работы, и её не стоит пропускать. Прежде всего укажем имена полей на русском языке. Перейдите на вкладку Поля и переименуйте их:
    LocalDateTime → Дата и время
    LocalDate → Дата
    Month → Месяц
    Day → День
    TempC → Температура
    Pressure → Давление
    RelHumidity → Влажность
    Тип WindSpeed10MinAvg → Скорость ветра
    VisibilityKm → Видимость
    City → Город
Поля Дата и время, Дата, Месяц, День, Город будут полями-измерениями, а Температура, Давление, Влажность, Скорость ветра, Видимость — полями-показателями. Зададим для показателей тип агрегации Среднее.
Чарты
Приступим к созданию первого чарта. Нажмите кнопку Создать чарт. Выберите тип чарта Линейная диаграмма и перетащите Дата в раздел X , а Температура — в раздел Y.
На этом примере видно, что средства визуализации иногда помогают быстро проверить качество датасета: есть ли в нём пропущенные или странные, выбивающиеся из общей тенденции данные.
В нашем случае можно сделать вывод о том, что в датасете не хватает данных примерно с середины 2015-го по середину 2016-го.
Разделим показатели температуры для двух городов. Для этого перетащим Город в раздел Цвета. Кроме того, округлим значения поля Дата до месяцев, чтобы лучше увидеть, как различаются данные для Москвы и Санкт-Петербурга. Для этого слева от поля Дата нажмите зелёный значок календаря и в разделе Группировка выберите округление по месяцам.
Из этого графика уже можно делать выводы. В целом температура в Москве выше, чем в Санкт-Петербурге. Летом примерно на 5 градусов, зимой — на 1−2 градуса.
Сохраните чарт, чтобы затем использовать его для дашборда.
Чтобы окончательно разобраться с температурой, построим ещё один чарт — Столбчатую диаграмму — и сравним среднегодовую температуру. Выберите тип диаграммы. Добавьте поле Город в раздел X, чтобы разделить отображение значений температуры. Также для поля Дата выберите группировку по годам.
Кроме того, для чарта понадобится задать фильтр по датам. Поскольку мы сравниваем среднегодовые значения, неполные данные за 2009 и 2019 годы отбросим. В разделе Фильтры нажмите + и выберите поле Дата.
Для этого чарта мы возьмём только данные из диапазона с начала 2010-го по конец 2018-го. Нажмите кнопку Применить фильтр и сохраните чарт.
Сделайте сами два таких же чарта с данными о скорости ветра: линейную диаграмму со среднемесячными значениями скорости ветра в городах и столбчатую диаграмму со среднегодовыми значениями.
Теперь у нас достаточно чартов для информативного дашборда.
Дашборд
На панели слева выберите Дашборды и нажмите кнопку Создать дашборд. Введите название дашборда и нажмите Создать.
Если в каталоге это первый дашборд — он откроется сразу после создания. Если в каталоге есть другие дашборды, вы увидите список. В этом случае выберите из списка только что созданный дашборд.
Теперь добавим созданные нами чарты на дашборд. Нажмите Добавить и в выпадающем списке выберите Чарт. Поочерёдно выбирайте из списка и добавляйте чарты.
В результате на дашборде появятся четыре виджета с чартами. Меняйте размеры и положение виджетов для лучшей визуализации.
Осталось лишь несколько последних штрихов. В том же пункте меню Добавить создадим пару заголовков и селектор по датам. В правом верхнем углу каждого виджета нажмите значок шестерёнки, чтобы изменить названия.
Сохраним дашборд. У вас могло получиться примерно так:
То, какие чарты сделать и как их разместить на дашборде, бывает понятно не сразу. Рассмотрите несколько вариантов, когда строите дашборд, чтобы разобраться, какая именно визуализация лучше помогает ответить на вопросы.
В маркетплейсе DataLens вы найдёте ещё один дашборд с погодой. Он хорошо демонстрирует возможности визуализации данных этого сервиса.
Чтобы открыть публичный доступ к дашборду, справа от его названия нажмите ··· и выберите Публичный доступ. Скопируйте ссылку. По ней дашборд будет доступен всем, с любых устройств и без аутентификации.
Task:
Выберите правильные утверждения. Отказоустойчивая система:
Decision:
-может выйти из строя, но быстро восстанавливается после сбоя
+имеет избыточность ресурсов
+способна работать даже в случае отказа на уровне дата-центра
+продолжает работать при выходе из строя одного из ее компонентов
Task:
Управляемые БД снимают с пользователя большую часть задач по обслуживанию системы. В частности, они обеспечивают:
Decision:
+мониторинг работы хостов и БД
+обновление ПО
+репликацию данных между хостами кластера
-автоматическое масштабирование системы при изменении нагрузки
+резервное копирование БД
Task:
Вы создаете кластер управляемой БД в облаке и хотите, чтобы ваша БД могла выдержать высокую нагрузку. Какой тип хранилища данных вам следует выбрать?
Decision:
-network-ssd
-network-hdd
+local-ssd
Task:
Объектное хранилище может использоваться для хранения:
Decision:
+очень больших файлов размером в несколько терабайт
+статических веб-сайтов
-реплики БД
+резервной копии БД
Task:
Шардирование позволяет:
Decision:
+распределить нагрузку на БД и повысить ее производительность
+ускорить выполнение запросов к БД
+повысить доступность БД
Task:
Для OLAP-сценариев работы с БД характерно, что:
Decision:
+большинство запросов — это операции чтения
+данные добавляются в БД большими порциями
-к консистентности данных предъявляются строгие требования
+на выходе данные фильтруют или агрегируют
-устанавливаются ослабленные режимы изоляции транзакций
Task:
Технологии работы с большими данными основаны на необходимости:
Decision:
+горизонтальной масштабируемости используемых вычислительных ресурсов
-надежной обработки транзакций
+обеспечения отказоустойчивости систем
+распределенного хранения и обработки данных
Task:
В датасете, который вы используете для визуализации данных в DataLens, есть такие поля: «Название производителя», «Категория товара», «Бренд», «Год выпуска». Эти поля, как правило, являются:
Decision:
-показателями
+измерениями
Task:
Вы разрабатываете приложение, для хранения данных в котором используется кластер управляемой БД MySQL c двумя хостами типа s2.large и быстрым локальным хранилищем размером 100 ГБ. Завершив очередной этап тестирования приложения, вы остановили кластер. За какие ресурсы будет продолжать начисляться оплата?
Decision:
+хосты БД
+хранилище данных
+хранение резервных копий
Task:
С помощью политик доступа к объектному хранилищу вы можете:
Decision:
-выдать пользователям определённые роли
+настроить дополнительные условия действий с бакетами и объектами
-задать шаблон для генерирования подписанных ссылок на объекты
Task:
CLI Yandex Cloud
Decision:
Если вы разработали отличную программу или сервис — это только часть успеха. Важно, чтобы ваш сервис был надёжным и быстрым, выдерживал пиковые нагрузки и не падал при каждом обновлении.
Лет 10–20 назад, чтобы запустить новый сервис иногда было достаточно скопировать пачку скриптов на сервер. Сегодня сервис нужно не только создать, но еще и правильно внедрить и поддерживать. На это повлияло несколько причин:
    Сами сервисы стали сложнее, они часто используют сторонние библиотеки, причем строго определенных версий. Чтобы избежать конфликтов, сервисы стали изолировать друг от друга — помещать в виртуальные машины или контейнеры.
    С появлением методологии Agile, когда развитие приложений стало практически непрерывным, пришлось создавать и новые практики IT-администрирования.
    Рост числа пользователей интернета привел к тому, что практически каждый публичный сервис приходится проектировать как высоконагруженный.
    Многие сервисы стали жизненно важными для миллионов пользователей. Такие сервисы должны оставаться доступными невзирая на пиковые нагрузки и стихийные бедствия.
Как правило, поддержка IT-инфраструктуры не входит в сферу ответственности разработчиков. Раньше инфраструктурой занимались системные администраторы. Сегодня, когда эта область знаний стала гораздо шире, ее принято называть Operations. На стыке разработки и администрирования возникла новая специализация — DevOps.
Область интересов DevOps — выстраивание эффективных процессов внедрения приложений и сервисов, управления серверами и сетями. Специально для DevOps создаются инструменты для поддержки инфраструктуры, в том числе облачной.
Чем выше нагрузка на сервис, тем важнее становится роль DevOps. И поскольку сегодня одни из самых нагруженных сервисов — это поисковики, именно эти компании стали «гуру» в сфере DevOps. Так, Google собрал ключевые знания в книге SRE Book (SRE — Site Reliability Engineering).
Практики DevOps ориентированы на то, чтобы найти баланс между простотой внедрения, скоростью внесения изменений и надёжностью сервисов.
Вот основные направления современного DevOps, которые мы рассмотрим в рамках данного курса:
    Автоматизация. Чем меньше операций выполняется вручную, тем меньше вероятность ошибок. Кроме того, автоматизация позволяет внедрить в практику регулярно выполняемые операции (например резервное копирование).
    Infrastructure as Code (IaC). Суть этого подхода заключается в том, что инфраструктура описывается в текстовых файлах специального формата — как настройки отдельных объектов и взаимосвязи между этими объектами. Такие файлы обычно называют спецификациями или манифестами. С этими файлами можно работать так же, как с программным кодом — отслеживать изменения в них, отдавать на ревью, хранить историю версий и обмениваться.
    Простота настройки окружения. Как мы уже говорили выше, современные сервисы принято изолировать друг от друга и упаковывать в контейнеры или отдельные виртуальные машины.
    Простота масштабирования. Поскольку нагрузка на сервисы меняется, нужно уметь выделять необходимые ресурсы и сворачивать их, когда они не нужны. Это особенно актуально в облачной инфраструктуре.
    Отказоустойчивость. Чтобы сервис работал бесперебойно, нужно правильно накатывать обновления, не допускать отказов и выявлять потенциальные проблемы до того, как их заметят пользователи.
Task:
Как пользоваться CLI Yandex Cloud
Decision:
Что такое интерфейс командной строки Yandex Cloud и для чего он нужен
Предположим, вы пишете скрипт, который автоматизирует рутинные задачи в облаке: выгружает аналитику об аккаунте или пересоздаёт виртуальные машины (ВМ) при обновлении операционной системы. Или, возможно, вы гик и писать команды в консоли вам куда приятнее, чем кликать мышкой в веб-интерфейсе. В обоих случаях вам понравится утилита CLI Yandex Cloud (yc).
Эта утилита предоставляет интерфейс командной строки (command-line interface, CLI) для управления Yandex Cloud. На курсе мы станем использовать термины yc и CLI как синонимы, потому что других консольных интерфейсов у нас не будет. Утилита yc также подходит для взаимодействия с облаком из систем без графического интерфейса, например прямо с ВМ на Linux.
В этой теме вам предстоит выполнить три практические работы по утилите yc. А пока давайте просто познакомимся с ней поближе.
Работа в командной строке — это всегда последовательный запуск команд. В нашем CLI команды выглядят примерно так:
yc compute instance list 
Эта команда выводит список ваших ВМ:
+----------------------+------+---------------+---------+-------------+
|          ID          | NAME |    ZONE ID    | STATUS  | INTERNAL IP |
+----------------------+------+---------------+---------+-------------+
| epdnq5kcsgs8tg31c2id | api1 | ru-central1-b | RUNNING | 172.17.0.23 |
| ef3jbvule3hhd05fd8s6 | api2 | ru-central1-c | RUNNING | 172.18.0.5  |
| fhm90d339g7vhu2971f9 | api3 | ru-central1-a | RUNNING | 172.16.0.5  |
+----------------------+------+---------------+---------+-------------+ 
Важно! Из-за особенностей синтаксиса командных оболочек cmd и PowerShell в ОС Windows некоторые команды yc могут выполняться некорректно. Рекомендуем использовать альтернативные варианты:
    сторонняя командная оболочка, например Git BASH;
    подсистемы Windows для Linux (WSL);
    терминал на компьютере или виртуальной машине с ОС Linux / macOS.
CLI и сервисы Yandex Cloud
Команды CLI разделены на группы, каждая из которых соответствует сервису или компоненту Yandex Cloud. Например:
    yc resource-manager … — управление облаками и каталогами;
    yc compute … — управление ВМ;
    yc load-balancer … — управление балансировщиками нагрузки.
Управление кластерами баз данных:
    yc managed-mysql … — MySQL;
    yc managed-postgresql … — PostgreSQL;
    yc managed-clickhouse … — ClickHouse.
Есть ещё небольшая группа служебных команд:
    yc init — первоначальная настройка CLI;
    yc version — показывает версию CLI;
    yc help — выводит описание всех команд или справку о команде.
Документация о CLI
На предыдущих курсах вы пользовались консолью управления облаком и документацией сервисов Yandex Cloud. Так вот: большинство действий в облаке можно выполнить не только из консоли управления, но и из командной строки — с помощью утилиты yc.
На странице документации, где описывается операция, можно посмотреть, как выполнить её в консоли управления и в CLI. Это понадобится, например, если вы захотите автоматизировать действия и написать скрипт.
Иногда проще найти команду в документации о CLI: там есть краткий хорошо структурированный справочник команд.
Кроме того, в документации о CLI описано, как установить yc и что делать, если возникают ошибки.
Структура команды CLI
Большинство команд yc строится по следующему принципу:
    Сначала указывается сервис Yandex Cloud (или группа служебных команд). Например, compute.
    Затем ресурс, которым управляет сервис. Например, instance.
    Потом действие, которое необходимо выполнить над ресурсом. Например, get.
    И в конце — параметры и флаги, меняющие поведение команды, а также (если нужно) идентификатор ресурса.
Например:
В первой команде мы обращаемся к сервису resource-manager и просим его создать (create) каталог (folder), указав имя каталога в параметре --name. Создадим каталог с именем project-1:
yc resource-manager folder create --name project-1 
Результат:
id: b1g4i1fr007e94ut1kc7
cloud_id: b1gttd235imdk2fdud9p
created_at: "2021-03-27T06:26:39.656888Z"
name: project-1
status: ACTIVE 
Где:
    id — идентификатор созданного каталога.
    cloud_id — идентификатор облака, в котором размещён каталог.
    created_at — дата и время создания каталога.
    name — имя каталога.
    status — текущий статус каталога.
Идентификаторы и параметры ресурсов уникальны. Идентификаторы и параметры ваших ресурсов будут отличаться от тех, что приведены в примерах. Копируя команды из урока, подставляйте свои значения. Чтобы выделить часть команды, которую нужно заменить, мы будем использовать <плейсхолдеры>. При подстановке своих значений воспроизводить символы <> не нужно.
Некоторые команды принимают сразу несколько параметров. Например, давайте поменяем описание только что созданного каталога:
yc resource-manager folder update \
  --name project-1 \
  --description "Learning CLI" 
Теперь в информации о каталоге выводится и его описание:
id: b1g4i1fr007e94ut1kc7
cloud_id: b1gttd235imdk2fdud9p
created_at: "2021-03-27T06:26:39Z"
name: project-1
description: Learning CLI
status: ACTIVE 
Чтобы команды лучше читались, их разбивают на несколько строк с помощью символа \ (благодаря ему командная строка поймёт, что вы ввели единую команду, а не три разных строчки).
В командных оболочках cmd и PowerShell вместо символа \ для переноса строки используются ^ и ` соответственно. В наших примерах мы будем использовать \.
Использование CLI
Во всех вызовах yc есть что-то общее. Например, для любой команды можно указать, в каком облаке и каталоге её выполнить. Для некоторых команд можно указать, в каком формате вы хотите получить данные: в текстовом, JSON или YAML.
Параметры, которые можно использовать с любой командой, называются глобальными. Пример такого параметра — название каталога, в котором надо выполнить команды:
yc compute instance list --folder-name project-1 
Некоторые глобальные параметры нельзя указывать вместе. Например, вы можете выбрать либо название каталога (--folder-name …), либо его идентификатор (--folder-id …), но не оба сразу.
Профили
Обычно для каждого проекта в Yandex Cloud создаётся отдельный каталог, работа ведётся в нём. Если у вас несколько каталогов — вам быстро надоест указывать --folder-id или --folder-name в каждой команде. Чтобы облегчить вашу жизнь, придуманы профили. Профиль — это набор настроек, таких как имя облака и каталога, которые автоматически применяются к каждой команде. Чтобы переключиться на другой каталог, вы создаёте профиль (или переключаетесь на созданный ранее) и продолжаете работу уже в нём.
Представьте, что вы только начали проект. Создали для него каталог и теперь создаёте профиль project-1:
yc config profile create project-1
yc config set folder-name project-1 --profile project-1 
Убедимся, что профиль запомнил имя каталога:
yc config list --profile project-1 
Результат выполнения команды должен быть таким:
folder-name: project-1 
В большинстве случаев при работе с облаком достаточно профиля по умолчанию — default. Он создаётся при первом использовании программы. Давайте снова переключимся из профиля project-1 в профиль по умолчанию и продолжим работу в нём:
yc config profile activate default 
Какие параметры можно задать в профиле, а какие — только для команд, смотрите в разделе Конфигурация CLI документации Yandex Cloud.
Task:
Утилита yc нужна, чтобы...
Decision:
-подключаться к Yandex Cloud
+управлять сервисами Yandex Cloud
-управлять аккаунтами Yandex Cloud
Task:
Как можно задавать параметры выполнения команд в yc?
Decision:
-С помощью профиля
-С помощью глобальных флагов
+Оба варианта верны
Task:
Практическая работа. Начало работы в CLI
Decision:
На этой практической работе мы установим утилиту yc, познакомимся с режимом подсказок --help и выполним несколько простых команд.
Установка CLI
Первым делом скачайте и установите CLI (если вы ещё не сделали этого в предыдущих курсах).
Теперь настройте программу для работы с вашим аккаунтом и облаком. Для этого запустите командную строку и введите команду:
yc init 
В консоли появится предложение перейти по ссылке, чтобы программа получила доступ к аккаунту и облаку. Перейдите по ссылке, согласитесь с условиями, затем скопируйте токен и вставьте его в окно командной строки.
Срок жизни токена — один год. Через год необходимо получить новый токен и повторить аутентификацию.
Если кто-то узнал ваш токен, отзовите его и запросите новый.
Продолжайте установку: выберите облако, каталог и зону доступности по умолчанию. Следуйте указаниям системы и завершите настройку.
Готово!
При установке автоматически создается профиль default, в него записываются выбранные настройки.
Список профилей можно посмотреть командой:
yc config profile list 
Флаг --help
На прошлом уроке вы узнали о флагах, которые используются при запуске команд. Пожалуй, самый важный и частый флаг — --help (сокращенно -h), поэтому поговорим о нём подробнее.
Команд для управления ресурсами облака много. Вы запомните некоторые, но помнить всё невозможно. Пользуйтесь флагом --help, когда пишете команду. В этом случае она не выполняется, а в консоль выводится информация о ресурсах, которыми управляет команда, и параметрах запуска.
Помните, на предыдущем уроке мы говорили о типичной структуре большинства команд CLI (сервис — ресурс — действие — флаги)?
Важная особенность флага --help заключается в том, что его можно применять для каждого уровня этой структуры и писать команду постепенно.
В этой практической работе нам понадобится ВМ в облаке. Если у вас нет ВМ — создайте её в консоли управления, как мы это делали в предыдущих курсах.
Допустим, вы хотите перезапустить эту ВМ, но не помните полный синтаксис команды.
    Сначала вызовите описание сервиса Yandex Compute Cloud:
yc compute --help 
Вы увидите синтаксис команды (раздел Usage), список ресурсов, которыми можно управлять (Groups), а также действий (Commands) и флагов:
Usage:
  yc compute <group|command>
Groups:
  instance               Manage virtual machine instances
  disk                   Manage disks
  disk-type              Show available disk types
  image                  Manage images
  snapshot               Manage snapshots
  zone                   Show availability zones
  instance-group         Manage instance groups
  placement-group        Manage placement groups
  host-type              Show available host types
  host-group             Manage host groups
  disk-placement-group   Manage disk placement groups
  filesystem             Manage filesystems
Commands:
  connect-to-serial-port Connect to serial port
Global Flags:
      --profile string       Set the custom configuration file.
      --debug                Debug logging.
      --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
      --no-user-output       Disable printing user intended output to stderr.
      --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                             Pass 0 to disable retries. Pass any negative value for infinite retries.
                             Even infinite retries are capped with 2 minutes timeout.
      --cloud-id string      Set the ID of the cloud to use.
      --folder-id string     Set the ID of the folder to use.
      --folder-name string   Set the name of the folder to use (will be resolved to id).
      --endpoint string      Set the Cloud API endpoint (host:port).
      --token string         Set the OAuth token to use.
      --format string        Set the output format: text (default), yaml, json, json-rest.
  -h, --help                 Display help for the command. 
    Вам нужна ВМ — ресурс instance. Теперь узнайте, как получить список активных ВМ и какая команда отвечает за перезапуск:
yc compute instance --help 
Вы увидите список возможных действий (команд) над ВМ:
Manage virtual machine instances
Usage:
  yc compute instance <command>
Aliases:
  instances
Commands:
  get                      Show information about the specified virtual machine instance
  list                     List virtual machine instances
  create                   Create a virtual machine instance
  create-with-container    Create a virtual machine instance running Docker container
  update                   Update the specified virtual machine instance
  update-container         Update the specified virtual machine instance running Docker container
  add-metadata             Add or update metadata for the specified virtual machine instance
  remove-metadata          Remove keys from metadata for the specified virtual machine instance
  add-labels               Add labels to specified virtual machine instance
  remove-labels            Remove labels from specified virtual machine instance
  delete                   Delete the specified virtual machine instance
  get-serial-port-output   Return the serial port output of the specified virtual machine instance
  stop                     Stop the specified virtual machine instance
  start                    Start the specified virtual machine instance
  restart                  Restart the specified virtual machine instance
  attach-disk              Attach existing disk to the the specified virtual machine instance
  attach-new-disk          Attach new disk to the the specified virtual machine instance
  detach-disk              Detach disk from the the specified virtual machine instance
  attach-filesystem        Attach existing filesystem to the specified virtual machine instance
  detach-filesystem        Detach filesystem from the specified virtual machine instance
  update-network-interface Update the specified network interface of virtual machine instance
  add-one-to-one-nat       Add one-to-one NAT to the the specified network interface of virtual machine instance
  remove-one-to-one-nat    Remove one-to-one NAT to the the specified network interface of virtual machine instance
  list-operations          List operations for the specified instance
Global Flags:
      --profile string       Set the custom configuration file.
      --debug                Debug logging.
      --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
      --no-user-output       Disable printing user intended output to stderr.
      --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                             Pass 0 to disable retries. Pass any negative value for infinite retries.
                             Even infinite retries are capped with 2 minutes timeout.
      --cloud-id string      Set the ID of the cloud to use.
      --folder-id string     Set the ID of the folder to use.
      --folder-name string   Set the name of the folder to use (will be resolved to id).
      --endpoint string      Set the Cloud API endpoint (host:port).
      --token string         Set the OAuth token to use.
      --format string        Set the output format: text (default), yaml, json, json-rest.
  -h, --help                 Display help for the command. 
    Сначала получите список ВМ, это команда list:
yc compute instance list 
Результатом выполнения команды будет список машин в том каталоге, где вы работаете, с именами и идентификаторами:
+----------------------+-------------+---------------+---------+----------------+-------------+
|          ID          |    NAME     |    ZONE ID    | STATUS  |  EXTERNAL IP   | INTERNAL IP |
+----------------------+-------------+---------------+---------+----------------+-------------+
| fhm2p20bifmg3k3voda7 | my-instance | ru-central1-a | RUNNING | ХХХ.ХХХ.ХХХ.ХХ | ХХ.ХХХ.Х.ХХ |
+----------------------+-------------+---------------+---------+----------------+-------------+ 
    На шаге 2 вы нашли команду перезапуска — restart, теперь можно посмотреть её синтаксис:
yc compute instance restart --help 
Результат выполнения будет таким:
Restart the specified virtual machine instance
Usage:
  yc compute instance restart <INSTANCE-NAME>|<INSTANCE-ID> [Global Flags...]
Flags:
      --id string     Instance id.
      --name string   Instance name.
      --async         Display information about the operation in progress, without waiting for the operation to complete.
Global Flags:
      --profile string       Set the custom configuration file.
      --debug                Debug logging.
      --debug-grpc           Debug gRPC logging. Very verbose, used for debugging connection problems.
      --no-user-output       Disable printing user intended output to stderr.
      --retry int            Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.
                             Pass 0 to disable retries. Pass any negative value for infinite retries.
                             Even infinite retries are capped with 2 minutes timeout.
      --cloud-id string      Set the ID of the cloud to use.
      --folder-id string     Set the ID of the folder to use.
      --folder-name string   Set the name of the folder to use (will be resolved to id).
      --token string         Set the OAuth token to use.
      --format string        Set the output format: text (default), yaml, json, json-rest.
  -h, --help                 Display help for the command. 
    Наконец, вы получили полный синтаксис команды перезапуска. Примените её к ВМ:
yc compute instance restart --name <имя_ВМ> 
По такому принципу вы можете сформировать в консоли любую команду.
Итак, подведём итоги. Чтобы уточнить синтаксис или порядок действий при работе с CLI, выберите один из трёх путей:
    найдите нужное действие на странице сервиса Yandex Cloud в документации и там переключитесь на вкладку CLI;
    найти команду в справочнике о yc;
    воспользуйтесь флагом --help, шаг за шагом уточняя возможности и синтаксис команды.
Синхронный и асинхронный режимы работы
Некоторые команды выполняются очень быстро. Например, создание каталога или просмотр настроек профиля. Такие команды можно выполнять подряд, одну за другой — без задержек.
Если при выполнении команды ресурс изменяет состояние, создается операция. Примеры операций — перезапуск ВМ после обновления или резервное копирование базы данных.
Если каждая следующая команда ждёт завершения предыдущей операции, такой режим выполнения называется синхронным. Когда какая-то операция в синхронном режиме выполняется долго, CLI выводит в консоли точки и время с начала операции, чтобы показать, что процесс не завис:
...1s...6s...12s...17s 
Операция может выполняться довольно долго, а ожидание — затормозить процесс. В таких случаях важно оценить, нужен ли результат операции для выполнения следующих команд. Если нет — можно не ждать её завершения и сразу же переходить к следующей команде. Этот режим называется асинхронным.
Чтобы выполнить команду асинхронно, используйте флаг --async.
Перезапустите одну из ранее созданных ВМ в асинхронном режиме (в команде укажите имя этой ВМ):
yc compute instance restart --name <имя_ВМ> --async 
В ответ на асинхронный вызов CLI выводит идентификатор операции (в поле id) и информацию о ней:
id: fhm5k7iq03rm2s7enhdk
description: Restart instance
created_at: "2021-03-27T08:32:47.562595036Z"
created_by: aje9cb7k03512mrugcee
modified_at: "2021-03-27T08:32:47.562595036Z"
metadata:
  '@type': type.googleapis.com/yandex.cloud.compute.v1.RestartInstanceMetadata
  instance_id: fhm2p20bifmg3k3voda7 
С помощью идентификатора операции вы можете проверить результаты её выполнения:
yc operation get <идентификатор_операции> 
Когда операция завершится, вы увидите результат:
id: fhm5k7iq03rm2s7enhdk
...
done: true
... 
Decision:
$ yc init
$ yc config profile list
$ yc compute --help
$ yc compute instance --help
$ yc compute instance list
$ yc compute instance restart --help
$ yc compute instance restart --name ubuntu-test
$ yc compute instance restart --name ubuntu-test --async
$ yc operation get fhmq5a4aren4lmigbt03
Task:
Практическая работа. Создание виртуальных машин с помощью CLI
Decision:
Создание ВМ — одна из самых сложных команд CLI, потому что в ней очень много параметров. Давайте потренируемся работать с ней.  Но сначала подготовим окружение.
Мы будем работать в каталоге по умолчанию. Создадим в нём сеть, в ней — три подсети, а затем по ВМ в каждой подсети.
    Создайте сеть my-network в Virtual Private Cloud. Эта команда относится к группе vpc.
Проверьте синтаксис команды
Выполнение операции займёт какое-то время. В итоге сеть появится в каталоге, который вы выбрали в предыдущей практической работе.
    Теперь создадим три подсети в разных зонах доступности (ru-central1-a, ru-central1-b, ru-central1-c). Чтобы проще было выполнять задания дальше, назовите их my-subnet-1, my-subnet-2 и my-subnet-3. А пространства IP-адресов для подсетей укажите, соответственно, как 192.168.1.0/24, 192.168.2.0/24 и 192.168.3.0/24.
Не забудьте указать, что вы создаёте подсети в новой сети, созданной на предыдущем шаге. Иначе они появятся в сети по умолчанию, которая есть в каждом каталоге.
Вот так выглядит команда создания подсети в зоне доступности ru-central1-a:
yc vpc subnet create \
  --name my-subnet-1 \
  --zone ru-central1-a \
  --range 192.168.1.0/24 \
  --network-name my-network 
Где:
    name — имя подсети.
    zone — зона доступности.
    range — адресное пространство подсети.
    network-name — имя сети, в которой создаётся подсеть.
Создайте две другие подсети сами.
Проверьте синтаксис команд
yc vpc subnet create \
  --name my-subnet-2 \
  --zone ru-central1-b \
  --range 192.168.2.0/24 \
  --network-name my-network
yc vpc subnet create \
  --name my-subnet-3 \
  --zone ru-central1-c \
  --range 192.168.3.0/24 \
  --network-name my-network 
      Осталось создать три ВМ в нужных зонах доступности и привязать к ним подсети.
Пусть машины работают под управлением ОС Ubuntu 20.04 LTS, имеют диски объёмом 30 Гб, 4 Гб оперативной памяти и два виртуальных процессорных ядра.
ВМ могут создаваться долго, поэтому запускайте команды в асинхронном режиме.
Пример команды для первой ВМ:
yc compute instance create \
  --name my-instance-1 \
  --hostname my-instance-1 \
  --zone ru-central1-a \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
  --async 
Посмотрите внимательно на все параметры. Подумайте, какой из них за что отвечает.
Создайте две другие машины сами.
Проверьте синтаксис команд
yc compute instance create \
  --name my-instance-2 \
  --hostname my-instance-2 \
  --zone ru-central1-b \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
  --async
 yc compute instance create \
  --name my-instance-3 \
  --hostname my-instance-3 \
  --zone ru-central1-c \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
  --async 
После выполнения каждой команды благодаря флагу --async вы получите идентификатор операции и ее описание в виде:
id: c9q9v4bsn1hs9api4b13
description: Create instance
created_at: "2021-03-01T03:23:00.079888Z"
created_by: aje8s4vd4pp7cduq2o4k
modified_at: "2022-07-29T09:14:53.567744154Z"
metadata:
  '@type': type.googleapis.com/yandex.cloud.compute.v1.CreateInstanceMetadata
  instance_id: fhmepiaciq5l9slqid3k 
    Проследите за статусом одной из операций, используя ее идентификатор.
Проверьте синтаксис команды
yc operation get <идентификатор_операции> 
    Дождитесь, пока операция завершится. Используйте для этого команду wait.
Проверьте синтаксис команды
yc operation wait <идентификатор_операции> 
    Убедитесь, что ВМ созданы. Для этого выведите их список.
yc compute instance list 
По умолчанию список выдается в виде таблицы:
+----------------------+---------------+---------------+---------+---------------+--------------+
|          ID          |     NAME      |    ZONE ID    | STATUS  |  EXTERNAL IP  | INTERNAL IP  |
+----------------------+---------------+---------------+---------+---------------+--------------+
| ef34r4fs8dsva3qtsivs | my-instance-3 | ru-central1-c | RUNNING | 51.250.44.130 | 192.168.3.34 |
| epdj7u79isrolup3vfo8 | my-instance-2 | ru-central1-b | RUNNING | 158.160.6.249 | 192.168.2.28 |
| fhmepiaciq5l9slqid3k | my-instance-1 | ru-central1-a | RUNNING | 62.84.126.39  | 192.168.1.30 |
+----------------------+---------------+---------------+---------+---------------+--------------+ 
Список можно вывести в формате YAML или JSON (эта возможность пригодится вам на следующих уроках):
yc compute instance list --format json 
Список в формате JSON содержит больше информации, чем таблица.
Посмотрите результат
[
  {
    "id": "ef3rutmaas72bsujcja7",
    "folder_id": "b1gfdbij3ijgopgqv9m9",
    "created_at": "2021-06-21T12:41:10Z",
    "name": "my-instance-3",
    "zone_id": "ru-central1-c",
    "platform_id": "standard-v2",
    "resources": {
      "memory": "4294967296",
      "cores": "2",
      "core_fraction": "100"
    },
    "status": "RUNNING",
    "metadata_options": {
      "gce_http_endpoint": "ENABLED",
      "aws_v1_http_endpoint": "ENABLED",
      "gce_http_token": "ENABLED",
      "aws_v1_http_token": "ENABLED"
    },
    "boot_disk": {
      "mode": "READ_WRITE",
      "device_name": "ef3v2lor1u4pfn3ce1al",
      "auto_delete": true,
      "disk_id": "ef3v2lor1u4pfn3ce1al"
    },
    "network_interfaces": [
      {
        "index": "0",
        "mac_address": "d0:0d:1b:f7:6c:a5",
        "subnet_id": "b0c4h992tbuodl5hudpu",
        "primary_v4_address": {
          "address": "10.128.0.32",
          "one_to_one_nat": {
            "address": "178.154.212.5",
            "ip_version": "IPV4"
          }
        }
      }
    ],
    "fqdn": "my-instance-3.ru-central1.internal",
    "scheduling_policy": {},
    "network_settings": {
      "type": "STANDARD"
    },
    "placement_policy": {}
  },
  {
    "id": "epd928ffks7m8ssc4i3k",
    "folder_id": "b1gfdbij3ijgopgqv9m9",
    "created_at": "2021-06-21T12:40:35Z",
    "name": "my-instance-2",
    "zone_id": "ru-central1-b",
    "platform_id": "standard-v2",
    "resources": {
      "memory": "4294967296",
      "cores": "2",
      "core_fraction": "100"
    },
    "status": "RUNNING",
    "metadata_options": {
      "gce_http_endpoint": "ENABLED",
      "aws_v1_http_endpoint": "ENABLED",
      "gce_http_token": "ENABLED",
      "aws_v1_http_token": "ENABLED"
    },
    "boot_disk": {
      "mode": "READ_WRITE",
      "device_name": "epddf7t0ljn9i1jp2pbs",
      "auto_delete": true,
      "disk_id": "epddf7t0ljn9i1jp2pbs"
    },
    "network_interfaces": [
      {
        "index": "0",
        "mac_address": "d0:0d:91:21:ef:a7",
        "subnet_id": "e2l1fgq2fbhnp6b929t7",
        "primary_v4_address": {
          "address": "10.129.0.9",
          "one_to_one_nat": {
            "address": "84.201.176.134",
            "ip_version": "IPV4"
          }
        }
      }
    ],
    "fqdn": "my-instance-2.ru-central1.internal",
    "scheduling_policy": {},
    "network_settings": {
      "type": "STANDARD"
    },
    "placement_policy": {}
  },
  {
    "id": "fhm1op9id0dc6bubfags",
    "folder_id": "b1gfdbij3ijgopgqv9m9",
    "created_at": "2021-06-21T12:39:43Z",
    "name": "my-instance-1",
    "zone_id": "ru-central1-a",
    "platform_id": "standard-v2",
    "resources": {
      "memory": "4294967296",
      "cores": "2",
      "core_fraction": "100"
    },
    "status": "RUNNING",
    "metadata_options": {
      "gce_http_endpoint": "ENABLED",
      "aws_v1_http_endpoint": "ENABLED",
      "gce_http_token": "ENABLED",
      "aws_v1_http_token": "ENABLED"
    },
    "boot_disk": {
      "mode": "READ_WRITE",
      "device_name": "fhmms7r7ia4uteikv1to",
      "auto_delete": true,
      "disk_id": "fhmms7r7ia4uteikv1to"
    },
    "network_interfaces": [
      {
        "index": "0",
        "mac_address": "d0:0d:1c:65:32:68",
        "subnet_id": "e9bcvlanhbum9ggdvkh2",
        "primary_v4_address": {
          "address": "10.130.0.6",
          "one_to_one_nat": {
            "address": "178.154.225.167",
            "ip_version": "IPV4"
          }
        }
      }
    ],
    "fqdn": "my-instance-1.ru-central1.internal",
    "scheduling_policy": {},
    "network_settings": {
      "type": "STANDARD"
    },
    "placement_policy": {}
  }
] 
    Чтобы избежать ненужных расходов, удалите три созданные ВМ (в следующих практических работах они не понадобятся).
yc compute instance delete my-instance-1 my-instance-2 my-instance-3 
Decision:
$ yc vpc network create --name my-network
$ yc vpc subnet create \
  --name my-subnet-1 \
  --zone ru-central1-a \
  --range 192.168.1.0/24 \
  --network-name my-network
$ yc vpc subnet create \
  --name my-subnet-2 \
  --zone ru-central1-b \
  --range 192.168.2.0/24 \
  --network-name my-network
$ yc vpc subnet create \
  --name my-subnet-3 \
  --zone ru-central1-c \
  --range 192.168.3.0/24 \
  --network-name my-network
$ yc compute instance create \
  --name my-instance-1 \
  --hostname my-instance-1 \
  --zone ru-central1-a \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-1,nat-ip-version=ipv4 \
  --async
$ yc compute instance create \
  --name my-instance-2 \
  --hostname my-instance-2 \
  --zone ru-central1-b \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-2,nat-ip-version=ipv4 \
  --async
$ yc compute instance create \
  --name my-instance-3 \
  --hostname my-instance-3 \
  --zone ru-central1-c \
  --create-boot-disk image-family=ubuntu-2004-lts,size=30,type=network-nvme \
  --image-folder-id standard-images \
  --memory 4 --cores 2 --core-fraction 100 \
  --network-interface subnet-name=my-subnet-3,nat-ip-version=ipv4 \
  --async 
$ yc operation get ef3vcn9383adr1anudcg
$ yc operation get epdk80vb9jjjc3190s6a
$ yc operation get fhm0u5fo2bq3cl2gagpr
$ yc operation wait fhm0u5fo2bq3cl2gagpr
$ yc compute instance list
$ yc compute instance list --format json
$ yc compute instance delete my-instance-1 my-instance-2 my-instance-3
Task:
Практическая работа. Использование файлов спецификаций
Decision:
В этой практической работе вы создадите, обновите и удалите группу ВМ.
Вы уже убедились, что создать даже одну ВМ через yc непросто: нужно установить много разных параметров. Создание группы ВМ требует ещё больше параметров. Чтобы не указывать их все в командной строке, конфигурацию описывают в файле, который используют при создании группы. Такой файл называется спецификацией. Использование спецификаций — это первый шаг в освоении подхода Infrastructure as Code (IaC), который мы будем применять на следующих уроках.
Спецификации пишутся в разных форматах. Для группы ВМ используется язык YAML. Если вы не знакомы с ним — ничего страшного. В документации есть шаблоны спецификаций, и на первых порах вам будет достаточно лишь немного их изменять. Ниже мы разберём, как составлять спецификации.
Часть 1. Создание Instance Group
    Для разворачивания группы ВМ потребуется сеть. Если сети ещё нет, создайте её.
    Посмотрите информацию об имеющихся сетях.
 yc vpc network list
Сохраните идентификатор сети, он понадобиться нам в дальнейшем.
По умолчанию все операции в Instance Groups выполняются от имени сервисного аккаунта c ролью editor на каталог. Если сервисного аккаунта нет, то тоже создайте его и назначьте эту роль.
Посмотрите информацию об имеющихся сервисных аккаунтах.
 yc iam service-account list
    Сохраните идентификатор сервисного аккаунта, он понадобится нам в дальнейшем.
    Для создания группы необходимо подготовить её спецификацию. Создайте в любом текстовом редакторе файл с расширением yaml, например specification.yaml.
Обратите внимание: в формате YAML важны отступы слева. Даже если текст правильный, но отступы не соблюдены, при выполнении спецификации возникнут ошибки.
    Сначала внесите информацию о группе. Пусть группа называется my-group. Укажите идентификатор сервисного аккаунта, от имени которого будете работать (см. шаг 2).
Идентификаторы ресурсов уникальны. Копируя команды из текста урока, не забывайте подставлять свои идентификаторы.
    name: my-group
    service_account_id: <идентификатор_сервисного_аккаунта>
Наша группа будет содержать три одинаковые ВМ. Машины создадим из публичного образа Ubuntu 18.04 LTS (возьмём не последнюю версию, чтобы потренироваться обновлять ВМ). Узнайте идентификатор образа с помощью команды:
 yc compute image list --folder-id standard-images
В столбце FAMILY найдите ubuntu-1804-lts, в столбце ID будет указан нужный идентификатор.
Опишите в спецификации ВМ. Это раздел instance_template.
Пусть каждая машина использует платформу Intel Broadwell (посмотрите поддерживаемые платформы в документации Yandex Compute Cloud),  имеет 2 Гб оперативной памяти и два процессорных ядра.
 instance_template:
     platform_id: standard-v1
     resources_spec:
         memory: 2g
         cores: 2
Добавьте описание загрузочного диска. Он будет использоваться на чтение и запись (режим READ_WRITE). Укажите идентификатор образа, который получили на шаге 5. Выделите сетевой HDD объёмом 32 Гб.
     boot_disk_spec:
        mode: READ_WRITE
        disk_spec:
            image_id: <идентификатор_образа>
            type_id: network-hdd
            size: 32g
Теперь опишите сеть: идентификатор сети из каталога по умолчанию (см. шаг 1). Задайте публичный IP-адрес, чтобы к ВМ можно было обращаться извне.
     network_interface_specs:
         - network_id: <идентификатор_сети>
           primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
В политике планирования укажите, что машина не прерываемая.
     scheduling_policy:
         preemptible: false
В политике развертывания (раздел deploy policy) укажите, что в каждый момент времени может быть неработоспособной только одна машина, не больше. Запретите увеличивать число ВМ, т. е. создавать больше трех машин одновременно. Мы чуть подробнее разберём эти настройки, когда будем обновлять ВМ в группе.
     deploy_policy:
         max_unavailable: 1
         max_expansion: 0
Мы создаем группу фиксированного размера из трёх ВМ. Укажите это в политике масштабирования (раздел scale_policy):
    scale_policy:
        fixed_scale:
        size: 3 
Наконец, в политике распределения машин по зонам (раздел allocation_policy) укажите, что будет использоваться зона ru-central1-a. Мы делаем это для простоты. Лучше распределять ВМ группы по разным зонам доступности — это позволит пережить краткие сбои или выход зоны из строя.
    allocation_policy:
        zones:
            - zone_id: ru-central1-a 
Для балансировщика нагрузки (раздел load_balancer_spec) укажите целевую группу, к которой он будет привязан (это мы рассмотрим чуть ниже).
    load_balancer_spec:
        target_group_spec:
        name: my-target-group 
Нашей спецификации уже достаточно, чтобы создать группу ВМ. Но на эти машины не будет установлено никакого ПО, только операционная система из публичного образа. Если не менять конфигурацию, то после создания ВМ вам придётся устанавливать программы вручную.
Чтобы сэкономить время и сократить число ошибок, давайте максимально автоматизируем создание ВМ, включая установку ПО. Для этого добавим в конфигурацию машины секцию, где будут вызываться команды установки программ. В этой же секции можно описать создание пользователей, но мы этого делать не будем, так как заходить на ВМ не планируем.
Установим на машины веб-сервер NGINX и на веб-странице index.nginx-debian.html, которая создается по умолчанию и выводит приветственное сообщение «Welcome to nginx», заменим слово nginx идентификатором активной ВМ и версией ОС. Поскольку мы подключим балансировщик нагрузки, идентификатор активной ВМ будет различаться для разных пользователей. Это и позволит нам убедиться в том, что балансировщик работает.
Для установки ПО используйте cloud-init — пакет, выполняющий команды на ВМ при первом запуске. Вы узнали о нём из курса о ВМ. Команды опишите в блоке конфигурации #cloud-config. Примеры команд смотрите в документации cloud-init.
Содержимое #cloud-config описывается в разделе instance_template в секции metadata:
    metadata:
      user-data: |-
        #cloud-config
          package_update: true
          runcmd:
            - [apt-get, install, -y, nginx ]
            - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html'] 
    Спецификация готова. Вот ее полный текст. Помните, что в формате YAML важно соблюдать отступы слева.
name: my-group
service_account_id: ajeu495h1s9tn1rorulb
instance_template:
    platform_id: standard-v1
    resources_spec:
        memory: 2g
        cores: 2
    boot_disk_spec:
        mode: READ_WRITE
        disk_spec:
            image_id: fd8fosbegvnhj5haiuoq 
            type_id: network-hdd
            size: 32g
    network_interface_specs:
        - network_id: enpnr4onfs6ihtoao32u
          primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
    scheduling_policy:
        preemptible: false
    metadata:
      user-data: |-
        #cloud-config
          package_update: true
          runcmd:
            - [ apt-get, install, -y, nginx ]
            - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
deploy_policy:
    max_unavailable: 1
    max_expansion: 0
scale_policy:
    fixed_scale:
        size: 3
allocation_policy:
    zones:
        - zone_id: ru-central1-a
load_balancer_spec:
    target_group_spec:
        name: my-target-group 
Теперь создайте группу ВМ по подготовленной спецификации. Уточните синтаксис команды сами:
yc compute instance-group --help 
Проверить синтаксис команды
yc compute instance-group create --file <путь_к_файлу_specification.yaml>  
Для тренировки можете вызвать эту команду в асинхронном режиме, а затем проверить её статус и дождаться завершения.
    Убедитесь, что группа создана, в веб-консоли или выведя список групп с помощью yc.
yc compute instance-group list 
В списке вы должны увидеть свою группу машин my-group:
+----------------------+------------+------+
|          ID          |    NAME    | SIZE |
+----------------------+------------+------+
| amc65sbgfqeqf00m02sc | my-group   |    3 |
+----------------------+------------+------+ 
Часть 2. Балансировщик
    Создайте балансировщик my-load-balancer. Посмотрите, какие параметры должны быть у соответствующей команды:
yc load-balancer network-load-balancer create --help 
В выводе справки обратите внимание, что при создании балансировщика можно сразу создать и обработчик входящего трафика (параметр --listener).
Формат параметра --listener достаточно хитрый: в нём можно указать сразу несколько подпараметров через запятую:
...
--listener name=my-listener,external-ip-version=ipv4,port=80
... 
Помимо имени обработчика, здесь указывается версия IP-протокола и порт, на котором балансировщик будет принимать трафик.
Проверить синтаксис команды
yc load-balancer network-load-balancer create \
  --region-id ru-central1 \
  --name my-load-balancer \
  --listener name=my-listener,external-ip-version=ipv4,port=80 
Затем подключите к балансировщику целевую группу (команда attach-target-group). Вам понадобится идентификатор целевой группы. Чтобы узнать его, запросите с помощью yc список доступных целевых групп и выберите ту, которую вы указали в спецификации specification.yaml. 
Проверить синтаксис команды
yc load-balancer target-group list 
Целевая группа также подключается с помощью нескольких подпараметров, которые соответствуют настройкам в консоли управления (их вы изучали на первом курсе). Для целевой группы укажите такие параметры:
    target-group-id — идентификатор группы;
    healthcheck-name, healthcheck-interval, healthcheck-timeout, healthcheck-unhealthythreshold, healthcheck-healthythreshold, healthcheck-http-port — параметры проверки состояния (см. документацию). Эти параметры аналогичны тем, что задаются в консоли управления при создании балансировщика. Вы изучали их в первом курсе.
Укажите 80-й порт, на котором запущен NGINX.
Проверить синтаксис команды
yc load-balancer network-load-balancer attach-target-group my-load-balancer \ 
  --target-group target-group-id=<идентификатор целевой группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
Можно не выполнять две команды (создание балансировщика и подключение целевой группы) по очереди, а одной командой create создать балансировщик с привязанной целевой группой.
    Убедитесь, что балансировщик создан, а целевая группа подключена через консоль управления или с помощью yc.
Часть 3. Доступ к машинам группы
    Проверьте состояние машин группы. Для этого запросите список машин и дождитесь статуса HEALTHY.
yc load-balancer network-load-balancer target-states my-load-balancer \
    --target-group-id <идентификатор_целевой_группы> 
Теперь откройте в браузере страницу балансировщика. IP-адрес балансировщика вы можете узнать с помощью консоли управления или yc.
На странице вы увидите приветственное сообщение и в нём идентификатор одной из машин.
Часть 4. Обновление Instance Group
    При создании на ВМ группы была установлена ОС Ubuntu 18.04 LTS. Теперь обновите её до Ubuntu 20.04 LTS (ubuntu-2004-lts в столбце FAMILY). Ещё раз посмотрите список доступных образов (см. часть 1) и в файле спецификации specification.yaml измените параметр image_id.
...
boot_disk_spec:
   mode: READ_WRITE
   disk_spec:
       image_id: <идентификатор_образа>
       type_id: network-hdd
       size: 32g
... 
    Теперь запустите обновление группы с изменённым файлом спецификации.
Проверить синтаксис команды
yc compute instance-group update \
  --name my-group \
  --file <путь_к_файлу_specification.yaml> 
Группа будет обновляться постепенно: когда одна машина из группы удаляется, ей на замену создаётся новая. Общее число машин в группе не увеличится. Именно такую политику обновления мы задали в файле спецификации (см. часть 1):
...
deploy_policy:
    max_unavailable: 1
    max_expansion: 0
... 
Есть и другой режим обновления: сначала в группу добавляется ВМ с новой конфигурацией, а затем отключается старая машина. Это повторяется, пока не обновятся все машины. Такому режиму соответствовала бы другая конфигурация:
...
deploy_policy:
    max_unavailable: 0
    max_expansion: 1
... 
Убедитесь, что машины обновились. На приветственной странице должна выводиться новая версия ОС.
Часть 5. Удаление машины из группы
На приветственной странице балансировщика посмотрите имя активной машины и попробуйте удалить ее. Убедитесь, что приветственная страница остаётся доступна всё время: балансировщик переключит трафик на другую машину группы. А Yandex Cloud тем временем пересоздаст удалённую машину.
Проверить синтаксис команды
yc compute instance delete <имя_ВМ> 
Часть 6. Удаление Instance Group
Теперь удалите группу и балансировщик командами yc.
Проверить синтаксис команд
yc compute instance-group delete --name my-group
yc load-balancer network-load-balancer delete --name my-load-balancer 
Кстати, ключевой параметр --name можно и не писать. Достаточно указать имя группы или балансировщика.
Убедитесь, что группы и балансировщика больше нет, через консоль управления или с помощью yc. 
Decision:
$ yc vpc network list
$ yc iam service-account list
$ vim specification.yaml
$ cat specification.yaml
name: my-group
service_account_id: ajeq7kga9ms7bhup4gbe
$ yc compute image list --folder-id standard-images
$ vim specification.yaml
$ cat specification.yaml
name: my-group
service_account_id: ajeq7kga9ms7bhup4gbe
instance_template:
    platform_id: standard-v1
    resources_spec:
        memory: 2g
        cores: 2
    boot_disk_spec:
        mode: READ_WRITE
        disk_spec:
            image_id: fd8k6joqhuk8ts8eb1ao 
            type_id: network-hdd
            size: 32g
    network_interface_specs:
        - network_id: enpboucd6803lg6jspnh
          primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
    scheduling_policy:
        preemptible: false
    metadata:
      user-data: |-
        #cloud-config
          package_update: true
          runcmd:
            - [ apt-get, install, -y, nginx ]
            - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
deploy_policy:
    max_unavailable: 1
    max_expansion: 0
scale_policy:
    fixed_scale:
        size: 3
allocation_policy:
    zones:
        - zone_id: ru-central1-a
load_balancer_spec:
    target_group_spec:
        name: my-target-group
$ yc compute instance-group --help 
$ yc compute instance-group create --file /home/administrator/specification.yaml
$ yc compute instance-group list
$ yc load-balancer network-load-balancer create --help
$ yc load-balancer network-load-balancer create \
  --region-id ru-central1 \
  --name my-load-balancer \
  --listener name=my-listener,external-ip-version=ipv4,port=80 
$ yc load-balancer target-group list
$ yc load-balancer network-load-balancer attach-target-group my-load-balancer \ 
--target-group target-group-id=enp3edjdaoot0v64qth0,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80
$ yc load-balancer network-load-balancer target-states my-load-balancer \
--target-group-id enp3edjdaoot0v64qth0
$ yc compute instance-group update \
  --name my-group \
  --file /home/administrator/specification.yaml
$ yc compute instance-group delete --name my-group
$ yc load-balancer network-load-balancer delete --name my-load-balancer
Task:
О Packer
Decision:
На этом уроке речь пойдёт об инструменте Packer. Он позволяет удобно создавать образы ВМ.
Для чего нужны образы
Когда вы завершаете создавать проект, нужно перенести его из тестовой среды в рабочую. И тут перед вами встают две проблемы. Во-первых, чтобы решение гарантированно работало, рабочая среда должна минимально отличаться от той, в которой проект создавался и тестировался. И во-вторых, проект нужно тиражировать, т. е. придётся много раз устанавливать и настраивать ПО на серверах или облачных платформах. Чтобы ускорить процесс и меньше ошибаться, лучшее решение — создать шаблон: образ ВМ с настроенным софтом.
Готовые образы, доступные в маркетплейсе, содержат только разные версии операционных систем или наборы программ. Такие образы не решают проблему быстрого масштабирования проекта. Образ с софтом, который нужен именно вам, как раз можно создать с помощью Packer — продукта компании HashiCorp. Packer умеет создавать образы для разных платформ, в том числе Yandex Cloud.
Установите Packer. Он поддерживает все популярные операционные системы — Windows, macOS, Linux и FreeBSD.
Как создать образ при помощи Packer
Packer работает так: на входе вы даёте ему текстовый файл — спецификацию — с описанием сборки образа, а на выходе получаете готовый образ. Все просто!
Для примера давайте создадим образ с Ubuntu и веб-сервером NGINX. Описание образа можно составить на языке HCL (HashiCorp Language) или с помощью обычного JSON. Вариант с HCL более современный, так что рассмотрим именно его.
Если у вас есть готовые конфигурации в JSON, то их можно конвертировать в HCL с помощью команды packer hc2_upgrade.
Наш образ my-ubuntu-nginx будет довольно простым. Для его создания достаточно такой спецификации:
source "yandex" "ubuntu-nginx" {
  token               = "<OAuth-токен>"
  folder_id           = "<идентификатор_каталога>"
  source_image_family = "ubuntu-2004-lts"
  ssh_username        = "ubuntu"
  use_ipv4_nat        = "true"
  image_description   = "my custom ubuntu with nginx"
  image_family        = "ubuntu-2004-lts"
  image_name          = "my-ubuntu-nginx"
  subnet_id           = "<идентификатор подсети>"
  disk_type           = "network-ssd"
  zone                = "ru-central1-a"
}
build {
  sources = ["source.yandex.ubuntu-nginx"]
   provisioner "shell" {
    inline = ["sudo apt-get update -y", 
              "sudo apt-get install -y nginx", 
              "sudo systemctl enable nginx.service"
             ]
  }
} 
В секции source указывается, что собирать образ мы будем именно в Yandex Cloud. В документации Packer есть раздел с настройками, специфичными для Yandex Cloud.
Packer должен аутентифицироваться в Yandex Cloud, чтобы создать образ от вашего имени. Есть несколько способов сделать это. В конфигурации выше, например, задан ключ token. Еще один способ — записать IAM-токен или OAuth-токен в переменную окружения YC_TOKEN, тогда в самой спецификации можно дополнительно ничего не указывать.
В параметре image_name мы указываем имя образа.
В секции provisioner — команды, которые нужно выполнить при сборке образа. В нашем случае это установка NGINX.
Сохраним конфигурацию в файл my-ubuntu-nginx.pkr.hcl и попросим Packer на его основе создать образ ВМ:
packer build <путь_к_my-ubuntu-nginx.pkr.hcl> 
Остальные параметры, которые можно использовать в спецификации, подробно разобраны в документации Packer.
Можно хранить спецификации для Packer (текстовые файлы) в системе контроля версий, а значит, можно отслеживать изменения, откатывать к более раннему состоянию, смотреть, кто и когда что-то поменял. Как мы уже говорили, такой подход называется Infrastructure as Code.
Task:
Packer нужен, чтобы ...
Decision:
+создавать образы ВМ для определенной платформы
-тиражировать ВМ на основе готового публичного образа
-создавать универсальные образы ВМ, которые можно использовать на любой платформе
Task:
Секция source в конфигурации служит для описания ...
Decision:
-Всех платформ, с которыми может использоваться образ. Все платформы перечисляются в одной секции source
-Платформы, с которой может использоваться образ. В конфигурации может быть несколько секций source
+Платформы, с которой может использоваться образ. В конфигурации может быть только одна секция source
Task:
Практическая работа. Создаём образ виртуальной машины
Decision:
В этой практической работе вы установите Packer, подготовите с его помощью образ, а затем создадите из образа виртуальную машину.
    Установите Packer, если ещё не сделали это на предыдущем уроке. Он поддерживает все популярные операционные системы — Windows, macOS, Linux и FreeBSD.
    Скачать дистрибутив Packer для вашей ОС также можно с зеркала Yandex Cloud.
    Подготовьте файл в формате HCL со спецификацией образа, например my-ubuntu-nginx.pkr.hcl.
    При создании файла опирайтесь на документацию Packer.
    В качестве примера можете взять спецификацию из предыдущего урока:
 source "yandex" "ubuntu-nginx" {
   token               = "<OAuth-токен>"
   folder_id           = "<идентификатор_каталога>"
   source_image_family = "ubuntu-2004-lts"
   ssh_username        = "ubuntu"
   use_ipv4_nat        = "true"
   image_description   = "my custom ubuntu with nginx"
   image_family        = "ubuntu-2004-lts"
   image_name          = "my-ubuntu-nginx"
   subnet_id           = "<идентификатор_подсети>"
   disk_type           = "network-ssd"
   zone                = "ru-central1-a"
 }
 build {
   sources = ["source.yandex.ubuntu-nginx"]
   provisioner "shell" {
     inline = ["sudo apt-get update -y",
           "sudo apt-get install -y nginx",
           "sudo systemctl enable nginx.service"]
   }
 }
Не забудьте подставить в спецификацию идентификаторы своего каталога и подсети (подсеть должна быть в той же зоне доступности, которая указана в параметре zone). Также укажите свой OAuth-токен (или воспользуйтесь переменной окружения YC_TOKEN при сборке образа).
Теперь создайте образ ВМ на основе файла спецификации:
 packer build <путь_к_файлу_my-ubuntu-nginx.pkr.hcl>
    После того как команда отработает, убедитесь, что образ появился в каталоге. Для этого в консоли управления перейдите в сервис Compute Cloud. Найдите образ на вкладке Образы.
    Перейдите на вкладку Виртуальные машины и начните создавать ВМ.
    Раньше для создания загрузочного диска вы выбирали один из публичных образов, например Ubuntu 20.04. Теперь вместо этого переключитесь на вкладку Пользовательские. Нажмите кнопку Выбрать и в открывшемся окне переключитесь на вкладку Образ.
    Выберите созданный образ и нажмите Применить.
    Из образа создастся загрузочный диск.
    Завершите создание ВМ.
    Проверьте ВМ: введите её IP-адрес в адресную строку браузера. Убедитесь, что веб-сервер работает.
    Удалите ВМ: на следующих уроках она не понадобится. А вот образ удалять не стоит.
Decision:
$ vim my-ubuntu-nginx.pkr.hcl
$ cat my-ubuntu-nginx.pkr.hcl
 source "yandex" "ubuntu-nginx" {
   token               = "y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI"
   folder_id           = "fhmql8bhca7pvlq35r3q"
   source_image_family = "ubuntu-2004-lts"
   ssh_username        = "ubuntu"
   use_ipv4_nat        = "true"
   image_description   = "my custom ubuntu with nginx"
   image_family        = "ubuntu-2004-lts"
   image_name          = "my-ubuntu-nginx"
   subnet_id           = "enpboucd6803lg6jspnh"
   disk_type           = "network-ssd"
   zone                = "ru-central1-a"
 }
 build {
   sources = ["source.yandex.ubuntu-nginx"]
   provisioner "shell" {
     inline = ["sudo apt-get update -y",
           "sudo apt-get install -y nginx",
           "sudo systemctl enable nginx.service"]
   }
 }


Не работает!





Task:
О терраформе
Decision:
На предыдущих уроках вы создали образ ВМ, описав его текстовым файлом — спецификацией. Сейчас мы пойдём ещё дальше: познакомимся с программой Terraform, которая позволяет похожим образом создавать облачную инфраструктуру (не только ВМ, но и балансировщики, сети, базы данных, хранилища и т. д.). Подготовив один файл спецификации, вы автоматически развернёте из него готовую инфраструктуру. Риски ошибок ручной сборки сводятся к минимуму.
Вот так выглядит каркас спецификации для Terraform. Он состоит из описания ресурсов: ВМ, сетей, подсетей и т. д.
resource "yandex_compute_instance" "vm-1" {
  ...
}
resource "yandex_vpc_network" "network-1" {
  ...
}
resource "yandex_vpc_subnet" "subnet-1" {
  ...
} 
Terraform позволяет предварительно посмотреть план выполнения: что будет создано и удалено в процессе работы. Благодаря этому вы можете удостовериться, что получите инфраструктуру нужной конфигурации, а ничего лишнего не появится и не пропадёт.
Вывод команды с проверкой создаваемых ресурсов:
Terraform will perform the following actions:
  # yandex_compute_instance.vm-1 will be created
  + resource "yandex_compute_instance" "vm-1" {
    ...
    }
  # yandex_vpc_network.network-1 will be created
  + resource "yandex_vpc_network" "network-1" {
    ...
    }
  # yandex_vpc_subnet.subnet-1 will be created
  + resource "yandex_vpc_subnet" "subnet-1" {
    ...
    }
Plan: ... to add, 0 to change, 0 to destroy. 
В Terraform объекты можно связывать друг с другом. Например, можно подключить ВМ к сети, созданной в этой же спецификации.
resource "yandex_compute_instance" "vm-1" {
  ...
  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
  }
}
resource "yandex_vpc_subnet" "subnet-1" {
  ...
} 
Спецификации Terraform
Terraform, как и Packer, разработала компания HashiCorp. Облачные провайдеры, в том числе Yandex Cloud, поддерживают спецификации Terraform. Обычно они пишутся на языке HCL и хранятся в файлах формата .tf. Для удобства таких файлов может быть несколько. При запуске Terraform просматривает все файлы в директории и воспринимает их как единую спецификацию.
Посмотрите пример файла спецификации. Привязка к провайдеру (в данном случае это Yandex Cloud) задаётся в секциях required_providers и provider:
terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
}
provider "yandex" {
  token     = "<OAuth-токен>"
  cloud_id  = "<идентификатор_облака>"
  folder_id = "<идентификатор_каталога>"
  zone      = "<зона_доступности_по_умолчанию>"
} 
Как и Packer, Terraform поддерживает различные способы аутентификации. В спецификации выше в параметре token задан OAuth-токен от Yandex Cloud. Другой способ аутентифицироваться — использовать переменную окружения YC_TOKEN, в которую можно записать не только OAuth-токен, но и IAM-токен.
Значения параметров или задаются в спецификации, или передаются в качестве переменных, чтобы адаптировать спецификацию для конкретных задач. Например, с помощью одной спецификации вы сможете развернуть одинаковую инфраструктуру в разных каталогах — для тестирования и для рабочей эксплуатации:
variable "folder-id" {
  type = string
}
provider "yandex" {
  token     = "<OAuth-токен>"
  cloud_id  = "<идентификатор_облака>"
  folder_id = var.folder-id
  zone      = "<зона_доступности_по_умолчанию>"
} 
При этом ключевые ресурсы и зависимости остаются зафиксированы в спецификации и обеспечивают ее работоспособность.
Как использовать спецификации Terraform
Инфраструктура разворачивается в три этапа:
    Команда terraform init инициализирует провайдеров, указанных в файле спецификации.
    Команда terraform plan запускает проверку спецификации. Если есть ошибки — появятся предупреждения. Если ошибок нет, отобразится список элементов, которые будут созданы или удалены.
    Команда terraform apply запускает развёртывание инфраструктуры.
Если инфраструктура больше не нужна, её можно удалить командой terraform destroy.
Оптимизация создания инфраструктуры
На самом деле Terraform не всегда создаёт заново все ресурсы, описанные в спецификации. Terraform ведёт реестр, в котором фиксирует состояние инфраструктуры в облаке. Этот реестр называется State (стейт-файл), он имеет формат JSON.
State поддерживает связь между описанием ресурсов в спецификации и реальными ресурсами в облаке. При запуске команд plan и apply стейт-файл сравнивается с ресурсами, которые нужно создать из спецификации. По итогам сравнения недостающие ресурсы создаются, лишние — удаляются, а некоторые изменяются на ходу. Такой подход позволяет существенно улучшить производительность операций развёртывания, особенно для масштабных инфраструктур. После выполнения команды apply стейт-файл обновляется.
С помощью команд и стейт-файлов вы можете управлять конфигурацией облачной инфраструктуры: импортировать ее описание в стейт-файл (команда terraform import), исключить ресурсы из стейт-файла (terraform state rm), выгрузить описание (terraform output и terraform show).
Task:
Переменные в Terraform нужны, чтобы...
Decision:
-описывать объекты, которые меняют состояние при работе облака
-описывать объекты, которые различаются в разных версиях спецификации
-задавать значения, которые могут различаться в разных инсталляциях одной спецификации
Task:
Переменные в Terraform нужны, чтобы...
Decision:
-описывать объекты, которые меняют состояние при работе облака
-описывать объекты, которые различаются в разных версиях спецификации
+задавать значения, которые могут различаться в разных инсталляциях одной спецификации
Task:
Что делает команда terraform plan?
Decision:
-Подсчитывает стоимость тарифицируемых сервисов и проверяет, достаточно ли средств на балансе
+Проверяет правильность спецификации и формирует список элементов инфраструктуры, которые будут созданы, изменены или удалены
-Подбирает оптимальные значения переменных для развертывания
-Планирует работы, которые должны быть выполнены в облаке сразу после развертывания
Task:
Практическая работа. Создаём виртуальную машину из образа и базу данных
Decision:
В этой практической работе вы установите Terraform и подготовите спецификацию, с помощью которой создадите виртуальную машину, а затем управляемую базу данных.
Подсказки для создания спецификации смотрите в документации Yandex Cloud и в справочнике ресурсов (раздел Resources).
Установите Terraform. Дистрибутив для вашей платформы можно скачать из зеркала. После загрузки добавьте путь к папке, в которой находится исполняемый файл, в переменную PATH.
Настройте провайдер.
Если раньше у вас был настроен провайдер из реестра Hashicorp, сохраните его настройки:
mv ~/.terraformrc ~/.terraformrc.old
Укажите источник, из которого будет устанавливаться провайдер.
Откройте файл конфигурации Terraform CLI:
nano ~/.terraformrc
Добавьте в него следующий блок:
provider_installation {
  network_mirror {
    url = "https://terraform-mirror.yandexcloud.net/"
    include = ["registry.terraform.io/*/*"]
  }
  direct {
    exclude = ["registry.terraform.io/*/*"]
  }
}
Подробнее о настройках зеркал см. в документации.
В начале конфигурационного файла .tf добавьте следующие блоки:
terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"
}
provider "yandex" {
  zone = "<зона доступности по умолчанию>"
}
Где:
    source — глобальный адрес источника провайдера.
    required_version — минимальная версия Terraform, с которой совместим провайдер.
    provider — название провайдера.
    zone — зона доступности, в которой по умолчанию будут создаваться все облачные ресурсы.
Выполните команду terraform init в папке с конфигурационным файлом .tf. Эта команда инициализирует провайдеров, указанных в конфигурационных файлах, и позволяет работать с ресурсами и источниками данных провайдера.
Если провайдер не установился, создайте обращение в поддержку с именем и версией провайдера.
Если вы использовали файл .terraform.lock.hcl, то перед инициализацией выполните команду terraform providers lock, указав адрес зеркала, откуда будет загружаться провайдер, и платформы, на которых будет использоваться конфигурация:
terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 -platform=darwin_arm64 yandex-cloud/yandex
Если вы использовали модули, то сначала выполните terraform init, затем удалите lock-файл, а затем выполните команду terraform providers lock.
    Создайте файл спецификации my-config.tf и укажите в нём Yandex Cloud в качестве провайдера.
 terraform {
   required_providers {
     yandex = {
       source = "yandex-cloud/yandex"
     }
   }
 }
 provider "yandex" {
   token  =  "<OAuth-токен>"
   cloud_id  = "<идентификатор_облака>"
   folder_id = "<идентификатор_каталога>"
   zone      = "<зона_доступности_по_умолчанию>"
 }
Далее мы будем считать, что в качестве зоны доступности по умолчанию выбрана ru-central1-a.
Добавьте в файл блок, описывающий создание ВМ. Его сложно написать с нуля, поэтому опирайтесь на пример из документации. Чтобы вам было проще опознать в консоли управления объекты, созданные по этой спецификации, указывайте уникальные имена для ВМ, сети и подсети, а не оставляйте имена по умолчанию (default).
Для создания ВМ используйте образ, созданный с помощью Packer в предыдущей практической работе.
Можно использовать переменные в спецификации Terraform и передавать в них разные значения при запуске команд. Например, если сделать переменную для идентификатора образа image-id, тогда с помощью одного и того же файла спецификации вы сможете создавать ВМ с разным наполнением.
Переменные Terraform хранятся в файлах с расширением .tfvars. Создайте файл my-variables.tfvars и укажите в нём идентификатор своего образа Packer (узнайте идентификатор с помощью команды yc compute image list):
 image-id = "<идентификатор_образа>"
В файле спецификации my-config.tf объявите эту переменную (ключевое слово variable). Тогда в секции, где описываются настройки ВМ, вы сможете обратиться к переменной как var.image-id:
 ...
 variable "image-id" {
     type = string
 }
 resource "yandex_compute_instance" "vm-1" {
 ...   
     boot_disk {
         initialize_params {
             image_id = var.image-id
         }
     }
 ...
Скорректируйте описание для сети и подсети.
Для сети достаточно указать имя:
  resource "yandex_vpc_network" "network-1" {
      name = "from-terraform-network"
  }
Для подсети укажите зону доступности и сеть, а также внутренние IP-адреса, уникальные в рамках сети. Используйте адреса из адресного пространства 10.0.0.0/16.
  resource "yandex_vpc_subnet" "subnet-1" {
      name           = "from-terraform-subnet"
      zone           = "ru-central1-a"
      network_id     = "${yandex_vpc_network.network-1.id}"
      v4_cidr_blocks = ["10.2.0.0/16"]
  }
Проверьте синтаксис спецификации:
variable "image-id" {
  type = string
}
resource "yandex_compute_instance" "vm-1" {
  name = "from-terraform-vm"
  platform_id = "standard-v1"
  zone = "ru-central1-a"
  resources {
    cores  = 2
    memory = 2
  }
  boot_disk {
    initialize_params {
      image_id = var.image-id
    }
  }
  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
  }
  metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
  }
}
resource "yandex_vpc_network" "network-1" {
  name = "from-terraform-network"
}
resource "yandex_vpc_subnet" "subnet-1" {
  name           = "from-terraform-subnet"
  zone           = "ru-central1-a"
  network_id     = "${yandex_vpc_network.network-1.id}"
  v4_cidr_blocks = ["10.2.0.0/16"]
}
output "internal_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}
output "external_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}
    Теперь попробуйте применить спецификацию. Перейдите в папку с файлом спецификации и выполните инициализацию.
 terraform init
Если всё сделано верно, Terraform покажет сообщение:
 ...
 Terraform has been successfully initialized!
 ...
Важно: выполняйте команды Terraform в папке, где находится файл спецификации.
    Проверьте спецификацию с помощью команды terraform plan.
    Terraform использует все файлы .tf из папки, в которой запущена команда. Поэтому название файла спецификации my-config.tf указывать не нужно: его Terraform подхватит и так.
    Если файл с переменными называется стандартно (terraform.tfvars), его тоже можно не указывать при запуске команды. А если название файла нестандартное, то его нужно указывать:
 terraform plan -var-file=my-variables.tfvars
Terraform выведет план: объекты, которые будут созданы, и т. п.:
 ...
 Terraform will perform the following actions:
 ...
На самом деле необязательно помещать переменные в файл, их можно просто указывать при запуске команды. Поскольку у вас только одна переменная, это было бы несложно:
 terraform plan -var="image-id=<идентификатор_образа>"
Создайте в облаке инфраструктуру по описанной вами спецификации. Выполните команду:
 terraform apply -var-file=my-variables.tfvars
Terraform запросит подтверждение:
 ...
 Do you want to perform these actions?
      Terraform will perform the actions described above.
      Only 'yes' will be accepted to approve.
      Enter a value:
В ответ введите yes.
Когда команда будет выполнена, вы увидите сообщение:
  Apply complete! Resources: ... added, 0 changed, 0 destroyed.
  Outputs:
  external_ip_address_vm_1 = "84.201.133.49"
  internal_ip_address_vm_1 = "10.2.0.24"
В консоли управления убедитесь, что ВМ создана. Откройте в браузере страницу с указанным IP-адресом и проверьте, доступна ли ВМ.
Как мы говорили на предыдущем уроке, Terraform хранит описание инфраструктуры в стейт-файлах. Посмотрите, как выглядит стейт-файл сейчас:
 terraform state list
Вы увидите список объектов:
 yandex_compute_instance.vm-1
 yandex_vpc_network.network-1
 yandex_vpc_subnet.subnet-1
    Теперь добавьте в файл спецификации блок, описывающий создание кластера БД PostgreSQL. Подсказки ищите в справочнике ресурсов. Не забудьте заменить в спецификации имя подсети.
Проверьте синтаксис спецификации:
resource "yandex_mdb_postgresql_cluster" "postgres-1" {
  name        = "postgres-1"
  environment = "PRESTABLE"
  network_id  = yandex_vpc_network.network-1.id
  config {
    version = 12
    resources {
      resource_preset_id = "s2.micro"
      disk_type_id       = "network-ssd"
      disk_size          = 16
    }
    postgresql_config = {
      max_connections                   = 395
      enable_parallel_hash              = true
      vacuum_cleanup_index_scale_factor = 0.2
      autovacuum_vacuum_scale_factor    = 0.34
      default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
      shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
    }
  }
   database {
    name  = "postgres-1"
    owner = "my-name"
  }
  user {
    name       = "my-name"
    password   = "Test1234"
    conn_limit = 50
    permission {
      database_name = "postgres-1"
    }
    settings = {
      default_transaction_isolation = "read committed"
      log_min_duration_statement    = 5000
    }
  }
  host {
    zone      = "ru-central1-a"
    subnet_id = yandex_vpc_subnet.subnet-1.id
  }
}
Сохраните файл спецификации.
Проверьте синтаксис спецификации:
terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
}
provider "yandex" {
  token  =  "<OAuth-токен>"
  cloud_id  = "<идентификатор_облака>"
  folder_id = "<идентификатор_каталога>"
  zone      = "ru-central1-a"
}
variable "image-id" {
  type = string
}
resource "yandex_compute_instance" "vm-1" {
  name = "from-terraform-vm"
  platform_id = "standard-v1"
  zone = "ru-central1-a"
  resources {
    cores  = 2
    memory = 2
  }
  boot_disk {
    initialize_params {
      image_id = var.image-id
    }
  }
  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
  }
  metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
  }
}
resource "yandex_vpc_network" "network-1" {
  name = "from-terraform-network"
}
resource "yandex_vpc_subnet" "subnet-1" {
  name           = "from-terraform-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.network-1.id
  v4_cidr_blocks = ["10.2.0.0/16"]
}
resource "yandex_mdb_postgresql_cluster" "postgres-1" {
  name        = "postgres-1"
  environment = "PRESTABLE"
  network_id  = yandex_vpc_network.network-1.id
  config {
    version = 12
    resources {
      resource_preset_id = "s2.micro"
      disk_type_id       = "network-ssd"
      disk_size          = 16
    }
    postgresql_config = {
      max_connections                   = 395
      enable_parallel_hash              = true
      vacuum_cleanup_index_scale_factor = 0.2
      autovacuum_vacuum_scale_factor    = 0.34
      default_transaction_isolation     = "TRANSACTION_ISOLATION_READ_COMMITTED"
      shared_preload_libraries          = "SHARED_PRELOAD_LIBRARIES_AUTO_EXPLAIN,SHARED_PRELOAD_LIBRARIES_PG_HINT_PLAN"
    }
  }
  database {
    name  = "postgres-1"
    owner = "my-name"
  }
  user {
    name       = "my-name"
    password   = "Test1234"
    conn_limit = 50
    permission {
      database_name = "postgres-1"
    }
    settings = {
      default_transaction_isolation = "read committed"
      log_min_duration_statement    = 5000
    }
  }
  host {
    zone      = "ru-central1-a"
    subnet_id = yandex_vpc_subnet.subnet-1.id
  }
}
output "internal_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}
output "external_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}
Теперь примените обновлённую спецификацию. В папке с файлом спецификации выполните команду terraform plan:
terraform plan -var-file=my-variables.tfvars 
Если появляются сообщения об ошибках — исправьте ошибки и снова выполните команду.
Обновите инфраструктуру в соответствии с дополненной спецификацией командой terraform apply:
terraform apply -var-file=my-variables.tfvars 
Поскольку спецификация теперь включает создание БД, команда может выполняться довольно долго (около 10 минут).
В консоли управления откройте раздел Managed Service for PostgreSQL и убедитесь, что кластер postgres-1 создан и имеет статус Alive.
Проверьте, как изменился стейт-файл:
terraform state list 
В списке появился новый объект:
yandex_compute_instance.vm-1
yandex_mdb_postgresql_cluster.postgres-1
yandex_vpc_network.network-1
yandex_vpc_subnet.subnet-1 
Удалите инфраструктуру:
terraform destroy -var-file=my-variables.tfvars 
В конце вы увидите сообщение о выполнении команды:
...
Destroy complete! Resources: 4 destroyed. 
    В консоли управления убедитесь, что объекты удалены.
Task:
Контейнеризация
Decision:
Мы много говорили о пользе виртуальных машин. Они предоставляют изолированную среду, в которой приложения гарантированно работают, и поэтому упрощают перенос и тиражирование приложений.
Но у ВМ есть и недостаток: они создают рабочую среду полностью, включая операционную систему (ОС) и весь установленный на ней софт. Когда на одном сервере создаются несколько ВМ, каждая запускает свои отдельные экземпляры ОС и прочих приложений. В результате ресурсы сервера — вычислительная мощность процессора, дисковое пространство и т. д. — расходуются неэффективно.
Новое решение: контейнеризация
Главное и принципиальное отличие контейнера от ВМ в том, что он использует ресурсы и ядро хостовой ОС. Несколько контейнеров, размещённых на одном сервере, используют ресурсы сервера совместно, тем самым экономя их.
Так же, как и ВМ, контейнер изолирован от других контейнеров и хостовой ОС. Он может содержать различные приложения и запускаться на различных платформах.
Хорошей практикой считается принцип «один контейнер — один сервис». Так проще обновлять приложения и создавать резервные копии. Например, если вы написали для веб-сервера NGINX веб-приложение на Python, поместите сервер и приложение в отдельные контейнеры.
Слоистая архитектура контейнеров
Говоря о контейнерах, часто употребляют термин слой. Любое изменение окружения — установка программы, создание директории — создаёт новый слой. Эти слои накладываются друг на друга.
Если на одном сервере оказываются несколько контейнеров с общими слоями (например, библиотеками), то слои не дублируются: они устанавливаются один раз и затем используются совместно.
Преимущества контейнеров
С контейнерами разработка стала эффективнее и проще. Чем же они хороши?
    Экономия ресурсов. Во-первых, контейнеры занимают меньший объём, чем ВМ: они не содержат отдельных копий ОС и дополнительных программ и утилит. Во-вторых, благодаря общим слоям контейнеры оптимизируют использование ресурсов хоста.
    Независимость. Контейнер самодостаточен. Всё, что нужно для работы (библиотеки, настройки, среда запуска), находится внутри.
    Переносимость. Контейнер независим. Платформа, на которой его запускают, неважна: он везде будет работать одинаково. Можно спокойно переносить контейнер с одной платформы на другую.
    Скорость разворачивания контейнеров и работы в них. Это преимущество следует из предыдущих. Сервер не тратит время на эмуляцию гостевой ОС, а высвободившиеся ресурсы можно направить на увеличение производительности приложений и сервисов.
    Тиражирование и масштабирование. Собрали контейнер однажды — копируйте его сколько угодно раз. Запускайте одновременно нужное количество копий контейнера. Всё будет работать одинаково.
    Оркестрация. Дирижёр одновременно управляет множеством музыкантов, играющих на разных инструментах. Вы можете создавать похожие системы из контейнеров, каждый из которых выполняет узкую задачу. Оркестрация — это управление такими системами, т. е. координация работы множества контейнеров. Подробнее о ней мы поговорим на следующих уроках.
Task:
Контейнеризация эффективна, потому что:
Decision:
-производит аппаратную оптимизацию процессов
+оптимально использует ресурсы системы
Task:
Docker
Decision:
Как и виртуальные машины, контейнеры создаются из образов. На сегодняшний день самая популярная и удобная платформа для создания и запуска образов — Docker.
Docker работает так. Предположим, вы с коллегами разработали приложение. Вы упаковываете его со всеми зависимостями — библиотеками, интерпретаторами, файлами и т. д. — в Docker-образ и отправляете в репозиторий (т. е. в хранилище). Чтобы развернуть приложение, нужно скачать из репозитория образ и создать из него контейнер на рабочем сервере.
Хранилища Docker-образов бывают публичные и приватные. Самое известное публичное хранилище — это Docker Hub. Однако если вы работаете с Yandex Cloud, лучше использовать собственное хранилище облака — Yandex Container Registry. О нём и его плюсах мы поговорим на следующем уроке.
Как создаются образы
Docker-образы создаются с помощью инструкций, таких как запуск команды, добавление файла или директории, создание переменной окружения. Инструкции хранятся в Dockerfile — это обычный текстовый файл, который можно редактировать в любом текстовом редакторе (что соответствует принципам Infrastructure as Code).
Вот простой пример Dockerfile для образа, в котором есть только ОС Ubuntu и веб-сервер NGINX:
FROM ubuntu:latest
RUN apt-get update -y
RUN apt-get install -y nginx
ENTRYPOINT ["nginx", "-g", "daemon off;"] 
Каждая инструкция создаёт новый слой образа, и эти слои накладываются друг на друга. В конце вы задаёте команду — исполняемый файл, который будет запущен при старте Docker-контейнера.
В примере выше первая строка определяет исходный образ (публичный образ с последней версией Ubuntu), на основе которого мы строим свой.
Вторая и третья строки устанавливают веб-сервер NGINX.
Последняя строка задаёт точку входа — запускает NGINX.
Процесс создания образа — это считывание и выполнение инструкций из Dockerfile. Чтобы создать образ из Dockerfile, используется команда build. Если файл со спецификацией называется стандартно (Dockerfile), не указывайте название. Если иначе — напишите название после ключа -f. После ключа -t указывается имя образа, который будет создан:
docker image build -f my-dockerfile -t my-image . 
Точка в примере означает, что для сборки используется текущая директория.
Как создаются контейнеры из образов
Для работы с хранилищем используются традиционные команды push и pull. Так мы помещаем образ в хранилище:
docker push my-image 
Чтобы создать контейнер, загрузите из хранилища образ и запустите его:
docker pull my-image
docker run my-image 
При создании контейнера из образа можно использовать параметры (флаги). Например, чтобы ограничить ресурсы памяти и процессора, загрузить свежую версию образа, передать значения переменных. Смотрите доступные флаги с помощью традиционного ключа --help.
Task:
С помощью Docker можно создавать образы ...
Decision:
-виртуальных машин
+контейнеров
-виртуальных машин и контейнеров (зависит от спецификации)
Task:
Образы, созданные в Docker, можно хранить ...
Decision:
-только в специальном публичном хранилище Docker Hub
-в любом публичном хранилище образов, в том числе в Yandex Container Registry
+в публичном или приватном хранилище образов, в том числе в Docker Hub и в Yandex Container Registry
Task:
Yandex Container Registry
Decision:
На прошлом уроке мы говорили о том, что Docker-образы хранятся в хранилищах. Если вы работаете с Yandex.Cloud, лучше всего использовать сервис Yandex Container Registry.
Преимущества Yandex Container Registry
    Бесплатный внутренний трафик. Для создания контейнеров придётся скачивать образы, которые могут весить несколько гигабайтов. Если вы берёте образы из Docker Hub или другого внешнего реестра, трафик тарифицируется. А если из Yandex Container Registry — такой трафик считается внутренним и не оплачивается.
    Приватный реестр. В Docker Hub это платная возможность. В Yandex Container Registry ваш реестр по умолчанию приватный. Чтобы сделать его публичным, предоставьте права системной группе allUsers.
    Политика автоматического удаления. При CI/CD после каждого изменения исходного кода создаётся новый образ. В итоге образов становится слишком много, приходится вручную управлять ими и удалять лишние. В Yandex Container Registry можно настроить автоматическое удаление. Это упростит управление образами в рамках CI/CD и сэкономит дисковые ресурсы и деньги, ведь стоимость хранения образов зависит от их объёма.
    Удобство. С Yandex Container Registry вы будете работать в привычном интерфейсе консоли управления и с командами утилиты yc.
Реестр, репозиторий и теги
Реестр в Yandex Container Registry — это хранилище Docker-образов, а репозиторий — набор образов с одинаковыми именами (т. е. версий образа).
Чтобы различать образы в репозитории и отбирать их по правилам, добавляйте к имени образа уникальный в рамках репозитория тег. Если тег не задан — последней версии образа автоматически присваивается тег latest.
При обращении к образу используется префикс cr.yandex. Он означает, что образ хранится Yandex Container Registry.
Так выглядит запись для обращения к образу:
cr.yandex/<реестр>/<имя образа>:<тег>. 
Пример полного имени: cr.yandex/my-registry/my-app:latest.
Регулярные выражения позволяют выбирать образы по правилам. Например, если тестовые образы приложения my-app создавались с тегами testVersion1, testVersion2, testVersion3 и т. д., то вы отберёте все тестовые образы вот так:
cr.yandex/my-registry/my-app:test.* 
Автоматическое удаление
Политики автоматического удаления настраиваются для каждого репозитория отдельно. Политика — это правила, по которым Docker-образы будут удаляться. Например, можно удалять все образы с тегами test.* и все образы с тегами prod.*, созданные более месяца назад. При этом вы можете на всякий случай сохранить несколько образов, подходящих под условия.
Политики удаления описываются в JSON-файле в виде списка опций и их значений. Обычно используются опции:
    tag_regexp — тег Docker-образа для фильтрации.
    untagged — флаг для применения правила к Docker-образам без тегов.
    expire_period — время, кратное 24 часам, через которое Docker-образ попадает под политику удаления.
    retained_top — количество Docker-образов, которые не будут удалены, даже если подходят по правилу.
Вот пример файла rules.json:
[
    {
     "description": "Delete prod Docker images older than 30 days but retain 20 last ones",
     "tag_regexp": "prod",
     "expire_period": "30d",
     "retained_top": 20
    },
    {
     "description": "delete all test Docker images except 10 last ones",
     "tag_regexp": "test.*",
     "retained_top": 10
    },
    {
     "description": "delete all untagged Docker images older than 48 hours",
     "untagged": true,
     "expire_period": "48h"
    }
] 
Удаление образа — это ответственное действие. Поэтому после настройки правил проверьте, как они будут работать в автоматическом режиме. Вам поможет тестовый запуск политики: dry-run.
Для репозитория можно настроить несколько политик, но активной будет только одна. Включайте и отключайте политики в зависимости от своих задач.
Container Optimized Image
Yandex Cloud позволяет создать из специального образа Container Optimized Image виртуальную машину, чтобы запустить на ней Docker-контейнер. При использовании Container Optimized Image не нужно устанавливать на машину Docker и скачивать образ с помощью команды docker pull. Мы опробуем эту возможность в следующей практической работе.
Task:
Что такое репозиторий Yandex Container Registry?
Decision:
-Альтернативная реализация реестра для хранения Docker-образов
+Набор Docker-образов с одинаковыми именами
-Группа Docker-контейнеров
Task:
Что такое политика автоматического удаления?
Decision:
-Соглашение об использовании Yandex Container Registry, которое заключает владелец аккаунта
-Набор правил для автоматического отключения виртуальных машин, созданных из Docker-образов
+Набор правил для автоматического удаления Docker-образов из Yandex Container Registry
Task:
Практическая работа. Создание докер-образа и загрузка его в Container Registry
Decision:
В этой практической работе вы создадите реестр в Yandex Container Registry, подготовите Docker-образ виртуальной машины и поместите его в реестр, а затем создадите машину из этого образа.
    Установите Docker.
    Создайте реестр в Yandex Container Registry:
 yc container registry create --name my-registry
Обратите внимание, что в выводе есть уникальный идентификатор (id) реестра. Он пригодится вам для следующих команд.
 id: crpfpd8jhhldiqah91rc
 folder_id: b1gfdbij3ijgopgqv9m9
 name: my-registry
 status: ACTIVE
 created_at: "2021-04-06T00:46:48.150Z"
Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper. Это нужно для того, чтобы внешняя платформа Docker могла от вашего имени отправить образ в ваш приватный реестр в Yandex Cloud.
 yc container registry configure-docker
Подготовьте Dockerfile. Можно использовать файл из урока о Docker:
 FROM ubuntu:latest
 RUN apt-get update -y
 RUN apt-get install -y nginx
 ENTRYPOINT ["nginx", "-g", "daemon off;"]
По умолчанию Docker использует файл с именем Dockerfile и без расширения.
Перейдите в папку с Dockerfile и соберите образ (не забудьте подставить идентификатор своего реестра):
 docker build . -t cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
Ключ -t позволяет задать образу имя.
Напоминаем, что в Yandex Container Registry можно загрузить только образы, названные по такому шаблону:
 cr.yandex/<ID реестра>/<имя Docker-образа>:<тег>
Загрузите Docker-образ в реестр:
 docker push cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
В консоли управления перейдите в реестр и предоставьте всем пользователям право использовать хранящиеся образы. Для этого перейдите на вкладку Права доступа, в правом верхнем углу нажмите кнопку Назначить роли. В открывшемся окне нажмите кнопку Выбрать пользователя, на вкладке Группы выберите All users. Нажмите кнопку Добавить роль и последовательно введите viewer и container-registry.images.puller. Нажмите кнопку Сохранить.
В консоли управления создайте ВМ с помощью Container Optimized Image.
При создании машины в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный образ, остальные настройки оставьте по умолчанию и нажмите Применить.
    Другие настройки ВМ мы уже разбирали.
    Когда новая ВМ получит статус Running, найдите её внешний IP адрес в консоли управления и убедитесь, что по этому адресу отображается приветственная страница NGINX.
Обратите внимание! C помощью Docker-образа вы создали и запустили виртуальную машину с предустановленным, нужным вам ПО. При этом вам даже не потребовалось заходить внутрь ВМ и выполнять установку или настройку ПО вручную.
Decision:
$ sudo apt update
$ sudo apt install apt-transport-https ca-certificates curl software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
$ sudo apt update
$ apt-cache policy docker-ce
$ sudo apt install docker-ce
$ sudo /etc/init.d/docker start
$ yc container registry create --name my-registry
done (1s)
id: crppr4k34p7u5pr94lvf
folder_id: b1geto6411pvmr7j3pd2
name: my-registry
status: ACTIVE
created_at: "2022-10-16T03:16:59.989Z"
$ yc container registry configure-docker
$ cd docker/
$ vim Dockerfile
$ cat Dockerfile
FROM ubuntu:latest
RUN apt-get update -y
RUN apt-get install -y nginx
ENTRYPOINT ["nginx", "-g", "daemon off;"]
$ sudo docker build . -t cr.yandex/crppr4k34p7u5pr94lvf/ubuntu-nginx:latest
$ sudo docker push cr.yandex/crppr4k34p7u5pr94lvf/ubuntu-nginx:latest
Task:
Оркестрация и Kubernetes
Decision:
На прошлых уроках мы обсудили, для чего нужны контейнеры. Вы уже знаете, что хорошей практикой считается упаковывать каждый сервис в отдельный контейнер. Даже для простого интернет-сайта создаются как минимум два контейнера: для CMS-системы (например WordPress) и для БД. Для работы серьёзного приложения и тем более набора приложений в облаке обычно приходится создавать множество контейнеров. Ими нужно управлять: отслеживать работоспособность и перезапускать при сбоях, обновлять, разворачивать, масштабировать, останавливать. Такое управление называется оркестрацией.
Наиболее популярная система управления контейнерами — Kubernetes®, также известная как K8s. Это система с открытым исходным кодом, которая автоматизирует операции с контейнерами: мониторинг, распределение нагрузки, предоставление ресурсов и пр.
С помощью Kubernetes решают разнообразные прикладные задачи. Например, кластеры Kubernetes разворачивают на телеком-вышках, при организации умного дома и даже для управления узлами автомобиля.
Из чего состоит Kubernetes
В Kubernetes контейнеры или наборы контейнеров размещаются на подах (pod). Под — это логический хост. Один или несколько подов, а также сервисы для управления подами образуют узел, или ноду (node). Узел — это рабочая машина, виртуальная либо физическая. Однотипные узлы образуют группу узлов.
В свою очередь, узлы объединяются в кластер. У каждого кластера есть своя панель управления (control plane), именно она и обеспечивает оркестрацию. Один из узлов кластера становится главным — мастером (master). Он запускает управляющие процессы Kubernetes: сервер Kubernetes API, планировщик и контроллеры основных ресурсов.
В одном физическом кластере могут находиться несколько виртуальных. Виртуальный кластер называется пространством имён (namespace). В отличие от нод и подов, которые в кластере есть всегда, пространства имён надо использовать тогда, когда в них возникает реальная необходимость. Например, если приложение состоит из сервисов, то для каждого сервиса стоит создать пространство имён. Это поможет управлять разделением ресурсов физического кластера между сервисами.
В кластер Kubernetes можно устанавливать расширения, облегчающие управление. Например, графический веб-интерфейс Dashboard или инструмент для мониторинга ресурсов кластера Container Resource Monitoring. Они необязательны. Единственное обязательное расширение — это внутренний DNS-сервер кластера. Он необходим для общения сервисов между собой.
Что делает Kubernetes
    Автоматическое развёртывание. Вы можете описать состояние контейнеров в виде конфигурации, и Kubernetes автоматически обеспечит заданное состояние: будет развёртывать и удалять контейнеры, перераспределять ресурсы.
    Мониторинг сервисов и балансировка. Kubernetes распределяет сетевой трафик так, чтобы развёртывание было стабильным.
    Оркестрация хранилища. Kubernetes позволяет автоматически смонтировать систему хранения: локальное или облачное хранилище.
    Самоконтроль. Kubernetes перезапускает отказавшие контейнеры, заменяет их и завершает работу контейнеров, которые не соответствуют заданному уровню работоспособности.
Yandex Managed Service for Kubernetes
Как вы наверняка догадались, Kubernetes довольно сложно администрировать, даже если использовать его вместе с облачным хранилищем и не заботиться о физическом предоставлении ресурсов. Чтобы упростить администрирование и интеграцию, в Yandex Cloud есть сервис Managed Service for Kubernetes.
    При использовании Yandex Managed Service for Kubernetes вы создаёте кластер и группы узлов. При этом мастер-ноды, пространство имён, сервис DNS и прочие необходимые элементы развёртываются автоматически. А за обслуживание и обновление всей инфраструктуры кластера отвечает облачный провайдер.
    Приложения, помещённые в такой кластер, автоматически масштабируются: при пиковых нагрузках ресурсы подтягивают, при спаде — освобождают.
    У Yandex Managed Service for Kubernetes есть свой графический интерфейс. Дополнительные расширения не требуются.
    Для хранения Docker-образов для подов кластера используйте Yandex Container Registry.
    Мастер-узел можно настроить так, что он будет автоматически реплицироваться во всех зонах доступности Yandex Cloud.
    Благодаря интеграции с сервисом Yandex Identity and Access Management можно добавлять пользователей в кластеры Kubernetes по учётным записям вашей организации или, например, почте на @yandex.ru.
Task:
Что такое Yandex Managed Kubernetes?
Decision:
-Сервис для управления трафиком и биллингом при использовании Yandex Cloud как облачного хранилища для кластеров Kubernetes
+Сервис для управления контейнеризованными приложениями с помощью Kubernetes в инфраструктуре Yandex Cloud
-Реестр образов контейнеров для развёртывания кластеров под управлением Kubernetes
Task:
Kubernetes нужен, чтобы ...
Decision:
-управлять большим количеством запущенных контейнеров
-автоматизировать развёртывание
-масштабировать приложения
+все ответы верны
Task:
Практическая работа. Создание кластера
Decision:
В этой практической работе вы создадите кластер Kubernetes и группу узлов в нём.
    Выберите каталог для кластера.
    Выберите сервис Managed Service for Kubernetes. Нажмите кнопку Создать кластер. Дальше заполним настройки кластера:
    Для Kubernetes необходим сервисный аккаунт для ресурсов и узлов.
    Сервисный аккаунт для ресурсов — это аккаунт, под которым сервису Kubernetes будут выделяться ресурсы в нашем облаке.
    Сервисный аккаунт для узлов необходим уже созданным узлам самого кластера Kubernetes для доступа к другим ресурсам. Например, чтобы получить Docker-образы из Container Registry.
    Этим аккаунтам нужны разные права, и поэтому у них бывают разные роли. В общем случае вы можете использовать один и тот же сервисный аккаунт. Выберите аккаунт, который создали на первом курсе, или заведите новый.
    Ключ шифрования Yandex Key Management Service позволяет защитить конфиденциальную информацию (пароли, OAuth-токены и SSH-ключи) и повысить безопасность. Это необязательно — кластер запустится и без ключа. Для этой практической работы не создавайте его.
    Релизные каналы RAPID, REGULAR и STABLE отличаются процессом обновления и доступными вам версиями Kubernetes.
    RAPID и REGULAR содержат все версии, включая минорные. STABLE — только стабильные версии. RAPID обновляется автоматически, а в REGULAR и STABLE обновление можно отключить. Когда появляется обновление, информация о нём отображается в консоли управления.
    Выберите REGULAR.
Внимательно выбирайте релизный канал! Изменить его после создания кластера Kubernetes нельзя.
Конфигурация мастера
Мастер — ведущая нода группы узлов кластера — следит за состоянием Kubernetes и запускает управляющие процессы. Сконфигурируем мастер:
    Выберите версию Kubernetes. Их набор зависит от релизного канала. Версии мастера и других нод могут не совпадать, но это достаточно тонкая настройка, могут возникнуть проблемы совместимости, которые повлияют на работу всего кластера.
    Кластеру может назначаться публичный IP-адрес. Выберите вариант Автоматически. В этом случае IP выбирается из пула свободных IP-адресов. Если вы не используете Cloud Interconnect или VPN для подключения к облаку, то без автоматического назначения IP-адресов вы не сможете подключиться к кластеру: он будет доступен только во внутренней сети вашего облака.
    Тип мастера влияет на отказоустойчивость. Зональный работает только в одной зоне доступности, а региональный — в трёх подсетях в каждой зоне доступности.
    Выберите зональный тип. В будущем для рабочей среды используйте региональные кластеры, а для разработки и тестирования — более дешёвые зональные.
    Выбор типа мастера также влияет на подсети, в которых будет развёрнут кластер. У вас уже есть подсети, созданные по умолчанию для функционирования облака. Выберите их.
Настройки окна обновлений
    Режимов обновления четыре: Отключено, В любое время, Ежедневно и В выбранные дни. Региональный мастер во время обновления остаётся доступен, зональный — нет.
    Группа узлов кластера обновляется с выделением дополнительных ресурсов, так как при обновлении создаются узлы с обновлённой конфигурацией. При обновлении поды с контейнерами будут переезжать с одного узла на другой.
    По умолчанию выставлен пункт В любое время. Оставьте его.
Сетевые настройки кластера
    Сетевые политики для кластера Kubernetes необязательны. Эта опция включает сетевой контроллер Calico, который позволяет применять тонкие настройки политик доступа для кластера.
    Не выбирайте эту опцию.
    Во время работы кластера подам с контейнерами и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы IP-адреса подов и сервисов Kubernetes не пересеклись с другими адресами в вашем облаке, задайте CIDR (Classless Inter-Domain Routing — бесклассовая междоменная маршрутизация). Оставьте адреса пустыми: они будут назначены автоматически.
    Маска подсети узлов влияет на количество подов, которые могут запускаться. Если адресов не хватит, под не запустится.
    Вы заполнили все настройки, теперь нажмите Создать кластер. Дождитесь, пока статус кластера станет RUNNING, а состояние — HEALTHY. Это может занять около 10 минут.
Создание группы узлов
    Зайдите в созданный кластер, перейдите на вкладку Управление узлами и нажмите Создать группу узлов. Группы узлов — это группы виртуальных машин.
    Введите имя и описание группы, выберите версию Kubernetes. Выберите Автоматический тип масштабирования и количество узлов от 1 до 5. Укажите среду запуска контейнеров — Docker.
В сетевых настройках задайте автоматический IP-адрес и выберите зону доступности (кластер зональный, поэтому зона доступности только одна). Задайте SSH-ключ, чтобы иметь доступ к виртуальным машинам кластера. Настройки обновления идентичны настройкам мастера.
Остальные настройки группы, которые мы не упомянули (вычислительные ресурсы, хранилище и т. д.), оставьте по умолчанию.
Нажмите Создать группу узлов и дождитесь, пока операция выполнится.
Task:
Практическая работа. Первое приложение в кластере
Decision:
На прошлом уроке вы создали в консоли управления Yandex Cloud кластер Kubernetes и группу узлов в нём. Теперь с помощью командной строки вы развернете в кластере приложение — веб-сервер NGINX.
    Основное средство взаимодействия с кластером — инструмент kubectl. Установите его по инструкции.
    В консоли управления войдите в созданный кластер Managed Service for Kubernetes и нажмите кнопку Подключиться. В открывшемся окне скопируйте команду для подключения:
 yc managed-kubernetes cluster get-credentials \
   --id <идентификатор_кластера> \
   --external
Чтобы проверить правильность установки и подключения, посмотрите на конфигурацию:
 kubectl config view
Ответ получится примерно таким (IP-адрес сервера и название кластера будут отличаться):
 apiVersion: v1
 clusters:
 - cluster:
     certificate-authority-data: DATA+OMITTED
     server: https://178.154.206.242
   name: yc-managed-k8s-cat2oek6hbp7mnhhhr4m
 contexts:
 ...
Создание манифеста
Для описания настроек приложения в кластере создадим файл my-nginx.yaml. Такой файл называется манифестом.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest 
Рассмотрим, из чего он состоит.
    Директива apiVersion определяет, для какой версии Kubernetes написан манифест. От версии к версии обозначение может меняться.
 apiVersion: apps/v1
Директива kind описывает механизм использования. Она может принимать значения Deployment, Namespace, Service, Pod, LoadBalancer и т. д. Для развёртывания приложения укажите значение Deployment.
 kind: Deployment
Директива metadata определяет метаданные приложения: имя, метки (labels), аннотации.
С помощью Меток можно идентифицировать, группировать объекты, выбирать их подмножества. Добавляйте и изменяйте метки при создании объектов или позднее, в любое время.
Аннотации используют, чтобы добавить собственные метаданные к объектам.
Укажем имя приложения:
 metadata:
   name: my-nginx-deployment
В основном блоке spec содержится описание объектов Kubernetes.
Директива replicas определяет масштабирование. Для первого запуска укажите, что приложению нужен один под. Позже вы посмотрите, как приложения масштабируются, и сможете увеличить число подов.
Директива selector определяет, какими подами будет управлять контейнер (подробнее о ней можно прочитать в документации). Поды отбираются с помощью метки (label).
Директива template определяет шаблон пода. Метка в шаблоне должна совпадать с меткой селектора — nginx.
В шаблоне содержится ещё одна, собственная директива spec. Она задаёт настройки контейнеров, которые будет развёрнуты на поде. Нам нужен один контейнер. Используйте для него образ, созданный ранее с помощью Docker и помещённый в реестр Yandex Container Registry.
 spec: 
   matchLabels: 
     app: nginx
   replicas: 1
   selector: ~
   template: 
     metadata: 
       labels: 
         app: nginx
     spec: 
       containers: 
         - name: nginx
           image: "cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest"
    Настройки манифеста для развёртывания приложения есть в документации Kubernetes.
Выполнение манифеста
    Для создания или обновления ресурсов в кластере используется команда apply. Файл манифеста указывается после флага -f.
 kubectl apply -f <путь_к_файлу_my-nginx.yaml>
Если результат будет успешным, вы увидите сообщение:
 deployment.apps/my-nginx-deployment created
Чтобы убедиться, что приложение создано, посмотрите список подов:
 kubectl get pods
Дождитесь статуса Running:
 NAME                                   READY   STATUS    RESTARTS   AGE
 my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          5m27s
Теперь получите более подробную информацию, выполнив ту же команду с флагом -o wide:
 kubectl get pods -o wide
Вы увидите внутренний IP-адрес, который присвоен поду. Это пригодится, если нужно узнать, где именно развёрнуто приложение.
Чтобы получить максимально подробную информацию о запущенном приложении, используйте команду describe:
 kubectl describe deployment/my-nginx-deployment
Масштабирование
    Теперь увеличьте количество подов. Вручную это можно сделать двумя способами:
        изменить файл манифеста, указав в директиве replicas нужное число подов, и снова выполнить команду apply;
        если файла манифеста нет под рукой — использовать команду scale:
kubectl scale --replicas=3 deployment/my-nginx-deployment 
Если всё получится, в выводе команды kubectl get pods вы увидите сообщение:
NAME                                   READY   STATUS    RESTARTS   AGE
my-nginx-deployment-65b9b678b6-6whpp   1/1     Running   0          117s
my-nginx-deployment-65b9b678b6-wtph9   1/1     Running   0          117s
my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          14m 
На следующей практической работе мы посмотрим, как обращаться извне к кластеру Kubernetes и развёрнутому в нём приложению.
Кластер как код
Как видите, управление кластерами Kubernetes отлично вписывается в концепцию Infrastructure as Code: вы можете описать конфигурацию кластера в текстовом файле — манифесте. Вы также можете разворачивать кластеры Kubernetes с помощью Terraform.
Task:
Практическая работа. Балансировка нагрузки
Decision:
Большинство веб-приложений созданы, чтобы взаимодействовать через интернет. Вы развернули в кластере приложение, но у вас пока нет к нему доступа из интернета. Чтобы исправить эту проблему, воспользуемся сервисом LoadBalancer.
У созданного пода есть внутренний IP-адрес.
Помните, мы говорили о том, что в кластере есть собственный сервис DNS? Он работает с внутренними IP-адресами объектов кластера, чтобы те могли взаимодействовать.
Однако внутренний IP-адрес может меняться, когда ресурсы группы узлов обновляются. Чтобы обращаться к приложению извне, требуется неизменный публичный IP-адрес — это и будет IP-адрес балансировщика.
    Создайте файл-манифест load-balancer.yaml:
 apiVersion: v1
 kind: Service
 metadata:
   name: my-loadbalancer
 spec:
   selector:
      app: nginx
   ports:
   - port: 80
     targetPort: 80
   type: LoadBalancer
Где:
port — порт сетевого балансировщика, на котором будут обслуживаться пользовательские запросы;
targetPort — порт контейнера, на котором доступно приложение;
selector — метка селектора из шаблона подов в манифесте объекта Deployment.
Выполните манифест:
 kubectl apply -f <путь_к_файлу_load-balancer.yaml>
Вы увидите сообщение:
 service/my-loadbalancer created
    В консоли управления откройте раздел Load Balancer. Там должен появиться балансировщик нагрузки с префиксом k8s в имени и уникальным идентификатором кластера Kubernetes.
    Скопируйте IP-адрес балансировщика в адресную строку браузера. Вы увидите приветственную страницу NGINX.
Если при создании ресурсов вы получаете ошибку failed to ensure cloud loadbalancer: failed to start cloud lb creation: Permission denied, убедитесь, что вашему сервисному аккаунту хватает прав. Подробнее читайте в документации. 
Task:
Автоматическое масштабирование
Decision:
Автомасштабирование в Managed Kubernetes
Масштабирование позволяет распределять нагрузку между контейнерами и снизить риск сбоя.
На прошлых уроках мы развернули приложение в кластере из одного пода, а затем масштабировали на три пода. Ручное масштабирование — занятие трудоёмкое и неэффективное. Посмотрим, как его автоматизировать.
Для автомасштабирования подходят инструменты, встроенные в Kubernetes: Horizontal Pod Autoscaler и Cluster Autoscaler. Они решают разные задачи и могут работать как по отдельности, так и совместно.
Horizontal Pod Autoscaler, как понятно из названия, масштабирует поды: увеличивает и уменьшает их количество, когда изменяется нагрузка. Cluster Autoscaler управляет количеством узлов, на которых поды запущены.
Давайте посмотрим на работу этих инструментов поближе.
Horizontal Pod Autoscaler
Horizontal Pod Autoscaler анализирует нагрузку на сервис и исходя из неё создаёт или удаляет поды.
Сервис ориентируется на лимиты (limits) и запросы (requests). Первые ограничивают ресурсы, доступные поду с контейнерами: процессор, память и др. Если их не указать, контейнер может забрать все ресурсы ноды. Запросы описывают количество свободных ресурсов, которыми должен располагать узел, чтобы на нём можно было запустить ещё один под с сервисом. Если ресурсов недостаточно, придётся создать дополнительный узел.
И тут в дело вступает наш второй инструмент.
Cluster Autoscaler
Cluster Autoscaler оценивает запросы подов и автоматически изменяет количество узлов кластера Kubernetes:
    Если из-за нехватки ресурсов не удаётся запустить поды, то новые узлы создаются и добавляются в кластер.
    Если узлы недостаточно утилизируются, а их поды можно перенести на другие узлы, то узлы освобождаются и удаляются из кластера.
В Yandex Managed Service for Kubernetes инструмент Cluster Autoscaler включён по умолчанию. Читайте об этом в разделе документации об автомасштабировании группы узлов
Task:
Чем отличаются сценарии автомасштабирования Horizontal Pod Autoscaler и Cluster Autoscaler?
Decision:
-Horizontal Pod Autoscaler наращивает поды, а Cluster Autoscaler — узлы
-Horizontal Pod Autoscaler анализирует метрики потребления ресурсов процессора или оперативной памяти, а Cluster Autoscaler опирается на политики, описывающие ресурсы, которые необходимы для запуска подов
+Оба ответа верны
Task:
Практическая работа. Автомасштабирование в Yandex Managed Kubernetes
Decision:
В этой работе вы увидите, как в Kubernetes® выполняется горизонтальное автомасштабирование.
    Создайте манифест load-balancer-hpa.yaml.
    Для начала скопируйте в него настройки спецификаций, которые вы составляли на предыдущих уроках: из my-nginx.yaml (в примере ниже это раздел Deployment) и из load-balancer.yaml (раздел Service).
    Поскольку новый балансировщик должен отслеживать отдельную группу контейнеров, используйте для контейнеров другие метки (labels), например nginx-hpa.
---
### Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-loadbalancer-hpa
  labels:
    app: nginx-hpa
spec:
  replicas: 1
  selector:
    matchLabels:
           app: nginx-hpa
  template:
    metadata:
              name: nginx-hpa
            labels:
              app: nginx-hpa
    spec:
            containers:
              - name: nginx-hpa
                image: k8s.gcr.io/hpa-example
---
### Service
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-hpa
spec:
  selector:
     app: nginx-hpa
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer 
    В разделе Deployment смените образ с Yandex Container Registry на k8s.gcr.io/hpa-example — это специальный тестовый образ из публичного репозитория, создающий высокую нагрузку на процессор. Так вам будет удобно отслеживать работу Horizontal Pod Autoscaler.
       ...
       spec:
          containers:
              - name: nginx-hpa
              image: k8s.gcr.io/hpa-example 
    Теперь добавьте в шаблон контейнера настройки requests и limits: мы попросим по умолчанию 256 мебибайтов памяти и 500 милли-CPU (половину ядра), а ограничим контейнер 500 мебибайтами и 1 CPU.
    ...
    spec:
     containers:
       - name: nginx-hpa
         image: k8s.gcr.io/hpa-example
         resources:
           requests:
             memory: "256Mi"
             cpu: "500m"
           limits:
             memory: "500Mi"
             cpu: "1" 
    Дополните манифест настройками для Horizontal Pod Autoscaler:
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-nginx-deployment-hpa
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 20 
    В результате должен получиться такой манифест:
---
### Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment-hpa
  labels:
    app: nginx-hpa
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-hpa
  template:
    metadata:
      name: nginx-hpa
      labels:
        app: nginx-hpa
    spec:
      containers:
        - name: nginx-hpa
          image: k8s.gcr.io/hpa-example
          resources:
            requests:
              memory: "256Mi"
              cpu: "500m"
            limits:
              memory: "500Mi"
              cpu: "1"
---
### Service
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-hpa
spec:
  selector:
    app: nginx-hpa
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
---
### HPA
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-nginx-deployment-hpa
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 20 
    Примените манифест:
kubectl apply -f <путь_к_load-balancer-hpa.yaml> 
Вы увидите три сообщения:
deployment.apps/my-nginx-deployment-hpa created
service/my-loadbalancer-hpa created
horizontalpodautoscaler.autoscaling/my-hpa created 
    В консоли управления перейдите в раздел Network Load Balancer. Дождитесь, пока статус my-nginx-deployment-hpa станет Running, после чего посмотрите IP-адрес балансировщика. Убедитесь, что в браузере этот адрес доступен. В терминале сохраните IP-адрес в переменную. Например, так:
LOAD_BALANCER_IP=<IP-адрес балансировщика> 
    Запустите в отдельном окне отслеживание интересующих вас компонентов кластера Kubernetes:
while true; do kubectl get pod,svc,hpa,nodes -o wide; sleep 5; done  
    Теперь сымитируйте рабочую нагрузку на приложение. Для этого подойдёт утилита wget (установите её с помощью пакетного менеджера или с сайта).
while true; do wget -q -O- http://$LOAD_BALANCER_IP; done  
Вы увидите, что сначала увеличится число подов, а затем добавятся узлы. Число узлов ограничено настройками группы узлов кластера, которые вы задали при создании кластера (в нашем случае максимальное количество узлов — пять).
    Остановите цикл создания нагрузки на приложение (комбинация клавиш Ctrl + C). В окне консоли с отслеживанием компонентов кластера вы увидите, как удаляются узлы и поды без нагрузки.
Task:
Мониторинг Managed Kubernetes
Decision:
На прошлом уроке вы узнали один из способов контролировать состояние работающего и нагруженного кластера:
kubectl get pod,svc,hpa,nodes -o wide 
И увидели в ответ на команду примерно следующее:
Воспринимать информацию в таком виде не очень удобно. К счастью, в Yandex Managed Service for Kubernetes есть дополнительные возможности управления кластерами, и одна из них — дашборды для мониторинга.
Чтобы подать нагрузку на кластер и смотреть, как распределяются ресурсы, запустите в отдельном окне цикл с утилитой wget — ровно так, как делали это на прошлом уроке:
while true; do wget -q -O- http://<IP_адрес_балансировщика>; done 
    В веб-консоли перейдите в раздел Managed Service for Kubernetes, войдите в свой кластер и переключитесь на панели слева на вкладку Рабочая нагрузка. Перейдите на вкладку Контроллеры Deployment.
    Вы увидите список запущенных сервисов. В нём будет ваш сервис my-loadbalancer-hpa. Войдите в него. На вкладках вы можете посмотреть количество и статус подов, события и другие данные.
Постарайтесь начать отслеживать состояние ресурсов сразу же после подачи нагрузки, так вы успеете застать процесс создания подов и узлов.
    На вкладке Поды вы увидите количество и статус подов, которые поддерживают сервис. Некоторые поды не созданы — у них в колонке Узел стоят прочерки.
Под может быть не создан, например потому что не хватило ресурсов процессора. Чтобы узнать причину, переключитесь на него и откройте вкладку События.
    Вернитесь в верхний раздел Кластер и перейдите к просмотру узлов. Вы увидите, что происходит автомасштабирование: создаётся или уже создан второй узел.
Откройте этот узел и посмотрите на дашборд мониторинга ресурсов — на общую картину и значения на конкретный момент:
В Yandex Managed Kubernetes у всех ресурсов кластера есть такие дашборды для мониторинга.
Для эксплуатации сервиса важно отслеживать не только состояние ресурсов, но и события в кластере.
Вернитесь в головной раздел кластера и перейдите в События. Их, как видите, много. Чтобы находить события быстрее, фильтруйте их с помощью поля Фильтр по сообщению и трёх выпадающих списков.
Вы можете настроить мониторинг и видеть только те данные, которые хотите. Вот как это делается.
На панели слева переключитесь в раздел Сеть. В последней колонке нажмите значок шестерёнки. Откроется список полей, которые выводятся в детализации. Включайте и отключайте их.
Теперь в разделе Сеть откройте любой сервис и убедитесь, что в детализации остались именно те поля, которые вы отметили.
Попробуйте сами исследовать возможности мониторинга для ресурсов кластера: узлов, подов, балансировщика, сервисов.
Закройте окно с запущенной утилитой wget. Понаблюдайте, как меняется количество активных узлов и подов. Через некоторое время лишние ресурсы освободятся. Найдите на графиках момент выключения нагрузки.
Task:
Отказоустойчивость Managed Kubernetes
Decision:
Отказоустойчивость — это подход, позволяющий увеличить доступность приложения или сервиса. Поговорим об отказоустойчивости приложений, развёрнутых в кластере Kubernetes.
Yandex Managed Kubernetes из коробки обеспечивает определённый уровень отказоустойчивости:
    Доступ к мастер-ноде Managed Kubernetes предоставляется только на уровне API, а значит, даже владелец кластера не сможет «залезть внутрь» и сломать её.
    Автомасштабирование тоже существенно повышает отказоустойчивость.
    Managed Kubernetes плотно интегрирован с инфраструктурой Yandex Cloud. Вы можете использовать Yandex Container Registry для хранения образов и Container Optimized Image для разворачивания контейнеров.
Но если ваши приложения и сервисы действительно критичны для бизнеса, стоит надёжно их защитить и дополнительно повысить отказоустойчивость.
Рассмотрим некоторые сценарии отказов.
1. Отказ ноды
Время от времени ломается всё, в том числе виртуальные машины, серверы или стойки в дата-центре. Это значит, что выходят из строя размещённые на них ноды. Kubernetes будет автоматически расселять поды по действующим нодам и зонам доступности. Он делает это случайно, так что все поды могут оказаться на одном узле.
Правильнее распределять копии контейнеров по разным узлам (виртуальным машинам), а в региональных кластерах — по разным зонам доступности. Вам поможет политика node anti-affinity, которая распределяет поды по узлам, или ещё более гибкий и современный инструмент Pod Topology Spread Constraints: он распределяет поды и по узлам, и по зонам доступности.
2. Обновление Kubernetes
Kubernetes развивается очень быстро, и необходимо регулярно обновлять его, чтобы получать доступ к новым возможностям. При обновлении Kubernetes автоматически перезапускает ноды, а вместе с ними перезапускаются и размещённые там поды. Если перезапустится несколько нод одновременно — это может привести к недоступности приложения или сервиса.
Чтобы управлять этим процессом, используйте Pod Disruption Budget. Его конфигурация позволяет ограничить число одновременно недоступных подов и обеспечить соблюдение SLA.
Пример, где задаётся минимальное число одновременно доступных подов (параметр minAvailable):
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: nginx 
Пример, где задаётся максимальное число одновременно недоступных подов (параметр maxUnavailable):
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx 
3. Сбой DNS
DNS — очень важный и самый нагруженный сервис внутри кластера Kubernetes. Сбой в нём приводит к недоступности всего кластера.  Инструмент NodeLocal DNSCache позволяет нодам обращаться в DNS-сервис кластера не напрямую, а через локальный кеш, который обновляется по протоколу TCP. Включение NodeLocal DNSCache существенно сокращает внутренний трафик и увеличивает отказоустойчивость кластера.
Несколько общих рекомендаций для Yandex Managed Kubernetes
    Обновляйте рабочую среду только вручную, используя каналы Regular или Stable. Отключите в рабочей среде автообновления в релизных каналах для мастера и для каждой группы узлов.
    Для рабочей среды используйте региональный тип мастера. О разнице между региональным и зональным мастером мы говорили, когда создавали кластер. SLA Yandex Managed Kubernetes распространяется именно на региональную конфигурацию.
    При настройке LoadBalancer явно прописывайте политику распределения трафика. За это отвечает параметр externalTrafficPolicy. Задавайте для него значение Local — тогда трафик будет напрямую попадать на те ноды, где запущены контейнеры приложений. При этом сокращается горизонтальный трафик между виртуальными машинами. Если же оставить значение по умолчанию (Cluster), то трафик будет случайно попадать на любой из узлов кластера, а затем перенаправляться на другие ноды, пока не встретит нужный под.
    Для всех сервисов настройте requests и limits, о которых мы говорили на уроке про автомасштабирование. Напомним: limits гарантируют, что сервис не превысит выделенные ему ресурсы процессора и оперативной памяти, а requests нужны для автоматического горизонтального масштабирования подов.
    При автомасштабировании с помощью Cluster Autoscaler узлы создаются в одной зоне доступности. Для дополнительной страховки создайте по группе узлов в каждой зоне доступности.
    Разворачивайте сервисы типа Deployment в нескольких экземплярах. Это гарантирует доступность сервиса при проблемах с подом, нодой или даже целой зоной доступности.
    С помощью readiness-проб отслеживайте ситуации, когда приложение временно не может обслуживать трафик (например, потому что ждёт готовности смежного сервиса).
Task:
Управление доступом
Decision:
Кластеры Kubernetes широко используются для решения самых разных задач. Поэтому надёжность — бесперебойная работа и защита данных — очень важны. Надёжность определяется уровнями доступа к объектам кластера и  операциям над ними.
Для разграничения доступа используется ролевая модель. Подробно о ней рассказывается в курсе «Безопасность». А в этом уроке вы узнаете, как её можно применять для работы с кластером Kubernetes.
Ролевой доступ
Роль — это набор разрешений, описывающих допустимые операции с ресурсом. Вы предоставляете пользователю доступ к ресурсу, назначая ему роли. Пользователем может быть не только человек, но и команда, отдел или даже автоматизированные инструменты (например инструменты CI/CD).
Для ограничения потребления ресурсов в облаке применяются квоты. Квота — это количество ресурсов, которое пользователь может употребить.
Вот несколько типичных ролей при использовании кластера Kubernetes:
    администратор облака — даёт доступ в облако и выделяет квоты;
    сетевой администратор — организует и разграничивает доступ к ресурсам, но сам их не использует;
    специалист по безопасности — наблюдает за тем, что происходит, не потребляет ресурсов;
    разработчик — пишет код, создаёт пул-реквесты и, соответственно, новые образы контейнеров;
    DevOps/SRE — управляет кластером Kubernetes и реестром образов;
    Инструменты CI/CD — запускают приложения в рабочей среде. Именно от их имени часто потребляются основные ресурсы.
Для работы с кластером Kubernetes можно выделить две группы ролей:
    Роли для управления ресурсами внутри кластера.
    Роли в Managed Kubernetes, которые обеспечивают интеграцию кластера в облако.
Рассмотрим вторую группу.
Ролевая модель в Yandex Cloud
Как вы знаете, в Yandex.Cloud доступ к ресурсам контролирует сервис Yandex IAM (Identity and Access Management) и его система сервисных аккаунтов. В IAM есть роли и для Managed Kubernetes, и для Container Registry. С их помощью можно настроить поддержку описанной выше ролевой модели.
Примитивные роли viewer, editor и admin можно выдавать на сервисы или ресурсы сервиса, и тогда их название строится по принципу сервис.роль или сервис.ресурс.роль. В Managed Kubernetes это выглядит так:
Идеология ролевой модели Yandex Cloud — чёткое разграничение публичных и непубличных ресурсов. Поэтому все роли, предоставляемые по умолчанию, не позволяют открывать публичный доступ к ресурсам. Чтобы пользователь мог создавать публичные ресурсы, необходимо осознанно предоставить ему дополнительные роли.
Как правило, для безопасной разработки и эксплуатации приложений поддерживаются как минимум две среды: рабочая (для пользователей) и среда разработки и тестирования. Можно разделить эти среды в облаке с помощью каталогов и настроить для каждого каталога свою политику ролей и квот. Любому пользователю для работы в каталоге понадобится роль viewer.
В документации Managed Kubernetes описаны все возможные роли. Ниже мы приведем несколько примеров того, как с помощью ролей Yandex Cloud предоставлять доступ пользователям в реальных задачах.
Роли для DevOps/SRE
Задача DevOps — обеспечить ресурсы для работы приложения и организовать регулярные обновления приложения. Посмотрим, какие роли для этого нужны.
Чтобы создавать кластеры Kubernetes, нужна роль k8s.admin. Это роль по умолчанию, и поэтому (как мы уже говорили выше) она не даёт возможности открывать публичный доступ. Чтобы создать кластер с публичным доступом, потребуется дополнительная роль vpc.publicAdmin.
Например, чтобы команда разработки случайно не создала публично доступный кластер, ограничьте её ролью k8s.admin, а роль vpc.publicAdmin предоставьте только сотрудникам DevOps/SRE.
Чтобы установить системные приложения в кластере, команде DevOps/SRE потребуется полный административный доступ. Это роль k8s.cluster-api.cluster-admin.
Чтобы подключить кластер к сети, нужна роль vpc.user.  Чтобы управлять сервисными аккаунтами кластера — iam.serviceAccounts.user.
Чтобы обновлять приложение, нужна возможность помещать новый образ в реестр, создавать машины с помощью образа из реестра, а также удалять ненужные образы. Для управления образами в реестре подойдет роль container-registry.admin.
Роли для разработчика
Разработчик будет создавать приложения в кластере, но добавлять и удалять узлы — нет, так как это сфера ответственности DevOps. С учётом этих ограничений разработчику подойдёт роль k8s.cluster-api.editor.
Каждое изменение приложения создаёт новый образ — разработчику нужна роль container-registry.images.pusher, чтобы добавлять образы в реестр. Но эту роль следует выдавать только на каталог для разработки, а не на продуктив.
Также разработчику будет полезно отслеживать работу кластера в целом, не вмешиваясь в его работу. Для этого подойдёт k8s.viewer.
Роли для кластера K8s
Роль k8s.cluster.agent позволяет создавать в кластере любые объекты, кроме публичных балансировщиков и публичных IP-адресов для нод. Также при создании кластера проверяются полномочия сервисного аккаунта, и эта роль позволит создать кластер без доступа в интернет.
Если же системному администратору потребуется предоставлять публичные доступы (доступ кластера в интернет и возможность создавать группы узлов с публичными адресами), ему нужно дать дополнительные роли vpc.publicAdmin или load-balancer.admin.
Роли для узлов K8s
Если вы работаете с Yandex Container Registry, для скачивания образов потребуется роль container-registry.images.puller. Если Container Registry не используется, эта роль не нужна.
Итоги и рекомендации
    Разворачивая кластеры Kubernetes, не забывайте о безопасности и продумывайте ролевую матрицу для доступа к продуктивной и непродуктивной среде.
    Используйте возможности Yandex IAM.
    Осознанно предоставляйте пользователям дополнительные роли для создания публичных ресурсов.
Task:
Кому могут предоставляться роли в Yandex Managed Kubernetes? Отметьте все подходящие варианты.
Decision:
+пользователям
+сервисам
+сервисным аккаунтам
-каталогам
Task:
Квота в Yandex Managed Kubernetes — это количество ...
Decision:
+ресурсов, которое пользователь с определённой ролью может употребить
-ролей, которые можно создать в одном пространстве имён
-пользователей, которых можно создать в одном облачном аккаунте
Task:
Принципы отказоустойчивости
Decision:
Вы наверняка время от времени встречали в новостях сообщения о том, что какой-то крупный интернет-магазин или востребованный сервис вышел из строя и был недоступен в течение нескольких часов или даже суток. Такие ситуации приводят к финансовым потерям (недополученной прибыли или даже штрафам) и становятся ударом по репутации провайдера.
Если вы работаете с юридическими лицами, обязательства по обеспечению доступности обычно фиксируются в виде SLA — Service Level Agreements, соглашении об уровне сервиса. Например, вы гарантируете, что сервис будет доступен 99,95% времени. Если вы работаете с обычными пользователями, формальные SLA могут не заключаться, но вы и сами заинтересованы в том, чтобы ваше приложение работало без сбоев.
Не существует универсального решения, которое устраняло бы все проблемы, связанные с отказоустойчивостью. Но если сочетать различные подходы со стороны разрабатываемого приложения и настройки инфраструктуры, то можно подобрать удачные решения для каждого конкретного случая и минимизировать риски отказа работы системы.
Мы уже затрагивали отдельные вопросы отказоустойчивости в предыдущих уроках: балансировка нагрузки, зоны доступности, автоматическое масштабирование и автоматическое восстановление — все это помогает поддерживать непрерывную работу системы.
Как можно обеспечивать отказоустойчивость
Есть несколько классических подходов к обеспечению отказоустойчивости.
    Масштабирование. Система должна быть готова быстро предоставить дополнительные ресурсы при повышении нагрузки и уметь освобождать лишние ресурсы при снижении нагрузки. Этот принцип заложен в облачной архитектуре.
    Избыточность и резервирование. Наличие независимых дублирующих компонентов повышает надёжность системы: например, если один диск выйдет из строя, данные можно быстро перенести на запасной диск.Отказоустойчивые системы часто строят, создавая несколько копий приложения или его компонентов: баз данных, очередей сообщений и т.д. Реализация может строиться двумя способами: так, чтобы взаимодействие происходило сразу с двумя компонентами одновременно, или так, чтобы постоянно работал только один компонент, а второй находился в ожидании.
    Мониторинг. Для быстрого реагирования нужно отслеживать состояние всех компонентов системы и анализировать историю событий. В небольших системах это можно делать вручную. В крупных или критически важных системах необходим автоматический мониторинг с уведомлениями при приближении к критическим значениям параметров. Такой мониторинг называют проактивным — он позволяет не исправлять проблемы, а не допускать их появления. Мониторингу мы посвятим отдельную тему.
    Реакция на сбои. Например, система может сама перенести данные с вышедшего из строя диска на резервный или развернуть дополнительные виртуальные машины при повышении нагрузки. Подобные действия тоже входят в проактивный мониторинг и их тоже можно автоматизировать.
Понимайте вашу систему
Чтобы выбрать оптимальное решение, хорошо изучите устройство вашей системы. Мы говорим именно «системы», а не «приложения» или «сервиса», потому что слабым звеном может оказаться не код, а часть инфраструктуры. Например, это могут быть сбои в работе сети или проблемы на стороне провайдера.
Вам нужно понимать слабые места системы — точки отказа. Точкой отказа называется компонент, при выведении которого из строя перестает корректно работать вся система. Выход из строя точки отказа часто приводит к неработоспособности других компонентов, что в свою очередь ведёт к отказу следующих. Такие ситуации называют каскадными сбоями, или эффектом домино, и они приводят к самым масштабным авариям.
Отказоустойчивость в облаке
При проектировании отказоустойчивых приложений важно понимать, что представляют собой отдельные сущности облачной инфраструктуры в реальном мире.
Например, понятие зона доступности — достаточно условное и определяется провайдером инфраструктуры. Какой-то провайдер может назвать три соседние стойки в дата-центре тремя разными зонами доступности.
В Yandex Cloud зоны доступности — это дата-центры в разных регионах России. За счет такого географического разделения обеспечивается более высокая надёжность. Но это и накладывает определенные ограничения — при отказе некоторые сущности физически не смогут мигрировать в другую зону доступности (например кеши записи при отказе диска). Значит, вам нужно настраивать репликацию на более высоком уровне, например, на уровне приложения.
Рассчитывайте целесообразность
Высокая доступность приложения или сервиса требует инвестиций. Когда вы планируете меры по повышению отказоустойчивости, старайтесь соотносить затраты и отдачу от них.
Например, если ваше приложение просто формирует какой-то ежемесячный отчет, вряд ли стоит добиваться для него времени работы 99,999%. И уж точно не стоит уделять большое внимание отказоустойчивости сред разработки и тестирования.
Вот пример оценки вложений в отказоустойчивость для небольшого интернет-магазина: стоит ли бороться за повышение времени доступности с 99% до 99,9%? Судя по этим расчетам, инвестиции окупаются только если выручка магазина составляет не менее 40 тыс. руб. в день.
Мы рекомендуем вам инвестировать в отказоустойчивость, но подходить к этому расчетливо и вдумчиво, с «калькулятором в руках» и четкой картинкой в голове.
В следующих четырех уроках мы рассмотрим четыре самых распространённых сценария сбоев и посмотрим, как обеспечивается в этих случаях отказоустойчивость системы.
Task:
Отказоустойчивость — это:
Decision:
-Быстрый отклик системы в ответ на действия пользователя
+Способность системы продолжить работу при неработоспособности одного или нескольких компонентов
-Средняя доступность, выраженная как среднее число сбоев на период предоставления сервиса
Task:
Точка отказа — это:
Decision:
+Компонент системы, при выведении из строя которого система перестаёт работать корректно
-Компонент системы, при выведении из строя которого уменьшается максимальная пропускная способность
-Компонент системы, при выведении из строя которого увеличивается средний отклик пользователя
Task:
Практическая работа. Сбой виртуальной машины
Decision:
Давайте посмотрим, как принципы построения отказоустойчивых систем реализованы в Yandex Cloud. В практических работах этой темы вы проверите четыре основных сценария отказов:
сбой виртуальной машины,
сбой всей зоны доступности,
обновление приложения
сбой приложения.
Вы сымитируете эти отказы и понаблюдаете, как Yandex Cloud обеспечивает доступность приложения и восстанавливает инфраструктуру после сбоев.
Начнем с самого простого сценария — сбоя виртуальной машины.
Создайте группу из трёх ВМ в трёх зонах доступности под балансировщиком нагрузки. Используйте образ с ОС Ubuntu 18.04 (потом мы обновим его на более свежую версию ОС).
Используйте спецификацию specification.yaml из практической работы по CLI Yandex Cloud, но адаптируйте её для того, чтобы на ней можно было проверить разные сценарии сбоев.
Во-первых, будут задействованы все три зоны доступности, поэтому нужно немного исправить блок allocation_policy:
allocation_policy:
    zones:
        - zone_id: ru-central1-a
        - zone_id: ru-central1-b
        - zone_id: ru-central1-c 
Также пропишите подсети для каждой зоны (не забывайте подставлять идентификаторы ваших подсетей):
    network_interface_specs:
        - network_id: <идентификатор_сети>
          subnet_ids: 
            - <идентификатор_подсети_№1>
            - <идентификатор_подсети_№2>
            - <идентификатор_подсети_№3> 
          primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }} 
Во-вторых, в секции #cloud-config укажите пользователя, которого нужно создать для входа в виртуальные машины по SSH (это понадобится позднее, на одной из следующих практических работ):
        users:
          - name: my-user
            groups: sudo
            lock_passwd: true
            sudo: 'ALL=(ALL) NOPASSWD:ALL'
            ssh-authorized-keys:
              - ssh-rsa AAAAB3Nza... 
Обновленный файл спецификации specification.yaml:
name: my-group
service_account_id: <идентификатор_сервисного_аккаунта>
instance_template:
    platform_id: standard-v1
    resources_spec:
        memory: 2g
        cores: 2
    boot_disk_spec:
        mode: READ_WRITE
        disk_spec:
            image_id: <идентификатор_образа_Ubuntu_18.04> 
            type_id: network-hdd
            size: 32g
    network_interface_specs:
        - network_id: <идентификатор_сети>
          subnet_ids: 
            - <идентификатор_подсети_№1>
            - <идентификатор_подсети_№2>
            - <идентификатор_подсети_№3> 
          primary_v4_address_spec: { one_to_one_nat_spec: { ip_version: IPV4 }}
    scheduling_policy:
        preemptible: false
    metadata:
      user-data: |-
        #cloud-config
          users:
            - name: my-user
              groups: sudo
              lock_passwd: true
              sudo: 'ALL=(ALL) NOPASSWD:ALL'
              ssh-authorized-keys:
                - <содержимое_публичной_части_SSH-ключа>
          package_update: true
          runcmd:
            - [ apt-get, install, -y, nginx ]
            - [/bin/bash, -c, 'source /etc/lsb-release; sed -i "s/Welcome to nginx/It is $(hostname) on $DISTRIB_DESCRIPTION/" /var/www/html/index.nginx-debian.html']
 deploy_policy:
    max_unavailable: 1
    max_expansion: 0
scale_policy:
    fixed_scale:
        size: 3
allocation_policy:
    zones:
        - zone_id: ru-central1-a
        - zone_id: ru-central1-b
        - zone_id: ru-central1-c
 load_balancer_spec:
    target_group_spec:
        name: my-target-group
Создайте группу по новой спецификации:
yc compute instance-group create --file <путь_к_файлу_specification.yaml> 
Если ранее вы удаляли балансировщик нагрузки, создайте его снова и привяжите к целевой группе:
yc load-balancer network-load-balancer create \
  --region-id ru-central1 \
  --name my-load-balancer \
  --listener name=my-listener,external-ip-version=ipv4,port=80 \
  --target-group target-group-id=<идентификатор_целевой_группы>,healthcheck-name=test-health-check,healthcheck-interval=2s,healthcheck-timeout=1s,healthcheck-unhealthythreshold=2,healthcheck-healthythreshold=2,healthcheck-http-port=80 
В консоли управления убедитесь, что ресурсы созданы. Проверьте вывод по внешнему IP-адресу балансировщика — должна отображаться приветственная страница с идентификатором одной из виртуальных машин группы.
Начните отслеживать состояние виртуальных машин группы и целевой группы балансировщика:
while true; do \
yc compute instance-group \
  --id <идентификатор_группы_ВМ> list-instances; \
yc load-balancer network-load-balancer \
  --id <идентификатор_балансировщика> target-states \
  --target-group-id <идентификатор_целевой_группы>; \
sleep 5; done 
Информация выводится в виде таблиц:
+----------------------+---------------------------+----------------+-------------+------------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS         | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+------------------------+----------------+
| ef34nv4tp3ha8gl6p3df | cl1m5ksvljnq5frekghi-uzex | 84.201.148.207 | 10.128.0.42 | RUNNING_ACTUAL [1m54s] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [13m]   |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]    |                |
+----------------------+---------------------------+----------------+-------------+------------------------+----------------+
+----------------------+-------------+---------+
|      SUBNET ID       |   ADDRESS   | STATUS  |
+----------------------+-------------+---------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
| e2luooifg8ruecr7g6fk | 10.128.0.6  | HEALTHY |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
+----------------------+-------------+---------+
Сбой виртуальной машины может произойти из-за падения физического хоста, на котором она запущена. Иногда виртуальную машину могут удалить случайно, по ошибке. Чтобы сымитировать сбой, удалим одну из виртуальных машин в группе через консоль управления.
Если бы это была единственная машина, на которую поступает трафик, система стала бы недоступна. Но у нас система развернута на нескольких виртуальных машинах, поэтому трафик будет перенаправлен на две оставшиеся. Через несколько секунд будет обнаружена проблема, и виртуальная машина будет выведена из-под балансировки. Об этом говорит статус UNHEALTHY.
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | RUNNING_ACTUAL [15m] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [32m] |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
+----------------------+-------------+-----------+
|      SUBNET ID       |   ADDRESS   |  STATUS   |
+----------------------+-------------+-----------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY   |
| e2luooifg8ruecr7g6fk | 10.128.0.6  | UNHEALTHY |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY   |
+----------------------+-------------+-----------+ 
Далее подсеть перейдет в статус DRAINING — ресурс удаляется, и с него снимается трафик. Балансировщик перестает передавать трафик этому ресурсу.
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [0s] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
+----------------------+-------------+----------+
|      SUBNET ID       |   ADDRESS   |  STATUS  |
+----------------------+-------------+----------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
| e2luooifg8ruecr7g6fk | 10.128.0.6  | DRAINING |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
+----------------------+-------------+----------+
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CLOSING_TRAFFIC [9s] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m] |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]  |                |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
+----------------------+-------------+----------+
|      SUBNET ID       |   ADDRESS   |  STATUS  |
+----------------------+-------------+----------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
+----------------------+-------------+----------+ 
После этого Instance Group начнет пересоздавать удалённую виртуальную машину. Процесс восстановления может занять некоторое время. Понаблюдаем за ним.
Сначала новая виртуальная машина появится в группе в статусе CREATING_INSTANCE.
+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
| ef330no5frc5de91v77n | cl1m5ksvljnq5frekghi-uzex | 84.201.147.33  | 10.128.0.6  | CREATING_INSTANCE [-1s] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [33m]    |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [6h]     |                |
+----------------------+---------------------------+----------------+-------------+-------------------------+----------------+
+----------------------+-------------+----------+
|      SUBNET ID       |   ADDRESS   |  STATUS  |
+----------------------+-------------+----------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY  |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY  |
+----------------------+-------------+----------+
Далее виртуальная машина будет открыта для трафика (статус OPEN_TRAFFIC). Балансировщик начнет процесс включения машины в список доступных машин.
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [-1s] | Adding target(s)               |
|                      |                           |                |             |                       | 10.128.0.32 to target group    |
|                      |                           |                |             |                       | b7rh0bhm9f82dglb2p9r           |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
+----------------------+-------------+---------+
|      SUBNET ID       |   ADDRESS   | STATUS  |
+----------------------+-------------+---------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
+----------------------+-------------+---------+
+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        |         STATUS MESSAGE         |
+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1s] | Adding target(s)               |
|                      |                           |                |             |                      | 10.128.0.32 to target group    |
|                      |                           |                |             |                      | b7rh0bhm9f82dglb2p9r           |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m] |                                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                                |
+----------------------+---------------------------+----------------+-------------+----------------------+--------------------------------+
+----------------------+-------------+---------+
|      SUBNET ID       |   ADDRESS   | STATUS  |
+----------------------+-------------+---------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
| e2luooifg8ruecr7g6fk | 10.128.0.32 | INITIAL |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
+----------------------+-------------+---------+
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS         |         STATUS MESSAGE         |
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [18s] | Awaiting HEALTHY state for     |
|                      |                           |                |             |                       | target(s) 10.128.0.32. Elapsed |
|                      |                           |                |             |                       | time: 3s.                      |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [34m]  |                                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]   |                                |
+----------------------+---------------------------+----------------+-------------+-----------------------+--------------------------------+
+----------------------+-------------+----------+
|      SUBNET ID       |   ADDRESS   |  STATUS  |
+----------------------+-------------+----------+
| b0c4h992tbuodl5hudpu | 10.128.0.32 | INITIAL  |
| b0c4h992tbuodl5hudpu | 10.128.0.37 | INACTIVE |
| b0c4h992tbuodl5hudpu | 10.128.0.9  | HEALTHY  |
+----------------------+-------------+----------+
+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |         STATUS          |         STATUS MESSAGE         |
+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | OPENING_TRAFFIC [1m32s] | [NLB unhealthy]; Awaiting      |
|                      |                           |                |             |                         | HEALTHY state for target(s)    |
|                      |                           |                |             |                         | 10.128.0.32. Elapsed time: 1m  |
|                      |                           |                |             |                         | 17s.                           |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m]    |                                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]     |                                |
+----------------------+---------------------------+----------------+-------------+-------------------------+--------------------------------+
+----------------------+-------------+---------+
|      SUBNET ID       |   ADDRESS   | STATUS  |
+----------------------+-------------+---------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
| e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
+----------------------+-------------+---------+ 
И в завершение подсеть перейдет в статус HEALTHY, а машина — в статус RUNNING_ACTUAL, и трафик будет снова разделен между тремя машинами.
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
|     INSTANCE ID      |           NAME            |  EXTERNAL IP   | INTERNAL IP |        STATUS        | STATUS MESSAGE |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
| ef374t9ghea78p471gal | cl1m5ksvljnq5frekghi-uzex | 84.252.135.153 | 10.128.0.32 | RUNNING_ACTUAL [-1s] |                |
| ef3nquhoicdq0ccl0tlq | cl1m5ksvljnq5frekghi-iduv | 84.201.171.248 | 10.128.0.9  | RUNNING_ACTUAL [35m] |                |
| ef3oio9su52imaod7rad | cl1m5ksvljnq5frekghi-ixac | 84.252.132.4   | 10.128.0.37 | RUNNING_ACTUAL [7h]  |                |
+----------------------+---------------------------+----------------+-------------+----------------------+----------------+
+----------------------+-------------+---------+
|      SUBNET ID       |   ADDRESS   | STATUS  |
+----------------------+-------------+---------+
| b0c4h992tbuodl5hudpu | 10.128.0.37 | HEALTHY |
| e2luooifg8ruecr7g6fk | 10.128.0.32 | HEALTHY |
| e9bn57jvjnbujnmk3mba | 10.128.0.9  | HEALTHY |
+----------------------+-------------+---------+ 
Восстановление произошло автоматически без ручного вмешательства.
Обратите внимание! Эту группу виртуальных машин мы будем использовать и в трех следующих практических работах, не удаляйте её. Если вы будете делать большой перерыв между практическими работами, вы можете остановить группу (чтобы не расходовать средства на балансе), а затем запустить её снова.
Task:
Практическая работа. Сбой зоны доступности
Decision:
В этом сценарии рассмотрим ситуацию, когда произошел сбой сразу всей зоны доступности. Такие ситуации возникают крайне редко и могут быть связаны с какими-то масштабными стихийными бедствиями, однако и их стоит предусмотреть.
Посмотрим, как будет решаться проблема неожиданного выхода из строя зоны доступности.
В нашем примере (см. предыдущий урок) используются все три зоны доступности — ru-central1-a, ru-central1-b и ru-central1-c. В каждой зоне располагается одна ВМ.
    Установите такие настройки политики развертывания — пусть группу можно расширять на 1 ВМ и уменьшать на 1 ВМ:
    Теперь в настройках группы виртуальных машин уберите одну зону доступности, например ru-central1-c. Переключитесь на вкладку Список ВМ и посмотрите, что будет происходить.
    Для ВМ, которая располагалась в зоне ru-central1-c, отключается трафик (статус Closing traffic), а затем сама машина удаляется (статус Deleting instance). Одновременно в другой зоне доступности создаётся и запускается новая ВМ. Остальные машины в группе продолжают работать без изменений.
Таким образом, даже при выходе из строя всей зоны доступности группа виртуальных машин продолжит работать и будет способна принимать прежнюю нагрузку.
Task:
Практическая работа. Обновление приложения
Decision:
На практической работе с CLI мы уже рассматривали обновление операционной системы для группы виртуальных машин. Любые приложения, установленные на ВМ, обновляются по тем же правилам. Давайте рассмотрим этот процесс ещё раз.
Первый вариант обновления
Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с ОС Ubuntu 20.04. Убедитесь, что параметры политики развёртывания такие: группу нельзя расширять, а уменьшать можно только на одну ВМ.
Политика развёртывания группы виртуальных машин (вариант 1)
Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например с fd8s2gbn4d5k2rcf12d9 на fd8ju9iqf6g5bcq77jns) и запустите обновление группы:
yc compute instance-group update \
  --name my-group \
  --file <путь_к_файлу_specification.yaml> 
В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
Сначала вы увидите статус Running outdated. Это означает, что машины работают со старой версией приложения.
Затем одна из машин начинает обновляться: для неё закрывается трафик (статус Closing traffic), она останавливается (статус Stopping instance), обновляется (статус Updating instance), затем трафик снова открывается (статус Opening traffic), и наконец статус меняется на Running actual. Обновление выполнено.
Затем то же самое последовательно выполняется для остальных машин в группе.
Порядок обновления зависит от политики развёртывания. Мы запретили увеличивать размер группы и указали, что одновременно неработоспособной может быть только одна машина. Именно так и произошло обновление: машины по одной выводились из строя, обновлялись и запускались снова.
Второй вариант обновления
Теперь давайте изменим настройки политики развёртывания.
Если вы работаете в консоли управления, измените шаблон ВМ и выберите образ с Ubuntu и NGINX, созданный ранее и помещённый в Container Registry.
Измените параметры развёртывания. Теперь группу можно расширять на одну ВМ, а уменьшать нельзя:
Политика развёртывания группы виртуальных машин (вариант 2)
Если вы работаете в командной строке, в спецификации specification.yaml измените параметр image_id (например, снова с fd8ju9iqf6g5bcq77jns на fd8s2gbn4d5k2rcf12d9). Параметры обновления измените так:
deploy_policy:
    max_unavailable: 0
    max_expansion: 1 
Запустите обновление группы.
В консоли управления на странице группы ВМ перейдите на вкладку Список ВМ и проследите, как меняются статусы машин.
Сначала вы увидите статус Running outdated. Затем создаётся новая машина (статус Creating instance), для неё открывается трафик (статус Opening traffic), статус машины меняется на Running actual, при этом одна из «устаревших» ВМ выводится из строя (статусы Closing traffic и Stopping instance).
Затем то же самое последовательно выполняется для остальных машин в группе.
Task:
Практическая работа. Сбой приложения
Decision:
Последний сценарий, который мы рассмотрим, это сбой приложения. Ситуация, когда сама ВМ работоспособна, но по каким-то причинам произошла ошибка в приложении. Это может быть потеря соединения с базой данных или какой-то баг в запущенном приложении (например утечка памяти). Давайте сымитируем такой сценарий. На наших виртуальных машинах запущен только веб-сервер NGINX, давайте остановим его. Но сначала включим проверку состояния ВМ.
В консоли управления откройте вкладку Обзор для вашей группы виртуальных машин, нажмите кнопку Изменить и активируйте проверку состояний. Сохраните изменения.
В браузере откройте страницу с внешним IP-адресом балансировщика, привязанного к вашей группе, и посмотрите, на какую из машин выводится трафик. Узнайте внешний IP-адрес этой машины.
В новой вкладке браузера откройте IP-адрес этой виртуальной машины и убедитесь, что выводится приветственная страница, т. е. сервер доступен.
Помните, когда вы меняли файл конфигурации для группы машин, вы добавили в него пользователя my-user? Теперь он вам пригодится — из консоли зайдите на ВМ от его имени:
 ssh my-user@<внешний_IP-адрес_ВМ>
Посмотрите список запущенных процессов:
 ps axu
Убедитесь, что в списке есть процессы nginx:
Теперь остановите эти процессы, чтобы сделать сервер недоступным:
 sudo killall nginx
В браузере обновите страницу балансировщика. Вы увидите, что теперь трафик направляется на другую виртуальную машину группы. Это означает, что Instance Group обнаружил сбой приложения и переключил трафик.
Теперь обновите страницу виртуальной машины, на которой вы остановили NGINX. Убедитесь, что сервер теперь недоступен.
Откройте список машин вашей группы и проследите, как меняется состояние одной из машин.
Сначала будет закрыт трафик (статус Closing traffic), затем виртуальная машина будет остановлена (статус Stopping instance), а затем перезапущена (статус Running actual).
Убедитесь, что веб-сервер на этой ВМ снова доступен.
Мы проверили четыре основных сценария сбоев и убедились, что Yandex Cloud автоматически отрабатывает их и восстанавливает работоспособность группы.
Теперь вы можете удалить группу виртуальных машин, в этом курсе она больше не понадобится.
Task:
Зачем нужен мониторинг. Yandex Monitoring
Decision:
Мониторинг — это контроль за характеристиками системы, меняющимися со временем. Это могут быть трафик сайта, утилизация памяти или ресурсов процессора и т.д. Такие характеристики называются метриками. Их изменение отслеживают для того, чтобы они не выходили за границы приемлемых значений. Например, если загруженность процессора близка к 100%, то стоит нарастить вычислительные ресурсы.
Задачи мониторинга — хранить и визуализировать метрики в реальном времени, а также оповещать пользователя, если что-то пошло не так.
Нужен ли вам мониторинг
Единственно верный ответ — да, мониторинг нужен всегда. Без мониторинга невозможно построить надёжное приложение или сервис.
Даже если ваше приложение небольшое и не критически важное, мониторинг будет полезен. Он поможет вовремя обнаружить, что на сервере заканчивается оперативная память или свободное место на диске, позволит отследить аномально быстрый рост числа аккаунтов и т.д.
В Yandex Cloud есть собственный инструмент для мониторинга облачной инфраструктуры и размещённых в ней сервисов — Yandex Monitoring.
Обзор Yandex Monitoring
У каждого ресурса в Yandex Cloud есть набор системных метрик. Такие метрики начинают регистрироваться сразу же в момент создания ресурса. Например, для виртуальной машины можно следить за загрузкой процессора и утилизацией оперативной памяти.
Метрики обычно визуализируются с помощью графиков (значения метрик можно выводить и в текстовом виде, но это менее наглядно). Блоки с графиками или текстом называются виджетами. Несколько виджетов можно расположить рядом, тогда получается дашборд.
В Yandex Monitoring есть готовые дашборды для облачных сервисов — виртуальных машин, балансировщиков, кластеров Managed Kubernetes и других. Сервисные дашборды не надо настраивать, они работают из коробки.
Когда вы наводите указатель мыши на какой-то график на дашборде, всплывает окно со значением метки.
Если на сервисных дашбордах информации недостаточно, можно настроить собственные, пользовательские дашборды и вывести на них практически любые системные или пользовательские метрики. Вы узнаете, как это сделать, в следующей практической работе.
Пользовательские метрики можно создавать не только для сервисов Yandex Cloud, но и для своих приложений. Так вы сможете собрать все критические показатели вместе и наблюдать за ними, не переключаясь между различными системами мониторинга.
Выгрузка и загрузка метрик
Вы можете использовать метрики Yandex Cloud в другом сервисе или приложении. Для этого их выгружают в файл. И наоборот, в Yandex Cloud Monitoring можно загружать метрики из других приложений и сервисов, чтобы было удобнее контролировать их работу без переключения на разные дашборды. Подробнее мы рассмотрим эти кейсы в практических работах.
За загрузку метрик отвечает агент для поставки метрик Yandex Unified Agent.
Хранение метрик и удаление устаревших метрик
Как мы уже говорили, системные метрики автоматически собираются со всех активных ресурсов Yandex Cloud. После удаления ресурса сбор метрик с него прекращается, но уже собранные метрики не удаляются сразу, а хранятся в течение 30 дней.
Например, если вы удалили виртуальную машину 1 августа, то её метрики будут доступны в Yandex Monitoring до 31 августа.
Любые системные метрики, для которых новые значения не поступали в течение 30 дней, считаются устаревшими. Такие метрики автоматически удаляются из Yandex Monitoring раз в сутки.
Например, даже если вы не удаляли виртуальную машину, а просто остановили её и не запускали в течение месяца, то собранные для неё метрики будут удалены.
Автоматическое удаление затрагивает только системные метрики и не распространяется на пользовательские.
Task:
Что такое метрика?
Decision:
+Количественная характеристика системы, меняющаяся во времени.
-Значение какого-то показателя на конкретный момент времени.
-График на дашборде, который отображает изменение какого-то показателя во времени.
Task:
Что такое сервисный дашборд?
Decision:
+Дашборд, отражающий текущее состояние какого-то сервиса и доступный без дополнительных настроек, «из коробки».
-Дашборд, доступный сотрудникам службы поддержки и пользователям с ролью service.
-Дашборд, доступный за отдельную плату и предоставляемый по принципу SaaS.
Task:
Начало работы с Yandex Monitoring
Decision:
В этой практической работе вы создадите свой дашборд в сервисе Yandex Monitoring.
Подготовка. Что мониторить
Для мониторинга используйте сайт клиники «Доктор Айболит», который вы создавали на уроке про Object Storage. Будем отслеживать объём трафика, поступающий на сайт. Чтобы имитировать нагрузку, мы воспользуемся утилитой wget, которую уже использовали в практической работе с кластером Managed Kubernetes. Если у вас не установлена эта утилита, её можно скачать здесь. Адрес же своего сайта можно посмотреть в Object Storage.
while true; do wget -q -O- <адрес_сайта>; done 
Создание дашборда и виджета
Откройте начальную страницу Yandex Monitoring. На вкладке Главная нажмите кнопку Создать дашборд.
Открылось окно создания дашборда. В правом верхнем углу нажмите кнопку Сохранить, введите название нового дашборда и сохраните его.
Новый дашборд пустой, он не содержит виджетов. Давайте добавим виджет с графиком. Нажмите кнопку Редактировать и в блоке Добавить виджет выберите График.
Метрики добавляются в виджет с помощью запросов. Но это совсем не сложно — запросы создаются с помощью удобного конструктора, размещённого в нижней части страницы.
Конструктор сам предлагает, какие параметры нужно заполнить, и показывает доступные значения.
Выберите service = Object Storage, затем name = traffic и resource_id = <имя_бакета_с_сайтом>. Больше ничего выбирать не нужно. Этот запрос означает, что вы хотите мониторить входящий и исходящий трафик своего сайта.
Сразу после сохранения запроса вверху отобразятся графики, которые соответствуют указанным параметрам.
Вы можете настроить период времени, который хотите видеть на графике. В нашем случае будет более наглядно, если вы укажете небольшой период, например, 10 или 15 минут.
Нажмите кнопку Сохранить, чтобы созданный виджет появился на вашем новом дашборде.
Вы можете растянуть или сжать виджет до нужного размера, потянув мышью за правый нижний угол с треугольником, и перетащить виджет на нужное место на дашборде.
В правом верхнем углу виджета отображаются значки для редактирования («карандаш») и настройки («шестеренка»). Вы можете в любой момент отредактировать запрос, по которому строится график, а в настройках можете изменить название виджета или включить отображение легенды. Также вы можете удалить виджет, если он больше не нужен.
Задайте название для вашего виджета, например: «Трафик сайта клиники Айболит».
Нажмите кнопку Сохранить, чтобы сохранить новый виджет на дашборде.
Использование функций в виджетах
На виджете вы видите два отдельных графика для входящего и исходящего трафика. Допустим, вы хотите также посмотреть и суммарный трафик. Вернёмся в созданный ранее виджет и добавим к нему еще один график.
Нажмите на виджете значок редактирования («карандаш»).
Внизу нажмите кнопку Добавить запрос. Повторите все настройки предыдущего запроса. Затем в строке ниже добавьте функцию — нажмите значок «+», выберите раздел Комбинирование и функцию series_sum(). В скобках в данном случае ничего указывать не нужно.
На графике появилась ещё одна линия, которая показывает значения суммарного трафика. Если вы наведете на неё указатель мыши, то во всплывающем окне увидите, что у этой метрики нет имени.
Чтобы добавить имя, в той же строке, где вы описывали функцию, снова нажмите на значок «+», выберите раздел Другое и функцию alias. В скобках введите имя метрики, например, Traffic_Total.
Теперь при наведении указателя мыши на график будет отображаться это имя.
Нажмите кнопку Сохранить. На дашборде вы увидите обновленный виджет с тремя графиками. Снова нажмите кнопку Сохранить, чтобы сохранить изменения на дашборде.
Виджеты можно копировать с одного дашборда на другой с помощью меню:
Decision:
$ while true; do wget -q -O- http://www.aibolit38.ru.website.yandexcloud.net; done
Task:
Практическая работа. Отправка собственных метрик
Decision:
Часто бывает полезно отслеживать более широкий набор метрик, чем тот, что доступен в Yandex Monitoring «из коробки».
Предположим, вам интересно узнать, сколько людей заходит на ваш сайт и как их число зависит от времени дня или дня недели. Вы можете выгружать эти данные из Яндекс Метрики или вашей собственной аналитической системы и самостоятельно загружать в Yandex Monitoring с помощью API.
Давайте попробуем сделать это с нашим сайтом.
Отправка метрик через API
Получите IAM-токен:
    Инструкция для аккаунта на Яндексе.
    Инструкция для сервисного аккаунта.
Обратите внимание — токены устаревают через 12 часов после создания. Поэтому если вы сделаете паузу при выполнении данной практической работы, для продолжения лучше запросить новый токен.
Сохраните токен в переменной окружения, так его будет проще использовать:
 export IAM_TOKEN=<IAM-токен>
Создайте файл с телом запроса, например my-metrics.json. В свойстве metrics указывается список метрик для записи. Пусть это будет количество пользователей сайта. В массиве timeseries указываются значения на разные моменты времени (измените число на сегодняшнее в формате год-месяц-день).
{
  "metrics": [
    {
      "name": "number_of_users",
      "labels": {
       "site": "aibolit"
      },
      "type": "IGAUGE",
      "timeseries": [
        {
          "ts": "2021-05-10T10:00:00Z",
          "value": "22"
        },
        {
          "ts": "2021-05-10T11:00:00Z",
          "value": "44"
        },
        {
          "ts": "2021-05-10T12:00:00Z",
          "value": "11"
        },
        {
          "ts": "2021-05-10T13:00:00Z",
          "value": "55"
        },
        {
          "ts": "2021-05-10T14:00:00Z",
          "value": "33"
        }
      ]
    }
  ]
} 
Отправьте запрос, указав в нем идентификатор каталога и имя сервиса custom (это имя указывается для всех пользовательских метрик):
 curl -X POST \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${IAM_TOKEN}" \
     -d '@<путь_к_файлу_my-metrics.json>' \
 'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=<идентификатор_каталога>&service=custom'
Мониторинг пользовательских метрик
Создайте на вашем дашборде новый виджет с графиком, назовите его «Число пользователей сайта».
В виджете создайте запрос с параметрами service = Custom Metrics и name = number_of_users. Убедитесь, что в виджете выбран нужный период:
Этот график станет нагляднее, если вместо точек отображать столбцы. Тип графика можно изменить с помощью кнопки в правом верхнем углу виджета:
Мониторинг метрик Linux
Другой пример — ваши приложения запущены на виртуальных машинах под Linux. По умолчанию вы можете посмотреть утилизацию ресурсов процессора или диска для ВМ в целом. Но вам будет полезно знать, сколько ресурсов потребляет каждое из них. В Yandex Monitoring вы можете отслеживать системные метрики Linux, такие как объём свободной памяти или загрузка процессора. Но для этого нужно дополнительно настроить отправку этих метрик с помощью Yandex Unified Agent, который мы уже упоминали.
Установка Yandex Unified Agent
Создайте виртуальную машину. На неё вы будете устанавливать Yandex Unified Agent. Можете использовать образ с ОС Ubuntu, который вы создали ранее и поместили в Container Registry. Назовите машину, например, for-ua.
При создании используйте ваш сервисный аккаунт. Задайте логин (например ua-user) и ssh-ключ.
Для сервисного аккаунта добавьте роль monitoring.editor.
Посмотрите публичный IP-адрес машины for-ua и зайдите на неё по ssh:
 ssh ua-user@<публичный_адрес_ВМ>
Теперь вы можете установить Yandex Unified Agent:
 ua_version=$(curl -s https://storage.yandexcloud.net/yc-unified-agent/latest-version) bash -c 'curl -s -O https://storage.yandexcloud.net/yc-unified-agent/releases/$ua_version/unified_agent && chmod +x ./unified_agent'
Также вы можете выбрать опцию Установить в поле Агент сбора метрик при создании ВМ, тогда Yandex Unified Agent будет установлен автоматически.
Создайте файл config.yml с типовой спецификацией для доставки метрик Linux.
В параметре folder_id укажите идентификатор вашего каталога.
status:
port: "16241"
storages:
- name: main
  plugin: fs
  config:
    directory: /var/lib/yandex/unified_agent/main
    max_partition_size: 100mb
    max_segment_size: 10mb
channels:
- name: cloud_monitoring
  channel:
    pipe:
      - storage_ref:
          name: main
    output:
      plugin: yc_metrics
      config:
        folder_id: "<идентификатор_каталога>"
        iam:
          cloud_meta: {}
routes:
- input:
    plugin: linux_metrics
    config:
      namespace: sys
  channel:
    channel_ref:
      name: cloud_monitoring
- input:
    plugin: agent_metrics
    config:
      namespace: ua
  channel:
    pipe:
      - filter:
          plugin: filter_metrics
          config:
            match: "{scope=health}"
    channel_ref:
      name: cloud_monitoring
import:
- /etc/yandex/unified_agent/conf.d/*.yml 
В секции status достаточно указать порт для просмотра статуса Yandex Unified Agent.
Секция storage содержит список хранилищ, в которых будут находиться выгруженные данные. Для практической работы достаточно одного файлового хранилища (fs).
Секция channels содержит список именованных каналов, к этим каналам можно обращаться по имени из других секций спецификации. Здесь обозначен один канал с именем cloud_monitoring. К нему идёт обращение из секции routes, которая содержит список маршрутов доставки метрик.
Подробнее о конфигурировании Yandex Unified Agent вы можете почитать в документации.
Скопируйте файл спецификации в виртуальную машину for-ua:
 scp config.yml ua-user@84.252.135.237:config.yml
Теперь запустите Unified Agent с созданной спецификацией:
 sudo ./unified_agent --config config.yml
Если запуск прошел успешно, в конце вы увидите сообщение такого вида:
 ... NOTICE agent started

Настройка виджета для мониторинга метрик Linux
Создайте на вашем дашборде новый виджет с графиком, назовите его «Метрики Linux».
В виджете создайте запрос с параметром service = Custom Metrics. В параметре name выберите любой параметр, начинающийся с sys — всё это системные метрики, поставляемые Unified Agent. Например, name = sys.memory.MemAvailable.
Теперь в виджете отображается график наличия свободной оперативной памяти в виртуальной машине for-ua.
Decision:
$ yc iam key create --service-account-name monitortest --output key.json
id: ajean75uh879rndj8293
service_account_id: ajehq9p412df22arccm1
created_at: "2022-10-16T05:25:41.122300972Z"
key_algorithm: RSA_2048
$ yc config profile create monitortest-profile
$ yc config set service-account-key key.json
$ yc iam create-token
$ export IAM_TOKEN=t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA
$ vim my-metrics.json
$ cat my-metrics.json
{
  "metrics": [
    {
      "name": "number_of_users",
      "labels": {
       "site": "aibolit"
      },
      "type": "IGAUGE",
      "timeseries": [
        {
          "ts": "2021-05-10T10:00:00Z",
          "value": "22"
        },
        {
          "ts": "2021-05-10T11:00:00Z",
          "value": "44"
        },
        {
          "ts": "2021-05-10T12:00:00Z",
          "value": "11"
        },
        {
          "ts": "2021-05-10T13:00:00Z",
          "value": "55"
        },
        {
          "ts": "2021-05-10T14:00:00Z",
          "value": "33"
        }
      ]
    }
  ]
}
$ curl -X POST \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${t1.9euelZqMx8vGx5rNj4qezM7MyJ2Vj-3rnpWal47Gj8vOzZuZzc2ejZycks7l8_cfU1Fl-e8cMVEZ_d3z918BT2X57xwxURn9zef1656Vmp6RyMqKl8fIxo2Rm5XHzcbM7_0.XW7UF1ymvhuU-4VWP34TmF1Yw4YEDuWUggt0pWoxIqaC8dkP8UKcbjfHwk3JLcYRsrkgvCosBvnHl1IRfUvECA}" \
     -d '@/home/administrator/my-metrics.json' \
 'https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write?folderId=ajehq9p412df22arccm1&service=custom'
Task:
Практическая работа. Выгрузка метрик в формате Prometheus
Decision:
Как мы уже говорили раньше, метрики можно выгружать из Yandex Cloud Monitoring в сторонние приложения и сервисы. Пожалуй, чаще всего их выгружают для сервера Prometheus.
На сегодняшний день Prometheus — один из самых популярных инструментов для мониторинга приложений и сервисов. В основе его лежит специализированная СУБД для анализа временных рядов, которая обеспечивает высокое быстродействие. В отличие от большинства систем мониторинга, Prometheus не ждёт, пока сторонние приложения передадут ему свои метрики, а сам опрашивает подключенные к нему приложения и собирает нужные данные.
Prometheus и Yandex Cloud Monitoring решают схожие задачи — хранят значения разных метрик. Prometheus фактически является стандартом для обмена метриками. Поэтому даже используя сервисы Yandex Cloud, IT-администраторы часто хотят отслеживать их работу с помощью Prometheus. Чтобы не лишать специалистов привычных инструментов, Yandex Cloud Monitoring поддерживает выгрузку данных в формате Prometheus. Для этого используется метод prometheusMetrics.
Для визуализации данных, собираемых Prometheus, можно использовать сервис Grafana (в нем можно зарегистрироваться бесплатно на тестовый период). Вы можете установить Grafana на свой компьютер, а можете работать в облачной версии.
Посмотрим, как происходит выгрузка метрик в Prometheus и работа с ними в Grafana. Вы снова будете мониторить сайт клиники «Доктор Айболит».
Подготовка
Создайте API-ключ через консоль управления Yandex Cloud или CLI.
Если вы создаете ключ в консоли управления, то перейдите в каталог, из которого будете выгружать метрики (например default). Затем перейдите на вкладку Сервисные аккаунты и выберите существующий аккаунт. Нажмите кнопку Создать новый ключ и выберите Создать API-ключ. В описании ключа можно указать, например, «для доступа к Prometheus». Сохраните секретную часть ключа в отдельный файл, например, prometheus-key.txt.
Назначьте сервисному аккаунту роль monitoring.viewer на выбранный каталог.
Создайте файл спецификации prometheus.yml (см. пример ниже, замените в нем значение параметра folderId на идентификатор каталога, а значение для bearer_token — на ключ доступа из файла prometheus-key.txt):
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).
rule_files:
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
    - targets: ['localhost:9090']
 
  - job_name: 'yc-monitoring-export'
    metrics_path: '/monitoring/v2/prometheusMetrics'
    params:
      folderId:
      - '<идентификатор_каталога>' 
      service:
      - 'storage' 
    bearer_token: '<секретная_часть_API-ключа>'
    static_configs:
    - targets: ['monitoring.api.cloud.yandex.net']
      labels:
          folderId: '<идентификатор_каталога>'
          service: 'storage' 
Запуск сервера Prometheus
Если вы уже работаете с Prometheus, пропустите все шаги по установке — просто добавьте секцию scrape_configs из примера выше в спецификацию вашего сервера Prometheus и перезапустите сервер, а затем переходите к настройке Grafana.
Для запуска сервера Prometheus используйте официальный Docker-образ prom/prometheus.
Сначала загрузите образ. Для этого запустите Docker Desktop (в терминале выполните команду):
 docker pull prom/prometheus
Чтобы на сервере сразу был ваш файл спецификации, создайте свой образ на основе prom/prometheus. Подготовьте Dockerfile с двумя командами:
 FROM prom/prometheus
 ADD prometheus.yml /etc/prometheus/
Сохраните этот файл в тот же каталог, где находится prometheus.yml. Назовите его именем по умолчанию: Dockerfile.
В терминале перейдите в каталог с Dockerfile. Создайте образ с вашей конфигурацией (используйте ваш идентификатор в Yandex Container Registry):
 docker build . -t cr.yandex/<идентификатор_реестра>/my-prometheus:latest -f Dockerfile
Аутентифицируйтесь в Yandex Container Registry с помощью Docker Credential helper (чтобы Docker мог от вашего имени отправить образ в ваш реестр):
 yc container registry configure-docker
Теперь отправьте образ в ваше хранилище в облаке:
 docker push cr.yandex/<идентификатор_реестра>/my-prometheus:latest
Создайте виртуальную машину с помощью Container Optimized Image, вы уже делали это раньше в практической работе (в разделе Выбор образа загрузочного диска переключитесь на вкладку Container Solution и нажмите Настроить. Выберите из реестра созданный вами образ, остальные настройки оставьте по умолчанию и нажмите Применить).
При создании виртуальной машины используйте ваш сервисный аккаунт. Задайте логин (например prom) и ssh-ключ.
Назовите машину, например, for-prometheus.
Проверьте статус сервера по адресу http://<публичный IP-адрес ВМ с Prometheus>:9090/targets. Через несколько минут после запуска статус процессов prometheus и yc-monitoring-export должен стать UP.
Подайте нагрузку на ваш сайт:
while true; do wget -q -O- <адрес_сайта>; done 
Подождите несколько минут и проверьте, как поставляются метрики в Prometheus.
В верхнем меню выберите пункт Graph. Нажмите на значок «Земли». Откроется меню с доступными метриками. Выберите метрику, которую вы хотите проверить, например, traffic и нажмите кнопку Execute.
Переключитесь на вкладку Graph. Выберите текущее время, для наглядности уменьшите интервал запроса данных (например до 15 минут). Вскоре вы увидите график изменения выбранной метрики.
Настройка Grafana
Теперь посмотрим, как метрики визуализируются в системе Grafana.
Если у вас еще нет аккаунта в Grafana, создайте его с помощью нескольких простых шагов, это бесплатно. Вам откроется интерфейс по адресу https://<ваш_логин>.grafana.net/.
Добавление источника данных
Настройте Prometheus в качестве источника данных. На главной странице нажмите кнопку Connect data. Из предложенного списка выберите источник Prometheus data source и нажмите кнопку Create Prometheus data source.
В следующем окне в поле URL введите endpoint сервера Prometheus http://<публичный IP-адрес ВМ с Prometheus>:9090. Больше никакие настройки менять не нужно.
Внизу нажмите кнопку Save & Test. Должна отобразиться надпись Data source is working.
Добавление дашборда
Вернитесь на главную страницу (нажав на логотип в левом верхнем углу) и нажмите Create your first dashboard. Откроется окно настройки дашборда.
В нижней части экрана на вкладке Query выберите источник данных — Prometheus.
Выберите метрику, которую вы хотите отслеживать. Нажмите на поле Metrics, в открывшемся списке выберите метрику traffic.
Сверху отобразится график выбранной метрики.
Вверху справа в поле Panel Title укажите название графика (например, «Трафик сайта»).
Теперь сохраните настройки — в правом верхнем углу нажмите кнопку Save и укажите название дашборда (например, «Мой дашборд»).
Вы научились отслеживать метрики Yandex.Cloud не только средствами Yandex Monitoring, но и с помощью сторонних систем, в том числе широко используемых Prometheus и Grafana. Но метрики можно использовать не только для визуальной оценки состояния облака и его ресурсов. В следующих двух уроках мы посмотрим, как можно облегчить работу специалиста DevOps и автоматизировать мониторинг.
Task:
Алерты
Decision:
Представьте, что на сервере закончилось место на диске, и ваше приложение больше не может обрабатывать запросы. Или про ваш интернет-магазин написал известный блогер и к вам пришло небывалое количество посетителей, но сайт не справился с подскочившим трафиком и «лёг». Как вы бы хотели узнать об этом — из ленты новостей? Или от недовольных пользователей?
Как можно оперативно отслеживать подобные проблемы или ещё лучше — узнавать о них заранее? На прошлых уроках вы познакомились с сервисом Yandex Monitoring. Но мониторинг требует постоянного активного внимания — кто-то (вы или ваши коллеги) постоянно должен быть на дежурстве, чтобы отслеживать значения метрик и предпринимать меры, когда эти значения приближаются к критическим. Если же дежурного нет, то скорее всего вы узнаете о проблеме только когда она уже случится.
Хорошо бы передать контроль автоматике — пока все показатели в норме, система молча следит за ними, но как только какой-то показатель начинает вызывать тревогу, автоматика срабатывает и «будит» человека. Такие сигналы называются алертами.
Алерты — обязательный элемент мониторинга. Как только вы начинаете мониторить какой-то сервис или приложение, вам стоит сразу же настроить один или несколько алертов.
Фактически алерт — это функция, которая регулярно вызывается и вычисляет значение метрики. В Yandex Monitoring значения метрик проверяются раз в минуту. Результатом работы функции будет статус:
    OK — если значение метрики укладывается в пределы установленной нормы,
    Warning — если значение метрики достигло порога предупреждения,
    Alarm — если значение метрики достигло критического порога.
Есть еще два возможных статуса: NoData (не хватает данных для расчета) и Error (невозможно вычислить значение).
Обработка пиковых значений
Иногда метрики могут резко возрастать до пиковых значений, а затем снова возвращаться в норму. Такие всплески нагрузки не считаются опасными.
Например, в кластере ресурсы одной виртуальной машины закончились. При этом была автоматически создана еще одна виртуальная машина, нагрузка была распределена между ними, и уровень утилизации ресурсов на каждой машине снова стал укладываться в норму. При таком сценарии отправлять алерт администратору не стоит, автоматика устранила проблему без вмешательства человека.
Чтобы исключить лишние алерты, можно использовать функции агрегации. Например, для алерта вычисляется средний уровень утилизации ресурса за период в 5 минут. Если в течение 5 минут автоматика устранила проблему, то средний уровень утилизации ресурса будет укладываться в норму, и алерт не отправляется. Если же критический уровень нагрузки сохраняется дольше 5 минут, это означает, что автоматика не справляется и нужно вмешательство администратора. Тогда ему отправляется алерт на почту, в виде SMS или push-уведомления в браузере.
В следующей практической работе мы посмотрим, как настраивается алерт и в каком виде его получает администратор.
Task:
Когда система мониторинга отправляет алерт администратору?
Decision:
+алерт отправляется при достижении порога предупреждения
+алерт отправляется при достижении критического порога
+алерт отправляется только если среднее значение за период достигает порога предупреждения или критического порога
Task:
Практическая работа. Создание алерта
Decision:
В этой практической работе вы создадите алерт для случая, если трафик на сайте вдруг начнет существенно расти. Снова используйте сайт клиники «Доктор Айболит», для которого настраивали графики на дашборде.
Вы можете перейти на вкладку Алерты и там настроить алерт с нуля. А можете отталкиваться от графиков, которые уже выведены в виджете. Ниже рассматривается именно второй вариант.
Создание алерта
Вернитесь на созданный вами дашборд и в меню виджета «Трафик сайта» выберите пункт Создать алерт.
Поскольку в виджете используются два запроса, вам будет предложено выбрать, для какого запроса вы хотите создать алерт. Выберите запрос с суммирующей функцией и нажмите Продолжить.
Теперь задайте имя и, если хотите, описание алерта. Укажите значение для статусов Alarm и Warning.
Откройте спойлер Показать дополнительные настройки. Там вы увидите, что система предложила вам использовать среднее значение за 5 минут. Оставьте эти параметры.
Теперь нужно выбрать канал для получения алертов. У вас пока ещё нет настроенных каналов, поэтому система предложить вам создать его. Нажмите кнопку Добавить канал и далее Создать канал.
Укажите имя канала, выберите метод — Email, SMS или Push-уведомления. Укажите получателей — себя. Затем нажмите кнопку Создать.
В настройках алерта выберите только что созданный канал.
Вы можете указать для одного алерта несколько каналов уведомлений. Например, если вы хотите получать алерты об увеличении трафика сайта не только в виде Push-уведомлений, но и по электронной почте, создайте еще один канал с методом Email и выберите также и его.
Для каждого канала можно настроить режим повторения уведомлений. Например, в данном случае при превышении трафика будет отправлен один алерт по электронной почте, а алерты в виде push-уведомлений будут отправляться каждые 5 минут до тех пор, пока проблема не будет устранена.
Нажмите кнопку Создать алерт.
Вы увидите настройки созданного алерта, а сверху — его текущий статус OK.
Нажмите слева на вкладку Алерты. Вы увидите ваш алерт, сейчас он единственный в списке. Когда алертов станет больше, вам понадобятся инструменты для работы с ними. Например, вы сможете отобрать из списка только алерты, имеющие статус Alarm или Warning, или временно деактивировать отдельные алерты.
Срабатывание алерта
Теперь посмотрим, как срабатывает алерт. Подайте трафик на сайт, который вы мониторите:
while true; do wget -q -O- <адрес_сайта>; done 
Подождите немного и понаблюдайте за ростом нагрузки. Через какое-то время трафик начнет превышать пороговое значение Warning, и вы начнете получать Push-уведомления.
Если у администратора настроены другие каналы для алертов, он получил бы SMS или Email с предупреждением о пороговом значении трафика.
Как видите, алерты позволяют вовремя привлекать внимание администратора и устранять даже потенциальные, ещё не случившиеся проблемы.
Task:
Интерфейс командной строки Yandex.Cloud (CLI) — это:
Decision:
-сторонняя утилита, которую можно установить в ваш аккаунт Yandex Cloud. Её использование оплачивается в соответствии с тарифами Yandex Cloud
-сервис, предоставляемый Yandex Cloud для управления ресурсами облака в случае, когда консоль управления недоступна
+бесплатная утилита, которую можно установить на компьютер пользователя и использовать для управления ресурсами в Yandex Cloud. Использование CLI не тарифицируется
Task:
В чём заключаются преимущества подхода Infrastructure as Code (можно отметить несколько пунктов)?
Decision:
-любые объекты IT-инфраструктуры можно описать в едином стандартном формате
+изменения, вносимые в спецификации, легко проконтролировать
+для спецификаций можно хранить историю версий и при необходимости можно быстро извлечь и использовать нужную версию
+описания отдельных объектов IT-инфраструктуры можно многократно использовать в разных инсталляциях, это ускоряет подготовку к развёртыванию и повышает надёжность за счет использования проверенных решений
Task:
Выберите правильное утверждение:
Decision:
+Packer создаёт образ виртуальной машины, готовый к использованию на конкретной облачной платформе
-Packer создаёт универсальный образ виртуальной машины, готовый к использованию на любых облачных платформах
-Packer создаёт образ-«полуфабрикат», который необходимо адаптировать для конкретной облачной платформы с помощью переменных, задаваемых при создании виртуальной машины из образа
Task:
Выберите правильное утверждение:
Decision:
-при развёртывании облачной инфраструктуры с помощью Terraform создаются новые ресурсы (то есть, те, которые есть в спецификации, но еще не существуют в облаке), а ресурсы, уже существующие в облаке, не изменяются и не удаляются
-при развёртывании облачной инфраструктуры с помощью Terraform создаются новые ресурсы, а ресурсы, уже существующие в облаке, могут быть обновлены, но не удаляются
+при развёртывании облачной инфраструктуры с помощью Terraform создаются новые ресурсы, а ресурсы, уже существующие в облаке, могут быть обновлены или удалены
Task:
Контейнеризация отличается от виртуализации тем, что:
Decision:
-контейнеры используют больше ресурсов, чем виртуальные машины, так как требуется дополнительное ПО для распределения ресурсов сервера между контейнерами
+контейнеры используют меньше ресурсов, чем виртуальные машины: несколько контейнеров могут использовать одну хостовую ОС, тогда как для каждой виртуальной машины нужна отдельная ОС
-контейнеры и виртуальные машины используют примерно одинаковые объёмы ресурсов, но контейнеры более стабильны, надёжны и защищены
Task:
Выберите верное утверждение:
Decision:
-Docker умеет создавать образы и виртуальных машин, и контейнеров — это указывается в спецификации
+Docker создаёт образы контейнеров на основе готовых образов с определёнными операционными системами
-Docker создаёт образы контейнеров на основе готовых образов виртуальных машин, которые называются Container Optimized Image
Task:
Где применяются кластеры Kubernetes:
Decision:
-кластеры полезны для управления только масштабной инфраструктурой с большим количеством взаимосвязанных приложений и сервисов
+кластеры используются для управления даже небольшими системами, развёрнутыми в облаке
Task:
Что такое проактивный мониторинг (можно отметить несколько пунктов)
Decision:
-автоматический мониторинг, который отправляет уведомления администратору, когда параметры превышают критические значения
+автоматический мониторинг, который отправляет уведомления администратору, когда параметры приближаются к критическим значениям
+автоматический мониторинг, который при возникновении проблем включает систему аварийного восстановления
Task:
Отметьте признаки отказоустойчивой архитектуры (можно отметить несколько пунктов):
Decision:
-готовность к быстрому масштабированию
-избыточность ресурсов
+использование компонентов одного вендора, которые хорошо интегрируются между собой
-наличие подсистемы проактивного мониторинга
Task:
Для чего нужны алерты:
Decision:
+для оповещения администраторов о возникновении критической ситуации в системе
+для оповещения администраторов о приближающейся критической ситуации в системе
Decision:
Task:
Decision:
Task:
Decision:
Task:
Decision:
Task:
Практическая работа. Запускаем функцию с помощью CLI
Decision:

Decision:
$ sudo apt install jq
$ yc init
$ yc config list
token: y0_AgAAAABk37oLAATuwQAAAADQHQt0BDkxnuByQ0u6AQZYOkKFsQJsOBI
cloud-id: b1gg01f1vt0rkid9qsuk
folder-id: b1g0mvtp1aqh3kfsgubt
$ export SERVICE_ACCOUNT=$(yc iam service-account create \
  --name service-account-for-cf \
  --description "service account for cloud functions" \
  --format json | jq -r .) 
$ yc iam service-account list
+----------------------+------------------------+
|          ID          |          NAME          |
+----------------------+------------------------+
| ajehljbngf4kdbkaf5aq | service-account-for-cf |
+----------------------+------------------------+
$ echo $SERVICE_ACCOUNT
$ echo "export SERVICE_ACCOUNT_ID=ajehljbngf4kdbkaf5aq" >> ~/.bashrc && . ~/.bashrc
$ echo $SERVICE_ACCOUNT_ID
$ echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
$ echo $FOLDER_ID
b1g0mvtp1aqh3kfsgubt
$ yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_ID \
  --role editor 
$ yc serverless function create --name my-first-function
done (1s)
id: d4erp0jk02hj3134gn7g
folder_id: b1g0mvtp1aqh3kfsgubt
created_at: "2022-10-18T11:51:03.815Z"
name: my-first-function
log_group_id: ckgaep2dmajejq7vjdvv
http_invoke_url: https://functions.yandexcloud.net/d4erp0jk02hj3134gn7g
status: ACTIVE
$ vim index.py
$ cat index.py
def handler(event, context):
    return {
        'statusCode': 200,
        'body': 'Hello World!',
    }
$ yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path index.py
id: d4e456dkn2shsmiqdq3i
function_id: d4erp0jk02hj3134gn7g
created_at: "2022-10-18T11:55:21.256Z"
runtime: python37
entrypoint: index.handler
resources:
  memory: "268435456"
execution_timeout: 5s
service_account_id: ajehljbngf4kdbkaf5aq
image_size: "4096"
status: ACTIVE
tags:
  - $latest
log_group_id: ckgaep2dmajejq7vjdvv
$ yc serverless function list
+----------------------+-------------------+----------------------+--------+
|          ID          |       NAME        |      FOLDER ID       | STATUS |
+----------------------+-------------------+----------------------+--------+
| d4erp0jk02hj3134gn7g | my-first-function | b1g0mvtp1aqh3kfsgubt | ACTIVE |
+----------------------+-------------------+----------------------+--------+
$ yc serverless function version list --function-name my-first-function
+----------------------+----------------------+----------+---------------+---------+---------------------+
|          ID          |     FUNCTION ID      | RUNTIME  |  ENTRYPOINT   |  TAGS   |     CREATED AT      |
+----------------------+----------------------+----------+---------------+---------+---------------------+
| d4e456dkn2shsmiqdq3i | d4erp0jk02hj3134gn7g | python37 | index.handler | $latest | 2022-10-18 11:55:21 |
+----------------------+----------------------+----------+---------------+---------+---------------------+
$ yc serverless function invoke d4erp0jk02hj3134gn7g
{"statusCode": 200, "body": "Hello World!"}
$ yc serverless function allow-unauthenticated-invoke my-first-function
$ yc serverless function get my-first-function
id: d4erp0jk02hj3134gn7g
folder_id: b1g0mvtp1aqh3kfsgubt
created_at: "2022-10-18T11:51:03.815Z"
name: my-first-function
log_group_id: ckgaep2dmajejq7vjdvv
http_invoke_url: https://functions.yandexcloud.net/d4erp0jk02hj3134gn7g
status: ACTIVE
Task:
Введение в Serverless
Decision:
Что такое Serverless
Что бы вы подумали, если бы с вас потребовали поминутную оплату за время, проведённое в магазине? Если бы ресторан предложил вам арендовать повара и официанта на время ужина? Или если бы вам в кофейне сказали закупить зёрна, арендовать бариста и научить его готовить ваш кофе?
То, что кажется бессмыслицей в обычной жизни, по иронии судьбы до сих пор является стандартом в мире разработки приложений. Чтобы исполнить код, нужно арендовать виртуальную машину определённой мощности, настроить на ней операционную систему и развернуть среду исполнения своего кода. Чтобы записывать и читать данные в БД, нужно определить, сколько потребуется CPU, какой будет нужен объём памяти и жёсткого диска. И оплачивать их, вне зависимости от того используете вы их или нет.
Разработчики привыкли к этому и всегда думают о серверах. Но иногда очень хочется иметь возможность просто узнать цену чашки кофе, операции с БД или запуска определённой функции кода приложения. И брать их сколько нужно и когда нужно. Не переплачивать, если операций мало и они не загружают арендованные сервера. Не отказывать в сервисе, если операций больше, чем сервера могут обработать. И конечно, получать скидки, если чашек много!
Выкинуть из головы сервера. Serverless — в этом и есть суть одного из значительных направлений в развитии облачных сервисов, о котором пойдёт речь в данном курсе.
Использование serverless-сервисов при разработке приложений даёт следующие преимущества:
    Позволяет разработчикам сфокусироваться на написании и улучшении прикладного кода. При этом им не нужно готовить, настраивать и обслуживать какую-либо инфраструктуру (серверы, виртуальные машины, контейнеры).
    Часто позволяет снизить операционные расходы приложений по сравнению с их запуском на виртуальной машине или в средах контейнерной виртуализации — за счёт более эффективного распределения мощностей провайдером.
    Позволяет владельцам приложений минимизировать риски непредсказуемости спроса — как в случае переоценки, так и недооценки будущей нагрузки. Это позволяет гораздо проще принимать решения о добавлении новых функций в приложения или о запусках рекламных кампаний.
Задача этого курса — дать практические навыки использования ключевых инструментов serverless-экосистемы в Yandex Cloud. Мы не только познакомим вас с технологиями и объясним, как их объединить для решения бизнес-задач, но и расскажем, как прогнозировать расходы.
Содержание курса «Serverless-разработка»
Этот курс построен вокруг четырёх основных сервисов экосистемы serverless в Yandex Cloud:
    Yandex Cloud Functions. Вы узнаете, как запускать код ваших приложений в изолированной, автоматически масштабируемой и безопасной среде исполнения, а также настраивать события, при наступлении которых этот код будет выполняться. Вы также научитесь отслеживать показатели работы приложений и читать журнал исполнения кода для работы над производительностью приложений.
    API Gateway. Вы научитесь создавать API-шлюзы для интеграции компонентов микросервисных приложений и разрозненных API.
    Yandex Database (YDB) Serverless. Вы узнаете, как получить быстрое, надёжное и дешёвое реляционное или документное хранилище ваших данных, без необходимости управлять его ресурсами. Вы также познакомитесь с возможностями работы с YDB через Document API, доступными только для serverless-режима.
    Yandex Message Queue. Вы научитесь создавать и настраивать очереди для обмена сообщениями между компонентами вашего приложения, а в самом конце — закрепите весь пройденный курс и сделаете практическую работу, которая объединяет все изученные сервисы для решения конкретной задачи.
Необходимые навыки для успешного прохождения курса
Чтобы быстрее освоить материалы курса, вам нужно быть знакомыми с некоторыми базовыми понятиями, которые освещены в других курсах этой профессии, а также в документации.
    Сервисные аккаунты. Их использование позволяет получать программный доступ к сервисам Yandex Cloud, а также гибко настраивать права доступа к ресурсам для написанных вами программ. Рекомендуем прочитать руководство «Как начать работать c сервисными аккаунтами» в документации к Yandex Identity and Access Management.
    Мониторинг. Во всех сервисах Yandex Cloud есть функция мониторинга тех или иных метрик. Мы рекомендуем агрегировать нужные метрики в сквозном сервисе Yandex Monitoring. Об этом вы можете узнать из предыдущего курса «DevOps и автоматизация», а также из документации по этому сервису.
    Yandex Database. В рамках данного курса мы познакомим вас с особенностями работы с YDB в бессерверном режиме: что можно получить бесплатно, как формируется стоимость и каким образом можно её минимизировать. Для полного понимания этой информации нужно знать, как работать с данными через SQL, о чем рассказывается в соответствующем разделе курса «Хранение и анализ данных».
    Биллинг. Важно понимать принципы тарификации используемых ресурсов и уметь считать расходы, чтобы правильно выбирать подход к проектированию приложений и сервисов. Этой теме посвящён отдельный курс профессии «Прогнозирование затрат и оптимизация расходов».
Task:
Отметьте ключевые особенности разработки приложений при использовании serverless-сервисов:
Decision:
-Вашему приложению не будут нужны сервера
+Нет необходимости создавать и обслуживать виртуальные машины
-Можно арендовать практически любой сервис в облаке, платя только за вызовы функций, запросы к БД, трафик и т.д.
+Нет необходимости отслеживать утилизацию оборудования и добавлять или удалять ресурсы, чтобы инфраструктура обходилась дешевле и справлялась с повышенной нагрузкой
+Можно писать приложения для Cloud Functions на любом удобном для вас языке программирования, для которого есть среда исполнения
+Вы сможете написать и запустить приложение в интернете, обладая меньшими знаниями о серверах и операционных системах, чем при использовании виртуальных машин
Task:
Обзор сервисов
Decision:
Базовым сервисом для serverless-разработки в Yandex Cloud является Yandex Cloud Functions (далее — CF). Он относится к категории Function-As-A-Service (FaaS), его главная задача — исполнять программный код, который написан на одном из поддерживаемых языков программирования (сейчас их восемь).
Допустим, вам нужно каждый день делать бэкап дисков от части виртуальных машин, запущенных в Yandex Cloud. В старой парадигме вы бы арендовали мощности под виртуальную машину, развернули в ней операционную систему, настроили утилиту cron на запуск бэкапа по расписанию, а затем были бы вынуждены поддерживать эту ВМ в работоспособном состоянии.
Если же вы применяете Cloud Functions, ранее написанный скрипт бэкапа вы можете адаптировать для использования в СF, создать новую функцию через веб-интерфейс и настроить расписание выполнения этой функции. Больше вам ни о чём думать не надо — сервис сделает всё остальное. При этом вы платите только за количество вызовов функции, а также время исполнения. Подробнее о тарификации этого и других serverless-сервисов вы узнаете из следующих уроков.
Второй типовой сценарий работы с CF — это обработка HTTP-запроса. При этом приложение, обрабатывающее запросы из интернета, редко доступно пользователям напрямую. Обычно между клиентом и приложением размещается промежуточный компонент, или API-шлюз (API Gateway), который обеспечивает балансировку, поставку статического контента, маршрутизацию запросов под разными URL на нужные сервисы, терминирование SSL.
На рынке существует множество готовых API-шлюзов (nginx, apigee, axway, 3scale и прочие), и вы можете арендовать несколько виртуальных машин для их развёртывания. Но гораздо удобнее воспользоваться готовым serverless-решением от Yandex Cloud — Yandex API Gateway. Чтобы его использовать, вам не нужно арендовать какие-либо ресурсы на почасовой основе.
Допустим, ваше приложение должно по запросу отдавать статический контент из хранилища. Раньше вы бы для этого подняли сервер с NGINX. Сейчас вы просто настраиваете API Gateway: Yandex Cloud сам примет запросы, терминирует HTTPS, маршрутизирует запросы к сервисам и отдаст контент из Object Storage. Вам нужно будет заплатить лишь за количество запросов к созданным API-шлюзам и исходящий трафик.
Когда на запрос клиента нужно ответить быстро, а обработка запроса требует времени, её можно сделать асинхронной и параллельной, используя очереди сообщений на базе Yandex Message Queue.
Допустим, в вашем веб-сервисе зарегистрировался новый пользователь. Отвечающий за регистрацию микросервис складывает в очередь несколько сообщений для других сервисов: положить данные в CRM, отправить приветственное письмо и т.д. И сразу после этого отвечает, что регистрация прошла успешно. Каждый микросервис забирает из очереди свою задачу и выполняет её. Благодаря такому «клею» между компонентами вы можете собирать достаточно сложные конструкции. При этом вы ещё и решаете задачи надёжности — ведь теперь регистрация пользователя не откажет, если CRM в течение какого-то времени будет недоступен, а когда он «поднимется» — обработает все накопившиеся сообщения о регистрации. А ещё вам не надо арендовывать серверы и обслуживать сложные технические решения, которые бы позволили надёжно передавать сообщения между компонентами вашего сервиса.
Набор serverless-сервисов был бы неполон, если бы среди них не было базы данных. Сервис Yandex Database (YDB) в Yandex Cloud предоставляет вам возможность работы с высоконадёжной реляционной или документной БД через SQL или Document API (AWS DynamoDB API) с практически неограниченными возможностями масштабирования нагрузки и оплатой только за успешно выполненные запросы и объём хранимых данных. Создание такой БД делается за несколько секунд одним кликом в UI или командой в CLI, после чего вы получаете URL для доступа к ней и можете начать работу.
Работу с перечисленными выше сервисами мы подробно разбираем в рамках этого курса. Однако к группе Serverless относятся ещё несколько сервисов. Для более подробного их изучения мы рекомендуем обратиться к документации. Здесь же остановимся на нескольких основных моментах.
Yandex Object Storage — масштабируемое облачное объектное хранилище данных, совместимое с Amazon S3 API. У Object Storage есть ряд преимуществ перед обычным сервером, на который можно писать данные в произвольную папку.
Во-первых, это плоское хранилище без иерархии файловой структуры, что ускоряет поиск данных. При этом внутри хранилища сохраняемые файлы группируются в бакеты — за счёт этого можно разделять данные разных проектов или пользователей.
Объекты в Object Storage реплицируются в несколько географически распределённых зон доступности, что позволяет увеличить доступность данных.
Наконец, S3-хранилище автоматически расширяется по мере необходимости, а значит, вам не нужно отслеживать, хватает ли места на диске.
Yandex Serverless Containers — сервис для запуска Docker-контейнеров без создания виртуальных машин и кластеров Kubernetes. Serverless Containers сам использует функции Yandex Cloud Functions для развёртывания контейнеров. Это даёт ряд преимуществ: функции автоматически масштабируются, нет необходимости настраивать балансировщик нагрузки, функцию можно развернуть за секунды.
Yandex Cloud Logging — сервис для агрегации и чтения логов пользовательских приложений и ресурсов Yandex Cloud. Вы можете создать свою лог-группу, объединяющую несколько облачных сервисов. Это удобно в тех случаях, когда вам нужно отладить работу, скажем, микросервисного приложения. Объединив логи Cloud Functions, API Gateway и Message Queue, вы будете видеть консоли управления запросы к API-шлюзу, действия с событиями в очереди и журнал исполнения программного кода.
Сервис Cloud Logging не только является частью serverless-экосистемы, но и сам построен на базе бессерверных технологий, в частности, хранит данные в Yandex Database Serverless.
Yandex IoT Core — сервис интернета вещей для двустороннего обмена сообщениями между реестрами и устройствами. Этот сервис использует протокол Message Queuing Telemetry Transport (MQTT), который применяется в автоиндустрии, логистике, платформах умных домов, умных бытовых устройствах и т.д.
IoT Core обеспечивает защиту данных по всем точкам подключения, поддерживает автоматическую балансировку и горизонтальное масштабирование во всех дата-центрах Yandex Cloud. Вы можете связывать IoT Core с Cloud Functions через триггеры. В этом случае ваши функции Cloud Functions смогут обрабатывать копии сообщений, передаваемых через MQTT-брокер.
Yandex Data Streams — масштабируемый сервис для управления потоками данных в режиме реального времени. Эта шина потоков данных может непрерывно собирать информацию из разных источников и накапливать её в принимающих системах, таких как ClickHouse, S3 и других. При этом передаваемые данные можно произвольным образом обрабатывать программным кодом из Cloud Functions.
К примеру, вы разрабатываете IoT-устройство с веб-сервисом и мобильным приложением. С помощью Data Streams вы можете собирать телеметрию с устройства, а также данные об использовании сервиса и приложения, чтобы в одной точке анализировать закономерности поведения пользователей и улучшать клиентский опыт.
На следующем уроке мы немного подробнее разберём экономические преимущества использования serverless-подхода при разработке и размещении приложений в облаке.
Task:
Сервис Yandex Cloud Functions относится к модели:
Decision:
+FaaS
-PaaS
-SaaS
Task:
Как определить, сколько серверов и какой мощности нужно арендовать под сервер БД для моего приложения?
Decision:
+Это не нужно делать, для создания serverless БД Yandex Database нужно только указать её имя.
-Нужно взять прогнозный профиль нагрузки, замерить максимальную производительность кластера из трёх серверов в разных зонах доступности на этом профиле, аппроксимировать результаты, заложить запас на непредвиденную нагрузку и постоянно отслеживать утилизацию серверов, чтобы не упустить момент, когда приложение окажется перегруженным. Также обязательно нужно отслеживать маркетинговые акции и готовиться к ним, так как они могут приводить к резким пикам нагрузки.
-У меня маленькое приложение, я ничего не буду считать и возьму один самый маленький сервер. Потом я напишу ещё одно маленькое приложение и создам вторую БД на том же сервере. Ведь сколько бы я ни сделал приложений, ни одно из них никогда не станет популярным и не создаст сколько-нибудь значимой нагрузки.
Task:
Тарификация Serverless
Decision:
На предыдущих уроках мы уже несколько раз упомянули, что у serverless-экосистемы свои особенности тарификации. Давайте разберёмся в принципах модели оплаты и экономических преимуществах такого подхода. Все serverless-сервисы объединяет следующий принцип: вместо почасовой или помесячной оплаты за использование ресурсов (например, vCPU, диска и памяти для ВМ) вы платите за исполнение ваших запросов к сервису.
Для каждого serverless-сервиса существуют свои правила, по которым определяется стоимость исполненного запроса. Они учитывают характеристики запроса, условия его исполнения или связанные с его обработкой показатели. Данные о том, какие ресурсы и сколько их на самом деле потрачено, остаются внутренней кухней провайдера. Хотя, конечно, стоимость запроса напрямую связана с фактическими потребностями в ресурсах для его исполнения.
Плата за исполнение запросов вместо платы за ресурсы принципиально меняет ваши отношения с сервисом. В такой модели у вас нет рисков, связанных с недостаточным использованием или недостатком ресурсов, из-за чего вам пришлось бы переплачивать провайдеру. А это значит, что теперь вам не нужно самостоятельно поддерживать конфигурацию ресурсов под сервисом в соответствии с нагрузкой на ваши приложения. Поэтому нет необходимости выделять время на изучение устройства сервиса и обслуживание приобретённых мощностей. Аналогично serverless-сервис решает все вопросы надёжности и доступности. Вам не надо настраивать разнообразные репликации данных, переключение между мастерами и репликами, выполнять апгрейд узлов, изучать, почему не применились логи или события передались в неправильном порядке.
Умножьте сэкономленное время на количество сервисов, которые нужны типичному приложению, и вы получите существенный выигрыш в сравнении с использовании «классических» сервисов. Освободившееся время вы можете вложить в развитие приложения, улучшив показатель time-to-market.
Ещё одна особенность serverless-сервисов — взаимная интеграция, упрощающая разработку решений. Например, API Gateway в курсе, что бывают не просто HTTP-эндпоинты, а Cloud Functions и сам решит внутри вопросы локализации трафика в пределах зон доступности, обеспечив для вашего приложения наименьшие задержки без какого-либо специального конфигурирования.
Кроме того, в Yandex Cloud есть так называемый Free tier — каждый пользователь может бесплатно использовать сервис в определённых пределах. Это позволяет попробовать serverless-экосистему в деле и даже успешно эксплуатировать в облаке приложения с небольшой нагрузкой. Также Free tier позволяет свободно экспериментировать, развёртывая в облаке приложения, в которых вы не уверены. А если приложение окажется успешным, нагрузка на него вырастет, и бесплатный предел будет превышен, всё продолжит работать. Тарификация каждого serverless-сервиса определяется его спецификой. Подробнее на примерах мы разберём это в последующих уроках.
Вводная тема на этом завершена. После короткого теста переходим к изучению флагманского serverless-сервиса в Yandex Cloud — Yandex Cloud Functions.
Task:
Что такое Serverless?
Decision:
-Нет серверов
-Сервер находится в Yandex Cloud
+Клиент сервиса не занимается конфигурацией, администрированием или управлением ресурсами сервиса
Task:
В чём основное отличие модели тарификации между классическим и serverless сервисами?
Decision:
-Serverless дороже
-Serverless дешевле
-При использовании serverless оплата зависит от фактического использования сервиса в интересах клиента, а классические сервисы оплачиваются по часам за выделенные клиенту ресурсы
Task:
Что такое функции?
Decision:
Давайте рассмотрим гипотетический пример. У вас есть рассылка для клиентов, и в каждом письме содержится ссылка для желающих отписаться. Пользователь, который больше не хочет читать вашу рассылку, переходит по этой ссылке, и его адрес убирают из списка получателей.
При классическом подходе к решению этой задачи вам нужно поднять ВМ, настроить на ней сервер, принимающий запросы, а также развернуть и настроить весь стек, на котором работает приложение, обрабатывающее запросы на отписку. Помимо этого, требуется обеспечить отказоустойчивость приложения: добавить реплики, развернуть балансировщик нагрузки, а также настроить сбор метрик, логов и настроить уведомления в случае нештатных ситуаций. При этом вы будете платить провайдеру за постоянно работающие ВМ, а также системному администратору за обслуживание всей этой инфраструктуры.
Вместо этого вы можете создать так называемую serverless-функцию. Это программа, которая предназначена для решения какой-то одной простой задачи. Код этой программы упаковывается в изолированный контейнер и выполняется мощностями дата-центра облачной платформы.
В вашем случае этот код будет обрабатывать HTTP-запрос, делать обращение к базе данных и удалять адрес пользователя из получателей рассылки. Платить вы будете только за выполнение функции и обращение к базе данных.
Решения класса FaaS (Function-as-a-Service) есть у разных поставщиков облачных технологий, например Cloud Functions в Google Cloud Platform, Azure Functions у Microsoft, AWS Lambda у Amazon. В Yandex Cloud этот сервис называется Cloud Functions. Давайте сначала разберёмся с возможностями и особенностями его реализации в Yandex Cloud.
Особенности реализации serverless functions в Yandex Cloud
Yandex Cloud Functions в целом следует общей концепции serverless functions. Для запуска кода облачных (бессерверных) функций используются специально созданные виртуальные машины, на которых установлены рантаймы, то есть среды выполнения кода на том или ином языке программирования, например Python версии 3.9. При этом для удобства пользователя работа с ними скрыта за абстракцией, позволяющей сосредоточиться на решаемой задаче.
Однако сервис Cloud Functions умеет не только это. Он также:
    Ведёт журнал выполнения функций. Сюда выводятся параметры и время запуска функции, время её выполнения и результат выполнения. У пользователя также есть возможность выводить в журнал собственную информацию при выполнении функции. Это удобно как при создании и отладке функций, так и при разборе нештатных ситуаций. Например, данные, которые вам нужно было записать в базу данных, в итоге туда не попали. Почему? Не вызвалась функция или была недоступна база данных? В коде функции была ошибка? Журнал поможет найти ответы на эти вопросы.
    Интегрируется с другими сервисами Yandex Cloud. Для этого он использует триггер — механизм, который позволяет запускать функцию по событиям в других сервисах. Например, вы можете вызывать функцию на каждый пакет данных от ваших IoT-устройств для их обработки.
    Предоставляет SDK для разработчиков. Это программные модули, которые можно подключить и вызвать из пользовательских функций. Они упрощают взаимодействие с другими сервисами Yandex Cloud из кода программы. Например, вы можете использовать SDK для сохранения данных в Yandex Object Storage.
Ограничения
При этом у сервиса есть два концептуальных ограничения:
    Функции должны быть конечными. Они не могут бесконечно выполнять какую-то задачу. Это ограничение заложено в суть самого сервиса, есть даже специальный таймер, по истечении которого функция будет автоматически завершена.
    Функции не могут сами по себе хранить состояния. Cloud Functions — среда выполнения, в которой не сохраняется состояние. Каждая запущенная копия (instance) функции отвечает только за один запрос одновременно, при этом в случае наличия большого количества запросов, экземпляр может последовательно обработать большое количество запросов. Конкурирующие запросы обрабатываются разными копиями функции, эти копии не могут обмениваться переменными или локальной памятью. Если вам нужно, чтобы функция сохраняла состояние, данные нужно писать в файл в бакете Object Storage или в БД, такую как Yandex Database.
Тарификация
При использовании этого сервиса вы будете платить за количество вызовов функции, за используемые для вызова выделенные вычислительные ресурсы, а также за исходящий трафик. Под вычислительными ресурсами подразумевается объём памяти, указанный при создании версии функции, а также время выполнения для каждого вызова функции.
Давайте рассмотрим следующий пример. Предположим, ваше приложение получает ровно один запрос в секунду с равномерной частотой в течение дня, что составит 2 678 400 за месяц в 31 день. Какова будет ежемесячная стоимость обработки этих запросов?
Предположим, обработка запроса занимает 200 мс, на её выполнение требуется 256 МБ памяти. Исходящего трафика нет. Стоимость вычислительных ресурсов для функции (ГБ×час) составляет 3,42 рубля*. Это означает, что все единицы измерения нужно привести к гигабайтам и часам. Стоимость вызовов функции составляет 10 рублей за 1 000 000 вызовов. Получаем следующую формулу:
3,42×(256/1024)×(200/1000/3600)×2678400+10×(2678400/1000000)= 1543,42 × (256 / 1024) × (200 / 1000 / 3600) × 2 678 400 + 10 × (2 678 400 / 1 000 000) = ~1543,42×(256/1024)×(200/1000/3600)×2678400+10×(2678400/1000000)= 154 ₽ в месяц
* Тарифы, использованные в этом и следующих примерах расчёта стоимости, могут отличаться от актуальных
Однако первый миллион вызовов функции в месяц не тарифицируется, равно как и первые 10 ГБ×час  в месяц, израсходованные на выполнение функций. Скорректируем вычисления:
3,42×(((256/1024)×(200/1000/3600)×2678400)−10)+10×((2678400–1000000)/1000000)= 109,803,42 × (((256 / 1024) × (200 / 1000 / 3600) × 2 678 400) - 10) + 10 × ((2 678 400 – 1 000 000) / 1 000 000) = ~109,803,42×(((256/1024)×(200/1000/3600)×2678400)−10)+10×((2678400–1000000)/1000000)= 109,80 ₽ в месяц
Теперь давайте рассчитаем стоимость аренды виртуальной машины. Для запуска приложения нам подойдет ВМ на базе любого дистрибутива Linux с дополнительно установленным рантаймом. Эта ВМ должна быть постоянно доступна, а значит, она не может быть прерываемой. Кроме того, она должна работать на 100% vCPU. Минимальная стоимость такой машины составит 1 360,83 ₽ в месяц.
Обратите внимание: эти расходы придётся увеличить, поскольку вам также нужны сетевой балансировщик (его использование тарифицируется) и ещё одна ВМ для перехвата трафика.
Более подробную информацию о тарификации Cloud Functions вы найдёте в документации.
На следующем уроке поговорим о том, как добавлять функцию в Cloud Functions и приводить её код к модели программирования.
Task:
Что такое serverless functions?
Decision:
-Функция на Python, которая запущена на виртуальной машине. Сервера-то нет, всё в облаке.
-Концепция облачных вычислений, при которой провайдер управляет и настраивает виртуальные машины, операционные системы и среды исполнения кода.
-Скрипт на Bash.
-Terraform-скрипт, разворачивающий виртуальную машину.
Task:
К какому типу сервисов относятся бессерверные вычисления?
Decision:
-IaaS
+FaaS
-SaaS
-ГЛОНАСС 
Task:
Какие ограничения есть у serverless-функций?
Decision:
+Функции не хранят своего состояния
-Функции не работают с базами данных
-Функции не могут работать больше 1 секунды
-Функции не могут работать с другими сервисами
+Функция не может быть бесконечной
Task:
Практическая работа. Создаём вашу первую функцию
Decision:
Мы уже достаточно сказали о том, что создавать облачные функции — просто. Давайте сделаем это на практике.
Как добавить код функции
    На главной странице консоли управления в списке сервисов выберите Cloud Functions:
На открывшейся странице нажмите кнопку Создать функцию:
Укажите имя функции, введите короткое описание того, что она будет делать, и нажмите кнопку Создать:
Затем выберите среду выполнения кода и нажмите кнопку Продолжить:
По умолчанию сервис предлагает создать Hello World — файл с примером кода на выбранном языке программирования. Этот файл будет создан и автоматически загружен в контейнер. В поле Способ укажите Редактор кода и выберите файл index.go.
По умолчанию сервис предлагает работать с редактором кода прямо в веб-интерфейсе (как на скриншоте выше). Однако вместо этого вы можете загрузить файл с кодом из бакета Object Storage (этот способ подойдёт для файлов больше 3,5 МБ) или загрузить ZIP-архив с кодом с локальной машины. Переключатель способа добавления кода находится прямо над окном редактора.
Код вашей функции может находиться как в одном файле, так и в нескольких. Вы также можете создавать папки. При этом обязательно нужно указывать точку входа — часть кода, которая будет вызываться первой и принимать параметры вызова. Формат точки входа — <имя файла с функцией>.<имя обработчика вызова>. Например, index.Handler.
Вверху справа нажмите кнопку Создать версию, чтобы сохранить текущее состояние функции.
Сервис создаст версию функции и покажет справочную страницу о ней.
Как протестировать созданную функцию
    Теперь в панели слева перейдите на вкладку Тестирование. В поле Шаблон данных выберите HTTPS-вызов. Сервис автоматически сгенерирует входные данные в формате JSON.
Под полем с входными данными нажмите кнопку Запустить тест. Сервис выполнит HTTPS-вызов созданной функции и сформирует ответ (также в формате JSON).
Task:
Практическая работа. Запускаем функцию с помощью CLI
Decision:
В предыдущей практической работе вы познакомились с созданием функции через консоль управления. На этом уроке вы научитесь создавать функцию с помощью интерфейса командной строки (утилиты yc).
Пользоваться консолью управления бывает очень удобно, но вести большой проект всё же лучше локально, с помощью среды разработки. Артефакты локальной разработки можно с лёгкостью переносить в облако с помощью консольных утилит. Выполняя последовательно шаги, вы изучите основные команды для создания функций в облаке.
Шаг 1. Создание сервисного аккаунта
Создание аккаунта
Для начала убедитесь, что у вас установлена и инициализирована утилита yc.
У вас уже есть сервисные аккаунты, созданные на предыдущих занятиях. Однако гораздо лучше, когда для каждой конкретной задачи (или блока задач) вы заводите отдельный сервисный аккаунт. Это обеспечивает прозрачность в управлении доступом и контроле за ролями в сервисах.
Предварительно установите утилиту jq, она потребуется для выполнения задания:
sudo apt install jq 
Создайте сервисный аккаунт с именем service-account-for-cf:
export SERVICE_ACCOUNT=$(yc iam service-account create \
  --name service-account-for-cf \
  --description "service account for cloud functions" \
  --format json | jq -r .) 
Проверьте текущий список сервисных аккаунтов:
yc iam service-account list
echo $SERVICE_ACCOUNT 
После проверки запишите идентификатор (ID) созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
echo "export SERVICE_ACCOUNT_ID=<идентификатор_сервисного_аккаунта>" >> ~/.bashrc && . ~/.bashrc
echo $SERVICE_ACCOUNT_ID 
Назначение роли сервисному аккаунту
Добавьте вновь созданному сервисному аккаунту роль editor:
echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc 
echo $FOLDER_ID
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_ID \
  --role editor 
Не удаляйте файл ~/.bashrc после прохождения практической работы, он понадобится нам в дальнейшем.
Шаг 2. Создание и настройка функции
Создание функции
Создайте функцию с именем my-first-function:
yc serverless function create --name my-first-function 
Вы получите URL, по которому можно будет сделать вызов функции http_invoke_url. По умолчанию функция будет непубличной.
Загрузка кода функции
Создайте файл  index.py :
sudo nano index.py 
Добавьте в index.py следующее содержимое:
def handler(event, context):
    return {
        'statusCode': 200,
        'body': 'Hello World!',
    } 
Успешное выполнение этой функции вернёт небольшую веб-страницу.
Загрузите код функции в облако и создайте её версию. Для этого перейдите в папку с файлом index.py и выполните команду:
yc serverless function version create \
    --function-name my-first-function \
    --memory 256m \
    --execution-timeout 5s \
    --runtime python37 \
    --entrypoint index.handler \
    --service-account-id $SERVICE_ACCOUNT_ID \
    --source-path index.py 
Успешное выполнение команды приведёт к созданию версии функции. С помощью консоли управления убедитесь, что версия создана.
Вызов функции
Получите список функций, а затем — информацию о функции my-first-function:
yc serverless function list
yc serverless function version list --function-name my-first-function 
В результате вызова последней команды из столбца FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
yc serverless function invoke <идентификатор_функции> 
По умолчанию функция создаётся непубличной. Чтобы сделать функцию my-first-function публичной, выполните следующую команду:
yc serverless function allow-unauthenticated-invoke my-first-function 
После этого вы сможете вызвать её в браузере. Получите параметр http_invoke_url для функции my-first-function:
yc serverless function get my-first-function 
Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции.
На следующем уроке мы разберёмся, как в работе помогают триггеры, ведение журнала и мониторинг выполнения функций.
Decision:

Task:
Как готовить функции к запуску и управлять ими
Decision:
Логика работы с функциями напоминает работу с контейнерами. Только что созданная функция — набор данных, который позволяет её распознать по уникальному идентификатору, имени и описанию. Этот набор также содержит тип среды выполнения, переменные окружения и параметры вызова кода. При этом кода, который будет запускаться в только что созданной функции, нет. В этом состоит сходство с контейнерами. Функция — это своеобразный контейнер, внутри которого будет выполняться код.
Как привести код к модели программирования
Хотя каждая среда выполнения предъявляет свои требования к коду, существуют единая модель программирования. Она является общей для всех языков и определяет интерфейс между вашим кодом и средой выполнения. Вот что это означает на практике.
Функция представляет собой логический контейнер с набором параметров. Внутри контейнера вызывают версию пользовательского кода, которая обязательно содержит метод. Его нужно указать в точке входа функции. Обработчик принимает поступающие на вход данные и вызывает необходимые функции и методы в коде. Он должен соответствовать требованиям модели программирования функций и уметь принимать данные и контекст вызова.
Контекст используют для взаимодействия программного кода функции с сервисом Yandex Cloud Functions. Например, с помощью контекста узнают время до завершения выполнения Cloud Functions.
Ещё одно важное понятие модели программирования — среда выполнения, т. е. логическая инфраструктура, которая содержит необходимое окружение для запуска пользовательского кода. Она соответствует выбранному языку программирования. Сервис позволяет использовать такие среды выполнения, как Python, Node.js или Java (с полным списком вы можете ознакомиться в документации).
Все поступающие запросы функция обрабатывает последовательно. Если запросы поступают быстрее, чем одна функция успевает их обрабатывать, сервис запускает дополнительные экземпляры этой функции.
Наконец, чтобы сообщить сервису об ошибке при выполнении функции, обрабатывайте ошибки с помощью исключений.
Версионирование и теги
Даже очень хорошо отлаженный код время от времени нуждается в обновлении. В Cloud Functions реализовано простое управление версиями исходного кода. Любое текущее состояние кода вы можете сохранить как новую версию. Для этого после внесения изменений справа вверху на странице редактора нажмите кнопку Создать версию.
Вы также можете создать новую версию с помощью CLI или через API. Вот шаблон команды для CLI:
yc serverless function version create \
  --function-id <идентификатор_функции> \
  --runtime <среда_выполнения> \
  --entrypoint <точка_входа> \
  --memory <объём_выделяемой_памяти,_МБ> \
  --execution-timeout <таймаут,_с> \
  --source-path <путь_к_ZIP-архиву_c_кодом_функции> 
Для каждой версии сохраняется свой уникальный идентификатор.
Есть несколько распространённых сценариев использования версий. Например, сложная микросервисная система может использовать разные версии одной и той же функции в разных сценариях. Скажем, более новая версия функции используется ради новых возможностей, а старая поддерживается для обратной совместимости с другими модулями системы, которые пока не обновлены.
И, конечно, если в новой версии кода оказались ошибки, вы всегда можете оперативно перенастроить триггер на выполнение более старой, проверенной версии исходников функции. Для этого используется механизм тегов, позволяющий вызывать определённую версию функции.
Вот как это работает. Последняя версия функции при создании автоматически получает специальный тег $latest, а с предыдущей версии функции этот тег автоматически снимается.
Допустим, вы создали новую функцию, внесли изменения и сохранили вторую версию кода, которая получила тег $latest. В этом случае у первой версии кода не остаётся никакого тега, и вы не можете обращаться именно к этой версии. Чтобы назначить ей тег, вы можете воспользоваться CLI или публичным API. Шаблон команды для CLI выглядит следующим образом:
yc serverless function version set-tag \
  --id <идентификатор_версии> \
  --tag <тег> 
Чтобы подставить в команду правильный идентификатор версии, со страницы со списком всех функций перейдите на страницу с обзором нужной функции.
В нижней части страницы обзора перечислены все версии функции. Вам нужна самая ранняя, она же самая нижняя в списке.
Скопируйте в буфер идентификатор этой версии и подставьте его в шаблон команды. Затем придумайте «говорящее» имя тега. При выборе имени руководствуйтесь простым принципом: если управление функциями в консоли передали новому сотруднику, он должен суметь сам без труда разобраться, что есть что. Допустим, самую первую версию вы решили так и назвать: first. Тогда окончательная команда для CLI будет выглядеть так:
yc serverless function version set-tag \
  --id <идентификатор_версии> \
  --tag first 
Внимание: имена тегов всегда начинаются со строчной латинской буквы, после которой можно использовать арабские цифры и некоторые спецсимволы, такие как дефис и подчёркивание. Если при именовании тега вы сделаете ошибку, CLI выведет справку в формате регулярного выражения.
После успешного назначения тега CLI выведет информацию о версии:
id: d4e5b6pm6pgspn6qff47
function_id: d4echmm41b3hd8hae1hs
created_at: "2022-08-05T11:58:28.838Z"
runtime: golang117
entrypoint: index.Handler
resources:
  memory: "134217728"
execution_timeout: 3s
image_size: "884736"
status: ACTIVE
tags:
  - first
log_group_id: ckgkat886nqg8qa541l4 
Вы можете проверить успешность назначения тега версии, обновив страницу с информацией о функции:
Поскольку в сложных системах у версий кода может быть разная семантика, вы можете назначить одной и той же версии больше одного тега. Для этого просто добавьте ещё один тег по тому же идентификатору, например вот так:
yc serverless function version set-tag \
  --id <идентификатор_первой_версии> \
  --tag v_1_0_0 
А второй версии кода тоже добавьте ещё один тег с инкрементированной версией:
yc serverless function version set-tag \
  --id <идентификатор_второй_версии> \
  --tag v_1_0_1 
Результат:
Теперь при выборе версии кода вы можете ориентироваться как по номеру релиза, так и по «говорящему» тегу.
Чтобы удалить тег, используйте аргумент remove-tag, например:
yc serverless function version remove-tag \
  --id <идентификатор_версии> \
  --tag first 
Работа с функцией в консоли управления
Инструменты работы с функциями разнесены по разным вкладкам боковой панели слева. Давайте коротко пройдёмся по ним.
    Обзор. Здесь находится сводка самой важной информации о функции: идентификатор, статус, имя, описание, ссылка для вызова, список версий и т.д.
    Редактор. Вы можете поменять среду выполнения, внести правки в исходный код, указать переменные окружения, изменить объём памяти, выделяемой для выполнения функции, и т.д.
    Тестирование. Здесь вы выбираете версию кода по тегу, готовый шаблон данных или свой собственный — со структурой запроса в формате JSON — и смотрите ответ, который вам возвращает функция.
    Мониторинг. Вы можете отслеживать такие показатели, как количество запросов, время задержки откликов на запросы и количество ошибок.
    Логи. Здесь вы можете посмотреть подробный журнал о выполнении функции. Он помогает находить ошибки в коде.
    Операции. На этой странице перечислены все операции с функцией, такие как обновление кода, назначение и удаление тегов и т.д.
На следующем уроке мы расскажем об инструментах для автоматизации запуска и отладки функций, а затем — в оставшихся уроках этой темы — займёмся практикой.
Task:
На что похожа функция в терминологии Yandex Cloud Functions?
Decision:
-На виртуальную машину
-На объект в хранилище
+На контейнер
-На очередь
Task:
Какими способами можно добавить код функции (укажите все подходящие)?
Decision:
+В редакторе кода консоли управления
+Загрузив ZIP-архив
-Добавив ссылки на файлы
+Указав объект в Yandex Cloud Storage
Task:
Как можно вызвать функцию сервиса Yandex Cloud Functions?
Decision:
+Используя HTTPS-запрос
-Через WebSocket
-C помощью TLS
-Используя ABS
Task:
Какой среды исполнения НЕТ в сервисе Yandex Cloud Functions?
Decision:
-Go
-Python
-Bash
+C++
Task:
Из каких параметров состоит функция в Yandex Cloud Functions?
Decision:
+Имя
+Теги
-Адреса воркеров
+Среда выполнения
-Почты аккаунтов с доступом на редактирование
+Описание
Task:
Указать точку входа — обязательное требование для создания функции. Какие из имён точек входа соответствуют требованиям?
Decision:
-yacloud
+alice-skill.myFunction
+alice_skill.myFunction
-162.198
-alice-skill, myFunction
-alice_skill_myFunction
Task:
Триггеры, логирование, мониторинг
Decision:
Итак, вы запустили свою первую облачную функцию. Теперь давайте поговорим об инструментах, которые помогут автоматизировать запуск функций и отладить их работу. Это триггеры, логирование и мониторинг.
Триггеры
Допустим, вы написали функцию, которая обращается к базе данных и считывает из неё напоминания. Если время напоминания пришло, то функция отправляет его в Telegram, скажем, каждые 5 минут.
Сами по себе функции имеют только один механизм вызова — HTTPS-запрос. Реализовать механизм автоматического формирования и отправки такого запроса непросто. К тому же, это противоречит концепции serverless, в которой пользователь не должен беспокоиться о вещах, не связанных с его вычислительной задачей. Поэтому для Cloud Functions были разработаны триггеры.
В терминологии Yandex Cloud триггером является некое условие, при выполнении которого автоматически запускается определённая функция. Общая схема работы выглядит следующим образом:
Как видите, триггеры позволяют автоматизировать работу с другими сервисами Yandex Cloud и вызывают функцию:
    Message Queue — при появлении нового сообщения для обработки.
    IoT Core — при получении посылки с данными от устройства.
    Object Storage — при создании, редактировании ACL или удалении объекта.
    Container Registry — при создании или удалении Docker-образа или тега Docker-образа.
Таймер позволяет запускать функции с интервалом, который задаётся при помощи cron-выражений. Таймер может быть полезен для периодической проверки доступности вашего сайта или для создания резервной копии данных по расписанию. Справку по использованию cron-выражений вы найдёте в документации.
Подробно создание триггеров мы разберём в следующем уроке.
Логирование
Если при выполнении функции возникли неполадки, вы можете зайти в журнал этой функции, увидеть детали выполнения и найти причину нештатного поведения. Этот журнал доступен в разделе Логи. В нём сохраняются все данные о запуске функции, времени её выполнения и потреблённых ресурсах.
Кроме того, вы можете писать в журнал собственные сообщения во время выполнения функции и использовать стандартные конструкции языка программирования. Подробнее про использование журнала в каждой из сред выполнения можно узнать из документации к сервису.
На одном из следующих практических занятий вы научитесь читать журнал для отладки функции.
Мониторинг
Статистика по работе функции наглядно представлена в разделе Мониторинг. Здесь вы можете увидеть нагрузку на сервис и количество ошибок, если они есть.
Всего вам доступно два вида мониторинга работы сервиса. Первый — встроенный в пользовательский интерфейс консоли управления в разделе Мониторинг:
Здесь доступны метрики:
    Количество вызовов функций в минуту — график Requests, [rps];
    Задержка выполнения функции — график Request latency, [ms];
    Количество ошибок, возникших при выполнении функции — график Errors, [count];
    Количество одновременно выполняющихся вызовов функции — график Inflight, [pcs];
    Объем памяти, потребляемой функцией — график Used memory, [MB];
    Количество инициализаций функции в секунду — график Initializations, [pcs/s].
Второй вариант — сервис Yandex Monitoring, где можно отслеживать все перечисленные выше метрики и их представление, а также настроить автоматическое оповещение (например при возникновении ошибок при выполнении функции).
Task:
Зачем нужны триггеры в сервисе Yandex Cloud Functions?
Decision:
-Чтобы функция могла писать данные в базу данных
-Чтобы оповестить пользователя о том, что функция была вызвана
+Чтобы автоматически вызывать функцию по тем или иным событиям
-Чтобы упростить авторизацию
Task:
Какие метрики можно отслеживать в мониторинге сервиса?
Decision:
-Duration — среднее время выполнения функции
-Workers — количество работающих воркеров
-Invocations — количество вызовов функций
-Functions — количество функций
-Load — нагрузка на сервис
+Errors — количество ошибок при выполнении
Task:
Практическая работа. Создание триггера от Object Storage
Decision:
В предыдущем практическом уроке вы познакомились с созданием одной функции с помощью интерфейса командной строки (yc). В этом уроке мы продолжим разработку этой функции: модифицируем её содержание, добавим переменные окружения и т.д.
Важно выполнить предыдущий практический урок, так как вы будете опираться на знания и результаты, полученные в нём.
Шаг 1. Модификация сервисного аккаунта
Добавление роли сервисному аккаунту
По итогам прохождения предыдущей практической работы у вас есть сервисный аккаунт с именем service-account-for-cf. Для работы с Object Storage добавьте этому сервисному аккаунту роль storage.editor:
yc resource-manager folder add-access-binding $FOLDER_ID \
    --role storage.editor \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID 
Создание ключа доступа для сервисного аккаунта
Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, а также в том случае, если на следующем шаге для создания бакета в Object Storage вы планируете использовать Terraform.
Для создания ключа доступа необходимо вызвать следующую команду:
yc iam access-key create --service-account-name service-account-for-cf 
В результате вы получите примерно следующее:
access_key:
    id: ajefraollq5puj2tir1o
    service_account_id: ajetdv28pl0a1a8r41f0
    created_at: "2021-08-23T21:13:05.677319393Z"
    key_id: BTPNvWthv0ZX2xVmlPIU
secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
Где:
    key_id — идентификатор ключа доступа, ACCESS_KEY.
    secret — секретный ключ, SECRET_KEY.
Переменные ACCESS_KEY и SECRET_KEY будут использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3 на следующих этапах.
Шаг 2. Object Storage
Самый простой способ создания бакета в Object Storage — через консоль управления. Более сложный, позволяющий автоматизировать разработку, — использование Terraform. Вы можете выбрать любой из них.
Способ 1. Консоль управления
В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
На странице создания бакета:
    Введите имя бакета, пусть это будет bucket-for-trigger.
    При необходимости ограничьте максимальный размер бакета, установив значение, например, 1 ГБ.
    Выберите тип доступа, в нашем уроке установим значения в Публичный во всех случаях.
    Выберите класс хранилища, по умолчанию используется Стандартное.
Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
Способ 2. Terraform
Прежде всего необходимо получить OAuth-токен для работы с Yandex Cloud. Для этого можно сделать запрос к сервису Яндекс.OAuth. Подробнее прочитать можно в документации.
Сохраните OAuth-токен в переменную OAuth, но никому не передавайте. Также вам потребуются значения переменных: идентификатор облака — CLOUD_ID и идентификатор каталога FOLDER_ID (сохранен в переменную ранее).
Также на предыдущем шаге вы получили ключ доступа для сервисного аккаунта. Нам потребуется идентификатор ключа доступа ACCESS_KEY и секретный ключ SECRET_KEY.
В файл main.tf, представленный далее, внесём все собранные переменные. Важно: переменная BUCKET_NAME содержит имя создаваемого бакета в Object Storage, куда будем загружать файлы. Допустим, переменная будет равна bucket-for-trigger. Сохраним все значения:
terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"
}
provider "yandex" {
  token     = "<OAuth>"
  cloud_id  = "<CLOUD_ID>"
  folder_id = "<FOLDER_ID>"
}
resource "yandex_storage_bucket" "bucket" {
  access_key = "<ACCESS_KEY>"
  secret_key = "<SECRET_KEY>"
  bucket = "<BUCKET_NAME>"
} 
После внесения правок, находясь в каталоге с файлом main.tf, последовательно выполните следующие команды:
terraform init
terraform plan
terraform apply 
Успешное выполнение команд приведёт к созданию бакета bucket-for-trigger в объектном хранилище в вашем рабочем каталоге.
Шаг 3. Модификация функции
В предыдущей практической работе мы создали функцию с именем my-first-function с помощью следующей команды:
yc serverless function create --name my-first-function 
При создании функции вы получили URL, по которому можно будет сделать вызов функции http_invoke_url.
Загрузка кода новой версии
Новая версия функции имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо файлы index.py и requirements.txt заархивировать и получить файл my-first-function.zip.
Новая версия index.py:
import os
import datetime
import boto3
import pytz
ACCESS_KEY = os.getenv("ACCESS_KEY")
SECRET_KEY = os.getenv("SECRET_KEY")
BUCKET_NAME = os.getenv("BUCKET_NAME")
TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
TEMP_FILENAME = "/tmp/temp_file"
TEXT_FOR_TEMP_FILE = "This is text file"
def write_temp_file():
    temp_file = open(TEMP_FILENAME, 'w')
    temp_file.write(TEXT_FOR_TEMP_FILE)
    temp_file.close()
    print("\U0001f680 Temp file is written")
def get_now_datetime_str():
    now = datetime.datetime.now(pytz.timezone(TIME_ZONE))    
    return now.strftime('%Y-%m-%d__%H-%M-%S')
def get_s3_instance():
    session = boto3.session.Session()
    return session.client(
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
        service_name='s3',
        endpoint_url='https://storage.yandexcloud.net'
    )
def upload_dump_to_s3():
    print("\U0001F4C2 Starting upload to Object Storage")
    get_s3_instance().upload_file(
        Filename=TEMP_FILENAME,
        Bucket=BUCKET_NAME,
        Key=f'file-{get_now_datetime_str()}.txt'
    )
    print("\U0001f680 Uploaded")
def remove_temp_files():
    os.remove(TEMP_FILENAME)
    print("\U0001F44D That's all!")
def handler(event, context):
    write_temp_file()
    upload_dump_to_s3()
    remove_temp_files()
    return {
        'statusCode': 200,
        'body': 'File is uploaded',
    } 
Первая версия requirements.txt:
boto3==1.13.10
botocore==1.16.10
python-dateutil==2.8.1
pytz==2020.1 
Находясь в каталоге с файлом my-first-function.zip вызовите следующую команду, это позволит вам загрузить код функции в облако и создать её версию:
yc serverless function version create \
  --function-name my-first-function \
  --memory 256m \
  --execution-timeout 5s \
  --runtime python37 \
  --entrypoint index.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-path my-first-function.zip 
Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой версии необходимо подготовить несколько переменных. Переменные ACCESS_KEY и SECRET_KEY вы получили на первом шаге, а значение BUCKET_NAME на втором:
echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
Определим идентификатор (ID) для последней загруженной версии функции:
yc serverless function version list --function-name my-first-function 
Создадим новую версию функции, задав при этом переменные окружения. Для этого выставим значение параметра source-version-id равное полученному ID в следующей команде:
yc serverless function version create \
  --function-name my-first-function \
  --memory 256m \
  --execution-timeout 5s \
  --runtime python37 \
  --entrypoint index.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-version-id <ID> \
  --environment ACCESS_KEY=$ACCESS_KEY \
  --environment SECRET_KEY=$SECRET_KEY \
  --environment BUCKET_NAME=$BUCKET_NAME 
Успешное выполнение команды приведёт к созданию версии функции.
Вызов функции
Получите список функций и информацию о функции my-first-function:
yc serverless function list
yc serverless function version list --function-name my-first-function 
В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью следующей команды:
yc serverless function invoke <идентификатор_функции> 
В предыдущей практической работе мы сделали функцию my-first-function публичной с помощью команды:
yc serverless function allow-unauthenticated-invoke my-first-function 
Теперь мы можем сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
yc serverless function get my-first-function 
Введите значение параметра http_invoke_url в браузере и наслаждайтесь вызовом вашей функции. Во время её вызова в Object Storage будет создан новый файл.
Шаг 4. Создание триггера
Создание функции
Для создания триггера нам необходима функция, которую триггер будет запускать. Аналогично предыдущему шагу создадим функцию my-trigger-function и её версию на основе файла index.py.
def handler(event, context):
    print("\U0001F4C2 Starting function after trigger")
    print(event)     
    return {
        'statusCode': 200,
        'body': 'File is uploaded',
    } 
Находясь в каталоге с файлом index.py, вызовите следующие команды:
yc serverless function create --name my-trigger-function
yc serverless function version create \
  --function-name my-trigger-function \
  --memory 256m \
  --execution-timeout 5s \
  --runtime python37 \
  --entrypoint index.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-path index.py
yc serverless function version list --function-name my-trigger-function 
Создание триггера
Чтобы создать триггер my-first-trigger, который вызывает функцию my-trigger-function при создании нового объекта в бакете BUCKET_NAME, выполните команду:
yc serverless trigger create object-storage \
  --name my-first-trigger \
  --bucket-id $BUCKET_NAME \
  --events 'create-object' \
  --invoke-function-name my-trigger-function \
  --invoke-function-service-account-id $SERVICE_ACCOUNT_ID 
Вызов цепочки событий
Чтобы запустить цепочку событий, вызовем первую функцию my-first-function. Получите список функций и информацию о функции my-first-function:
yc serverless function list
yc serverless function version list --function-name my-first-function 
В результате вызова последней команды в столбце FUNCTION ID вы узнаете идентификатор функции и сможете сделать вызов функции с помощью команды:
yc serverless function invoke <идентификатор_функции> 
После этого вы можете сделать её вызов в браузере. Получите параметр http_invoke_url для функции my-first-function
yc serverless function get my-first-function 
Введите значение параметра http_invoke_url в браузере. Во время вызова функции в Object Storage будет создан новый объект. Сразу после этого сработает триггер my-first-trigger, который вызовет функцию my-trigger-function. В итоге, наша вторая функция запишет в логи содержание переменной event. Убедиться в этом вы сможете как в UI, так и через CLI.
yc serverless function logs my-trigger-function 
На следующем практическом занятии мы создадим навык Алисы с помощью Cloud Functions.
Decision:
$ yc resource-manager folder add-access-binding $FOLDER_ID \
    --role storage.editor \
    --subject serviceAccount:$SERVICE_ACCOUNT_ID
$ yc iam access-key create --service-account-name service-account-for-cf
access_key:
  id: aje3no6b86uc0d91r7e0
  service_account_id: ajehljbngf4kdbkaf5aq
  created_at: "2022-10-18T11:59:47.221476943Z"
  key_id: YCAJEelqE-7H-C2l8n7dMrLUV
secret: YCMHsOHeLnCVuUwhMen01V2HkokHr0N2gehlSYBc
$ vim index.py
$ cat index.py
import os
import datetime
import boto3
import pytz
ACCESS_KEY = os.getenv("ACCESS_KEY")
SECRET_KEY = os.getenv("SECRET_KEY")
BUCKET_NAME = os.getenv("BUCKET_NAME")
TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
TEMP_FILENAME = "/tmp/temp_file"
TEXT_FOR_TEMP_FILE = "This is text file"
def write_temp_file():
    temp_file = open(TEMP_FILENAME, 'w')
    temp_file.write(TEXT_FOR_TEMP_FILE)
    temp_file.close()
    print("\U0001f680 Temp file is written")
def get_now_datetime_str():
    now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
    return now.strftime('%Y-%m-%d__%H-%M-%S')
def get_s3_instance():
    session = boto3.session.Session()
    return session.client(
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
        service_name='s3',
        endpoint_url='https://storage.yandexcloud.net'
    )
def upload_dump_to_s3():
    print("\U0001F4C2 Starting upload to Object Storage")
    get_s3_instance().upload_file(
        Filename=TEMP_FILENAME,
        Bucket=BUCKET_NAME,
        Key=f'file-{get_now_datetime_str()}.txt'
    )
    print("\U0001f680 Uploaded")
def remove_temp_files():
    os.remove(TEMP_FILENAME)
    print("\U0001F44D That's all!")
def handler(event, context):
    write_temp_file()
    upload_dump_to_s3()
    remove_temp_files()
    return {
        'statusCode': 200,
        'body': 'File is uploaded',
    }
$ vim requirements.txt
$ cat requirements.txt
boto3==1.13.10
botocore==1.16.10
python-dateutil==2.8.1
pytz==2020.1boto3==1.13.10
botocore==1.16.10
python-dateutil==2.8.1
pytz==2020.1
$ zip my-first-function.zip requirements.txt index.py
echo "export ACCESS_KEY=aje3no6b86uc0d91r7e0" >> ~/.bashrc && . ~/.bashrc
yc serverless function version create \
  --function-name my-first-function \
  --memory 256m \
  --execution-timeout 5s \
  --runtime python37 \
  --entrypoint index.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-version-id d4e456dkn2shsmiqdq3i \
  --environment ACCESS_KEY=$ACCESS_KEY \
  --environment SECRET_KEY=$SECRET_KEY \
  --environment BUCKET_NAME=$BUCKET_NAME

Не получилось!!!!!!!!

Task:
Практическая работа. Навык Алисы
Decision:
В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, добавили ему роли editor и storage.editor и создали ключ доступа.
Также вы создали бакет в Object Storage с именем bucket-for-trigger, триггер my-first-trigger для его обработки и вызываемую им функцию my-trigger-function.
Ещё была создана функция my-first-function, её использовали для того, чтобы запустить цепочку событий. Публичный вызов этой функции приводил к созданию нового объекта в бакете в Object Storage. Это запускало вызов триггера my-first-trigger, который стартовал функцию my-trigger-function. В итоге последняя функция записывала в логи содержание переменной event.
Если вы удалили бакет и сервисный аккаунт, необходимо вернуться к предыдущим урокам и повторить их создание.
Шаг 1. Создание функции
На предыдущем уроке мы создали функцию с именем my-first-function. Поменяем её исходный код так, чтобы обрабатывать запросы от Алисы.
На основе функции будет создан навык Попугай, который повторяет все, что ему написал или сказал пользователь.
Функция parrot
Создадим новую функцию с именем parrot с помощью команды:
yc serverless function create \
  --name parrot \
  --description "function for Alice" 
По умолчанию функция не является публичной.
Загрузка кода новой версии
Функция имеет зависимости, которые описаны в файле requirements.txt, а это значит, что для загрузки функции в облако необходимо заархивировать файлы parrot.py и requirements.txt и получить файл parrot.zip.
Содержание функции parrot.py:
import os
import datetime
import boto3
import pytz
ACCESS_KEY = os.getenv("ACCESS_KEY")
SECRET_KEY = os.getenv("SECRET_KEY")
BUCKET_NAME = os.getenv("BUCKET_NAME")
TIME_ZONE = os.getenv("TIME_ZONE", "Europe/Moscow")
TEMP_FILENAME = "/tmp/temp_file"
TEXT_FOR_TEMP_FILE = "This is text file"
def write_temp_file(text_for_s3):
    TEXT_FOR_TEMP_FILE = text_for_s3
    temp_file = open(TEMP_FILENAME, 'w')    
    temp_file.write(TEXT_FOR_TEMP_FILE)
    temp_file.close()
    print("\U0001f680 Temp file is written")
def get_now_datetime_str():
    now = datetime.datetime.now(pytz.timezone(TIME_ZONE))
    return now.strftime('%Y-%m-%d__%H-%M-%S')
def get_s3_instance():
    session = boto3.session.Session()
    return session.client(
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
        service_name='s3',
        endpoint_url='https://storage.yandexcloud.net'
    )
def upload_dump_to_s3():
    print("\U0001F4C2 Starting upload to Object Storage")
    get_s3_instance().upload_file(
        Filename=TEMP_FILENAME,
        Bucket=BUCKET_NAME,
        Key=f'file-{get_now_datetime_str()}.txt'
    )
    print("\U0001f680 Uploaded")
def remove_temp_files():
    os.remove(TEMP_FILENAME)
    print("\U0001F44D That's all!")
def handler(event, context):
    """
    Entry-point for Serverless Function.
    :param event: request payload.
    :param context: information about current execution context.
    :return: response to be serialized as JSON.
    """
    text = 'Hello! I\'ll repeat anything you say to me.'
    if 'request' in event and \
            'original_utterance' in event['request'] \
            and len(event['request']['original_utterance']) > 0:
        text = event['request']['original_utterance']
        write_temp_file(text)
        upload_dump_to_s3()
        remove_temp_files()
    return {
        'version': event['version'],
        'session': event['session'],
        'response': {
            # Respond with the original request or welcome the user if this is the beginning of the dialog and the request has not yet been made.
            'text': text,
            # Don't finish the session after this response.
            'end_session': 'false'
        },
    }
Содержание файла зависимостей requirements.txt:
boto3==1.13.10
botocore==1.16.10
python-dateutil==2.8.1
pytz==2020.1 
Находясь в каталоге с файлом parrot.zip, вызовите приведенную ниже команду. Это позволит вам загрузить код функции в облако и создать её версию:
yc serverless function version create \
  --function-name=parrot \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=parrot.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-path parrot.zip 
Шаг 2. Создание новой версии функции
Новая версия функции при вызове будет загружать в Object Storage новый файл. Для создания этой новой версии функции необходимы переменные.
Если переменные среды не сохранились, то в консоли управления можно посмотреть имя бакета, а ACCESS_KEY и SECRET_KEY скопировать из предыдущей функции my-first-function:
echo "export ACCESS_KEY=<ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
echo "export SECRET_KEY=<SECRET_KEY>" >> ~/.bashrc && . ~/.bashrc
echo "export BUCKET_NAME=bucket-for-trigger" >> ~/.bashrc && . ~/.bashrc 
Определим идентификатор (ID) последней загруженной версии функции:
yc serverless function version list --function-name parrot 
Создадим новую версию функции, задав переменные окружения. Для этого выставим значение параметра source-version-id равное полученному идентификатору версии функции (ID) в следующей команде:
yc serverless function version create \
  --function-name parrot \
  --memory 256m \
  --execution-timeout 5s \
  --runtime python37 \
  --entrypoint parrot.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --source-version-id <идентификатор_версии_функции> \
  --environment ACCESS_KEY=$ACCESS_KEY \
  --environment SECRET_KEY=$SECRET_KEY \
  --environment BUCKET_NAME=$BUCKET_NAME 
Успешное выполнение команды приведёт к созданию версии функции.
Шаг 3. Вызов функции и её тестирование
По умолчанию функция создаётся непубличной. Чтобы сделать функцию parrot публичной, вызовите следующую команду:
yc serverless function allow-unauthenticated-invoke parrot 
Протестируйте функцию parrot, чтобы проверить правильность кода перед созданием связки с Алисой. В консоли управления на странице сервиса Cloud Functions выберите созданную функцию и перейдите на вкладку Тестирование. В поле Шаблон данных данных укажите Навык Алисы и нажмите кнопку Запустить тест.
В блоке Результат тестирования убедитесь, что функция выполнена и приведен ответ.
Перейдите по ссылке https://dialogs.yandex.ru/developer/ и создайте новый диалог Алисы (подробности о создании навыков вы можете узнать из документации):
    Нажмите кнопку Создать диалог. Выберите тип диалога Навык в Алисе, у вас откроется форма на вкладке Настройки.
Заполните имя навыка, оно должно состоять минимум из двух слов, например My parrot.
В блоке Backend выберите вариант Функция в Яндекс.Облаке и в выпадающем списке выберите созданную вами функцию parrot.
В блоке Тип доступа в выпадающем списке выберите Приватный.
В блоке Публикация в каталоге выберите Примеры запросов, например Запусти навык - My parrot, Имя разработчика, Категорию, Описание и Иконку.
Нажмите кнопку Сохранить и перейдите на вкладку Тестирование.
Если вы сделали всё правильно, то на экране появится приветствие навыка. Далее навык будет повторять всё, что вы ему напишете. При этом фразы, которые вы отправите Алисе, будут сохраняться в новом файле в бакете. Вы можете это проверить в консоли управления.
Task:
Какой фразой вас приветствует навык?
Decision:
+Hello! I'll repeat anything you say to me.
-Привет! Я повторю всё, что вы мне скажете?
-Привет! Рада вас видеть! Чем я могу помочь?
-Hello! Glad to see you! How can I help you?
Task:
Практическая работа. Проверка доступности
Decision:
На этом практическом занятии вы создадите функцию для проверки доступности сайта yandex.ru, которая будет измерять время ответа. Результаты работы функции будут передаваться в базу данных сервиса Yandex Managed Service for PostgreSQL с использованием подключения к управляемой БД из функции. Также вы запустите триггер-таймер, который будет регулярно производить опрос сайта yandex.ru.
Шаг 1. Дополнительная роль для сервисного аккаунта
В предыдущих практических работах вы создали сервисный аккаунт с именем service-account-for-cf, назначили ему роли editor и  storage.editor и создали ключ доступа. Чтобы подключаться к управляемым БД из функции, нужно добавить сервисному аккаунту роль serverless.mdbProxies.user.
Для этого выполните следующую команду:
yc resource-manager folder add-access-binding $FOLDER_ID \
  --role serverless.mdbProxies.user \
  --subject serviceAccount:$SERVICE_ACCOUNT_ID 
Шаг 2. Создание базы данных
Создание кластера PostgreSQL
Конечно, кластер PostgreSQL можно создать с помощью консоли управления, но в этой практической работе мы используем CLI. Прежде всего, давайте определим подсеть, в которой будет расположен кластер. Разместим кластер в зоне ru-central1-c и с помощью следующей команды узнаем идентификатор(ID) соответствующей подсети:
yc vpc subnet list 
Создадим кластер версии PostgreSQL 13 с именем my-pg-database. Установим тип хоста burstable b2.nano — это самый дешёвый и простой вариант хоста. Из-за невысокой производительности он подходит только для тестовых целей. Используем для хоста жёсткий диск (HDD) размером 10 ГБ.
Сразу создадим пользователя с именем user1 и паролем user1user1, а также базу данных db1. Для удобства администрирования откроем доступ из консоли управления. Используйте опцию websql-access — это позволит выполнять SQL-запросы прямо в консоли управления. Чтобы открыть возможность подключения к PostgreSQL из функции, необходимо подключить опцию serverless-access.
Следующая команда за несколько минут создаст кластер PostgreSQL (не забудьте подставить идентификатор вашей подсети):
yc managed-postgresql cluster create \
  --name my-pg-database \
  --description 'For Serverless' \
  --postgresql-version 13 \
  --environment production \
  --network-name default \
  --resource-preset b2.nano \
  --host zone-id=ru-central1-c,subnet-id=<идентификатор_подсети> \
  --disk-type network-hdd \
  --disk-size 10 \
  --user name=user1,password=user1user1 \
  --database name=db1,owner=user1 \
  --websql-access \
  --serverless-access 
После успешного создания кластера проверьте результат:
yc managed-postgresql cluster list
yc managed-postgresql cluster get <имя или идентификатор кластера> 
Создание таблицы для хранения данных
При создании кластера мы использовали опцию websql-access, что открывает нам возможности по исполнению SQL-команд в консоли управления. Воспользуемся этим и сделаем таблицу в созданной нами базе данных. В эту таблицу мы будем складывать результаты выполнения функции. В консоли управления перейдите в каталог, в котором создан кластер PostgreSQL. Откройте сервис Managed Service for PostgreSQL и перейдите в кластер my-pg-database.
В боковом меню перейдите на вкладку SQL. Для базы данных db1введите имя user1 и пароль user1user1, нажмите кнопку Подключиться.
В открывшемся окне введите SQL-запрос и исполните его:
CREATE TABLE measurements (
    result integer,
    time float
); 
Успешное выполнение команды создаст таблицу, куда мы будем складывать результаты.
Шаг 3. Подключение к управляемой БД из функции
Создание подключения
В консоли управления перейдите в каталог, в котором хотите создать подключение. Откройте сервис Cloud Functions. В боковом меню перейдите на вкладку Подключения к БД. Нажмите кнопку Создать подключение.
    Введите имя, описание подключения и в выпадающем списке выберите тип подключения — PostgreSQL.
    Укажите кластер — my-pg-database.
    Укажите базу данных — db1.
    Введите данные пользователя БД: имя user1 и пароль user1user1.
    Нажмите кнопку Создать.
Выберите созданное подключение. На вкладке Обзор скопируйте параметры Идентификатор и Точка входа. Они будут использованы в функции на следующем шаге.
Шаг 4. Создание функции
Перед созданием функции определите переменные для инициации подключения: CONNECTION_ID — идентификатор подключения, DB_USER — имя пользователя БД, DB_HOST — точка входа. Используйте следующие команды:
echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
Они будут использованы в функции function-for-postgresql.py. Код функции:
import datetime
import logging
import requests
import os
#Эти библиотеки нужны для работы с PostgreSQL
import psycopg2
import psycopg2.errors
CONNECTION_ID = os.getenv("CONNECTION_ID")
DB_USER = os.getenv("DB_USER")
DB_HOST = os.getenv("DB_HOST")
# Настраиваем функцию для записи информации в журнал функции
# Получаем стандартный логер языка Python
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
#Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
def log(logString):
    if verboseLogging:
        logger.info(logString)
#Запись в базу данных
def save(result, time, context):
    connection = psycopg2.connect(
        database=CONNECTION_ID, # Идентификатор подключения
        user=DB_USER, # Пользователь БД
        password=context.token["access_token"],
        host=DB_HOST, # Точка входа
        port=6432,
        sslmode="require")
    cursor = connection.cursor()    
    postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
    record_to_insert = (result, time)
    cursor.execute(postgres_insert_query, record_to_insert)
    connection.commit()
# Это обработчик. Он будет вызван первым при запуске функции
def entry(event, context):
    #Выводим в журнал значения входных параметров event и context
    log(event)
    log(context)
    # Тут мы запоминаем текущее время, отправляем запрос к yandex.ru и вычисляем время выполнения запроса
    try:
        now = datetime.datetime.now()
        #здесь указано два таймаута: 1c для установки связи с сервисом и 3 секунды на получение ответа
        response = requests.get('https://yandex.ru', timeout=(1.0000, 3.0000))
        timediff = datetime.datetime.now() - now
        #сохраняем результат запроса
        result = response.status_code
    #если в процессе запроса сработали таймауты, то в результат записываем соответствующие коды
    except requests.exceptions.ReadTimeout:
        result = 601
    except requests.exceptions.ConnectTimeout:
        result = 602
    except requests.exceptions.Timeout:
        result = 603
    log(f'Result: {result} Time: {timediff.total_seconds()}')    
    save(result, timediff.total_seconds(), context)
    #возвращаем результат запроса
    return {
        'statusCode': result,
        'headers': {
            'Content-Type': 'text/plain'
        },
        'isBase64Encoded': False
    } 
Перейдем в директорию с кодом функции и создадим нашу функцию function-for-postgresql. При этом сразу зададим все необходимые переменные и сервисный аккаунт:
yc serverless function create \
  --name  function-for-postgresql \
  --description "function for postgresql"
yc serverless function version create \
  --function-name=function-for-postgresql \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=function-for-postgresql.entry \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --environment VERBOSE_LOG=True \
  --environment CONNECTION_ID=$CONNECTION_ID \
  --environment DB_USER=$DB_USER \
  --environment DB_HOST=$DB_HOST \
  --source-path function-for-postgresql.py 
Проверим работоспособность функции:
yc serverless function version list --function-name function-for-postgresql
yc serverless function invoke --name function-for-postgresql 
Успешный вызов функции приведёт к измерению времени ответа сайта и формированию записи в базе данных.
Шаг 5. Создание триггера
Создание триггера-таймера
Проверять доступность сайта лучше в автоматическом режиме через равные промежутки времени. Для этой задачи создайте триггер-таймер. Он будет использовать cron-выражения:
yc serverless trigger create timer \
  --name trigger-for-postgresql \
  --invoke-function-name function-for-postgresql \
  --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
  --cron-expression '* * * * ? *' 
Cron-выражение * * * * ? * означает вызов функции function-for-postgresql один раз в минуту. Успешное выполнение функции раз в минуту будет создавать запись в базе данных, в чём вы можете убедиться, просмотрев записи в таблице.
Убедились? Поздравляем: вы успешно создали функцию, которая через заданный промежуток времени выполняется по триггеру, чтобы проверить доступность yandex.ru и записать результат проверки в базу данных.
Удаление триггера-таймера
После завершения практической работы не забудьте удалить созданный триггер trigger-for-postgresql, иначе он будет продолжать работать:
yc serverless trigger delete trigger-for-postgresql 
Поздравляем, вы успешно закончили вторую тему. Пройдите короткий тест — и мы перейдём к изучению сервиса API Gateway.
Не удаляйте созданный кластер PostgreSQL, он понадобится для следующей практической работы.
Task:
Код Cloud Functions выполняется:
Decision:
-На виртуальных машинах, и у вас есть к ним прямой доступ.
+На виртуальных машинах, но они скрыты от пользователя высокоуровневой абстракцией.
-На выделенных серверах, арендуемых у Yandex Cloud.
Task:
Для чего нужны триггеры:
Decision
+Чтобы не писать сложную систему создания и отправки HTTPS-запросов.
-Чтобы упростить проверку прав доступа при вызове функций.
+Чтобы автоматически вызывать функцию по тем или иным событиям.
+Чтобы получить простую интеграцию с другими сервисами Yandex Cloud
Task:
Что такое API Gateway
Decision:
Представим, что вы разрабатываете интернет-магазин с микросервисной архитектурой. Клиент открывает сайт или приложение вашего магазина и переходит на страницу товара — делает один запрос. Чтобы сформировать ту же карточку товара с набором корректных данных, системе нужно сделать несколько запросов, например:
    Получить из базы данных название и подробные характеристики товара.
    Получить из базы данных номинальную стоимость товара.
    Выяснить, нет ли у клиента персональной скидки на товары этой категории, а затем пересчитать конечную стоимость и показать её в карточке рядом с номинальной.
    Выяснить, есть ли этот товар на складе или в офлайн-магазинах и в каком количестве.
    Рассчитать длительность доставки в город, который указал клиент.
Каждый из этих запросов выполняет отдельный микросервис со своим API. Если же вы хотите после оформления заказа и получения оплаты передавать клиенту трекинг почтового отправления, то вам придётся передать всем логистическим компаниям — партнёрам единый API для работы с вашей информационной системой.
Такое решение можно построить с нуля на базе веб-сервера NGINX или с помощью специализированных решений, которые, как правило, ориентированы на определенные стеки технологий и среды выполнения. В дальнейшем вам придется самостоятельно администрировать всю инфраструктуру: настраивать маршрутизатор и балансировку запросов по виртуальным машинам, обеспечивать горизонтальное масштабирование при росте посещаемости и т. д.
Вместо этого вы можете воспользоваться сервисом Yandex API Gateway, который работает по модели PaaS. Сервис предоставит:
    функциональность прокси-сервера, масштабируемость и отказоустойчивость инфраструктуры;
    возможность описывать API в стандартном виде с помощью спецификации OpenAPI;
    возможность применять интеграционные решения на основе расширений OpenAPI.
Давайте разберёмся с базовыми концепциями сервиса.
Знакомство с Yandex API Gateway
В основе этого сервиса лежит классический паттерн программирования: внутреннее представление отделено от способа взаимодействовать с ним через прокси. Мы не афишируем нашу реализацию и отдаём наружу некий контракт.
Yandex API Gateway — важная часть serverless-экосистемы в Yandex Cloud. Он позволяет интегрировать компоненты микросервисного приложения и разрозненные API.
По своей сути Yandex API Gateway является управляемым RESTful API. Он находится между внешним пользователем и сервисами, которые обрабатывают запросы этого пользователя. Его основное преимущество — возможность свести в единую точку взаимодействие пользователей с ресурсами, которые расположены как в Yandex Cloud, так и на других серверах. Допустим, у вас есть три сущности:
    Файлы в Object Storage.
    Сайт на базе WordPress, развёрнутый на виртуальной машине.
    API, который реализован с помощью Cloud Functions.
API Gateway позволит организовать взаимодействие со всеми тремя ресурсами через обращение к одному домену с единым API. Без него вам бы пришлось самостоятельно разворачивать шлюз или обратный прокси-сервер и администрировать его.
Поскольку API Gateway работает по модели PaaS, он гарантирует доступность вашего API в соответствии с SLA. А ещё HTTP/HTTPS-запросы ваших пользователей будут выполнены вне зависимости от их количества.
Логика работы Yandex API Gateway
Сервис принимает запросы по HTTPS через служебный поддомен apigw.yandexcloud.net или через подключение к вашему домену через Certificate Manager, разбирает эти запросы и определяет их путь и параметры. Кроме того, он использует механизм сервисных аккаунтов для подключения к другим сервисам Yandex Cloud.
Созданный API-шлюз может обрабатывать запросы пятью разными способами:
    Автоматически формировать статический ответ на запрос. Ответ будет разным в зависимости от параметров запроса.
    Вызывать функцию, созданную в сервисе Yandex Cloud Functions, которая передаёт параметры запроса и возвращает результаты вызова в ответе.
    Обращаться к сервису Yandex Object Storage, чтобы раздавать статические файлы.
    Отправлять запрос на другой URL и формировать ответ как есть.
    Вызывать ноду DataSphere, развёрнутую в виде отдельного микросервиса.
Общая схема работы сервиса выглядит следующим образом:
Запрос пользователя попадает в API-шлюз, который в соответствии с правилами в спецификации перенаправляет его по другим путям, таким как выполнение облачной функции, получение файла из Object Storage и т. д.
Например, на запрос https://<домен>/GetFileList может быть дан статический ответ со списком файлов, а на запрос https://<домен>/static/<file> — возвращён файл из Yandex Object Storage.
Тарификация
Модель тарификации для этого сервиса — 120 рублей за 1 млн запросов в месяц.* Работает free tier: первые 100 000 запросов в месяц не тарифицируются.
Давайте посчитаем стоимость использования API-шлюза с количеством запросов из примера выше:
120*((2000000–100000)/1000000)=228 p в месяц
Для сравнения создадим виртуальную машину (ВМ) с самыми скромными возможностями, чтобы запустить на ней NGINX для терминирования TLS и проксирования запросов. Пусть это будет Ubuntu 20.04 c HDD объёмом 13 ГБ, Intel Ice Lake, 2 vCPU с гарантированной долей 20% и RAM 1 ГБ. Ежемесячная стоимость ВМ с такой конфигурацией составит 1 045,96 ₽.
* Актуальные тарифы могут отличаться от приведенных в примерах выше.
Обратите внимание: функционально сравнение этих двух решений не будет корректным. Отказоустойчивость API-шлюза, работающего на базе NGINX в ВМ, будет предельно низкой, и у вас уйдёт заметно больше времени на настройку маршрутов.
Более подробную информацию вы найдёте в документации.
На следующем уроке мы разберём основы конфигурации Yandex API Gateway.
Task:
Что из себя представляет сервис Yandex API Gateway?
Decision:
-Балансировщик запросов
-Агрегатор запросов
+Прокси-сервер
-Сетевой тоннель
Task:
Что из нижеперечисленного НЕ УМЕЕТ делать Yandex API Gateway?
Decision:
+Отправлять почтовые сообщения
-Вызывать функцию из Yandex Serverless Functions
-Обращаться в Yandex Object Storage
-Формировать статический ответ
Task:
Какими способами Yandex API Gateway может обработать запрос?
Decision:
+Обратиться к Yandex Object Storage
+Вызвать функцию из Yandex Cloud Functions
+Автоматически сформулировать статический ответ
+Отправить запрос на другой URL
Task:
Как настроить Yandex API Gateway
Decision:
На прошлом уроке вы узнали, что такое Yandex API Gateway и как он работает, а сейчас научитесь настраивать этот сервис.
API Gateway использует спецификацию OpenAPI 3.0 для конфигурации вызовов и ответов. Чтобы настроить сервис, нужно написать спецификацию и создать экземпляр API-шлюза. Эта спецификация должна соответствовать требованиям OpenAPI и описанию расширений, а также учитывать специфику сервиса. Все детали взаимодействия с сервисом можно уточнить в документации, а мы сосредоточимся только на самых важных аспектах.
С сервисом можно работать через консоль управления или с помощью Yandex Cloud REST API. Первый способ проще, но Yandex Cloud REST API имеет чуть больше функций и позволяет настраивать сервис из стороннего программного обеспечения. Мы будем использовать консоль управления.
Чтобы создать новый API-шлюз, перейдите на главную страницу каталога, справа вверху нажмите кнопку Создать ресурс и в раскрывшемся списке выберите API-шлюз.
На открывшейся странице вы можете ввести имя и описание шлюза, а затем вставить в поле Спецификация текст спецификации, который указывает сервису, что нужно сделать в том или ином запросе.
Детально изучить спецификацию OpenAPI 3.0 вы можете в репозитории проекта на GitHub. Здесь же мы рассмотрим минимально необходимый набор параметров и тегов для работы с API Gateway на примере кода спецификации, который формируется для нового API-шлюза по умолчанию.
Базовая спецификация выглядит следующим образом:
openapi: 3.0.0
info:
  title: <Название API>
  version: <Версия API>
paths:
  <Путь>:
    <Метод>:
      x-yc-apigateway-integration:
        type: <Тип расширения> 
Где:
    Путь — путь строки вызова после домена.
    Метод — метод вызова (например GET).
    Тип расширения — один из четырех типов расширений, который указывает на действия при вызове по данному пути:
        dummy — статический ответ.
        cloud-functions — вызов функции сервиса Yandex Cloud Functions.
        http — отправка HTTP-запроса.
        object-storage — обращение к сервису Yandex Object Storage.
При этом расширение x-yc-apigateway-integration является точкой входа для интеграции API-шлюза с другими сервисами.
Статический ответ может выглядеть так:
openapi: 3.0.0
info:
  title: Test API
  version: 1.0.0
paths:
  /hello:
    get:
      x-yc-apigateway-integration:
        type: dummy
        http_code: 200
        http_headers:
          Content-Type: text/plain
        content:
          text/plain: |
            Hello, World! 
В примере выше при вызове метода GET для служебного домена API-шлюз сначала подключает общее расширение x-yc-apigateway-integration, а затем расширение dummy, которое в ответ на обращение выдает статический ответ с заранее определённой текстовой строкой Hello, World!.
У расширения x-yc-apigateway-integration есть дополнительные параметры (http_code, http_headers и т. д.). Их наличие зависит от типа расширения.
В примере выше всего один путь (hello), но вы можете создавать сколько угодно путей. Шаблон спецификации с двумя путями будет выглядеть так:
openapi: 3.0.0
info:
  title: <Название API>
  version: <Версия API>
paths:
  <Путь 1>:
    <Метод>:
      x-yc-apigateway-integration:
        type: <Тип расширения>
  <Путь 2>:
    <Метод>:
      x-yc-apigateway-integration:
        type: <Тип расширения> 
Адрес служебного домена в примере выше формируется после того, как вы вставили текст спецификации в одноимённое поле и сохранили изменения. При этом сервис автоматически добавляет в спецификацию параметр servers по следующему шаблону:
servers:
- url: https://<идентификатор вашего APIGateway>.apigw.yandexcloud.net 
Значением URI будет домен, который нужно использовать при обращении к вашему API.
Например, если вы используете простой тестовый пример спецификации, добавляемый по умолчанию, то для проверки ответа можете использовать HTTP-клиент cURL или просто открыть этот адрес в браузере.
В спецификации вы также можете указать и свой домен, но на него нужно заранее подтвердить права в Certificate Manager. Эта функция находится на стадии Preview, доступ к ней необходимо запросить в службе техподдержки. Подробно об использовании собственных доменов с API-шлюзами можно прочитать в документации.
Журналы и мониторинг
Чтобы вы могли проверять правильность настройки и лучше понять работу сервиса, разработчики добавили две функции: ведение журнала и мониторинг.
На странице Логи вы можете увидеть информацию о запросах к путям, которые вы указали в спецификации.
На странице Мониторинг размещён график с количеством обращений к сервису и информация об ошибках.
На следующем занятии вы познакомитесь с расширениями OpenAPI, которые позволяют работать с другими сервисами Yandex Cloud.
Task:
Что нужно сделать, чтобы настроить сервис?
Decision:
-Обратиться в службу поддержки за ID
+Написать спецификацию
+Создать экземпляр API-шлюза
-Настроить доступы к шлюзу
Task:
Ниже — пример статического ответа. Какие элементы из обязательных в нём пропущены или в них допущена ошибка?
openapi: 3.0.0
info:
  title: Test API
  version: 1.0.0
paths:
  /enter:
    get:
      x-yc-apigateway-integration:
        http_code: 200
        http_headers:
          Content-Type: text/plain
        content:
          text/plain: |
            Enter the world!
    operationId: enter 
Decision:
-enter:
-get:
-x-yc-apigateway-integration:
+operationId:
Task:
Расширения OpenAPI
Decision:
На прошлом уроке мы разобрались с базовой структурой спецификаций OpenAPI в сервисе API Gateway. Теперь давайте посмотрим на структуру её расширений.
Статический ответ
Расширение x-yc-apigateway-integration:dummy возвращает фиксированное содержимое с указанным кодом ответа и необходимыми заголовками без участия стороннего сервиса.
Структура этого расширения выглядит следующим образом:
x-yc-apigateway-integration:
  type: dummy
  http_code: <HTTP-код ответа>
  http_headers:
    <Список заголовков ответа>
  content:
    <Содержимое тела ответа> 
Вот пример скрипта, который возвращает ответ в формате JSON с кодом 302:
x-yc-apigateway-integration:
  type: dummy
  http_code: 302
  http_headers:
    Location: "/some/location"
  content:
    "application/json": "{ \"message\": \"You've been redirected.\" }" 
Вы можете использовать широкий спектр параметров и механик спецификации OpenAPI и делать сложные конструкции для статических ответов. Например, следующий скрипт меняет ответ в зависимости от параметров запроса:
openapi: "3.0.0"
info:
  version: 1.0.0
  title: Test API
paths:
  /hello:
    get:
      summary: Say hello
      operationId: hello
      parameters:
        - name: user
          in: query
          description: User name to appear in greetings
          required: false
          schema:
            type: string
            default: 'world'
      responses:
        '200':
          description: Greeting
          content:
            'text/plain':
               schema:
                 type: "string"
      x-yc-apigateway-integration:
        type: dummy
        http_code: 200
        http_headers:
          'Content-Type': "text/plain"
        content:
          'text/plain': "Hello, {user}!\n" 
Подробнее о том, как использовать расширение, рассказывается в документации.
Вызов облачной функции
Расширение x-yc-apigateway-integration:cloud_functions вызывает указанную облачную функцию. В качестве входных данных функция получает информацию об HTTP-запросе и значения параметров, указанных в спецификации. На выходе клиенту возвращается результат выполнения функции.
Информация о запросе передается в том же формате, что и в текущей версии HTTP-интеграции при вызове функции с указанием параметра строки запроса integration=raw, который по большей части совместим с форматом AWS API Gateway. Значения параметров, указанных в спецификации, передаются в поле params параметра data.
Вот общая структура расширения:
x-yc-apigateway-integration:
  type: cloud-functions
  function_id: <идентификатор функции>
  tag: <Тег функции>
  service_account: <идентификатор сервисного аккаунта> 
Давайте посмотрим на параметры:
    Среди них обязательным является только function_id. , который ссылается на идентификатор облачной функции.
    Параметр tag указывает версию функции, которую вы хотите вызвать. Он может быть опущен, и тогда будет вызвана самая последняя версия функции с тегом $latest.
    Параметр service_account указывает на сервисный аккаунт, от имени которого будет вызвана функция. Он также может быть опущен, и тогда будет использовано значение, которое вы указали для всей спецификации в параметре верхнего уровня. Если же вы не указали параметр верхнего уровня, функция будет вызываться без авторизации.
Вот пример скрипта для этого расширения:
x-yc-apigateway-integration:
  type: cloud-functions
  function_id: b095c95icnvbuf4v755l
  tag: stable
  service_account: ajehfe41hhliq4n93q1g 
Подробнее о том, как использовать расширение, читайте в документации.
Обращение по HTTP
Расширение x-yc-apigateway-integration:http перенаправляет запрос в указанный URL. Вот его структура:
x-yc-apigateway-integration:
  type: http
  url: <URL для вызова>
  method: <Метод вызова>
  headers:
     <Массив заголовков вызова>
  timeouts:
     <Таймаут вызова> 
Обязательными параметрами здесь являются url и headers. Если method не указан, то система будет использовать метод запроса к Yandex API Gateway. При отсутствии timeouts будут использованы значения по умолчанию.
Пример скрипта для данного расширения:
x-yc-apigateway-integration:
  type: http
  url: https://example.com/backend1
  method: POST
  headers:
    Authorization: Basic ZjTqBB3f$IF9gdYAGlMrs2fuINjHsz
  timeouts:
    connect: 0.5
    read: 5 
Подробнее о том, как использовать расширение, читайте в документации.
Интеграция с Object Storage
Расширение x-yc-apigateway-integration:object_storage передает управление обработки запроса в Object Storage с целью раздачи статических файлов. Оно позволяет управлять ключом для доступа к объекту и реализует возможность раздавать статические данные напрямую из Object Storage, используя перенаправление на подписанный URL.
Структура расширения такова:
x-yc-apigateway-integration:
        type: object-storage
        bucket: <Имя бакета>
        object: <Имя объекта>
        presigned_redirect: <Генерация пре-подписанного url>
        service_account: <идентификатор сервисного аккаунта, от имени которого идет обращение к Yandex Object Storage> 
Параметр service_account является единственным необязательным. Он работает так же, как и в функциях. Если он отсутствует, то будет использовано значение для всего скрипта в параметре верхнего уровня. Если параметр верхнего уровня не указан, то обращение будет выполнено без авторизации. При этом бакет должен быть публичным.
Если значение параметра presigned_redirect задано как true, то для запроса будет сгенерирован специальный тип URL, который содержит все данные для авторизации. Любой клиент сможет скачать этот файл вне зависимости от наличия у него данных для авторизации.
Вот пример скрипта для этого расширения:
/static/{file}:
    get:
      summary: Serve static file from Yandex Cloud Object Storage
      parameters:
        - name: file
          in: path
          required: true
          schema:
            type: string
      x-yc-apigateway-integration:
        type: object-storage
        bucket: my-example-bucket
        object: 'my-object'
        presigned_redirect: true
        service_account: ajehfe41hhliq4n93q1g 
Подробнее о том, как использовать расширение, читайте в документации.
Жадные параметры
Предположим, что все JS и CSS файлы вашего сайта должны быть доступны по URL с одинаковым префиксом /static/js/ и /static/CSS/соответственно. Чтобы не перечислять в спецификации каждый файл по отдельности и не писать для него дублирующиеся интеграции, вы может использовать так называемые жадные параметры.
Чтобы описать все файлы, доступные по URL с префиксом /static на любом уровне вложенности, добавьте знак плюс после имени параметра. Вот пример спецификации:
/static/{file+}:
    get:
      summary: Serve static file from Yandex Cloud Object Storage
      parameters:
        - name: file
          in: path
          required: true
          schema:
            type: string
      x-yc-apigateway-integration:
        type: object_storage
        bucket: my-example-bucket
        object: '{file}'
        error_object: error.html 
Обобщенный HTTP-метод
Если вам необходимо все ваши вызовы по одному URL для всех HTTP-методов  направить в соответствии с одной и той же интеграцией, например в одну функцию, вы можете использовать расширение x-yc-apigateway-any-method. Оно позволяет избежать дублирования одинаковых интеграций в спецификации для каждого HTTP-метода, делает спецификацию короче и понятнее.
Вот пример спецификации, где применяется это расширение:
/example/{ID}:
    x-yc-apigateway-any-method:
      summary: Operating with examples
      operationId: example
      tags:
        - example
      parameters:
        - name: ID
          in: path
          description: Return ID
          required: true
          schema:
            type: string
      x-yc-apigateway-integration:
        type: cloud_functions
        function_id: b095c95icnvbuf4v755l
        tag: "$latest"
        service_account_id: ajehfe41hhliq4n93q1g 
Использование сервисного аккаунта
Всё взаимодействие с сервисами в Yandex Cloud должно происходить от имени какого-либо аккаунта. В случае с Yandex API Gateway сервисный аккаунт используется для работы с функциями и Yandex Object Storage. Его можно указать как для конкретного расширения, так и для всех расширений разом. Для этого используется параметр верхнего уровня:
x-yc-apigateway:
  service_account: <идентификатор сервисного аккаунта> 
Он используется следующим образом:
openapi: 3.0.0
info:
  title: Test API
  version: 1.0.0
x-yc-apigateway:
  service_account: <идентификатор сервисного аккаунта> 
Теперь вы знакомы с возможными вариантами настройки сервиса. На следующем занятии закрепим пройденный материал на практике и создадим HTTP API с помощью Cloud Functions и API Gateway.
Task:
С какого параметра должна начинаться спецификация сервиса?
Decision:
-info
-paths
+openapi
-x-yc-apigateway
Task:
Зачем нужно указывать сервисный аккаунт в параметрах скрипта?
Decision:
-Чтобы записывать логи
-Чтобы работал мониторинг
Чтобы обращаться к функциям или Object Storage с авторизацией
-Чтобы разрешить запросы к сервису сторонним пользователям
Task:
Что должен сделать сервис при том или ином типе пути?
Decision:
dummy - Выдать статический ответ
cloud-functions - Вызвать функцию сервиса Yandex Cloud Functions
http - Отправить http-запрос
object-storage - Обратиться к сервису Yandex Object Storage
Task:
Практическая работа. Создание HTTP API с помощью Cloud Functions и API Gateway
Decision:
На предыдущем практическом занятии мы создали простую систему, которая проверяет доступность сайта yandex.ru и измеряет время ответа на запрос. Полученную информацию функция записывала в базу данных PostgreSQL. На этом уроке мы доработаем начатый проект и добавим REST API, который позволит получать до 50 результатов проверки из базы данных.
Шаг 1. Проверить наличие сервисного аккаунта
Для работы нам понадобится сервисный аккаунт с именем service-account-for-cf и ролями editor, serverless.mdbProxies.user, который мы создали ранее.
Шаг 2. Yandex API Gateway
Создание спецификации
В рабочем каталоге создадим спецификацию hello-world.yaml:
openapi: "3.0.0"
info:
  version: 1.0.0
  title: Test API
paths:
  /hello:
    get:
      summary: Say hello
      operationId: hello
      parameters:
        - name: user
          in: query
          description: User name to appear in greetings
          required: false
          schema:
            type: string
            default: 'world'
      responses:
        '200':
          description: Greeting
          content:
            'text/plain':
              schema:
                type: "string"
      x-yc-apigateway-integration:
        type: dummy
        http_code: 200
        http_headers:
          'Content-Type': "text/plain"
        content:
          'text/plain': "Hello, {user}!\n" 
Мы можем создать API-шлюз с помощью консоли управления, но сейчас воспользуемся CLI.
Инициализация спецификации
Чтобы развернуть API-шлюз, используем спецификацию hello-world.yaml:
yc serverless api-gateway create \
  --name hello-world \
  --spec=hello-world.yaml \
  --description "hello world" 
В результате успешного создания API-шлюза получим значение параметра domain:
yc serverless api-gateway list
yc serverless api-gateway get --name hello-world 
Скопируем служебный домен, чтобы проверить работоспособность API-шлюза. Вставим его в адресную строку браузера и допишем в конец /hello. Должно получиться следующее:
https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello 
Теперь протестируем запрос с параметрами. Добавьте к предыдущему запросу ?user=my_user. Должно получиться следующее:
https://<идентификатор API Gateway>.apigw.yandexcloud.net/hello?user=my_user 
В первом случае в окне браузера вы увидите «Hello, world!», во втором «Hello, my_user!».
Шаг 3. Создание функции
Работа с библиотеками и переменными
До этого момента мы использовали рантайм python37, который не требовал явного указания библиотек, но начиная с версии python39, нужно указывать библиотеки явно. Для работы с requirements.txt можно воспользоваться удобной Python-библиотекой pipreqs: чтобы сгенерировать requirements.txt с помощью pipreqs, достаточно указать рабочий каталог. В большинстве интерпретаторов Linux для указания текущего каталога предусмотрена переменная $PWD. Если файл requirements.txt уже существует, актуализируйте его с помощью флага --force, например:
pip install pipreqs
pipreqs $PWD --print
pipreqs $PWD --force 
Чтобы создать функцию, проверим доступность переменных для инициации подключения CONNECTION_ID, DB_USER, DB_HOST, которые мы создали в предыдущей работе с помощью следующих команд:
echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
Создание функции
Создадим функцию function-for-user-requests.py:
import json
import logging
import requests
import os
#Эти библиотеки нужны для работы с PostgreSQL
import psycopg2
import psycopg2.errors
import psycopg2.extras
CONNECTION_ID = os.getenv("CONNECTION_ID")
DB_USER = os.getenv("DB_USER")
DB_HOST = os.getenv("DB_HOST")
# Настраиваем функцию для записи информации в журнал функции
# Получаем стандартный логер языка Python
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения
verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
#Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
def log(logString):
    if verboseLogging:
        logger.info(logString)
#Запись в базу данных
def save(result, time, context):
    connection = psycopg2.connect(
        database=CONNECTION_ID, # Идентификатор подключения
        user=DB_USER, # Пользователь БД
        password=context.token["access_token"],
        host=DB_HOST, # Точка входа
        port=6432,
        sslmode="require")
    cursor = connection.cursor()   
    postgres_insert_query = """INSERT INTO measurements (result, time) VALUES (%s,%s)"""
    record_to_insert = (result, time)
    cursor.execute(postgres_insert_query, record_to_insert)
    connection.commit()
#Формируем запрос
def generateQuery():
    select = f"SELECT * FROM measurements LIMIT 50"
    result = select
    return result
#Получаем подключение
def getConnString(context):
    """
    Extract env variables to connect to DB and return a db string
    Raise an error if the env variables are not set
    :return: string
    """
    connection = psycopg2.connect(
        database=CONNECTION_ID, # Идентификатор подключения
        user=DB_USER, # Пользователь БД
        password=context.token["access_token"],
        host=DB_HOST, # Точка входа
        port=6432,
        sslmode="require")   
    return connection
def handler(event, context):
    try:
        secret = event['queryStringParameters']['secret']
        if secret != 'cecfb23c-bc86-4ca2-b611-e79bc77e5c31':
            raise Exception()
    except Exception as error:
        logger.error(error)
        statusCode = 401
        return {
            'statusCode': statusCode
        }
    sql = generateQuery()
    log(f'Exec: {sql}')
    connection = getConnString(context)
    log(f'Connecting: {connection}')
    cursor = connection.cursor()
    try:
        cursor.execute(sql)
        statusCode = 200
        return {
            'statusCode': statusCode,
            'body': json.dumps(cursor.fetchall()),
        }
    except psycopg2.errors.UndefinedTable as error:
        connection.rollback()
        logger.error(error)
        statusCode = 500
    except Exception as error:
        logger.error(error)
        statusCode = 500
    cursor.close()
    connection.close()
    return {
        'statusCode': statusCode,
        'body': json.dumps({
            'event': event,
        }),
    }
Обратите внимание, в коде функции мы заложили параметр secret и его значение cecfb23c-bc86-4ca2-b611-e79bc77e5c31, при котором функция будет выполняться. Таким образом мы обеспечиваем дополнительную защиту при доступе к БД.
При создании функции сразу зададим все необходимые переменные и сервисный аккаунт:
yc serverless function create \
  --name function-for-user-requests \
  --description "function for response to user"
yc serverless function version create \
  --function-name=function-for-user-requests \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=function-for-user-requests.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --environment VERBOSE_LOG=True \
  --environment CONNECTION_ID=$CONNECTION_ID \
  --environment DB_USER=$DB_USER \
  --environment DB_HOST=$DB_HOST \
  --source-path function-for-user-requests.py 
Шаг 4. Обновление спецификации API Gateway
Наша функция готова, но по умолчанию она не является публичной. Предоставим доступ к этой функции с помощью API-шлюза — обновим ранее созданную спецификацию hello-world.yaml. Не забудьте вставить в файл идентификаторы вашей функции и вашего сервисного аккаунта:
openapi: "3.0.0"
info:
  version: 1.0.0
  title: Updated API
paths:
  /results:
    get:
      x-yc-apigateway-integration:
        type: cloud-functions
        function_id: <идентификатор функции>
        service_account_id: <идентификатор сервисного аккаунта>
      operationId: function-for-user-requests 
Вызовем перезагрузку нашей спецификации:
yc serverless api-gateway update \
  --name hello-world \
  --spec=hello-world.yaml 
Для тестирования вызовем функцию в браузере сначала без параметра secret, а затем — с ним:
https://<идентификатор API Gateway>.apigw.yandexcloud.net/results
https://<идентификатор API Gateway>.apigw.yandexcloud.net/results?secret=cecfb23c-bc86-4ca2-b611-e79bc77e5c31 
В ответе увидим результаты тестирования сервиса yandex.ru из базы данных.
Иногда приходится тестировать функцию в процессе разработки: для этого в консоли управления на странице функции перейдите на вкладку Тестирование, в поле Шаблон данных выберите HTTPS-вызов. Нажмите кнопку Запустить тест, и вы увидите код ошибки.
Код функции проверяет параметр secret для авторизации, то есть при вызове вы должны передать секретную последовательность, чтобы функция выдала результат. Добавим secret в параметры запроса в поле Входные данные:
    "queryStringParameters": {
        "a": "2",
        "b": "1",
        "secret": "cecfb23c-bc86-4ca2-b611-e79bc77e5c31"
    }, 
Запустим тест ещё раз. В ответе отобразятся данные из базы, как и с запросами через браузер.
Task:
Что выдаёт наше тестовое API при вызове метода /hello без параметров?
Decision:
-Hello, User!
+Hello, world!
-Hello!
-Username
Task:
Какой код ошибки выдаёт функция, если не указан секрет?
Decision:
-400
+401
-402
-403
Task:
Среди способов обработки запросов API-шлюзом есть следующие:
Decision:
+Статический ответ
+Динамический ответ
+Вызов Cloud Function
+Обращение в Object Storage
+Перенаправление запроса на другой URL
-Перенаправление запроса на устройство IoT
Task:
Для работы с API Gateway можно использовать:
Decision:
+Консоль управления
-Любой терминал с установленным Yandex CLI
+Yandex Cloud REST API
Task:
Спецификацию шлюза можно написать на основе спецификации:
Decision:
-OpenAPI 2.8
+OpenAPI 3.0
-OpenAPI 3.1
-Все перечисленные варианты
Task:
Особенности serverless-режима Yandex Database
Decision:
Отличия от dedicated-режима Yandex Database
В рамках курса «Хранение и анализ данных» вы уже разобрались с тем, как устроена Yandex Database (YDB), научились проектировать новые базы данных, составлять запросы на языке YQL, проводить диагностику и создавать резервные копии БД.
В рамках данного курса мы подробнее остановимся на бессерверном (serverless) режиме и специфичных для него возможностях YDB. Давайте начнём с главных отличий этого режима от режима работы с выделенными ресурсами (dedicated).
Как вы уже знаете, в режиме с выделенными ресурсами вы самостоятельно задаёте число виртуальных хостов, их вычислительные ресурсы и параметры хранилища, а также настраиваете облачные сети.
Если нагрузка на вычислительные ресурсы растёт, а вы не добавляете их своевременно, скорость обработки запросов будет уменьшаться вплоть до отказа в обслуживании. Поэтому такой режим применим для случаев, когда у вас есть реалистичный прогноз пиковых нагрузок и вы готовы самостоятельно обслуживать созданную инфраструктуру. При этом оплата происходит за выделенные вычислительные ресурсы, объём хранилища и резервных копий и не зависит от числа операций с базой.
При использовании YDB в бессерверном режиме вы можете развернуть базу данных в обслуживаемой среде без создания отдельных хостов и заказа дискового пространства. Вам не нужно контролировать вычислительные ресурсы, сети и другие параметры. Все ресурсы, которые необходимы для работы базы, автоматически предоставляются сервисом и масштабируются в зависимости от нагрузки.
Бессерверный режим применим в различных сценариях. Это может быть небольшой проект, для которого не нужен отдельный сервер, или крупный проект с неравномерной и плохо прогнозируемой нагрузкой. В таком режиме вы платите за фактические операции с базой и за хранение данных и резервных копий.
Чтобы лучше понять отличия от dedicated-режима, давайте рассмотрим простую аналогию. Допустим, вы арендовали помещение под салон красоты и теперь платите ежемесячную фиксированную арендную плату владельцу. Такие расходы называются постоянными. К вам в салон приходят клиенты, каждый из них приносит некоторый доход, а вы тратите на его обслуживание расходные материалы — краску, лак…
Чтобы получить прибыль по результатам месяца, вам нужно обслужить некоторое минимальное количество клиентов. При этом арендная плата не зависит от того, сколько к вам пришло клиентов. А вот стоимость расходных материалов, наоборот, зависит от этого напрямую: чем меньше, тем меньше потрачено расходных материалов, чем их больше — тем больше потрачено. Такие расходы называются переменными.
Возможно, вы всё посчитали перед тем, как открывать салон, но вокруг постоянно могут происходить события, которые вы не могли предсказать или риск которых решили принять. Например, рядом может открыться ещё один салон или вас затопит сосед сверху. В результате количество обслуженных клиентов может резко снизиться или просто оказаться меньше, чем нужно, и вы начнёте терпеть убытки. Арендная плата за помещение при этом остаётся фиксированной вне зависимости от ваших сложностей.
Но проблема не только в том, что возможны непредсказуемые события. Она также и в том, что поток реальных клиентов никогда не бывает равномерным: ночью все спят, в выходные салон переполнен, а в будни клиенты есть только вечерами и немного с утра.
Получается, что арендованная площадь работает полностью только часть выходных дней, а в остальное время простаивает. Скорее всего, в выходные даже приходится отказывать каким-то клиентам в записи, что снижает доход, но в другое время салон пуст, и аренда большего помещения уже невозможна из-за недостатка общего количества клиентов.
То же самое происходит и с вычислительными ресурсами. Только «отказ в записи на причёску» в случае недостатка вычислительных ресурсов может превратиться в «отказ в обслуживании» и в общем случае недопустим. А это вынуждает вас арендовать мощности под пиковые нагрузки даже с некоторым запасом. Всё время между пиками эти ресурсы будут недогружены, но вы будете за них платить по фиксированной почасовой ставке.
Конечно, аренда облачных вычислительных ресурсов намного более гибкая, чем в примере выше. Она позволяет оперативно изменять ресурсы при необходимости, но это также требует оперативной реакции на окружающие события. Например, надо проанализировать, что в результате запуска рекламной кампании ресурсов перестало хватать, и вовремя их расширить, а после её окончания не забыть от лишних ресурсов отказаться.
Переход от аренды выделенных вычислительных ресурсов к serverless-сервисам позволяет превратить все ваши расходы на инфраструктуру в переменные! А это значит, что вы сможете прогнозировать расходы на обслуживание каждого клиента. Даже если клиентов не будет вообще, вы останетесь в нуле, но никогда не уйдёте в минус. И у вас никогда не будет недозагруженных ресурсов.
Способы доступа к данным YDB в serverless-режиме
У вас есть выбор, как проводить операции с базой данных YDB в serverless-режиме. Для работы с YDB-таблицами вы можете выполнять YQL-запросы через консоль управления или YDB CLI, а также из приложений с помощью YDB SDK.
При этом также доступен специфичный для serverless-режима Document API, полностью совместимый с AWS DynamoDB API. Это позволяет легко переносить существующие приложения из AWS или писать новые с использованием AWS SDK.
Как следствие этого, в бессерверном режиме доступен ещё один тип таблиц — документные. Они состоят из набора элементов, где каждый элемент — набор атрибутов в виде пары ключ-значение.
Вы можете работать с документными таблицами через HTTP-интерфейс (например, с использованием приложения cURL), с помощью AWS CLI и через AWS SDK.
В следующем уроке мы подробнее рассмотрим тарификацию YDB в бессерверном режиме. Это принципиально важная тема, которая влияет на проектирование базы данных и выбор функций для работы с ней.
Task:
При работе в serverless-режиме для доступа к данным YDB вы можете использовать:
Decision:
+YQL-запросы через консоль управления и YDB CLI
+YQL-запросы через консоль управления, YDB CLI, YDB API
+Document API и cURL (для документных таблиц)
+Document API через cURL, AWS CLI, AWS SDK (для документных таблиц)
Task:
Тарификация Yandex Database в бессерверном режиме
Decision:
Из предыдущих уроков вы уже знаете, что сервисы в группе Бессерверные вычисления тарифицируются по-своему. Давайте разберёмся с тем, как рассчитывается стоимость использования Yandex Database.
В бессерверном режиме YDB вы платите за исполнение запросов к БД и хранение данных. Кроме того, тарифицируется использование других сервисов (например, объектного хранилища для резервных копий «по требованию», или исходящий трафик из Yandex Cloud в интернет). Но в этой части начислений нет какой-либо serverless-специфики, поэтому мы не будем на них останавливаться в рамках этого курса.
Хранение данных
Начнём с того, о чем можно рассказать быстро, — стоимости хранения данных. В отличие от dedicated режима, где вы оплачиваете зарезервированное под ваши данные дисковое пространство, объём хранилища serverless БД условно бесконечен и никак не влияет на стоимость. В полном соответствии с концепцией serverless, вы платите только за тот объём, который фактически используете. Также, пока объём ваших данных меньше 1 ГБ, их хранение будет бесплатно (Free tier).
Тариф устанавливается за хранение 1 ГБ в месяц выше бесплатного порога, расчет производится с точностью до 1 КБ. Таким образом, по действующему на момент написания этого урока тарифу в 21,38 рублей хранение 2,5 ГБ данных обойдется вам в 32,07 рублей в месяц.
Исполнение запросов
Исполнение любого запроса к БД оценивается в специальной единице измерения, которая называется Request Unit (сокращенно RU). Она отражает, сколько различных ресурсов было задействовано для исполнения запроса. Чем больше ресурсов потребовалось, тем выше стоимость исполнения запроса в Request Units (RU) и, соответственно, конечная стоимость в рублях.
Для каждого вида запросов определены правила, по которым на основании собираемой в процессе исполнения запроса информации об использовании ресурсов определяется его стоимость в RU. Данные правила нужно понимать для того, чтобы представлять какие ресурсы требуются для исполнения  разных запросов, как их использование влияет на стоимость, и выбирать наиболее оптимальный вариант их исполнения.
Чтобы предотвратить неожиданное использование большого количества RU из-за неоптимальных запросов, может быть установлено ограничение по потреблению RU в секунду. По умолчанию для каждой создаваемой БД устанавливается ограничение в 10 RU/сек, к которому применяется накопление неиспользованных RU в течение 5 минут. С таким ограничением вы можете свободно вести разработку и тестирование приложений, не опасаясь получить большой незапланированный счет.
Суммарная стоимость исполнения всех запросов в RU за некоторый период является базой, на основании которой рассчитывается стоимость в рублях, с применением следующих правил тарификации:
    Первый миллион Request Units каждый месяц предоставляется бесплатно (Free tier).
    Если стоимость исполненных запросов превысила 1 миллион RU, то далее действует тариф «за фактическое потребление» (on-demand), составляющий на момент написания этого курса 21,38 рублей за 1 миллион RU, применяемый с точностью до 1 RU.
    Вы можете установить специальный параметр БД, который называется «выделенная пропускная способность» (provisioned capacity) и измеряется в RU/сек. Если данный параметр больше нуля, то вы перейдёте на почасовое начисление оплаты за пропускную способность в указанных пределах. Она рассчитывается по специальному тарифу, стоимость RU на котором существенно ниже чем на тарифе «за фактическое потребление». Стоимость исполненных запросов в пределах установленной выделенной пропускной способности (с учетом пятиминутного накопления неиспользованных RU) будет вычитаться из суммирования по тарифу on-demand, что позволяет существенно сэкономить, если нагрузка на ваше приложение более-менее постоянна.
    Если вы установили значение параметра «выделенная пропускная способность» равное параметру «ограничение пропускной способности», то получите точную стоимость в рублях в месяц вне зависимости от вашей нагрузки. Такой режим похож на работу с выделенными ресурсами, но отличается от них тем, что ограничения можно в любой момент изменить, а минимальный квант в 1 RU/сек будет стоить вам 7.2 рубля в месяц по тарифам, актуальным на момент подготовки этого урока. При этом, за счет накопления неиспользуемых RU в течение 5 минут, на таком ограничении вы можете успешно исполнять запросы стоимостью до 300 RU один раз в 5 минут.
Далее мы познакомим вас с правилами расчета стоимости запроса в Request Units для разных типов запросов:
    YQL
    DocumentAPI
    Специальные API
Правила расчета стоимости запросов через YQL
YQL — диалект SQL, который используется для работы с данными, представленными в виде реляционных таблиц. Вы познакомились с ним в курсе «Хранение и анализ данных».
Давайте рассмотрим пример вычисления стоимости запроса, объединяющего данные из двух таблиц, которые вы создавали, работая с YDB в dedicated режиме.
SELECT
    sa.title AS season_title,
    sr.title AS series_title,
    sr.series_id,            
    sa.season_id                                          
FROM
    seasons AS sa
INNER JOIN
    series AS sr
ON sa.series_id = sr.series_id
WHERE sa.series_id = 1
ORDER BY                      
    sr.series_id,
    sa.season_id
; 
Для того, чтобы определить стоимость выполнения этого запроса, перейдите на вкладку Стоимость. Вы увидите, что стоимость запроса составила 6 RU.
Теперь давайте рассмотрим статистику выполнения этого запроса и определим, из чего складывается стоимость.
Как вы можете видеть, статистика запроса содержит следующую информацию:
{
    "processCpuTimeUs": "8367",
    "queryPhases": [
        {
            "affectedShards": "2",
            "cpuTimeUs": "948",
            "durationUs": "40606",
            "tableAccess": [
                {
                    "name": "series",
                    "partitionsCount": "1",
                    "reads": {
                        "bytes": "16",
                        "rows": "1"
                    }
                },
                {
                    "name": "seasons",
                    "partitionsCount": "1",
                    "reads": {
                        "bytes": "96",
                        "rows": "4"
                    }
                }
            ]
        }
    ]
} 
Где:
    query_phases[].cpu_time_us — время CPU на выполнение запроса в мкс.
    compilation.cpu_time_us — время CPU на компиляцию запроса в мкс.
    process_cpu_time_us — время CPU на управление взаимодействием в мкс.
    query_phases[].reads.rows — число прочитанных записей данных.
    query_phases[].reads.bytes — число прочитанных байт данных.
    query_phases[].updates.rows — число записанных записей данных.
    query_phases[].updates.bytes — число записанных байт данных.
Для определения стоимости запроса через YDB API вычисляются стоимость использования CPU и стоимость ввода-вывода данных. Из вычисленных значений выбирается максимальное.
Расчет стоимости использования CPU
Суммируется время CPU, затраченное на компиляцию запроса, а также время использования CPU на всех этапах его выполнения. Сумма делится на продолжительность одного окна 1,5 мс, округляется в меньшую сторону и переводится в RU по тарифу из таблицы, с которой можно познакомиться в документации.
Чтобы получить стоимость использования CPU, сумма времени CPU запроса делится на 1,5 мс.
(8367+948)/1500=6,21(8367 + 948)/1500 = 6,21(8367+948)/1500=6,21.
Результат округляется в меньшую сторону и переводится в RU:
6∗16 * 16∗1 RU = 666 RU
Стоимость ввода-вывода данных
Рассчитываются следующие значения:
    Количество операций чтения с диска. Сравнивается число прочитанных записей и блоков данных, выбирается наибольшее из них. Число блоков вычисляется делением суммы прочитанных байт на размер блока 4 КБ, с округлением вверх.
    Количество операций записи на диск. Сравнивается число записанных записей и блоков данных, выбирается наибольшее из них. Число блоков вычисляется делением суммы записанных байт на размер блока 1 КБ, с округлением вверх.
Учитывается ввод-вывод данных в таблицы и индексы. При удалении записей учитывается только их количество.
Количества операций чтения и записи переводятся в RU по тарифу из таблицы, с которой можно познакомиться в документации, полученные стоимости складываются.
Чтобы получить стоимости ввода-вывода данных, вычисляется количество операций чтения с диска в записях и блоках, из них выбирается максимальное:
    прочитаны 5 записей, количество операций равно 5;
    прочитаны 112 байт,
    112/(4×1024)=0,027112 / ( 4 × 1024 ) = 0,027112/(4×1024)=0,027,
    результат округляется в большую сторону, количество операций равно 1.
Количество операций чтения равно 5.
Стоимость ввода-вывода:
5×15 × 15×1 RU = 555 RU
Стоимость выполнения запроса равна 6 RU, так как на CPU затрачено больше ресурсов.
Правила расчета стоимости запроса через DocumentAPI
Об этих правилах вы узнаете на следующем уроке, когда будете знакомиться с DocumentAPI.
Правила расчета стоимости запроса через специализированные API
Существует несколько специализированных API, расчет RU для которых делается по отдельным правилам:
    ReadTable — специальная операция, предназначенная для считывания больших объемов данных. Её оценка в RU зависит только от объема данных и составляет 128 RU за 1 МБ, с округлением вверх до ближайшего мегабайта.
    BulkUpsert — специальная операция, предназначенная для вставки больших объемов данных в БД. Её оценка в RU зависит только от объема данных и составляет 0,5 RU за 1 КБ данных, с округлением вверх до ближайшего килобайта.
Task:
Рассчитайте стоимость следующего запроса YQL. Ответ введите в текстовое поле ниже.
$to_update = (
    SELECT series_id,
           season_id,
           episode_id,
           Utf8("Yesterday's Jam UPDATED") AS title
    FROM episodes
    WHERE series_id = 1 AND season_id = 1 AND episode_id = 1
);
SELECT * FROM episodes WHERE series_id = 1 AND season_id = 1;
UPDATE episodes ON
SELECT * FROM $to_update; 
На основании статистики запроса:
{
    "processCpuTimeUs": "11254",
    "queryPhases": [
        {
            "affectedShards": "1",
            "cpuTimeUs": "590",
            "durationUs": "8939",
            "tableAccess": [
                {
                    "name": "episodes",
                    "partitionsCount": "1",
                    "reads": {
                        "bytes": "24",
                        "rows": "1"
                    }
                }
            ]
        },
        {
            "affectedShards": "1",
            "cpuTimeUs": "510",
            "durationUs": "4131",
            "tableAccess": [
                {
                    "name": "episodes",
                    "partitionsCount": "1",
                    "reads": {
                        "bytes": "48",
                        "rows": "2"
                    }
                }
            ]
        },
        {
            "affectedShards": "1",
            "cpuTimeUs": "380",
            "durationUs": "4056",
            "tableAccess": [
                {
                    "name": "episodes",
                    "partitionsCount": "1",
                    "reads": {
                        "bytes": "257",
                        "rows": "6"
                    }
                }
            ]
        },
        {
            "affectedShards": "1",
            "cpuTimeUs": "463",
            "durationUs": "8651",
            "tableAccess": [
                {
                    "name": "episodes",
                    "partitionsCount": "1",
                    "updates": {
                        "bytes": "47",
                        "rows": "1"
                    }
                }
            ]
        }
    ]
} 
Сколько получилось?
Decision:

Task:
Что такое RU для Serverless YDB?
Decision:
-Свидетельство того, что YDB разработана в России
-Сумма в российских рублях, которую пользователь платит за использование YDB
+Request Unit — универсальная единица измерения стоимости исполнения запроса YDB Serverless, к которой привязываются разные варианты тарификации
Task:
Document API
Decision:
В бессерверном режиме работы Yandex Database поддерживает Document API — HTTP API, совместимое с Amazon DynamoDB. Это низкоуровневый API для работы с таблицами и создания, чтения, изменения и удаления данных. С помощью этого API можно выполнять операции над документными таблицами, совместимыми с AWS DynamoDB.
Для работы с таблицами в Document API поддерживаются следующие методы:
    CreateTable — создаёт таблицу.
    DeleteTable — удаляет таблицу.
    DescribeTable — возвращает информацию о таблице.
    DescribeTimeToLive — возвращает информацию о состоянии времени жизни (TTL) в указанной таблице.
    ListTables — возвращает список таблиц.
    UpdateTimeToLive — включает или отключает время жизни (TTL) для указанной таблицы.
Для работы с элементами таблиц поддерживаются следующие методы:
    BatchGetItem — возвращает атрибуты элементов из нескольких таблиц.
    BatchWriteItem — записывает или удаляет элементы из таблиц.
    DeleteItem — удаляет элемент в таблице.
    GetItem — возвращает атрибуты элемента из одной таблицы.
    PutItem — перезаписывает элементы в таблице.
    Query — возвращает элементы из таблиц.
    Scan — возвращает элементы и атрибуты из таблицы.
    TransactGetItems — извлекает несколько элементов из таблиц.
    TransactWriteItems — синхронная операция записи.
    UpdateItem — обновляет элементы в таблице.
С подробным описанием сигнатур каждого метода вы можете познакомиться в справочнике API.
Совместимость API даёт возможность пользователю работать с YDB при помощи инструментов AWS, таких как утилиты командной строки AWS CLI и AWS SDK для всех языков программирования, а также переносить свои приложения из AWS DynamoDB в YDB.
Давайте пройдём базовый сценарий работы с данными: создадим таблицу, запишем в неё данные и прочитаем их.
Создайте базу данных YDB в бессерверном режиме.
Перейдите на вкладку Обзор созданной БД.
Сохраните значение эндпойнта, указанное в блоке Document API эндпоинт.
Для удобства сохраните значение эндпойнта в переменной окружения с помощью команды export.
export ENDPOINT=<значение_endpoint> 
Рекомендованным способом использования Document API являются AWS CLI и AWS SDK, с которыми вы познакомитесь в дальнейших уроках. Здесь же мы используем утилиту cURL как простой пример применения низкоуровневого HTTP API.
Выполните команду:
curl \
  -H 'X-Amz-Target: DynamoDB_20120810.CreateTable' \
  -H "Authorization: Bearer $(yc iam create-token)" \
  -H "Content-Type: application.json" \
  -d '{"TableName": "docapitest/series","AttributeDefinitions":[{"AttributeName": "series_id", "AttributeType": "N"},{"AttributeName": "title", "AttributeType": "S"}],"KeySchema":[{"AttributeName": "series_id", "KeyType": "HASH"},{"AttributeName": "title", "KeyType": "RANGE"}]}' \
  $ENDPOINT 
В консоли управления убедитесь, что таблица series появилась в директории docapitest указанной базы.
Добавьте запись в базу с помощью следующей команды:
curl \
  -H 'X-Amz-Target: DynamoDB_20120810.PutItem' \
  -H "Authorization: Bearer $(yc iam create-token)" \
  -H "Content-Type: application.json" \
  -d '{"TableName": "docapitest/series", "Item": {"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}, "series_info": {"S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."}, "release_date": {"S": "2006-02-03"}}}' \
  $ENDPOINT 
Прочитайте запись с помощью команды:
curl \
  -H 'X-Amz-Target: DynamoDB_20120810.GetItem' \
  -H "Authorization: Bearer $(yc iam create-token)" \
  -H "Content-Type: application.json" \
  -d '{"TableName": "docapitest/series", "Key": {"series_id":{"N":"1"}, "title":{"S":"IT Crowd"}}}' \
  $ENDPOINT 
Удалите созданную ранее таблицу.
curl \
  -H 'X-Amz-Target: DynamoDB_20120810.DeleteTable' \
  -H "Authorization: Bearer $(yc iam create-token)" \
  -H "Content-Type: application.json" \
  -d '{"TableName": "docapitest/series"}' \
  $ENDPOINT 
Мы показали вам, как работать с низкоуровневым HTTP API Amazon DynamoDB в YDB. В следующих уроках рассмотрим практические примеры работы с AWS CLI и AWS SDK, которые упрощают взаимодействие с базой данных.
Правила расчёта стоимости запроса через Document API
При работе через Document API стоимость обработки каждого документа равна стоимости единицы запроса в RU, умноженной на размер документа в блоках. Размер в блоках равен размеру в байтах, делённому на размер блока и округлённому в большую сторону. Запрос несуществующего документа приравнивается к чтению документа размером в 1 блок.
Если запрос обрабатывает несколько документов (например BatchGetItem), итоговая стоимость равна сумме стоимостей по каждому документу. Операции со схемой данных, такие как CreateTable, DeleteTable, DescribeTable, ListTables, не тарифицируются. В таблице ниже приведён пример расчёта стоимости для различных вызовов Document API.
Task:
Что означает совместимость с AWS DynamoDB API? Выберите все правильные варианты:
Decision:
+Доступность документных таблиц
-Возможность обращаться к таблицам, хранящимся в AWS DynamoDB.
+Доступность совместимого API для работы с документными таблицами
+Возможность использовать AWS CLI и AWS SDK для создания приложений
Task:
Практика. Загрузка данных, выполнение запросов AWS CLI
Decision:
а предыдущем уроке мы рассмотрели, как работать с YDB через Document API — низкоуровневый HTTP API, совместимый с AWS DynamoDB API. В этом уроке рассмотрим операции создания таблицы, записи, чтения, изменения и удаления данных в таблице с помощью AWS CLI.
Сервисный аккаунт и ключ доступа
Для работы инструментов AWS вам понадобится создать сервисный аккаунт в облаке.
Выберите вкладку Сервисные аккаунты в каталоге, где расположена БД.
Нажмите кнопку Создать сервисный аккаунт.
Введите имя сервисного аккаунта. Чтобы назначить сервисному аккаунту роль на текущий каталог, нажмите Добавить роль и выберите роль, например editor.
Нажмите кнопку Создать.
Выберите созданный сервисный аккаунт и нажмите на строку с его именем. Нажмите кнопку Создать новый ключ на верхней панели. Выберите пункт Создать статический ключ доступа.
Сохраните идентификатор и секретный ключ.
Работа с AWS CLI
Установите AWS CLI с сайта https://aws.amazon.com/ru/cli/.
Для Windows: загрузите и запустите 64- или 32-разрядный установщик.
Для Mac и Linux: установите AWS CLI с помощью утилиты pip (требуется Python 2.6.5 или более поздней версии).
pip install awscli 
Для настройки AWS CLI  запустите команду:
aws configure 
Введите сохраненные значения идентификатора ключа AWS Access Key ID и ключа AWS Secret Access Key и укажите ru-central1 в качестве Default region name.
Убедитесь, что в качестве переменной окружения ENDPOINT указано корректное значение эндпойнта вашей базы данных, либо добавьте его, как вы это делали в прошлом уроке: сохраните значение эндпойнта, указанное в строке Document API эндпоинт, в переменной окружения с помощью команды
export ENDPOINT=<значение endpoint> 
Создание таблицы
Создайте таблицу с помощью команды:
aws dynamodb create-table \
  --table-name docapitest/series \
  --attribute-definitions \
  AttributeName=series_id,AttributeType=N \
  AttributeName=title,AttributeType=S \
  --key-schema \
  AttributeName=series_id,KeyType=HASH \
  AttributeName=title,KeyType=RANGE \
  --endpoint $ENDPOINT 
Убедитесь, что в директории docapitest появилась таблица series.
Добавление данных в таблицу
Добавьте в таблицу две строки c помощью команд:
aws dynamodb put-item \
  --table-name docapitest/series \
  --item '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}, "series_info": {"S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."}, "release_date": {"S": "2006-02-03"}}' \
  --endpoint $ENDPOINT 
и
aws dynamodb put-item \
  --table-name docapitest/series \
  --item '{"series_id": {"N": "2"}, "title": {"S": "Silicon Valley"}, "series_info": {"S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."}, "release_date": {"S": "2014-04-06"}}' \
  --endpoint $ENDPOINT 
Чтение данных из таблицы
Для того чтобы прочитать данные из таблицы, выполните команду:
aws dynamodb get-item --consistent-read \
  --table-name docapitest/series \
  --key '{"series_id": {"N": "1"}, "title": {"S": "IT Crowd"}}' \
  --endpoint $ENDPOINT 
В качестве вывода вы увидите:
{
    "Item": {
        "release_date": {
            "S": "2006-02-03"
        },
        "series_id": {
            "N": "1"
        },
        "series_info": {
            "S": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris ODowd, Richard Ayoade, Katherine Parkinson, and Matt Berry."
        },
        "title": {
            "S": "IT Crowd"
        }
    }
} 
Для того, чтобы выбрать данные из таблицы series по ключу series_id, выполните следующую команду:
aws dynamodb query \
  --table-name docapitest/series \
  --key-condition-expression "series_id = :name" \
  --expression-attribute-values '{":name":{"N":"2"}}' \
  --endpoint $ENDPOINT 
В качестве результата вы увидите:
{
    "Items": [
        {
            "release_date": {
                "S": "2014-04-06"
            },
            "series_id": {
                "N": "2"
            },
            "series_info": {
                "S": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky."
            },
            "title": {
                "S": "Silicon Valley"
            }
        }
    ],
    "Count": 1,
    "ScannedCount": 1,
    "ConsumedCapacity": null
} 
Удаление таблицы
aws dynamodb delete-table \
  --table-name docapitest/series \
  --endpoint $ENDPOINT 
На следующем практическом занятии мы разберём пример использования AWS SDK для работы с YDB в serverless-режиме.
Decision:

Task:
Практическая работа. Запуск тестового приложения
Decision:
В предыдущем уроке вы прошли подготовительные этапы: создали и настроили сервисный аккаунт, выпустили статический ключ, а также научились работать с таблицами и данными с помощью низкоуровневого API и CLI.
В этом уроке вы продолжите работу с инструментами AWS и с помощью AWS SDK для языка Python научитесь выполнять такие базовые операции, как создание таблиц БД, запись и чтение данных.
Для выполнения работы вам понадобится Python версии 3.6 и выше и библиотека boto3.
Установить эту библиотеку можно с помощью команды:
pip install boto3 
Создание таблицы
Создайте файл с именем SeriesCreateTable.py и скопируйте в него исходный код программы:
import boto3
def create_series_table():
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.create_table(
        TableName = 'docapitest/series', # Series — имя таблицы 
        KeySchema = [
            {
                'AttributeName': 'series_id',
                'KeyType': 'HASH'  # Ключ партицирования
            },
            {
                'AttributeName': 'title',
                'KeyType': 'RANGE'  # Ключ сортировки
            }
        ],
        AttributeDefinitions = [
            {
                'AttributeName': 'series_id',
                'AttributeType': 'N'  # Целое число
            },
            {
                'AttributeName': 'title',
                'AttributeType': 'S'  # Строка
            },
        ]
    )
    return table
if __name__ == '__main__':
    series_table = create_series_table()
    print("Table status:", series_table.table_status) 
Отредактируйте исходный код файла и укажите значение endpoint_url вашей базы. Затем запустите написанный код:
python SeriesCreateTable.py 
С помощью консоли управления убедитесь, что  в директории docapitest появилась таблица series.
Первоначальная загрузка данных
Для того чтобы вставить данные в созданную таблицу series, создайте файл с именем SeriesLoadData.py и скопируйте в него следующий исходный код программы:
from decimal import Decimal
import json
import boto3
def load_series(series):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document API эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    for serie in series:
        series_id = int(serie['series_id'])
        title = serie['title']
        print("Series added:", series_id, title)
        table.put_item(Item = serie)
if __name__ == '__main__':
    with open("seriesdata.json") as json_file:
        serie_list = json.load(json_file, parse_float = Decimal)
    load_series(serie_list) 
Отредактируйте файл SeriesLoadData.py и укажите значение endpoint_url вашей базы.
Для загрузки данных приложение будет использовать данные, которые записаны в файл seriesdata.json. Создайте этот файл и скопируйте в него описание сериалов:
[{
    "series_id": 1,
    "title": "IT Crowd",
    "info": {
      "release_date": "2006-02-03T00:00:00Z",
      "series_info": "The IT Crowd is a British sitcom produced by Channel 4, written by Graham Linehan, produced by Ash Atalla and starring Chris O'Dowd, Richard Ayoade, Katherine Parkinson, and Matt Berry"
    }
  },
  {
    "series_id": 2,
    "title": "Silicon Valley",
    "info": {
      "release_date": "2014-04-06T00:00:00Z",
      "series_info": "Silicon Valley is an American comedy television series created by Mike Judge, John Altschuler and Dave Krinsky. The series focuses on five young men who founded a startup company in Silicon Valley"
    }
  },
  {
    "series_id": 3,
    "title": "House of Cards",
    "info": {
      "release_date": "2013-02-01T00:00:00Z",
      "series_info": "House of Cards is an American political thriller streaming television series created by Beau Willimon. It is an adaptation of the 1990 BBC miniseries of the same name and based on the 1989 novel of the same name by Michael Dobbs"
    }
  },
  {
    "series_id": 3,
    "title": "The Office",
    "info": {
      "release_date": "2005-03-24T00:00:00Z",
      "series_info": "The Office is an American mockumentary sitcom television series that depicts the everyday work lives of office employees in the Scranton, Pennsylvania, branch of the fictional Dunder Mifflin Paper Company"
    }
  },
  {
    "series_id": 3,
    "title": "True Detective",
    "info": {
      "release_date": "2014-01-12T00:00:00Z",
      "series_info": "True Detective is an American anthology crime drama television series created and written by Nic Pizzolatto. The series, broadcast by the premium cable network HBO in the United States, premiered on January 12, 2014"
    }
  },
  {
    "series_id": 4,
    "title": "The Big Bang Theory",
    "info": {
      "release_date": "2007-09-24T00:00:00Z",
      "series_info": "The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom served as executive producers on the series, along with Steven Molaro"
    }
  },
  {
    "series_id": 5,
    "title": "Twin Peaks",
    "info": {
      "release_date": "1990-04-08T00:00:00Z",
      "series_info": "Twin Peaks is an American mystery horror drama television series created by Mark Frost and David Lynch that premiered on April 8, 1990, on ABC until its cancellation after its second season in 1991 before returning as a limited series in 2017 on Showtime"
    }
  }
] 
Запустите программу (для её успешного выполнения может понадобиться указать в скрипте SeriesLoadData.py полный путь к файлу seriesdata.json):
python SeriesLoadData.py 
В результате выполнения вы увидите вывод программы:
Series added: 1 IT Crowd
Series added: 2 Silicon Valley
Series added: 3 House of Cards
Series added: 3 The Office
Series added: 3 True Detective
Series added: 4 The Big Bang Theory
Series added: 5 Twin Peaks 
Работа с записями
Создание записи
Теперь создайте файл SeriesItemPut.py и скопируйте в него следующий код:
from pprint import pprint
import boto3
def put_serie(series_id, title, release_date, series_info):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    response = table.put_item(
      Item = {
            'series_id': series_id,
            'title': title,
            'info': {
                'release_date': release_date,
                'series_info': series_info
            }
        }
    )
    return response
if __name__ == '__main__':
    serie_resp = put_serie(3, "Supernatural", "2015-09-13",
                          "Supernatural is an American television series created by Eric Kripke")
    print("Series added successfully:")
    pprint(serie_resp, sort_dicts = False) 
В результате выполнения этого кода в таблице добавится запись о сериале Supernatural.
Чтение записи
Создайте файл SeriesItemGet.py и скопируйте в него следующий код:
from pprint import pprint
import boto3
from botocore.exceptions import ClientError
def get_serie(title, series_id):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    try:
        response = table.get_item(Key = {'series_id': series_id, 'title': title})
    except ClientError as e:
        print(e.response['Error']['Message'])
    else:
        return response['Item']
if __name__ == '__main__':
    serie = get_serie("Supernatural", 3,)
    if serie:
        print("Record read:")
        pprint(serie, sort_dicts = False) 
Результатом будет сообщение в формате JSON с данными о сериале.
Обновление записи
В файле SeriesItemUpdate.py разместите код обновления записи:
from decimal import Decimal
from pprint import pprint
import boto3
def update_serie(title, series_id, release_date,  rating):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    response = table.update_item(
        Key = {
            'series_id': series_id,
            'title': title
        },
        UpdateExpression = "set info.release_date = :d, info.rating = :r ",
        ExpressionAttributeValues = {
            ':d': release_date,
            ':r': Decimal(rating)
        },
        ReturnValues = "UPDATED_NEW"
    )
    return response
if __name__ == '__main__':
    update_response = update_serie(
        "Supernatural", 3, "2005-09-13", 8)
    print("Series updated:")
    pprint(update_response, sort_dicts = False) 
Результатом будет сообщение в формате JSON с измененными данными.
Удаление записи
Создайте файл SeriesItemDelete.py и скопируйте в него следующий код:
from decimal import Decimal
from pprint import pprint
import boto3
from botocore.exceptions import ClientError
def delete_underrated_serie(title, series_id, rating):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    try:
        response = table.delete_item(
            Key = {
                'series_id': series_id,
                'title': title
            },
            ConditionExpression = "info.rating <= :val",
            ExpressionAttributeValues = {
                ":val": Decimal(rating)
            }
        )
    except ClientError as e:
        if e.response['Error']['Code'] == "ConditionalCheckFailedException":
            print(e.response['Error']['Message'])
        else:
            raise
    else:
        return response
if __name__ == '__main__':
    print("Deleting...")
    delete_response = delete_underrated_serie("Supernatural", 3, 8)
    if delete_response:
        print("Series data deleted:")
        pprint(delete_response, sort_dicts = False) 
Убедитесь, что данные о сериале Supernatural удалены из таблицы.
Поиск по ключам партицирования и сортировки
Код поиска разместите в новом файле SeriesQuery.py:
from pprint import pprint
import boto3
from boto3.dynamodb.conditions import Key
def query_and_project_series(series_id, title_range):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    response = table.query(
        ProjectionExpression = "series_id, title, info.release_date",
        KeyConditionExpression = Key('series_id').eq(series_id) & Key('title').begins_with(title_range)
    )
    return response['Items']
if __name__ == '__main__':
    query_id = 3
    query_range = 'T'
    print(f"Series with ID = {query_id} and names beginning with "
          f"{query_range}")
    series = query_and_project_series(query_id, query_range)
    for serie in series:
        print(f"\n{serie['series_id']} : {serie['title']}")
        pprint(serie['info']) 
Результатом будет сообщение:
Series with ID = 3 and names beginning with T
3 : The Office
{'release_date': '2005-03-24T00:00:00Z'}
3 : True Detective
{'release_date': '2014-01-12T00:00:00Z'} 
Запуск операции Scan
Создайте файл SeriesTableScan.py и скопируйте в него следующий код:
from pprint import pprint
import boto3
from boto3.dynamodb.conditions import Key
def scan_series(id_range, display_series):
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")
    table = ydb_docapi_client.Table('docapitest/series')
    scan_kwargs = {
        'FilterExpression': Key('series_id').between(*id_range),
        'ProjectionExpression': "series_id, title, info.release_date"
    }
    done = False
    start_key = None
    while not done:
        if start_key:
            scan_kwargs['ExclusiveStartKey'] = start_key
        response = table.scan(**scan_kwargs)
        display_series(response.get('Items', []))
        start_key = response.get('LastEvaluatedKey', None)
        done = start_key is None
if __name__ == '__main__':
    def print_series(series):
        for serie in series:
            print(f"\n{serie['series_id']} : {serie['title']}")
            pprint(serie['info'])
    query_range = (1, 3)
    print(f"Series with IDs from {query_range[0]} to {query_range[1]}...")
    scan_series(query_range, print_series) 
Результатом будет сообщение:
Series with IDs from 1 to 3...
3 : House of Cards
{'release_date': '2013-02-01T00:00:00Z'}
3 : The Office
{'release_date': '2005-03-24T00:00:00Z'}
3 : True Detective
{'release_date': '2014-01-12T00:00:00Z'}
1 : IT Crowd
{'release_date': '2006-02-03T00:00:00Z'}
2 : Silicon Valley
{'release_date': '2014-04-06T00:00:00Z'} 
Удаление таблицы
Создайте файл SeriesTableDelete.py и скопируйте в него следующий код:
import boto3
def delete_serie_table():
    ydb_docapi_client = boto3.resource('dynamodb', endpoint_url = "<Document_API_эндпоинт>")

    table = ydb_docapi_client.Table('docapitest/series')
    table.delete()
if __name__ == '__main__':
    delete_serie_table()
    print("Table Series deleted") 
Убедитесь, что таблица удалена из базы данных.
Поздравляем, вы завершили изучение темы Serverless Yandex Database!
Decision:

Task:
Что такое очереди
Decision:
Зачем нужны очереди
Предположим, вы создаете поисковую систему. У вас есть роботы (поставщики), которые собирают ссылки и передают их обработчикам (потребителям) для разбора страниц и записи результата в базу данных. Самый простой способ передавать ссылки от роботов к обработчикам — вызывать обработчики напрямую из роботов. Однако, такая реализация имеет ряд недочётов, например:
    Как правило, роботы работают быстрее обработчиков: данные будут накапливаться внутри роботов, а значит, их память или диск будут быстро переполняться.
    Роботам нужно знать рабочий интерфейс обработчиков, который может меняться по мере совершенствования системы, а значит, будет нужно адаптировать и код роботов.
    Нет гарантии, что обработчик разберет ссылку целиком.
Практически все эти проблемы решаются при помощи очередей, т. е. последовательности некоторой информации. Главная задача очередей — организовать независимую работу поставщиков и потребителей информации в системах реального времени. Вот что это означает в нашем примере.
Робот пишет сообщение в очередь: «У меня есть новая ссылка, вот она». Обработчик ссылок читает записанное сообщение из очереди и забирает себе ссылку на разбор. Чтобы другой обработчик не начал обрабатывать ту же самую ссылку, очередь скрывает это сообщение на некоторый промежуток времени (таймаут видимости), достаточный для его обработки потребителем.
Если сообщение обработано успешно, оно удаляется из очереди, а обработчик забирает себе следующее. Если обработчик вышел из строя или произошёл обрыв соединения, по истечении таймаута видимости сообщение снова становится доступным в очереди, и его может взять другой обработчик.
Таким образом, очередь служит буфером между поставщиком и потребителем, позволяя развязать два параллельных процесса. Если потребитель начинает медленнее вычитывать элементы, то очередь будет их накапливать. А шанс догнать процесс и обработать данные побыстрее даст потом.
Очереди используются внутри программ при взаимодействии потоков и в организации больших систем, где взаимодействуют несколько программ или сервисов. В таких системах очереди помогают решать проблемы интеграции приложений, их отказоустойчивости и масштабирования, например:
    надёжной передачи данных и команд между компонентами;
    обработки данных от множества устройств IoT;
    обработки событий, по которым должна быть вызвана функция.
Стоит учитывать, что у очередей есть ряд ограничений. Главным из них является неопределённое время на обработку принимающей стороной. Сложно предсказать, когда потребитель получит и обработает элемент.
Виды очередей
Существует два распространённых типа очередей: FIFO и стандартная.
FIFO означает «first in, first out», т. е. порядок размещения данных в очереди совпадает с порядком, в котором данные из нее считываются. Очереди FIFO используются в тех случаях, когда нужно обеспечить строгий порядок доставки и однократную обработку сообщений. Например, соблюдение исходного порядка важно для финансовых транзакций. Если представить, что у клиента на счету было 2 000 рублей, и он сначала положил туда 10 000, а затем потратил 5 000, то очевидно, что порядок выполнения транзакций имеет значение. Стоит отметить, что у очередей FIFO сравнительно небольшое количество вызовов API в секунду.
В свою очередь из стандартных очередей сообщения по возможности считываются последовательно, но соблюдение исходного порядка при доставке сообщений не гарантируется. Преимуществом этого подхода является более высокая в сравнении с FIFO пропускная способность: стандартные очереди поддерживают сравнительно большое количество вызовов API в секунду (отправка, принятие или удаление сообщения).
В Yandex Cloud сервисом очередей является Yandex Message Queue. Он относится к PaaS-слою и объединён с другими serverless-сервисами в группу Бессерверные вычисления. В следующем уроке мы поговорим о его специфике.
Task:
Как называется тип очередей, который сохраняет последовательность элементов?
Decision:
-LIFO
+FIFO
-UFO
-Stack
Task:
В каких задачах полезны очереди?
Decision:
-Хранения данных
-Обработки данных
+Передачи данных
-Генерации данных
Task:
Сколько поставщиков и потребителей может быть у очереди?
Decision:
-1
-2
-3
+В теории — бесконечное количество. На практике — зависит от конкретной очереди
Task:
Знакомство с Yandex Message Queue
Decision:
На прошлом уроке вы узнали, что такое очереди и зачем они нужны. На этом уроке вы разберётесь со спецификой реализации очередей в Yandex Message Queue и их параметрами.
Что такое сообщение
Основным понятием для очередей в Yandex Message Queue (YMQ) является сообщение. Сообщение состоит из тела — ваших данных — и метаданных, его дополнительных атрибутов.
Тело сообщения обрабатывается вашим приложением, а метаданные удобно использовать для других самых разнообразных целей: например, для подтверждения обработки сообщения, оценки времени его обработки или чтобы удостовериться, что оно было передано без изменений.
Представим, что мы считали это простое сообщение из очереди, давайте рассмотрим его содержимое:
    Body — т. е. тело сообщения. Здесь хранится то, что собственно передаётся.
    Attributes — набор атрибутов сообщения, указывающих время первого получения (время UNIX), количество попыток обработки этого сообщения, время отправки (время UNIX).
    ReceiptHandle — идентификатор получения. Он указывает на факт получения сообщения и назначается системой при его считывании. Этот идентификатор используется для удаления полученного сообщения из очереди или изменения его таймаута видимости.
    MD5OfBody — хеш-сумма тела сообщения, созданная 128-битным алгоритмом хеширования MD5.
    MessageId — уникальный идентификатор сообщения. Он возвращается вам из YMQ, когда вы отправляете сообщение. Этот идентификатор удобно использовать для различной диагностики.
Как работает сервис YMQ
Задача YMQ — организовать очередь между приложениями-отправителями и приложениями получателями сообщений. Чтобы отправить и получить сообщение, отправители и получатели должны обращаться к сервису сами.
Каждое сообщение за время жизни проходит следующие этапы:
    отправка в очередь;
    хранение в очереди, пока оно не будет считано или пока не истечёт время хранения;
    чтение сообщения потребителем и пометка сообщения на это время, как находящегося в обработке;
    удаление из очереди, если сообщение было успешно обработано или перенесено в Dead Letter Queue:
Типы очередей в YMQ
Сервис YMQ поддерживает два типа очередей — стандартные и FIFO. На предыдущем уроке мы уже разобрали их основные отличия. Давайте остановимся теперь на них подробнее.
Стандартные очереди позволяют сохранять сообщения, которые затем читаются приложениями в произвольном порядке. Такой подход упрощает систему обработки сообщений. Приложения должны быть рассчитаны на ситуации приёма данных не в хронологическом порядке их поступления.
Стандартные очереди обеспечивают гарантию, что каждое сообщение будет доставлено до получателя хотя бы один раз. В исключительных случаях данные могут быть доставлены до считывающего приложения несколько раз. Обрабатывающие системы должны быть готовы к подобным ситуациям. Такие очереди лучше подходят для обработки не связанных между собой сообщений и обеспечивают более высокую пропускную способность, то есть работают быстрее.
Очереди FIFO позволяют обеспечить строгую очерёдность выдачи сообщений запрашивающей/обрабатывающей стороне и обеспечивают семантику строгой однократной гарантированной доставки сообщений. Такие очереди подходят для передачи связанных сообщений и работают медленнее из-за того, что сообщения должны быть обработаны по очереди.
FIFO очереди часто используются для обработки финансовых данных. Представьте, что в одну FIFO очередь отправляются действия с банковскими счетами разных пользователей. Данные из очереди обрабатывают несколько получателей, и было бы удобно, чтобы все действия одного пользователя попадали одному получателю. Для такой цели можно использовать группировку сообщений.
При помощи специальных идентификаторов группы можно обеспечить отправку сразу нескольких потоков упорядоченных сообщений для разных получателей сообщений в рамках одной очереди FIFO. Вместо одной очереди FIFO получается несколько очередей по количеству групп. В каждой группе запись сообщений и их считывание происходит по схеме FIFO.
Параметры очередей
Давайте разберём, какие параметры бывают у очередей в Yandex Message Queue.
В Базовых параметрах помимо имени и типа очереди вы можете указать:
    Стандартный таймаут видимости. Это время, на которое сообщение скрывается из очереди после чтения получателем. Пока сообщение скрыто, другие получатели не могут получить сообщение из очереди. Минимальный таймаут видимости — 30 секунд, максимальный — 12 часов.
    Срок хранения сообщений. Вы можете указать, как долго каждое сообщение может храниться в очереди в ожидании чтения получателем. Это значение должно быть в промежутке от 60 секунд до 14 дней.
    Максимальный размер сообщения. Может составлять от 1 до 256 КБ.
    Задержка доставки. Иногда нужно, чтобы обработчик получил сообщение не сразу, а позже. Здесь вы можете указать время, в течение которого новое сообщение нельзя получить из очереди. Значение должно быть в промежутке от 0 секунд до 15 минут.
    Время ожидания при получении сообщения. В течение этого времени получатель будет ожидать поступления сообщений. Если в очереди появятся сообщения, вызов будет сделан раньше, чем указано в этой настройке. Если же по истечении этого времени сообщения не появились, будет возвращен пустой список.
В блоке Настройки очередей недоставленных сообщений вы можете настроить работу с так называемой Dead Letter Queue (DLQ, дословно — очередь невостребованных писем). Это специальная очередь, куда могут перенаправляться сообщения, которые получатели не смогли обработать в обычных очередях. Собирая такие сообщения в отдельной очереди, вы можете исследовать ошибки, возникающие при их обработке.
Чтобы воспользоваться этой функцией, вам придется сначала завести отдельную очередь того же типа, что и очередь, откуда перенаправляются сообщения, которые дошли до адресата, но не были обработаны. Включите функцию Перенаправлять недоставленные сообщения, выберите заранее созданную DLQ, а затем укажите количество попыток, после которых необработанное сообщение направляется в эту очередь.
Как можно работать с очередями в Yandex Message Queue
Помещение данных в очередь выполняется при помощи программных вызовов через специальный API, либо с использованием AWS CLI. Пример использования AWS CLI с YMQ вы можете найти в документации.
YMQ поддерживает API и другие подходы, которые используют в сервисе Amazon SQS, поэтому для работы с ними вы можете использовать уже существующие инструменты, например библиотеки boto3 для Python .
Тарификация
В рамках сервиса Message Queue тарифицируется количество запросов к стандартным очередям и очередям FIFO, а также исходящий трафик. Для целей тарификации каждые 64 КБ данных запроса считаются отдельным запросом.
Первые 100 000 запросов в месяц к очередям любого типа не оплачиваются. А далее их стоимость зависит от типа очереди: для стандартных очередей это 48,7600 ₽ за 1 миллион запросов, для FIFO — 61,1500 ₽.
Исходящий трафик до 10 ГБ не тарифицируется, а затем оплата за него  составляет 1,5300 p за 1 ГБ.
Допустим, за месяц было сделано 2,75 млн запросов объемом 48 КБ к стандартной очереди. Это значит, что нам нужно вычесть нетарифицируемое количество запросов из общего и привести его к тарифу за миллион запросов, а затем приплюсовать исходящий трафик исходя из того, что каждое сообщение умещается в 64 КБ, приведя размер каждого сообщения к гигабайтам. Считаем:
48,7600*((2750000−100000)/1000000)+1,53×((2750000*48/1024/1024)−10)=129,214+177,30=306,514 p
Более подробную информацию вы найдёте в документации.
На следующем уроке вы расширите созданное ранее приложение для проверки доступности yandex.ru, добавив в него возможность ставить задачи по проверке доступности других веб-ресурсов при помощи очередей.
Task:
Что происходит с сообщением, когда получатель начал обрабатывать сообщение?
Decision:
-Передаётся другому получателю
+Остаётся в очереди, но скрывается от других
-Удаляется
Task:
Какие типы очередей поддерживает сервис YMQ?
Decision:
-Стандартные
-По модели FIFO
+Оба типа
Task:
Практическая работа. Проверка доступности веб-ресурсов
Decision:
В этом уроке вы доработаете систему проверки доступности веб-ресурсов, которую создали на предыдущих практических занятиях. В текущем варианте она проверяет только доступность сайта yandex.ru. Теперь давайте добавим в неё возможность ставить задачи по проверке доступности других веб-ресурсов.
Общая архитектура системы
У системы есть два метода:
    CheckUrl — ставит задачу на проверку указанного URL.
    GetResult — считывает результаты проверки.
Метод CheckUrl обрабатывается функцией, которая будет складывать все запросы в очередь. Функция-обработчик будет вызываться раз в секунду, считывать URL из очереди,  проверять его доступность и записывать результат в базу данных. Оттуда этот результат можно будет получить с помощью метода GetResult.
Мы не будем менять уже созданные функции и таблицу в PostgreSQL, сделаем новые.
Работать с YMQ из функций мы будем с помощью библиотеки boto3. Чтобы её использовать, нужно создать сервисный аккаунт с секретным ключом доступа, а затем настроить зависимости функции. Сделаем это после того, как создадим очередь.
Шаг 1. Проверить наличие сервисного аккаунта
Если вы ранее создавали сервисный аккаунт с именем service-account-for-cf, добавляли вновь созданному сервисному аккаунту роли editor и другие, то вам остаётся только создать ключ доступа:
yc iam access-key create --service-account-name service-account-for-cf 
В результате вы получите примерно следующее:
    access_key:
        id: ajefraollq5puj2tir1o
        service_account_id: ajetdv28pl0a1a8r41f0
        created_at: "2021-08-23T21:13:05.677319393Z"
        key_id: BTPNvWthv0ZX2xVmlPIU
    secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
Здесь key_id — это идентификатор ключа доступа ACCESS_KEY. А secret — это секретный ключ SECRET_KEY. Переменные ACCESS_KEY и SECRET_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
Шаг 2. Создание очереди Yandex Message Queue
Вы можете создать очередь одним из трёх способов:
    через консоль управления;
    с помощью консольной утилиты aws;
    с помощью Terraform.
В этом уроке мы будем использовать консоль управления. Откройте раздел Message Queue и нажмите кнопку Создать очередь.
В настройках создаваемой очереди задайте имя очереди my-first-queue, затем выберите тип очереди Стандартная и нажмите кнопку Создать.
Очередь создана.
Теперь зайдите в настройки очереди, чтобы посмотреть параметры подключения к ней. Нам потребуется значение URL.
Шаг 3. Создание функции
Для создания функции зададим ряд переменных:
    VERBOSE_LOG — определяет, пишет ли функция подробности своего выполнения в журнал.
    AWS_ACCESS_KEY_ID — значение «Идентификатор ключа» из сервисного аккаунта, который мы сделали ранее.
    AWS_SECRET_ACCESS_KEY — значение «Секретный ключ» из того же сервисного аккаунта.
    QUEUE_URL — URL на очередь, его можно получить на обзорной странице созданной ранее очереди.
Чтобы задать переменные, в консоли выполните следующие команды:
echo "export VERBOSE_LOG=True" >> ~/.bashrc && . ~/.bashrc
echo "export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>" >> ~/.bashrc && . ~/.bashrc
echo "export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>" >> ~/.bashrc && . ~/.bashrc
echo "export QUEUE_URL=<QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc 
Воспользуйтесь командой pipreqs $PWD --force для формирования файла requirements.txt. Затем создайте функцию my-url-receiver-function.py:
import logging
import os
import boto3
logger = logging.getLogger()
logger.setLevel(logging.INFO)
verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
queue_url = os.environ['QUEUE_URL']
def log(logString):
    if verboseLogging:
        logger.info(logString)
def handler(event, context):
    # Get url
    try:
        url = event['queryStringParameters']['url']
    except Exception as error:
        logger.error(error)
        statusCode = 400
        return {
            'statusCode': statusCode
        }
    # Create client
    client = boto3.client(
        service_name='sqs',
        endpoint_url='https://message-queue.api.cloud.yandex.net',
        region_name='ru-central1'
    )
    # Send message to queue
    client.send_message(
        QueueUrl=queue_url,
        MessageBody=url
    )
    log('Successfully sent test message to queue')
    statusCode = 200
    return {
        'statusCode': statusCode
    } 
Перейдите в директорию с исходными файлами и упакуйте файлы с функцией и требованиями в ZIP-архив. При этом сразу задайте все необходимые переменные и сервисный аккаунт:
zip my-url-receiver-function my-url-receiver-function.py requirements.txt
yc serverless function create \
  --name  my-url-receiver-function \
  --description "function for url"
yc serverless function version create \
  --function-name=my-url-receiver-function \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=my-url-receiver-function.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --environment VERBOSE_LOG=$VERBOSE_LOG \
  --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  --environment QUEUE_URL=$QUEUE_URL \
  --source-path my-url-receiver-function.zip 
Тестирование функции
Находясь в вашем рабочем каталоге, перейдите в раздел Cloud Functions консоли управления и выберите ранее созданную функцию my-url-receiver-function. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон HTTPS-вызов и замените раздел queryStringParameters:
    "queryStringParameters": {
        "a": "2",
        "b": "1",
    },     
на аналогичный, но с параметром url с любым сайтом. Важно указывать ссылку целиком.
    "queryStringParameters": {
        "url": "https://ya.ru/"
    },     
Нажмите кнопку Запустить тест.
Если вы всё сделали правильно, то увидите код статуса 200. При этом в очереди увеличится количество сообщений.
Шаг 4. Обновление спецификации API Gateway
Функция готова, но по умолчанию она не является публичной. Предоставим доступ к ней с помощью API-шлюза. Для этого необходимо обновить ранее созданную спецификацию hello-world.yaml. Если у вас нет её под рукой, выгрузите её из облака:
yc serverless api-gateway get-spec \
  --name hello-world >> hello-world-new.yaml 
Внесите изменения, добавив секцию о ранее созданной функции:
    /check:
        get:
            x-yc-apigateway-integration:
                type: cloud-functions
                function_id: <идентификатор функции>
                service_account_id: <идентификатор сервисного аккаунта>
            operationId: add-url 
Обновите конфигурацию:
yc serverless api-gateway update \
  --name hello-world \
  --spec=hello-world-new.yaml 
Для тестирования выполните вызов функции в браузере:
https://<идентификатор API Gateway>.apigw.yandexcloud.net/check?url=https://ya.ru/ 
После каждого запроса количество сообщений в очереди будет увеличиваться на одно.
Шаг 5. Создание функции для чтения из очереди
В предыдущих работах мы создавали функцию, использующую подключение к БД. Здесь мы повторим этот опыт.
Проверим, что нам доступны переменные для инициации подключения: CONNECTION_ID, DB_USER, DB_HOST. Мы создавали их в предыдущей работе с помощью следующих команд:
echo "export CONNECTION_ID=<CONNECTION_ID>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_USER=<DB_USER>" >> ~/.bashrc && . ~/.bashrc
echo "export DB_HOST=<DB_HOST>" >> ~/.bashrc && . ~/.bashrc 
Также для работы с очередью нам потребуются переменные VERBOSE_LOG, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY и QUEUE_URL, заданные на предыдущих шагах.
Создадим функцию function-for-url-from-mq.py и воспользуемся командой pipreqs $PWD --force, чтобы сформировать для нее файл requirements.txt.
import logging
import os
import boto3
import datetime
import requests
#Эти библиотеки нужны для работы с PostgreSQL
import psycopg2
import psycopg2.errors
import psycopg2.extras
CONNECTION_ID = os.getenv("CONNECTION_ID")
DB_USER = os.getenv("DB_USER")
DB_HOST = os.getenv("DB_HOST")
QUEUE_URL = os.environ['QUEUE_URL']
# Настраиваем функцию для записи информации в журнал функции
# Получаем стандартный логер языка Python
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# Вычитываем переменную VERBOSE_LOG, которую мы указываем в переменных окружения 
verboseLogging = eval(os.environ['VERBOSE_LOG'])  ## Convert to bool
#Функция log, которая запишет текст в журнал выполнения функции, если в переменной окружения VERBOSE_LOG будет значение True
def log(logString):
    if verboseLogging:
        logger.info(logString)
#Получаем подключение
def getConnString(context):
    """
    Extract env variables to connect to DB and return a db string
    Raise an error if the env variables are not set
    :return: string
    """
    connection = psycopg2.connect(
        database=CONNECTION_ID, # Идентификатор подключения
        user=DB_USER, # Пользователь БД
        password=context.token["access_token"],
        host=DB_HOST, # Точка входа
        port=6432,
        sslmode="require")
    return connection
"""
    Create SQL query with table creation
"""
def makeCreateDataTableQuery(table_name):
    query = f"""CREATE TABLE public.{table_name} (
    url text,
    result integer,
    time float
    )"""
    return query
def makeInsertDataQuery(table_name, url, result, time):
    query = f"""INSERT INTO {table_name} 
    (url, result,time)
    VALUES('{url}', {result}, {time})
    """
    return query
def handler(event, context):
    # Create client
    client = boto3.client(
        service_name='sqs',
        endpoint_url='https://message-queue.api.cloud.yandex.net',
        region_name='ru-central1'
    )
    # Receive sent message
    messages = client.receive_message(
        QueueUrl=QUEUE_URL,
        MaxNumberOfMessages=1,
        VisibilityTimeout=60,
        WaitTimeSeconds=1
    ).get('Messages')
    if messages is None:
        return {
            'statusCode': 200
        }
    for msg in messages:
        log('Received message: "{}"'.format(msg.get('Body')))
    # Get url from message
    url = msg.get('Body');
    # Check url
    try:
        now = datetime.datetime.now()
        response = requests.get(url, timeout=(1.0000, 3.0000))
        timediff = datetime.datetime.now() - now
        result = response.status_code
    except requests.exceptions.ReadTimeout:
        result = 601
    except requests.exceptions.ConnectTimeout:
        result = 602
    except requests.exceptions.Timeout:
        result = 603
    log(f'Result: {result} Time: {timediff.total_seconds()}')
    connection = getConnString(context)
    log(f'Connecting: {connection}')    
    cursor = connection.cursor()
    table_name = 'custom_request_result'
    sql = makeInsertDataQuery(table_name, url, result, timediff.total_seconds())
    log(f'Exec: {sql}')
    try:
        cursor.execute(sql)
    except psycopg2.errors.UndefinedTable as error:
        log(f'Table not exist - create and repeate insert')
        connection.rollback()
        logger.error(error)
        createTable = makeCreateDataTableQuery(table_name)
        log(f'Exec: {createTable}')
        cursor.execute(createTable)
        connection.commit()
        log(f'Exec: {sql}')
        cursor.execute(sql)
    except Exception as error:
        logger.error( error)
    connection.commit()
    cursor.close()
    connection.close()
    # Delete processed messages
    for msg in messages:
        client.delete_message(
            QueueUrl=QUEUE_URL,
            ReceiptHandle=msg.get('ReceiptHandle')
        )
        print('Successfully deleted message by receipt handle "{}"'.format(msg.get('ReceiptHandle')))
    statusCode = 200
    return {
        'statusCode': statusCode
    } 
При создании сразу задайте все необходимые переменные и сервисный аккаунт:
zip function-for-url-from-mq function-for-url-from-mq.py requirements.txt
yc serverless function create \
  --name function-for-url-from-mq \
  --description "function for url from mq"
yc serverless function version create \
  --function-name=function-for-url-from-mq \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=function-for-url-from-mq.handler \
  --service-account-id $SERVICE_ACCOUNT_ID \
  --environment VERBOSE_LOG=True \
  --environment CONNECTION_ID=$CONNECTION_ID \
  --environment DB_USER=$DB_USER \
  --environment DB_HOST=$DB_HOST \
  --environment AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  --environment AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  --environment QUEUE_URL=$QUEUE_URL \
  --source-path function-for-url-from-mq.zip 
Протестируйте функцию.
После её выполнения количество сообщений в очереди уменьшится, а в базе данных появится новая таблица с результатами тестирования доступности функции.
Шаг 6. Создание триггера
Создадим триггер, который будет вызывать функцию обработки сообщений из очереди один раз в минуту. Он будет использовать cron-выражение:
yc serverless trigger create timer \
  --name trigger-for-mq \
  --invoke-function-name function-for-url-from-mq \
  --invoke-function-service-account-id $SERVICE_ACCOUNT_ID \
  --cron-expression '* * * * ? *' 
Cron-выражение * * * * ? * означает вызов функции function-for-url-from-mq один раз в минуту. Подробнее про cron-выражения можно прочитать в документации.
Теперь у нас есть функция, которая раз в минуту будет пробовать взять из очереди URL и проверить его. Также есть метод REST API, который позволяет записывать URL в очередь независимо от работы обработчика. Мы можем вызывать созданный метод как угодно часто. Очередь будет просто накапливаться, а затем обработчик будет постепенно её разбирать.
В итоге вы получили асинхронную систему проверки доступности URL с доступом по REST API. Вы не создали ни одной виртуальной машины, но решили вопросы масштабирования и отказоустойчивости системы.
Удаление триггера-таймера
По завершении практической работы не забудьте удалить созданный вами триггер trigger-for-mq, иначе он будет работать, пока не исчерпает деньги на аккаунте:
yc serverless trigger delete trigger-for-mq 
Не забудьте удалить или остановить все созданные вами ресурсы: триггеры, очереди YMQ и кластер базы данных.
Следующий практический урок завершает тему. Вы попробуете создать онлайн-сервис, конвертирующий произвольные видеофайлы в GIF-анимацию. Для этого вы объедините в одно решение сервисы Yandex Cloud Functions, Yandex Message Queue, Yandex Database и Yandex Object Storage. А заодно закрепите использование консольных инструментов yc и aws.
Decision:

Task:
Практическая работа. Однократная отправка сообщений
Decision:
В этой практической работе мы реализуем проект, который позволит пользователям конвертировать видеофайлы в GIF. Такая задача хорошо подходит для Cloud Functions, потому что конвертирование отнимает немало ресурсов процессора, и чем качественнее видео, тем больше ресурсов требуется на его обработку.
Почему для решения этой задачи нужны очереди?
Представим, что мы попытались решить эту задачу «в лоб». Пользователь заходит на страницу и вводит ссылку на видеофайл. Сервис скачивает его, конвертирует и отдает ссылку на GIF. Возникают две серьёзные проблемы:
    Синхронное соединение не всегда стабильно. Чем дольше вы его держите, тем выше вероятность, что оно разорвётся. В этом случае всё придётся сделать заново. А если соединение нестабильно, то пользователь может и не дождаться результата.
    Задача ресурсоёмкая: если сервисом одновременно воспользуются много пользователей с большими видеороликами, мощностей может не хватить.
Чтобы избежать этих проблем, в архитектуру сервиса необходимо встроить очередь.
Шаг 1. Сервисный аккаунт и Lockbox
Создание сервисного аккаунта
Создайте сервисный аккаунт с именем ffmpeg-account-for-cf:
export SERVICE_ACCOUNT=$(yc iam service-account create --name ffmpeg-account-for-cf \
  --description "service account for serverless" \
  --format json | jq -r .) 
Проверьте текущий список сервисных аккаунтов:
yc iam service-account list 
После проверки запишите ID созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_ID:
echo "export SERVICE_ACCOUNT_FFMPEG_ID=<ID>" >> ~/.bashrc && . ~/.bashrc
echo $SERVICE_ACCOUNT_FFMPEG_ID 
Назначение роли сервисному аккаунту
Добавим вновь созданному сервисному аккаунту роли storage.viewer, storage.uploader, ymq.reader, ymq.writer, ydb.admin, serverless.functions.invoker, и lockbox.payloadViewer:
echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
echo $FOLDER_ID
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role storage.viewer
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role storage.uploader
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role ymq.reader
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role ymq.writer
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role ydb.admin
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role serverless.functions.invoker
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role lockbox.payloadViewer
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --role editor 
Вы можете назначить несколько ролей и с помощью команды set-access-binding. Но эта команда полностью перезаписывает права доступа к ресурсу и все текущие роли на него будут удалены! Поэтому сначала убедитесь, что ресурсу не назначены роли, которые вы не хотите потерять:
yc resource-manager folder list-access-bindings $FOLDER_ID
yc resource-manager folder set-access-bindings $FOLDER_ID \
  --access-binding role=storage.viewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=storage.uploader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=ymq.reader,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=ymq.writer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=ydb.admin,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=serverless.functions.invoker,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=lockbox.payloadViewer,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID \
  --access-binding role=editor,subject=serviceAccount:$SERVICE_ACCOUNT_FFMPEG_ID 
Создание ключа доступа для сервисного аккаунта
Этот этап нужен для получения идентификатора ключа доступа и секретного ключа, которые будут использованы для загрузки файлов в Object Storage, работы с Yandex Message Queue и т. д. Для создания ключа доступа необходимо вызвать следующую команду:
yc iam access-key create --service-account-name ffmpeg-account-for-cf 
В результате вы получите примерно следующее:
    access_key:
        id: ajefraollq5puj2tir1o
        service_account_id: ajetdv28pl0a1a8r41f0
        created_at: "2021-08-23T21:13:05.677319393Z"
        key_id: BTPNvWthv0ZX2xVmlPIU
    secret: cWLQ0HrTM0k_qAac43cwMNJA8VV_rfTg_kd4xVPi 
Здесь key_id — это идентификатор ключа доступа ACCESS_KEY_ID. А secret — это секретный ключ SECRET_ACCESS_KEY. Переменные ACCESS_KEY_ID и SECRET_ACCESS_KEY могут быть использованы для задания соответствующих значений aws_access_key_id и aws_secret_access_key при использовании библиотеки boto3.
Создание элемента в сервисе Lockbox
В сервисе Lockbox (находится на стадии Preview) создайте ваш первый секрет, состоящий из набора версий, в которых хранятся ваши данные. Версия содержит наборы ключей и значений:
    Ключ — несекретное название для значения, по которому вы будете его идентифицировать.
    Значение — это секретные данные.
Версия не изменяется. Для любого изменения количества пар ключей-значений или их содержимого необходимо создать новую версию. Создадим секрет с именем ffmpeg-sa-key и парой ключей ACCESS_KEY_ID и SECRET_ACCESS_KEY:
yc lockbox secret create --name ffmpeg-sa-key \
  --folder-id $FOLDER_ID \
  --description "keys for serverless" \
  --payload '[{"key": "ACCESS_KEY_ID", "text_value": <ACCESS_KEY_ID>}, {"key": "SECRET_ACCESS_KEY", "text_value": "<SECRET_ACCESS_KEY>"}]' 
Получим и запишем значение SECRET_ID, оно нам потребуется при создании функции:
yc lockbox secret list
yc lockbox secret get --name ffmpeg-sa-key
echo "export SECRET_ID=<SECRET_ID>" >> ~/.bashrc && . ~/.bashrc
echo $SECRET_ID 
Шаг 2. Создание очереди Yandex Message Queue
Для создания очереди Yandex Message Queue вы можете использовать три разных способа:
    консоль управления;
    консольная утилита aws;
    Terraform.
Создание очереди с помощью утилиты aws
Воспользуемся AWS CLI. Для начала задайте конфигурацию с помощью команды aws configure. При этом от вас потребуется ввести:
    AWS Access Key ID — идентификатор ключа доступа key_id сервисного аккаунта, полученный на предыдущем шаге.
    AWS Secret Access Key — секретный ключ secret сервисного аккаунта, полученный на предыдущем шаге.
    Default region name — используйте значение ru-central1.
По завершению конфигурации вы сможете создать очередь:
aws configure
aws sqs create-queue --queue-name ffmpeg --endpoint https://message-queue.api.cloud.yandex.net/ 
В результате успешного выполнения предыдущей команды в ответ вы получите URL:
    {
        "QueueUrl": "https://message-queue.api.cloud.yandex.net/b1ga4gj7agij03ln6aov/dj6000000003kv2t02b3/ffmpeg"
    } 
Запишем значения URL в переменную YMQ_QUEUE_URL. Она потребуется нам при создании функции:
echo "export YMQ_QUEUE_URL=<YMQ_QUEUE_URL>" >> ~/.bashrc && . ~/.bashrc
echo $YMQ_QUEUE_URL 
Ещё вам потребует значение атрибута QueueArn, получим его:
aws sqs get-queue-attributes \
  --endpoint https://message-queue.api.cloud.yandex.net \
  --queue-url $YMQ_QUEUE_URL \
  --attribute-names QueueArn 
В результате вы получите ответ вида:
    {
        "Attributes": {
            "QueueArn": "yrn:yc:ymq:ru-central1:b1gl21bkgss4msekt08i:ffmpeg"
        }
    } 
Сохраним значение QueueArn в переменную YMQ_QUEUE_ARN:
echo "export YMQ_QUEUE_ARN=<YMQ_QUEUE_ARN>" >> ~/.bashrc && . ~/.bashrc
echo $YMQ_QUEUE_ARN 
Шаг 3. Создание базы данных в сервисе Yandex Database
Создадим базу данных YDB с именем ffmpeg и типом serverless, используя для этого флаг --serverless:
yc ydb database create ffmpeg \
  --serverless \
  --folder-id $FOLDER_ID
yc ydb database list 
Сразу получим и сохраним document_api_endpoint в значение переменной DOCAPI_ENDPOINT:
yc ydb database get --name ffmpeg
echo "export DOCAPI_ENDPOINT=<DOCAPI_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
echo $DOCAPI_ENDPOINT 
Как только база данных создана, воспользуемся ранее использованной утилитой AWS CLI для создания документной таблицы в этой базе данных. Всю конфигурацию возьмем из файла tasks.json:
{
  "AttributeDefinitions": [
    {
      "AttributeName": "task_id",
      "AttributeType": "S"
    }
  ],
  "KeySchema": [
    {
      "AttributeName": "task_id",
      "KeyType": "HASH"
    }
  ],
  "TableName": "tasks"
} 
Находясь в одном каталоге с файлом tasks.json, вызовите следующую команду для создания таблицы:
aws dynamodb create-table \
  --cli-input-json file://tasks.json \
  --endpoint-url $DOCAPI_ENDPOINT \
  --region ru-central1 
В консоли управления убедитесь, что БД ffmpeg создана, и в ней есть пустая таблица tasks.
Шаг 4. Создание бакета в сервисе Object Storage
Самый простой способ создания бакета в Object Storage — это использование консоли управления.
В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет. На странице создания бакета введите имя, в нашем примере это будет storage-for-ffmpeg, остальные параметры не меняйте.
Нажмите кнопку Создать бакет для завершения операции. Далее вы всегда сможете поменять класс хранилища, его размер и настройки доступа.
Сохраним название бакета для дальнейшего использования:
echo "export S3_BUCKET=<имя бакета>" >> ~/.bashrc && . ~/.bashrc
echo $S3_BUCKET 
Шаг 5. Создание функций
При создании функций нам потребуется ряд переменных:
    SECRET_ID — идентификатор секрета (можно получить из таблицы со списком секретов);
    YMQ_QUEUE_URL — URL очереди (можно получить на странице обзора);
    DOCAPI_ENDPOINT — его можно получить на странице обзора БД, нужен именно Document API;
    S3_BUCKET — имя бакета, в нашем случае это storage-for-ffmpeg.
Проверим заданные ранее переменные:
echo $SERVICE_ACCOUNT_FFMPEG_ID
echo $SECRET_ID
echo $YMQ_QUEUE_URL
echo $DOCAPI_ENDPOINT
echo $S3_BUCKET 
Для обработки видео нам потребуется утилита FFmpeg. Скачайте статический релизный бинарный файл для Linux amd64 на сайте ffmpeg.org. Обычно он находится в разделе FFmpeg Static Builds и называется примерно так: ffmpeg-release-amd64-static.tar.xz. Распакуйте архив. Из него вам понадобится только файл ffmpeg. Поскольку есть ограничение на размер  файла, который можно приложить через консоль, загрузим код функций и ffmpeg в Object Storage.
Исходный код в файле index.py содержит обе необходимые нам функции:
import json
import os
import subprocess
import uuid
from urllib.parse import urlencode
import boto3
import requests
import yandexcloud
from yandex.cloud.lockbox.v1.payload_service_pb2 import GetPayloadRequest
from yandex.cloud.lockbox.v1.payload_service_pb2_grpc import PayloadServiceStub
boto_session = None
storage_client = None
docapi_table = None
ymq_queue = None
def get_boto_session():
    global boto_session
    if boto_session is not None:
        return boto_session
    # initialize lockbox and read secret value
    yc_sdk = yandexcloud.SDK()
    channel = yc_sdk._channels.channel("lockbox-payload")
    lockbox = PayloadServiceStub(channel)
    response = lockbox.Get(GetPayloadRequest(secret_id=os.environ['SECRET_ID']))
    # extract values from secret
    access_key = None
    secret_key = None
    for entry in response.entries:
        if entry.key == 'ACCESS_KEY_ID':
            access_key = entry.text_value
        elif entry.key == 'SECRET_ACCESS_KEY':
            secret_key = entry.text_value
    if access_key is None or secret_key is None:
        raise Exception("secrets required")
    print("Key id: " + access_key)
    # initialize boto session
    boto_session = boto3.session.Session(
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key
    )
    return boto_session
def get_ymq_queue():
    global ymq_queue
    if ymq_queue is not None:
        return ymq_queue
    ymq_queue = get_boto_session().resource(
        service_name='sqs',
        endpoint_url='https://message-queue.api.cloud.yandex.net',
        region_name='ru-central1'
    ).Queue(os.environ['YMQ_QUEUE_URL'])
    return ymq_queue
def get_docapi_table():
    global docapi_table
    if docapi_table is not None:
        return docapi_table
    docapi_table = get_boto_session().resource(
        'dynamodb',
        endpoint_url=os.environ['DOCAPI_ENDPOINT'],
        region_name='ru-central1'
    ).Table('tasks')
    return docapi_table
def get_storage_client():
    global storage_client
    if storage_client is not None:
        return storage_client
    storage_client = get_boto_session().client(
        service_name='s3',
        endpoint_url='https://storage.yandexcloud.net',
        region_name='ru-central1'
    )
    return storage_client
# API handler
def create_task(src_url):
    task_id = str(uuid.uuid4())
    get_docapi_table().put_item(Item={
        'task_id': task_id,
        'ready': False
    })
    get_ymq_queue().send_message(MessageBody=json.dumps({'task_id': task_id, "src": src_url}))
    return {
        'task_id': task_id
    }
def get_task_status(task_id):
    task = get_docapi_table().get_item(Key={
        "task_id": task_id
    })
    if task['Item']['ready']:
        return {
            'ready': True,
            'gif_url': task['Item']['gif_url']
        }
    return {'ready': False}
def handle_api(event, context):
    action = event['action']
    if action == 'convert':
        return create_task(event['src_url'])
    elif action == 'get_task_status':
        return get_task_status(event['task_id'])
    else:
        return {"error": "unknown action: " + action}
# Converter handler
def download_from_ya_disk(public_key, dst):
    api_call_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?' + \
                   urlencode(dict(public_key=public_key))
    response = requests.get(api_call_url)
    download_url = response.json()['href']
    download_response = requests.get(download_url)
    with open(dst, 'wb') as video_file:
        video_file.write(download_response.content)
def upload_and_presign(file_path, object_name):
    client = get_storage_client()
    bucket = os.environ['S3_BUCKET']
    client.upload_file(file_path, bucket, object_name)
    return client.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': object_name}, ExpiresIn=3600)
def handle_process_event(event, context):
    for message in event['messages']:
        task_json = json.loads(message['details']['message']['body'])
        task_id = task_json['task_id']
        # Download video
        download_from_ya_disk(task_json['src'], '/tmp/video.mp4')
        # Convert with ffmpeg
        subprocess.run(['ffmpeg', '-i', '/tmp/video.mp4', '-r', '10', '-s', '320x240', '/tmp/result.gif'])
        result_object = task_id + ".gif"
        # Upload to Object Storage and generate presigned url
        result_download_url = upload_and_presign('/tmp/result.gif', result_object)
        # Update task status in DocAPI
        get_docapi_table().update_item(
            Key={'task_id': task_id},
            AttributeUpdates={
                'ready': {'Value': True, 'Action': 'PUT'},
                'gif_url': {'Value': result_download_url, 'Action': 'PUT'},
            }
        )
    return "OK" 
Сгенерируйте файл requirements.txt:
pipreqs $PWD --force 
Находясь в директории с исходными файлами, упакуем все нужные файлы в ZIP-архив.
zip src.zip index.py requirements.txt ffmpeg 
В Object Storage для простоты используем тот же бакет, куда далее будем складывать видео. На вкладке Объекты, вверху справа нажмите кнопку Загрузить и выберите созданный архив.
Создадим функции ffmpeg-api и ffmpeg-converter, при этом сразу зададим все необходимые переменные и сервисный аккаунт:
yc serverless function create \
  --name ffmpeg-api \
  --description "function for ffmpeg-api"
yc serverless function create \
  --name ffmpeg-converter \
  --description "function for ffmpeg-converter"
yc serverless function version create \
  --function-name ffmpeg-api \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=index.handle_api \
  --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
  --environment SECRET_ID=$SECRET_ID \
  --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
  --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
  --package-bucket-name $S3_BUCKET \
  --package-object-name src.zip
yc serverless function version create \
  --function-name ffmpeg-converter \
  --memory=2048m \
  --execution-timeout=600s \
  --runtime=python37 \
  --entrypoint=index.handle_process_event \
  --service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
  --environment SECRET_ID=$SECRET_ID \
  --environment YMQ_QUEUE_URL=$YMQ_QUEUE_URL \
  --environment DOCAPI_ENDPOINT=$DOCAPI_ENDPOINT \
  --environment S3_BUCKET=$S3_BUCKET \
  --package-bucket-name $S3_BUCKET \
  --package-object-name src.zip 
Тестирование функции
В консоли управления перейдите из рабочего каталога в раздел Cloud Functions и выберите ранее созданную функцию ffmpeg-api. Перейдите на вкладку Тестирование в боковом меню, выберите шаблон данных Без шаблона и добавьте во вводные данные JSON:
{"action":"convert", "src_url":"https://disk.yandex.ru/i/38RbVC0spb_jQQ"} 
Нажмите кнопку Запустить тест. Этим самым мы загрузим файл в хранилище и создадим задачу в БД. Если всё сделано правильно, то вы увидите такой результат:
    {
        "task_id": "133e05c2-1b98-41cc-9aab-b816d71af343"
    } 
Воспользуемся полученным идентификатором задачи task_id для получения статуса из базы данных. Для этого внесите в вводные данные JSON следующие изменения:
{"action":"get_task_status", "task_id":"<идентификатор задачи>"} 
Нажмите кнопку Запустить тест. Так как мы ещё не обрабатывали задачи в очереди, результат очевиден:
    {
        "ready": false
    } 
Шаг 6. Создание триггера
Теперь создайте триггер, который будет вызывать функцию обработки сообщений из очереди. После создания триггер начинает работать через пять минут. Он будет брать по одному сообщению и раз в 10 секунд отправлять в функцию:
yc serverless trigger create message-queue \
  --name ffmpeg \
  --queue $YMQ_QUEUE_ARN \
  --queue-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
  --invoke-function-name ffmpeg-converter  \
  --invoke-function-service-account-id $SERVICE_ACCOUNT_FFMPEG_ID \
  --batch-size 1 \
  --batch-cutoff 10s 
С этого момента очередь начнёт обрабатываться. Можно проверить, готова ли задача, и, если это так, запросить по URL результат обработки из Object Storage.
Теперь у нас есть функция, которая выполняет функцию API, через которую мы можем ставить задачи в очередь на исполнение. Триггер раз в 10 секунд берет по одному сообщению в очереди и передает функции обработчику. Функция-обработчик формирует результат и обновляет данные в базе данных. При этом мы получаем сконвертированные GIF-файлы из видео.
Протестируйте систему, используя полученный ранее идентификатор задачи task_id для получения статуса из базы данных. Для этого внесите изменения в вводные данные JSON:
{"action":"get_task_status", "task_id":"133e05c2-1b98-41cc-9aab-b816d71af343"} 
Нажмите кнопку Запустить тест. Если задача уже успела обработаться, то вы получите URL.
Удаление триггера
По завершении работы не забудьте удалить созданный триггер ffmpeg, иначе он будет продолжать работать:
yc serverless trigger delete ffmpeg 
Не забудьте также удалить или остановить все созданные вами ресурсы.
Task:
Очереди FIFO в Yandex Message Queue поддерживают:
Decision:
+Любое количество поставщиков и потребителей
-Ограниченное количество поставщиков и потребителей
-Ограниченное количество потребителей
Task:
Стандартные очереди в Yandex Message Queue:
Decision:
+Пытаются сохранять порядок полученных сообщений при передаче поставщикам, но не гарантируют его
-Гарантируют сохранение порядка полученных сообщений при передаче поставщику
-Всегда отдают сообщения поставщикам в произвольном порядке
Decision:

Task:
Практическая работа. Сокращатель ссылок
Decision:
В рамках этого курса вы изучили несколько ключевых сервисов Yandex Cloud, относящихся к группе Serverless. Давайте объединим их для решения ещё одной практической задачи и создадим сервис, который конвертирует длинные ссылки в короткие.
Шаг 1. Сервисный аккаунт
Создание аккаунта
Создайте сервисный аккаунт с именем serverless-shortener:
 export SERVICE_ACCOUNT_SHORTENER_ID=$(yc iam service-account create --name serverless-shortener \
  --description "service account for serverless" \
  --format json | jq -r .) 
Проверьте текущий список сервисных аккаунтов:
yc iam service-account list 
После проверки запишите идентификатор созданного сервисного аккаунта в переменную SERVICE_ACCOUNT_SHORTENER_ID:
echo "export SERVICE_ACCOUNT_SHORTENER_ID=<идентификатор сервисного аккаунта>" >> ~/.bashrc && . ~/.bashrc
echo $SERVICE_ACCOUNT_SHORTENER_ID 
Назначение ролей
Добавьте созданному сервисному аккаунту роли editor, storage.viewer и ydb.admin:
echo "export FOLDER_ID=$(yc config get folder-id)" >> ~/.bashrc && . ~/.bashrc
echo $FOLDER_ID
echo "export OAUTH_TOKEN=$(yc config get token)" >> ~/.bashrc && . ~/.bashrc
echo $OAUTH_TOKEN
echo "export CLOUD_ID=$(yc config get cloud-id)" >> ~/.bashrc && . ~/.bashrc
echo $CLOUD_ID
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
  --role editor
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
  --role ydb.admin
yc resource-manager folder add-access-binding $FOLDER_ID \
  --subject serviceAccount:$SERVICE_ACCOUNT_SHORTENER_ID \
  --role storage.viewer 
Шаг 2. Создание бакета в Object Storage
Сделаем для нашего сервиса веб-интерфейс. Поскольку это будет статическая веб-страница, разместим её в объектном хранилище.
В консоли управления в вашем рабочем каталоге выберите сервис Object Storage. Нажмите кнопку Создать бакет.
На странице создания бакета:
    Введите имя бакета. В нашем примере это будет storage-for-serverless-shortener.
    Ограничьте максимальный размер бакета (например 1 ГБ).
    Выберите тип доступа Публичный во всех случаях.
    Выберите класс хранилища Стандартное.
Нажмите кнопку Создать бакет для завершения операции.
Создайте и загрузите файл index.html в созданный бакет — это будет стартовая страничка для нашего сокращателя:
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Сокращатель URL</title>
    <!-- предостережет от лишнего GET запроса на адрес /favicon.ico -->
    <link rel="icon" href="data:;base64,iVBORw0KGgo=">
</head>
<body>
<h1>Добро пожаловать</h1>
<form action="javascript:shorten()">
    <label for="url">Введите ссылку:</label><br>
    <input id="url" name="url" type="text"><br>
    <input type="submit" value="Сократить">
</form>
<p id="shortened"></p>
</body>
<script>
    function shorten() {
        const link = document.getElementById("url").value
        fetch("/shorten", {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: link
        })
            .then(response => response.json())
            .then(data => {
                const url = data.url
                document.getElementById("shortened").innerHTML = `<a href=${url}>${url}</a>`
            })
            .catch(error => {
                document.getElementById("shortened").innerHTML = `<p>Произошла ошибка ${error}, попробуйте еще раз</p>`
            })
    }
</script>
</html> 
Шаг 3. Создание базы данных
    Создадим бессерверную базу данных YDB с именем for-serverless-shortener. Чтобы не переключаться из терминала, снова воспользуемся CLI. Обязательно укажите флаг --serverless для выбора типа создаваемой базы данных:
yc ydb database create for-serverless-shortener \
  --serverless \
  --folder-id $FOLDER_ID
yc ydb database list 
    Выполните команду:
yc ydb database get --name for-serverless-shortener 
В выводе вы увидите значение endpoint. Оно состоит из двух частей: собственно эндпоинта (обычно это ydb.serverless.yandexcloud.net:2135) и пути базы данных (он указывается после ключевого слова database и начинается с символа /, например /ru-central1/...). Сохраним адрес эндпоинта в переменную YDB_ENDPOINT, а путь базы данных — в переменную YDB_DATABASE. Они пригодятся нам для подключения функции.
yc ydb database get --name for-serverless-shortener
echo "export YDB_ENDPOINT=<YDB_ENDPOINT>" >> ~/.bashrc && . ~/.bashrc
echo $YDB_ENDPOINT
echo "export YDB_DATABASE=<YDB_DATABASE>" >> ~/.bashrc && . ~/.bashrc
echo $YDB_DATABASE 
    Для дальнейшей работы нам понадобится утилита ydb:
curl https://storage.yandexcloud.net/yandexcloud-ydb/install.sh | bash 
    С помощью CLI создадим авторизованный ключ сервисного аккаунта serverless-shortener:
    yc iam key create \
    --service-account-name serverless-shortener \
    --output serverless-shortener.sa 
Сохраним путь к файлу с ключом в переменную окружения:
echo "export SA_KEY_FILE=$PWD/serverless-shortener.sa" >> ~/.bashrc && . ~/.bashrc
echo $SA_KEY_FILE 
    Проверим работоспособность с помощью команды:
ydb \
  --endpoint $YDB_ENDPOINT \
  --database $YDB_DATABASE \
  --sa-key-file $SA_KEY_FILE \
  discovery whoami \
  --groups 
    Сохраним в файл links.yql SQL-скрипт для создания таблицы:
CREATE TABLE links
(
    id Utf8,
    link Utf8,
    PRIMARY KEY (id)
);
COMMIT; 
    Запустите создание таблицы, а затем проверьте результат:
ydb \
  --endpoint $YDB_ENDPOINT \
  --database $YDB_DATABASE \
  --sa-key-file $SA_KEY_FILE \
  scripting yql --file links.yql
ydb \
  --endpoint $YDB_ENDPOINT \
  --database $YDB_DATABASE \
  --sa-key-file $SA_KEY_FILE \
  scheme describe links 
Шаг 4. Создание функции
    В рабочем каталоге создайте файл index.py:
from kikimr.public.sdk.python import client as ydb
import urllib.parse
import hashlib
import base64
import json
import os
def decode(event, body):
    # тело запроса может быть закодировано
    is_base64_encoded = event.get('isBase64Encoded')
    if is_base64_encoded:
        body = str(base64.b64decode(body), 'utf-8')
    return body
def response(statusCode, headers, isBase64Encoded, body):
    return {
        'statusCode': statusCode,
        'headers': headers,
        'isBase64Encoded': isBase64Encoded,
        'body': body,
    }
def get_config():
    endpoint = os.getenv("endpoint")
    database = os.getenv("database")
    if endpoint is None or database is None:
        raise AssertionError("Нужно указать обе переменные окружения")
    credentials = ydb.construct_credentials_from_environ()
    return ydb.DriverConfig(endpoint, database, credentials=credentials)
def execute(config, query, params):
    with ydb.Driver(config) as driver:
        try:
            driver.wait(timeout=5)
        except TimeoutError:
            print("Connect failed to YDB")
            print("Last reported errors by discovery:")
            print(driver.discovery_debug_details())
            return None
        session = driver.table_client.session().create()
        prepared_query = session.prepare(query)
        return session.transaction(ydb.SerializableReadWrite()).execute(
            prepared_query,
            params,
            commit_tx=True
        )
def insert_link(id, link):
    config = get_config()
    query = """
        DECLARE $id AS Utf8;
        DECLARE $link AS Utf8;

        UPSERT INTO links (id, link) VALUES ($id, $link);
        """
    params = {'$id': id, '$link': link}
    execute(config, query, params)
def find_link(id):
    print(id)
    config = get_config()
    query = """
        DECLARE $id AS Utf8;

        SELECT link FROM links where id=$id;
        """
    params = {'$id': id}
    result_set = execute(config, query, params)
    if not result_set or not result_set[0].rows:
        return None
    return result_set[0].rows[0].link
def shorten(event):
    body = event.get('body')
    if body:
        body = decode(event, body)
        original_host = event.get('headers').get('Origin')
        link_id = hashlib.sha256(body.encode('utf8')).hexdigest()[:6]
        # в ссылке могут быть закодированные символы, например, %. это помешает работе api-gateway при редиректе,
        # поэтому следует избавиться от них вызовом urllib.parse.unquote
        insert_link(link_id, urllib.parse.unquote(body))
        return response(200, {'Content-Type': 'application/json'}, False, json.dumps({'url': f'{original_host}/r/{link_id}'}))
    return response(400, {}, False, 'В теле запроса отсутствует параметр url')
def redirect(event):
    link_id = event.get('pathParams').get('id')
    redirect_to = find_link(link_id)
    if redirect_to:
        return response(302, {'Location': redirect_to}, False, '')
    return response(404, {}, False, 'Данной ссылки не существует')
# эти проверки нужны, поскольку функция у нас одна
# в идеале сделать по функции на каждый путь в api-gw
def get_result(url, event):
    if url == "/shorten":
        return shorten(event)
    if url.startswith("/r/"):
        return redirect(event)
    return response(404, {}, False, 'Данного пути не существует')
def handler(event, context):
    url = event.get('url')
    if url:
        # из API-gateway url может прийти со знаком вопроса на конце
        if url[-1] == '?':
            url = url[:-1]
        return get_result(url, event)
    return response(404, {}, False, 'Эту функцию следует вызывать при помощи api-gateway') 
Создайте файл requirements.txt:
pipreqs $PWD --force 
Находясь в директории с исходными файлами, упакуйте все нужные файлы в zip-архив:
zip src.zip index.py requirements.txt 
    В переменные окружения функции необходимо добавить:
    endpoint — нужно указать протокол grpcs:// и добавить значение Эндпоинт из секции YDB эндпоинт, обычно получается grpcs://ydb.serverless.yandexcloud.net:2135.
    database — это значение поля База данных из секции YDB эндпоинт (начинается с /ru-central1/....).
    USE_METADATA_CREDENTIALS — выставите значение переменной в 1.
    Создадим нашу функцию for-serverless-shortener. При этом сразу зададим все необходимые переменные, сервисный аккаунт и сделаем ее публичной:
yc serverless function create \
  --name for-serverless-shortener \
  --description "function for serverless-shortener"
yc serverless function version create \
  --function-name for-serverless-shortener \
  --memory=256m \
  --execution-timeout=5s \
  --runtime=python37 \
  --entrypoint=index.handler \
  --service-account-id $SERVICE_ACCOUNT_SHORTENER_ID \
  --environment USE_METADATA_CREDENTIALS=1 \
  --environment endpoint=grpcs://ydb.serverless.yandexcloud.net:2135 \
  --environment database=$YDB_DATABASE \
  --source-path src.zip
yc serverless function allow-unauthenticated-invoke for-serverless-shortener 
Шаг 5. Конфигурирование Yandex API Gateway
    Создадим спецификацию for-serverless-shortener.yml со следующим содержанием:
openapi: 3.0.0
info:
  title: for-serverless-shortener
  version: 1.0.0
paths:
  /:
    get:
      x-yc-apigateway-integration:
        type: object_storage
        bucket:             <bucket_name>        # <-- имя бакета
        object:             <html_file>          # <-- имя html-файла
        presigned_redirect: false
        service_account:    <service_account_id> # <-- идентификатор сервисного аккаунта
      operationId: static
  /shorten:
    post:
      x-yc-apigateway-integration:
        type: cloud_functions
        function_id:  <function_id>               # <-- идентификатор функции
      operationId: shorten
  /r/{id}:
    get:
      x-yc-apigateway-integration:
        type: cloud_functions
        function_id:  <function_id>               # <-- идентификатор функции
      operationId: redirect
      parameters:
        - description: id of the url
          explode: false
          in: path
          name: id
          required: true
          schema:
            type: string
          style: simple 
Не забудьте подставить в спецификацию актуальные для вас значения переменных.
    Используем спецификацию для инициализации:
yc serverless api-gateway create \
  --name for-serverless-shortener \
  --spec=for-serverless-shortener.yml \
  --description "for serverless shortener" 
В результате успешного создания API-шлюза получим значение параметра domain:
yc serverless api-gateway list
yc serverless api-gateway get --name for-serverless-shortener 
    Чтобы проверить работоспособность API-шлюза и созданного приложения целиком, скопируйте служебный домен (вида https://<идентификатор API Gateway>.apigw.yandexcloud.net/) и вставьте адрес в браузер.
Добавляйте адреса сайтов в форму, они будут сохранятся в базу данных. А вам будет доступна ссылка, за которой будет скрываться оригинальный адрес. Ваше приложение полностью работоспособно. Теперь вы умеете использовать serverless-стеком технологий Yandex Cloud.
Итак, вы создали приложение с использованием Cloud Functions, API Gateway, Object Storage и Yandex Database. Конечно, вы можете развивать его и дальше, расширяя функциональность.
Вводный курс по serverless-разработке на этом завершён. Вам осталось пройти лишь заключительный тест, который проверит ваши знания по всем рассмотренным в курсе сервисам.
Decision:

Task:
Какие ключевые понятия бессерверных решений выделяются их поставщиками?
Decision:
-Знаю! Serverless используется для создания микросервисов на Go, разработка ведётся по Agile, без которой теряется весь смысл использования архитектуры с изолированными компонентами. Поэтому микросервисы, GoLang, Agile!
+Единого определения нет, но чаще всего упоминают контейнеризацию, динамическую оркестрацию, микросервисы. Впрочем, спорить на эту тему можно бесконечно.
Task:
За что разработчик платит при использовании Cloud Functions? Выберите все правильные ответы.
Decision:
+Количество вызовов функции
+Использованные вычислительные ресурсы
-Резервирование ресурсов
+Исходящий трафик
Task:
Какие концептуальные ограничения есть у сервиса? Выберите все правильные ответы.
Decision:
+Функции не могут бесконечно выполнять какую-то задачу.
-Лог ограничен последней тысячей вызовов функции.
+При вызове у функции нет информации о результате прошлых запусков.
Task:
Какие методы допустимо использовать в спецификации API-шлюза?
Decision:
-GET и POST.
-GET, POST, PUT.
-GET, POST, DELETE.
+Можно использовать все HTTP-методы.
Task:
Можно ли использовать в спецификации свой домен?
Decision:
-Нет, эта функция пока недоступна.
-Да, но сначала надо подтвердить права на домен в Certificate Manager.
+Да, но эту функцию надо отдельно подключить, а затем подтвердить права на домен.
Task:
Ключевые отличия serverless-режима YDB от managed-режима:
Decision:
+Оплата идёт за фактические запросы к базе данных, а не за резервирование мощностей
+Можно перенести своё приложение из AWS
-Для использования в приложениях доступен только Document API, нельзя использовать YQL
Task:
Что учитывается при тарификации YDB в serverless-режиме:
Decision:
+стоимость использования CPU;
-стоимость использования памяти;
+хранение данных;
+ввод и вывод данных.
Task:
Для работы с документными таблицами в YDB вы можете использовать:
Decision:
+HTTP API;
+gRPC;
+AWS CLI;
+AWS SDK.
Task:
Сказывается ли тип очереди на производительности?
Decision:
-Да, в очередях FIFO гарантируется исходный порядок сообщений, она разбираются потребителями быстрее, чем стандартные очереди.
+Да, стандартные очереди разбираются быстрее FIFO.
-Технически — да, но разница в производительности незначительна и не является решающим фактором при выборе типа очереди.
Task:
Какую функцию выполняет таймаут видимости?
Decision:
+Скрывает сообщение от получателей на заданное пользователем время, чтобы первый забравший его получатель успел завершить обработку.
-Определяет время, через которое сообщения отправляются в Dead Letter Queue.
-Определяет время, на которое добавленное в очередь сообщение скрывается от получателей.
Task:
Принципы информационной безопасности. Концепция разделения ответственности
Decision:
Среди специалистов в области информационной безопасности давно популярен мем — все компании можно разделить на две группы: те, которых уже взломали, и те, которые еще об этом не знают. Действительно, развитие интернет-технологий и увеличение объёма цифровых данных сопровождается не менее быстрым ростом киберпреступности. Так, например, по данным Генпрокуратуры России, в 2020 году в стране было совершено более полумиллиона преступлений в сфере информационно-телекоммуникационных технологий. За последние пять лет число таких преступлений увеличилось в 11 раз, а их доля среди прочих правонарушений выросла с 2 до 25 процентов.
Основными целями киберпреступников является получение доступа к персональным данным, финансовой информации и интеллектуальной собственности, а также внедрение в компьютерные системы программ-шифровальщиков.  Для компании, которая оказывается в роли жертвы, это означает не только удар по бизнесу и репутации, но и существенные финансовые потери. IBM Security проанализировала 537 недавно произошедших случаев утечки данных и пришла к выводу, что в среднем пострадавшая компания терпит финансовый ущерб в размере примерно от 3,5 до 5 миллионов долларов. А по оценкам экспертов Cybersecurity Ventures, общий ущерб от киберпреступлений в 2021 году составит около 6 триллионов долларов.
Может быть, ваша компания невелика и приведенные выше цифры кажутся вам нереальными. Как бы там ни было, информационная безопасность — это та проблема, которой необходимо уделять внимание. И делать это нужно независимо от того, в локальной или облачной инфраструктуре вы работаете. Принципы безопасности информационных систем одинаковы в обоих случаях: это защита конфиденциальности, целостности и доступности информации.
Конфиденциальность означает, что доступ к информации должны иметь только те, кто имеет на это право. К конфиденциальным данным относятся, например, персональные данные пользователей вашего приложения или коммерческие секреты вашей компании.
Целостность подразумевает защиту данных от несанкционированного изменения или удаления, а если такое все-таки произошло — у вас должна быть возможность всё восстановить.
Доступность означает, что тот, кто владеет информацией, или тот, для кого она предназначена, имеет к ней надежный бесперебойный доступ.То есть, например, вы должны знать, как защититься от распределенной сетевой атаки (DDoS, Distributed Denial of Service), направленной на то, чтобы вывести из строя ваш веб-ресурс.
И еще один момент. Обеспечение безопасности — это не разовое мероприятие, а постоянный процесс, основанный на следующих принципах:
    планирование технических и организационных мер с учетом имеющихся рисков и угроз;
    выполнение этих мер;
    мониторинг и анализ функционирования вашей системы;
    совершенствование применяемых инструментов защиты.
С точки зрения безопасности использование облака имеет свои особенности. Важные для вас данные находятся не на сервере, который надежно заперт в соседней комнате, а где-то в интернете. Многим кажется, что поэтому данные защищены хуже, и хранить их в облаке небезопасно. С другой стороны, к защите ваших данных подключается облачный провайдер, который, как правило, имеет для этого существенно больше ресурсов и квалифицированных специалистов.
В то, чтобы работа в облаке была безопасной, вносят вклад и облачный провайдер, и пользователь. Это называется разделяемой ответственностью (shared responsibility).
Есть аспекты безопасности, за которые отвечает провайдер, и вам не нужно о них заботиться. Например о том, кто имеет физический доступ к серверам или как утилизировать вышедший из строя жёсткий диск, чтобы данные не попали в чужие руки. Но какими бы возможностями для защиты данных ни обладал провайдер, он не может взять на себя полную ответственность за безопасность вашего облака.
Используя облачные инфраструктуру и сервисы, пользователь конфигурирует их под решение своих задач. Провайдер не видит, правильно ли вы настроили, например, права доступа к своим ресурсам или группы безопасности в виртуальной сети. Ответственность за это несёте вы сами.
Концепцию разделения ответственности за обеспечение безопасности проще всего проиллюстрировать такой схемой:
Если пользователь работает по модели IaaS, облачный провайдер отвечает за безопасность всего, что связано с оборудованием. При использовании моделей PaaS и SaaS провайдер несёт ответственность еще и за безопасность программного обеспечения, которое он предоставляет пользователю. И независимо от модели облачная платформа предоставляет пользователю инструменты для обеспечения безопасности.
В этом курсе мы разберём, как настроить доступ к своим ресурсам, обеспечить сетевую безопасность и шифровать данные. Но сначала кратко остановимся на требованиях к обеспечению безопасности и на том, как Yandex Cloud их выполняет.
Task:
За какой из элементов обеспечения информационной безопасности всегда отвечает пользователь независимо от модели использования облачной инфраструктуры?
Decision:
-Резервное копирование данных
-Обеспечение безопасности ОС
+Управление правами доступа к ресурсам
-Ведение журнала аудита
Task:
Правовые аспекты и основные стандарты
Decision:
Информационная безопасность — это настолько серьёзно, что многие её вопросы регулируются законами и регламентируются международными и национальными стандартами. Поэтому, рассказывая о безопасности, нельзя не затронуть и правовую сторону.
Когда вы создаёте информационную систему или приложение, сразу определите, какие требования к безопасности необходимо учитывать при обработке и хранении информации. Чаще всего эти требования связаны с персональными данными пользователей и с данными платежных систем. В этих случаях уровень безопасности, который вам предстоит обеспечить, определяют законодательство и стандарты.
Законодательство о защите персональных данных
Чтобы получать и обрабатывать персональные данные, необходимо соблюдать требования Федерального закона №152-ФЗ. Этот закон даёт определение персональных данных, описывает права субъектов персональных данных, то есть тех людей, чьи данные вы собираете и обрабатываете, и ваши обязанности как оператора.
Исходя из положений законодательства, персональные данные могут быть четырёх категорий:
    специальные: например национальность, политические взгляды, состояние здоровья;
    биометрические: фото, отпечатки пальцев;
    общедоступные: например ФИО, дата рождения, номер телефона;
    иные: например, какие покупки делал конкретный человек в вашем интернет-магазине.
Категория персональных данных влияет на уровень их защиты, который должен обеспечить оператор.
Фактически подавляющее большинство компаний в России являются операторами персональных данных — у каждой есть сотрудники, клиенты, поставщики и т. д. По закону на оператора возлагается ряд обязанностей. В частности, он должен:
    заранее четко сформулировать цели обработки персональных данных и действовать в соответствии с этими целями;
    обрабатывать персональные данные только после получения согласия на это от субъектов (например, от сотрудников компании или пользователей приложения);
    обеспечить защиту персональных данных от незаконного доступа;
    хранить персональные данные российских граждан на серверах, расположенных на территории России.
Оператор персональных данных обязан зарегистрироваться в Роскомнадзоре. Этот орган может проводить проверки соблюдения законодательства о защите персональных данных и, если выявит нарушения, налагать штрафы в соответствии со статьей 13.11 КоАП.
Закон формулирует лишь общие принципы защиты персональных данных. То, что нужно сделать на практике, определяется подзаконными актами: постановлением Правительства от 01.11.2012 № 1119 «Об утверждении требований к защите персональных данных при их обработке в информационных системах персональных данных» и приказом ФСТЭК России от 18.02.2013 № 21 «Об утверждении состава и содержания организационных и технических мер по обеспечению безопасности персональных данных при их обработке в информационных системах персональных данных».
Суть постановления заключается в том, что защита персональных данных должна осуществляться дифференцированно. Для этого вводятся понятия типа угрозы и уровня защищённости.
Тип угроз определяется самим оператором с учетом факторов, создающих опасность несанкционированного, в том числе случайного, доступа к персональным данным. Выделяют три типа угроз:
    первый — угрозы, связанные с уязвимостями в системном ПО;
    второй — угрозы, связанные с уязвимостями в прикладном ПО;
    третий — угрозы, не связанные с ПО.
Чтобы определить тип угрозы, нужно подумать, кто может быть заинтересован в незаконном доступе к персональным данным, с которыми вы работаете, зачем ему это нужно и какими возможностями он для этого располагает.
Требуемый уровень защищенности персональных данных зависит от типа угрозы, категории персональных данных, числа субъектов, данные о которых хранятся в вашей системе (больше или меньше 100 000), и от того, являются ли эти субъекты сотрудниками вашей компании. Существует четыре уровня защищенности. Определить подходящий уровень поможет таблица.
То, какие именно меры — технические и организационные — нужно принять, чтобы обеспечить каждый из этих уровней защищенности, определяет приказ ФСТЭК России от 18.02.2013 № 21, в котором 109 таких мер. Они могут быть простыми — например располагать мониторы таким образом, чтобы случайный посетитель не мог просматривать на них информацию. А могут быть довольно сложными и затратными — например обеспечить сбор и анализ информации о событиях безопасности и реагирование на неё. Для четвертого, минимального уровня защищённости требуется выполнить 27 мер, а для первого, самого высокого — 69.
Персональные данные защищают не только в России. Так, если вы собираетесь работать с пользователями из стран Европейского союза, то вам придётся учитывать требования регламента GDPR (General Data Protection Regulation). Это закон о защите персональных данных, действующий во всех странах ЕС.
Цели у 152-ФЗ и GDPR одинаковы, а требования несколько отличаются. Например, GDPR в случае обнаружения утечки персональных данных предписывает в течение 72 часов уведомить об этом регулятора, а ещё в ЕС cookies и IP-адреса считаются персональными данными. В российском законодательстве этого нет.
Незнание закона не только не освобождает от ответственности, но и негативно сказывается на бизнесе. Если вы работаете с персональными данными, изучите и выполняйте требования законодательства.
Стандарты
При определении мер, которые будут обеспечивать информационную безопасность, не всегда понятно, какого набора мер достаточно, чтобы снизить риски до разумных пределов. Это не праздный вопрос — реализация большинства мер защиты требует времени и денег. Для ответа на него существуют стандарты.
Важно понимать, что, в отличие от требований законодательства, нарушение которых грозит санкциями и штрафами, стандарты безопасности — это просто правила и практики, которые подавляющее большинство экспертов считает эффективными. От этого, правда, применение стандартов не становится менее полезным.
Наличие сертификата соответствия стандартам будет гарантировать вашим партнёрам и пользователям то, что информация, которую они вам доверяют, надёжно защищена. Ведь для того, чтобы получить такой сертификат, нужно успешно пройти аудит безопасности, который могут проводить только аккредитованные организации. А если потенциальный контрагент считает наличие у вас сертификата необходимым условием для совместной работы, то обойтись без этого документа не получится.
Стандарты серии ISO/IEC 27000
Основными в сфере информационной безопасности признаны стандарты серии ISO/IEC 27000, которые разработали Международная организация по стандартизации и Международная электротехническая комиссия. Документы этой серии определяют, как правильно создать систему управления информационной безопасностью (СУИБ). Серия включает почти два десятка общих стандартов и руководств и более двадцати специальных руководств по отдельным аспектам обеспечения информационной безопасности.
Для облачных систем наиболее важны три документа: ISO/IEC 27001, ISO/IEC 27017 и ISO/IEC 27018.
Стандарт ISO/IEC 27001 содержит требования к СУИБ. Он определяет, как её правильно внедрять, поддерживать и совершенствовать.
Стандарт ISO/IEC 27017 включает практические рекомендации по обеспечению информационной безопасности для облачных провайдеров. Эти рекомендации дополняют требования, которые изложены в стандарте ISO/IEC 27001.
Стандарт ISO/IEC 27018 содержит практические рекомендации по защите персональных данных при их обработке провайдерами облачных сервисов.
PCI DSS
Международный стандарт PCI DSS (Payment Card Industry Data Security Standard) содержит основные требования для защиты данных держателей платёжных карт. То есть он определяет, какие меры обеспечения безопасности вам необходимо выполнять, если вы обрабатываете такие данные.
Если ваше облачное приложение использует системы оплаты и работает с данными платёжных карт, то нужно размещать его в инфраструктуре, которая имеет соответствующий сертификат PCI DSS. Помимо этого, необходимо учитывать требования этого стандарта при проектировании логики работы приложения.
Для обработки данных платежных систем (Visa, Mastercard, Мир и некоторых других) соответствовать этому стандарту обязательно. Соответствие облака требованиям PCI DSS позволяет его клиентам использовать облачную инфраструктуру и сервисы для обработки данных платёжных карт.
ГОСТ Р 57580.1‑2017
Для обеспечения безопасности банковских и финансовых операций существует и национальный российский стандарт — ГОСТ Р 57580. Его требованиям должны соответствовать все кредитные и некредитные финансовые организации.
Соответствие сервисов облачной платформы требованиям этого стандарта помогает организациям, размещающим в облаке свои системы и приложения, выполнять требования Центрального банка и обеспечивать соответствие стандарту своих облачных систем.
Подытожим: применять стандарты и рекомендации для обеспечения информационной безопасности компании полезно. Ещё большую пользу — на этот раз бизнесу компании — принесет сертификат соответствия этим стандартам. В некоторых случаях, например для работы с платежными системами, иметь сертификат необходимо.
Task:
Что влияет на необходимый уровень защищённости информации по российскому законодательству:
Decision:
+Обрабатываются ли данные сотрудников или внешних пользователей
-Перечень актуальных угроз безопасности
+Категория персональных данных
-Число сотрудников оператора персональных данных
Task:
К какой категории персональных данных относится состояние здоровья:
Decision:
+Специальные
-Биометрические
-Общедоступные
-Иные
Task:
Как Yandex Cloud защищает облачную инфраструктуру и сервисы
Decision:
На предыдущем уроке мы выяснили, что для обеспечения безопасности требуется предпринять целый набор мер: технических и организационных. Хорошая новость заключается в том, что, если вы работаете в облаке, часть этих мер выполняет облачный провайдер.
Возникает вопрос: а насколько можно быть уверенным, что облачный провайдер выполняет эти меры и обеспечивает высокий уровень безопасности своего облака? Чтобы ответить на этот вопрос, нужно посмотреть, какие аттестаты, сертификаты и (или) другие заключения о соответствии он имеет.
Давайте разберемся на примере Yandex Cloud.
Информацию об относящихся к информационной безопасности аттестатах и сертификатах вы найдете в соответствующем разделе сайта Yandex Cloud. В этом разделе вы увидите, что Yandex Cloud имеет аттестат соответствия своей ИСПДн (информационной системы персональных данных) требованиям российского законодательства по защите персональных данных и обеспечивает первый уровень защищенности. То есть пользователи Yandex Cloud могут работать в облаке с персональными данными любой категории.
В заключении о соответствии приведена информация о том, какие меры для обеспечения первого уровня защищенности персональных данных предпринимает Yandex Cloud, а какие понадобится выполнить пользователю. Есть также краткая памятка, содержащая примерный перечень шагов, которые пользователю облака нужно будет сделать на своей стороне.
Здесь важно подчеркнуть: когда вы как оператор размещаете персональные данные в облаке, то вы поручаете облачному провайдеру лишь их обработку. Иными словами, вы не передаёте Yandex Cloud свои функции оператора персональных данных и связанную с этим ответственность. Это четко прописано в договоре: Yandex Cloud обязуется соблюдать конфиденциальность персональных данных, обеспечивать их безопасность при обработке и выполнять все установленные законодательством требования к защите обрабатываемых персональных данных.
Yandex Cloud выполняет ключевые требования GDPR (General Data Protection Regulation):
    предпринимает меры по защите данных;
    поддерживает процессы обработки обращений, связанных с получением, изменением и удалением персональных данных;
    информирует заказчиков в случае возникновения инцидентов.
Такие обязательства, как правило, включены в соглашение об обработке данных.
В этом же разделе вы найдёте информацию об имеющихся у Yandex Cloud сертификатах соответствия стандартам информационной безопасности.
В частности, система управления информационной безопасностью Yandex Cloud прошла аудит и получила сертификаты соответствия стандартам ISO/IEC 27001, ISO/IEC 27017 и ISO/IEC 27018.
Ещё у Yandex Cloud есть сертификаты соответствия стандартам PCI DSS и ГОСТ Р 57580.1-2017. Иными словами, на платформе Yandex Cloud пользователи могут создавать информационные системы или приложения, работающие с данными платежных карт и финансовыми или банковскими операциями. Чтобы узнать, как разделяется ответственность между облаком и пользователем в этих случаях, посмотрите матрицы разделения ответственности при выполнении требований PCI DSS или ГОСТ Р 57580.1-2017.
Важно понимать, что соответствие облачного провайдера и его облака каким-либо требованиям безопасности не означает, что система пользователя соответствует этим требованиям автоматически. Здесь работает концепция разделения ответственности, о которой мы говорили в первом уроке этой темы.
А теперь рассмотрим на примере Yandex Cloud, как облачные провайдеры обеспечивают безопасность системы.
Организационный уровень
В Yandex Cloud внедрена система управления информационной безопасностью (СУИБ), что подтверждается сертификатом соответствия стандарту ISO/IEC 27001.
СУИБ представляет собой набор политик и процедур, которые позволяют обеспечить информационную безопасность и минимизировать риски. В СУИБ включены следующие механизмы.
1.    Инвентаризация активов. Это, прежде всего, учет систем, которые обрабатывают данные клиентов. Правила использования информации и активов, которые связаны с обработкой информации, описаны во внутренних документах организации и регулярно объясняются сотрудникам. При увольнении сотрудники возвращают все корпоративные активы, а доступ к системам отзывается автоматически.
2.    Контроль доступа. Доступ к помещениям, зонам безопасности, серверным и сетевым ресурсам имеет только авторизованный персонал. У сотрудников есть доступ только к тем ресурсам, которые они должны использовать по должностным обязанностям.
3.    Организация физической безопасности. Аппаратные ресурсы Yandex Cloud располагаются в собственных дата-центрах. Доступ в помещения дата-центров контролируется, а все помещения оборудованы системами видеонаблюдения.
4.    Управление обновлениями и уязвимостями. Перед запуском в рабочей среде все обновляемые системы тестируются на наличие уязвимостей. За сроками установки обновлений ведется автоматический контроль.
5.    Проведение внутренних и внешних аудитов, а также тестов на проникновение для проверки эффективности процессов обеспечения информационной безопасности.
6.    Управление инцидентами. Инциденты — это любые события, которые нарушают или снижают качество обслуживания (или могут стать причиной таких нежелательных последствий). Процесс управления инцидентами осуществляет центр операционной безопасности в составе службы информационной безопасности. В случае выявления каких-либо инцидентов пользователям направляются уведомления.
7.    Управление персоналом для минимизации рисков, связанных с действиями сотрудников: проверка, обучение по регламентам информационной безопасности и т. д.
8.    Система управления непрерывностью бизнеса, которая описывает последовательность действий сотрудников при возможных негативных сценариях. Эта система также предусматривает резервирование для всех критичных компонентов облачной платформы и хранилищ данных, что позволяет восстановить информацию пользователей в случае отказа оборудования.
Технический уровень
Владельцем данных всегда является пользователь облачной платформы. Все данные пользователей хранятся в так называемом слое данных (Storage Layer), который является ключевым элементом облачной платформы.
Чтобы обеспечить конфиденциальность и целостность данных, Yandex Cloud шифрует их на уровне физической инфраструктуры и на уровне Storage Layer.
Криптографическая защита применяется и в некоторых сервисах. Так, например, резервные копии, которые создаются сервисами управляемых баз данных, шифруются перед отправкой в хранилище. Кроме того, все передаваемые пользовательские данные шифруются с использованием протокола TLS.
Доступность данных в Storage Layer обеспечивается путем их размещения в трёх зонах доступности и репликации между зонами.
Чтобы избежать атак на инфраструктуру облака с пользовательских виртуальных машин, применяется механизм изоляции ресурсов.
Критические, с точки зрения безопасности, облачные сервисы изолируются физически: они запускаются на выделенных физических серверах и в изолированных сетях, где пользовательские виртуальные машины не создаются. Для пользовательских машин используется логическая изоляция на уровне гипервизора. Гипервизор — это системное приложение, которое управляет несколькими виртуальными машинами на одной физической машине и отвечает за то, чтобы они работали полностью отдельно друг от друга.
Для обеспечения безопасности Yandex Cloud использует и другие средства и методы защиты, в том числе:
    инструменты контроля выполнения AppArmor и Seccomp, которые формируют среду изоляции и отслеживают работу приложений;
    систему обнаружения вторжений, которая собирает логи с хостов (в том числе логи AppArmor и Seccomp), дополняет их при помощи утилиты osquery и отправляет в систему Security Information and Event Management (SIEM);
    несколько типов межсетевых экранов и пакетных фильтров;
    систему мониторинга и оповещения о подозрительном поведении.
В соответствии с СУИБ всё установленное ПО регулярно проверяется на наличие уязвимостей и обновляется до последних версий. Конфигурации операционных систем описываются кодом и хранятся в репозитории. Все изменения конфигураций перед переносом в продуктивную среду проходят обязательную проверку в тестовых средах.
Администраторы инфраструктуры и разработчики провайдера имеют доступ к продуктивной среде через бастионный хост, который записывает сессию пользователя. Информация из записанных сессий передается в SIEM и регулярно анализируется. Для доступа используются аппаратные ключи, на которых хранятся аутентификационные данные.
Еще один важный для безопасности аспект работы облачного провайдера заключается в том, чтобы предоставить пользователям эффективные инструменты защиты своих систем и данных. О них вы узнаете на следующем уроке.
Task:
Какие действия должен выполнять пользователь для реализации третьего уровня защищённости при работе с персональными данными в облаке в соответствии с приказом ФСТЭК России от 18.02.2013 № 21?
Decision:
+Настраивать средства аутентификации внутри виртуальных машин
+Контролировать доступ к виртуальным машинам
+Обеспечивать антивирусную защиту виртуальных машин
+Защищать данные от раскрытия и модификации при их передаче через интернет
+Регистрировать события безопасности
Task:
Клиентские инструменты обеспечения безопасности
Decision:
Ответственность за безопасную работу в облаке разделяется между провайдером и пользователем. На прошлом уроке мы выяснили, что для безопасности делает облачный провайдер. Теперь давайте рассмотрим, какими инструментами для защиты своих ресурсов и данных в облаке располагает пользователь.
Некоторые из этих инструментов — например, группы безопасности или NAT — доступны при настройке ресурсов. Некоторые представлены в виде отдельных сервисов платформы безопасности. Кроме того, нужное ПО можно найти в Marketplace облака.
В этом уроке мы сделаем краткий обзор имеющихся инструментов. Подробно они будут разбираться в следующих темах.
Контроль доступа
За идентификацию пользователей и контроль доступа к ресурсам отвечает сервис IAM (Identity and Access Management). Сервис даёт возможность настроить права доступа: вы определяете, кто и какие права имеет на конкретный ресурс, а IAM предоставляет к нему доступ в соответствии с назначенными правами.
С помощью IAM вы можете:
    добавлять и удалять пользователей своих ресурсов;
    тонко управлять правами доступа, назначая и отзывая роли;
    создавать сервисные аккаунты — специальные учётные записи, от имени которых программы могут управлять ресурсами в Yandex CloudI;
    получать IAM-токен, необходимый для авторизации в API Yandex Cloud и выполнения операций с ресурсами.
Шифрование данных
Для защиты конфиденциальности и целостности пользовательских данных Yandex Cloud использует их шифрование. При этом возможность использовать криптографическую защиту данных есть и у самого пользователя.
С помощью сервиса KMS (Key Management Service) :
    создавать ключи шифрования и управлять ими;
    использовать ключи в популярных библиотеках;
    шифровать и расшифровывать данные небольшого объёма;
    организовывать криптографическую защиту больших объёмов данных с использованием схемы envelope encryption.
Сервис Lockbox помогает защитить ваши секреты, то есть любую информацию, которую нужно хранить в тайне. Это могут быть, например, логины и пароли или ключи сервисного аккаунта.
Используйте Lockbox, чтобы обеспечить:
    безопасное хранение и доступ к значениям секретов;
    централизованное хранение секретов в облаке;
    удобное управление жизненным циклом секретов.
Чтобы защищать передаваемые данные с использованием протокола HTTPS, необходимо использовать сертификаты. Управлять жизненным циклом сертификатов — устанавливать их, отслеживать окончание срока действия, формировать запросы на получение новых — можно с помощью сервиса Certificate Manager.
Сетевая безопасность
Важным элементом безопасности является защита виртуальных сетей. Чтобы её обеспечить, нужно управлять входящим и исходящим трафиком, а также трафиком между сетями в облаке.
Для управления входящим трафиком используется балансировщик нагрузки. Он позволяет снизить риски для безопасности, поскольку уменьшает поверхность атаки и ограничивает трафик на виртуальные машины только необходимыми протоколами. Балансировщик может быть интегрирован с сервисом DDoS Protection для защиты от DDoS-атак.
Чтобы управлять исходящим трафиком, рекомендуется предоставлять виртуальным машинам доступ в интернет через NAT (Network Address Translation) — механизм преобразования сетевых адресов, который работает в качестве сетевого шлюза или прокси-сервера.
Контроль над всеми видами трафика обеспечивается группами безопасности. Этот инструмент — фактически набор правил получения и отправки трафика — позволяет управлять доступом виртуальных машин к ресурсам, расположенным в облаке или в интернете.
Безопасный доступ к облачной инфраструктуре можно обеспечить с помощью виртуальных каналов VPN (Virtual Private Network). Для этого на отдельной виртуальной машине настраивают VPN-сервер или используют готовые образы VPN-серверов из Marketplace.
Выделенное сетевое соединение между локальной инфраструктурой и ресурсами в облаке организуют при помощи сервиса Cloud Interconnect. Такое соединение надёжнее, быстрее и безопаснее подключения через интернет.
Мониторинг доступности ресурсов
Одним из принципов информационной безопасности является обеспечение доступности ресурсов. Но только спроектировать систему в соответствии с этим принципом недостаточно. Нужно иметь инструменты, позволяющие контролировать работу ваших ресурсов.
Для этих целей предназначен сервис Yandex Monitoring. О его возможностях подробно рассказывается в курсе «DevOps и автоматизация».
Внешнее сканирование безопасности
Чтобы убедиться в том, что ваша система хорошо защищена, её можно проверить на наличие уязвимостей. Для этого проводят внешний поиск уязвимостей (пентесты).
Внешнее сканирование безопасности может проводиться как самим пользователем, так и привлечённым подрядчиком. В любом случае обязательно делать это по согласованию с Yandex Cloud и в соответствии с правилами организации сканирования.
Для проведения пентеста нужно отправить анкету в службу технической поддержки и получить одобрение этой заявки со стороны службы информационной безопасности.
Marketplace
Необходимые вам инструменты обеспечения безопасности можно найти и в Marketplace. В нём размещено несколько комплексных систем защиты ресурсов, межсетевых экранов, в том числе для веб-приложений (WAF, Web Application Firewall), образы для развёртывания серверов удалённого доступа OpenVPN и IPSec VPN.
ПО в Marketplace в основном платное. Оплата производится по моделям Pay as you go или BYOL. В первом случае тарифицируется только время фактического использования ПО. Во втором вам понадобится приобрести у поставщика ПО собственную лицензию.
Task:
Обзор сервиса IAM, типы аккаунтов
Decision:
Чтобы оградить дом или офис от нежданных гостей, люди придумали дверные замки: достаточно позаботиться о том, чтобы ключи не достались чужакам. В облаке примерно то же самое. Только в роли замка выступает сервис Yandex Identity and Access Management (IAM), который отвечает за доступ пользователей к вашим облачным ресурсам. На этом уроке мы рассмотрим, какие «ключи» используются для аутентификации пользователей в облаке и их авторизации в сервисах.
Аутентификация — это проверка, является ли входящий тем, за кого себя выдаёт. Для аутентификации может использоваться пароль, физическое устройство, сканирование биометрических данных. Если нужно обеспечить высокий уровень безопасности, используют двухфакторную аутентификацию.
Авторизация — это проверка прав на выполнение каких-либо действий. Например, DevOps-инженер Петров имеет права создавать и конфигурировать в облаке своей компании любые ресурсы, которые могут понадобиться. Но поскольку у Петрова нет прав на доступ к бакету бизнес-аналитика Васечкина в Object Storage, то пройти авторизацию и посмотреть, что же там хранится, он не сможет.
Теперь рассмотрим, как получить доступ к ресурсам Yandex Cloud. Предусмотрены три типа аккаунтов.
Тип 1. Учетная запись Яндекс ID. Это единый аккаунт для всех сервисов Яндекса, в котором хранятся личные данные пользователей, выполняется их аутентификация и авторизация. Если аутентификация прошла успешно, то Яндекс ID устанавливает в браузере пользователя cookie на домен верхнего уровня .yandex.
Когда вы входите в консоль управления Yandex Cloud, облако — точнее, сервис IAM — использует сookie Яндекс ID для аутентификации и, если она прошла успешно, устанавливает собственные cookie на поддомены cloud.yandex.TLD. В консоли эти cookie обмениваются на IAM-токены. IAM-токен — это информация для авторизации,  подписанная секретным ключом IAM. Он содержит, например, ID пользователя, время жизни самого токена, ресурсы или сервисы, которые могут быть доступны с этим токеном. С помощью IAM-токена пользователь авторизуется в API Yandex Cloud и выполняет операции с ресурсами.
Если вы работаете в интерфейсе командной строки (CLI), то на вашем компьютере будет сохранен OAuth-токен Яндекс ID. Утилита yc будет обменивать этот токен на IAM-токены со сроком жизни 12 часов  для взаимодействия с API Yandex Cloud.
Тип 2. Федерация удостоверений. Удостоверения (credentials) — это информация об атрибутах пользователей, таких как логин или адрес электронной почты, и способах аутентификации. При использовании федерации удостоверений они хранятся не у облачного провайдера, а у третьей доверенной стороны — поставщика удостоверений (IdP, Identity Provider).
В Yandex Cloud вы можете создать федерацию удостоверений с помощью сервиса Yandex Cloud Organization. Этот сервис работает с любой системой управления учётными записями, которая поддерживает протокол SAML (например Active Directory или Google Workspace).
Упрощённо аутентификация с использованием федерации удостоверений происходит следующим образом.
    Пользователь хочет открыть в браузере консоль управления Yandex Cloud.
    Если пользователь уже проходил аутентификацию и время жизни cookie в его браузере не истекло, то он переходит на главную страницу консоли управления. Если это не так, то консоль отправляет его на сервер поставщика удостоверений для аутентификации.
    Пользователь переходит на страницу аутентификации на сервере поставщика удостоверений и вводит свои логин и пароль.
    Если аутентификация прошла успешно, сервер поставщика удостоверений переадресует пользователя обратно на страницу входа в консоль управления.
    Поставщик удостоверений направляет сервису IAM облака подписанное сообщение (SAML Response) с информацией о том, что пользователь прошел аутентификацию.
После проверки подписи сообщения IAM устанавливает пользователю cookie на поддомен cloud.yandex. Сookie обменивается на IAM-токен, который позволяет пользователю выполнять операции с защищенными ресурсами. Криптографические ключи для контроля целостности cookie хранятся в IAM.
Тип 3. Аутентифицированные пользователи могут выполнять операции с облаком не только под своими пользовательскими аккаунтами, но и через сервисные аккаунты.
Сервисные аккаунты — это специальный тип аккаунтов, которые используются для доступа к ресурсам Yandex Cloud от имени приложений. Также под сервисными аккаунтами могут выполнять операции облачные функции и сервисы, которые запущены в виртуальных машинах.
С помощью сервисного аккаунта нельзя войти в консоль управления Yandex Cloud. Подразумевается, что операции от имени сервисного аккаунта выполняют программы, а не люди.
С помощью сервисного аккаунта другие пользователи могут управлять вашими ресурсами в Yandex Cloud. Учётным записям этих пользователей нужно лишь предоставить права на использование созданного вами сервисного аккаунта.
Для сервисного аккаунта можно создать ключи для аутентификации в Yandex Cloud с помощью API, CLI и других инструментов. Эти ключи будут удалены при удалении сервисного аккаунта. Если сервисный аккаунт привязан к виртуальным машинам или функциям, в которых запускается ваша программа, то его токен доступа будет вызываться из машины или функции, то есть хранить ключи доступа и управлять ими не придётся.
Аутентификацию сервисного аккаунта можно выполнять тремя способами:
    Авторизованные ключи. Используются для получения IAM-токена с помощью открытого и секретного ключа. Этот способ вы примените в следующей практической работе.
    API-ключи. Используются в некоторых сервисах для упрощённой аутентификации вместо IAM-токена. Могут пригодиться для работы с сервисами SpeechKit, Vision и Translate.
    Статические ключи доступа. Необходимы при использовании AWS-совместимых сервисов, например в Object Storage или Message Queue.
Task:
Можно ли авторизоваться без предварительной аутентификации?
Decision:
+Можно. Если доступ к ресурсу или объекту открыт для всех пользователей интернета.
-Нет
Task:
Какой тип аккаунтов лучше использовать для сотрудников компании, у которой есть своя система учёта пользователей?
Decision:
-Сервисный
+Федеративный
-Яндекс ID
Task:
Ресурсная модель и права доступа
Decision:
Сервис IAM используется не только для того, чтобы контролировать доступ к облачным ресурсам. С его помощью также назначают права доступа. Для этого применяют ролевую модель управления доступом — RBAC (Role Based Access Control). То есть каждому пользователю можно назначить определённую роль для доступа к тому или иному ресурсу.
Ролевая модель
Во вводном курсе про виртуальные машины вы уже познакомились с ролевой моделью Yandex Cloud. Теперь разберем ее подробнее.
Роль — это набор разрешений, которые описывают допустимые операции над ресурсом. После назначения пользователю роли на ресурс он сможет выполнять заданный набор операций над этим ресурсом.
Роли на ресурс назначаются в виде списка связей «роль — субъект». Такие связи называются привязками прав доступа (access bindings), их можно добавлять и удалять, то есть контролировать права доступа к ресурсу.
Одна привязка — это назначение субъекту одной роли. Чтобы назначить пользователю несколько ролей на один ресурс, нужно задать отдельную привязку для каждой из них.
Роли наследуются: все разрешения от родительского ресурса переходят на его дочерние ресурсы. Например, если назначить пользователю роль на каталог, в котором есть виртуальная машина, то все разрешения этой роли будут действовать и для виртуальной машины.
Если у пользователя есть роли и на родительский, и на дочерний ресурс, то для дочернего ресурса будет действовать объединенный список разрешений. Иными словами, ограничить список разрешений, унаследованных от родительского ресурса, на уровне дочернего не получится.
В реализованной в Yandex Cloud ролевой модели имеется два типа ролей: примитивные (общие для всех сервисов) и сервисные (выдаются только на один сервис). Сервисные роли зависят от специфики каждого отдельного сервиса и могут назначаться на объекты, для которых эти роли предназначены. Некоторые сервисные роли назначаются на ресурс (например на ключ шифрования в сервисе KMS), а некоторые — только на контейнер, в котором размещены ресурсы (например роль compute.admin назначается на каталог или облако).
К примитивным ролям относятся:
    viewer — позволяет просматривать информацию о ресурсе или список ресурсов, если назначена на каталог;
    editor — позволяет управлять ресурсами: создавать, изменять и удалять объекты;
    admin — позволяет управлять ресурсами и правами доступа к ним.
Примитивные роли стоит использовать только для быстрой настройки и тестирования инфраструктуры. Сервисные роли обеспечивают более гранулярный контроль, учитывающий специфику каждого отдельного сервиса.
Для каждого сервиса можно расписать структуру ролей в виде иерархии, где разрешения нижележащих ролей входят в обобщённые вышележащие роли.
Например, в сервисе управления ключами шифрования (KMS) реализована следующая структура ролей.
Примитивные роли admin, editor, viewer позволяют управлять или просматривать информацию о сервисе в целом. Сервисные роли обеспечивают гранулированное разделение прав.
Так, например, роль kms.admin предоставляет права на администрирование ключей: их просмотр, создание, изменение, удаление, ротацию, шифрование и расшифрование данных. Она также позволяет назначать роль kms.keys.encrypterDecrypter на конкретные ключи. Роль kms.keys.encrypterDecrypter — пользовательская. Она позволяет выполнять операции шифрования и расшифрования для указанных ключей и просматривать информацию о ключе. Но управлять жизненным циклом ключей с ее помощью нельзя.
Разрешения роли kms.keys.encrypterDecrypter входят в разрешения ролей kms.admin и editor. А разрешения kms.admin входят в разрешения примитивной роли admin.
Управление ролями
Владелец облака обладает ролью resource-manager.clouds.owner. Она позволяет выполнять любые операции с облаком и ресурсами в нём.
Вторая важная роль — это resource-manager.clouds.member, участник облака. Она дает возможность пользователю с Яндекс ID пройти аутентификацию в облаке и необходима для выполнения операций с ресурсами. Например, если у пользователя есть роли resource-manager.clouds.member и editor, то он сможет управлять ресурсами облака. Но если роль resource-manager.clouds.member отозвать, то управлять уже не сможет, несмотря на наличие роли editor.
Исключение здесь составляют ресурсы с публичным доступом. Для управления ими роль resource-manager.clouds.member не требуется, доступ к ним можно предоставить иначе.
Рассмотрим сценарий использования сервисных ролей на примере сервиса KMS.
    Владелец (роль resource-manager.clouds.owner) или администратор (роль admin) облака назначает роль kms.admin администратору KMS.
    Администратор KMS создаёт необходимые ключи шифрования и назначает роли для их использования с помощью интерфейса командной строки (CLI) или API.
    Пользователи или сервисные аккаунты получают роль kms.keys.encrypterDecrypter на необходимые им ключи шифрования.
В самом сервисе IAM используется роль iam.serviceAccounts.user. Эта роль предоставляет пользователю права на использование сервисных аккаунтов. Например, при создании группы виртуальных машин вы хотите использовать ваш сервисный аккаунт, у которого есть права на эту операцию, вместо своего пользовательского аккаунта. Сервис IAM проверяет, есть ли у вас разрешение на использование этого сервисного аккаунта, и разрешает или запрещает данную операцию.
В роль iam.serviceAccounts.user входит следующий набор разрешений:
    получение списка сервисных аккаунтов;
    получение информации о сервисном аккаунте;
    использование сервисного аккаунта при выполнении операций от его имени.
Task:
Какие типы ролей есть в сервисе IAM Yandex.Cloud?
Decision:
-Администраторские
+Сервисные
-Пользовательские
+Примитивные
Task:
Один из типов ролей в IAM называется сервисным, потому что:
Decision:
-такие роли предназначены только для сервисных аккаунтов
+такие роли даются только на один сервис
-они входят в ролевую модель сервиса IAM
Task:
Практическая работа. Права доступа и роли для сервисного аккаунта
Decision:
В этом уроке вы научитесь работать с сервисными аккаунтами и назначать для них роли и права доступа к объектам. В качестве объекта будет выступать созданный в сервисе KMS ключ шифрования (подробнее об этом сервисе вы узнаете на одном из следующих занятий). Предположим, что перед вами стоит задача использовать сервисный аккаунт для ротации ключей.
Чтобы решить эту задачу, понадобится выполнить следующие шаги:
    Создать сервисный аккаунт.
    Получить права на управление этим сервисным аккаунтом.
    Создать статические ключи доступа и привязать их к сервисному аккаунту, чтобы он мог пройти авторизацию в сервисе KMS.
    Создать в сервисе KMS ключ шифрования и назначить сервисному аккаунту роль kms.admin для управления этим ключом.
    И, наконец, ротировать ключ, то есть создать его новую версию с такими же параметрами, из-под сервисного аккаунта.
Нужно заметить, что через консоль управления сервисному аккаунту можно назначить роль только на каталог, в котором он был создан. Роли на все остальные ресурсы назначаются с помощью CLI или API. Поэтому для выполнения этой практической работы вам понадобится вспомнить, как пользоваться утилитой yc, чему вы уже научились в курсе «DevOps и автоматизация».
Ну что же, поехали!
ШАГ 1
Для начала создадим сервисный аккаунт. В консоли управления войдите в каталог облака, в котором вы будете выполнять эту практическую работу, и перейдите на вкладку Сервисные аккаунты. Нажмите кнопку Создать сервисный аккаунт.
В открывшемся окне задайте для нового сервисного аккаунта имя и, при желании, добавьте описание. Здесь аккаунту также можно добавить роли на каталог, в котором он создаётся.
Оставьте поле с ролями пустым и нажмите Создать.
ШАГ 2
Настройте для вашего аккаунта на Яндексе доступ на авторизацию под созданным сервисным аккаунтом.
    Для начала убедитесь, что вы авторизованы в аккаунте с ролью admin. Чтобы это проверить, выполните команду
yc iam role list 
Вы увидите список ролей вашего аккаунта. Примерно такой (роль admin должна в нем присутствовать):
    Узнайте идентификатор своего аккаунта. Он понадобится, чтобы добавить вашему аккаунту роль editor на созданный сервисный аккаунт (сервисный аккаунт тоже является ресурсом, и для работы с ним нужна соответствующая роль). Воспользуйтесь для этого командой:
yc iam user-account get <имя_вашего_аккаунта> 
    Кроме того, нужно узнать идентификатор созданного сервисного аккаунта. Это можно сделать в разделе Сервисные аккаунты консоли управления. Выбрав нужный аккаунт в списке, вы перейдёте на страницу с детальной информацией о нем.
    Теперь предоставьте вашему пользовательскому аккаунту права на управление созданным сервисным аккаунтом. Для этого нужно выполнить команду
yc iam service-account add-access-binding <ID_сервисного_аккаунта> \
--role editor --subject userAccount:<ID_пользовательского_аккаунта> 
ШАГ 3
Настройте аутентификацию под сервисным аккаунтом с вашей машины.
    Сначала нужно создать статические ключи доступа и сохранить их в json-файле (например key.json).
Воспользуйтесь для этого командой
yc --folder-name <имя_каталога> iam key create \
--service-account-name <имя_сервисного_аккаунта> --output key.json 
После выполнения команды вы получите идентификатор созданной ключевой пары. Используя статические ключи доступа, можно получить IAM-токен для авторизации в сервисах.
    Теперь нужно создать профиль, от имени которого будут выполняться операции из-под сервисного аккаунта. Для этого придумайте имя этого профиля (например yc-lab23-profile) и выполните команду:
yc config profile create <имя_профиля> 
Привяжите к этому профилю ранее созданный статический ключ доступа с помощью команды:
yc config set service-account-key key.json 
    Чтобы убедиться, что всё сделано правильно, выведите информацию об авторизации и ключах доступа
yc config list 
Вы должны получить примерно такой результат:
ШАГ 4
    Теперь нужно создать ключ шифрования, ротацией которого вы будете управлять с помощью сервисного аккаунта. Для этого перейдите в дашборд каталога в консоли управления, нажмите кнопку Создать ресурс и выберите Ключ шифрования.
В открывшемся окне задайте для ключа имя и нажмите кнопку Создать. Новый ключ появится в списке в открывшемся разделе Ключи.
    Чтобы назначить сервисному аккаунту роль для какого-либо ресурса, нужно знать идентификатор этого ресурса. Нажмите строку с созданным ключом, чтобы перейти на страницу детальной информации о нём, и скопируйте ID ключа.
    Перейдем к назначению сервисному аккаунту роли kms.admin для управления созданным ключом шифрования. Перед этим нужно сначала вернуться в профиль вашего аккаунта.
yc config profile activate <имя_профиля> 
Выполните команду:
yc --folder-name <имя_каталога> kms symmetric-key \
add-access-binding <ID_ключа_шифрования> --role kms.admin \
--subject serviceAccount:<ID_сервисного_аккаунта> 
Теперь с помощью сервисного аккаунта вы можете управлять этим ключом шифрования.
ШАГ 5
    В CLI переключитесь обратно в профиль сервисного аккаунта:
yc config profile activate <имя_профиля_сервисного_аккаунта> 
Ротируйте ключ шифрования:
yc kms symmetric-key rotate <ID_ключа> 

    После выполнения команды перейдите на страницу детальной информации о ключе и откройте вкладку Операции.
Вы увидите, что операция по ротации ключа выполнена под вашим сервисным аккаунтом. Значит, всё получилось и задача решена!
Decision:

Task:
Лучшие практики по работе с учетными записями
Decision:
В последних уроках этой и следующих тем мы будем рассказывать о лучших практиках обеспечения безопасности. Возможно, не все из них будут применимы для вашего проекта, но это полезные практические приемы, и о них в любом случае стоит знать.
Управление привилегированными пользователями
К привилегированным пользователям относятся учетные записи со следующими ролями:
    resource-manager.clouds.owner;
    billing.accounts.owner;
    роль admin, назначенная всему облаку;
    роль admin, назначенная каталогу;
    роль admin, назначенная платежному аккаунту.
Привилегированными они называются потому, что могут делать в облаке гораздо больше, чем обычный пользователь. Поэтому и результат их действий может оказаться крайне неприятным.
Пользователь, который создает облако, автоматически получает роль resource-manager.clouds.owner. Она позволяет выполнять любые операции с облаком и ресурсами в нем, а также выдавать доступ другим пользователям путем назначения и отзыва ролей.
Если ваша компания использует федерацию удостоверений, то рекомендуется назначить эту роль одному или нескольким сотрудникам. Их федеративные аккаунты должны быть надёжно защищены с помощью:
    двухфакторной аутентификации;
    запрета на аутентификацию с посторонних устройств;
    мониторинга попыток входа и заданных порогов предупреждений.
Для аккаунта Яндекс ID, под которым создано облако, нужно назначить сложный пароль, а использовать его — только в случае крайней необходимости (например, если федерация сломалась).
Роли admin на облака, каталоги и платежные аккаунты рекомендуется назначать только федеративным учетным записям. При этом число таких учетных записей должно быть минимально необходимым, а потребность пользователей в такой роли следует регулярно перепроверять.
Еще одна важная роль — это billing.accounts.owner, которая автоматически выдается при создании платёжного аккаунта и не может быть переназначена другому пользователю. Она позволяет выполнять любые действия с платёжным аккаунтом. Роль billing.accounts.owner может быть назначена только аккаунту Яндекс ID. Аккаунт с этой ролью используется при настройке способов оплаты и подключении облаков.
Безопасности такого аккаунта следует уделять повышенное внимание, поскольку он обладает значительными полномочиями и не может быть подключен к федерации корпоративных аккаунтов.
Наиболее правильным подходом можно считать отказ от регулярного использования аккаунта с этой ролью, то есть его нужно использовать только при первоначальной настройке и при внесении изменений. На время активного использования этого аккаунта включите двухфакторную аутентификацию (2FA) в Яндекс ID. Затем, если вы не используете способ оплаты банковской картой (доступный только для данной роли), назначьте этому аккаунту сложный пароль, сгенерированный с помощью специализированного ПО, отключите 2FA и не используйте этот аккаунт без необходимости.
После каждого использования генерируйте новый пароль. Отключение 2FA для этого аккаунта важно в ситуации, если аккаунт не закреплён за конкретным сотрудником. Это позволяет избежать привязки критически важного аккаунта к личному устройству.
Использование ресурсной модели
Если система должна соответствовать требованиям PCI DSS, то при разработке модели доступа для создаваемой инфраструктуры рекомендуется использовать следующий подход:
    все ресурсы, которые входят в область соответствия PCI DSS, нужно поместить в отдельное облако;
    группы ресурсов, которые требуют разного административного доступа, помещают в разные каталоги (например, DMZ, security, backoffice и т.д.);
    общие ресурсы (например, сеть и группы безопасности) помещают в отдельный каталог для разделяемых ресурсов.
Последние два пункта стоит иметь в виду при построении любой сложной инфраструктуры в облаке, даже если требования PCI DSS вас пока не беспокоят.
Использование сервисных аккаунтов
При использовании сервисных аккаунтов рекомендуется:
    для назначения сервисного аккаунта на виртуальную машину и получения токена использовать сервис метаданных;
    настроить на виртуальной машине локальный файрвол;
    обеспечить безопасное хранение и управление ключами сервисного аккаунта;
    следовать принципу минимальных привилегий и назначать сервисному аккаунту только те роли, которые необходимы для работы приложения;
    следовать принципу минимальных привилегий и в отношении доступа к самому сервисному аккаунту, то есть выдавать роли на него минимальному кругу пользователей и только при необходимости.
Task:
Принципы обеспечения сетевой безопасности
Decision:
 Чтобы передавать информацию между ресурсами облака и подключить облако к интернету, нужны облачные сети. В курсе «Виртуальные машины» вы уже познакомились с сервисом Yandex Virtual Private Cloud. С помощью этого сервиса создают сети и подсети в разных зонах доступности Yandex Cloud, назначают облачным ресурсам — виртуальным машинам и кластерам баз данных — внутренние и публичные IP-адреса, конфигурируют сеть так, чтобы она работала надёжно и безопасно.
На этом уроке мы разберём принципы, лежащие в основе безопасного функционирования облачных сетей. Их общую архитектуру можно представить следующим образом.
По этой схеме можно понять, какие механизмы и инструменты обеспечения безопасности следует использовать.
Доступность ресурсов
Прежде всего, сеть должна надёжно выполнять свои функции — в том числе, обеспечивать высокую доступность ресурсов. Для этого в Yandex Cloud используются три зоны доступности. Облачная сеть является глобальным объектом, а в зонах доступности размещаются зональные элементы инфраструктуры — например, виртуальные машины. Для связи элементов внутри каждой зоны создаются подсети.
Чтобы обеспечить высокую доступность ресурсов, следует размещать их в нескольких зонах доступности. Некоторые сервисы, например, управляемые базы данных, поддерживают размещение хостов в разных дата-центрах сразу из коробки.
Периметр безопасности
Следующий принцип построения сетей — создание периметра безопасности. Идея заключается в том, чтобы предусмотреть минимально необходимое количество точек выхода в интернет и обеспечить их защиту.
Здесь напрашивается аналогия. Представьте две крепости. В первую можно войти только через одни хорошо укреплённые ворота. Во вторую — через несколько ворот: часть из них укреплена хорошо, часть — так себе, а о существовании одного прохода с обычной калиткой помнят лишь немногие старожилы. Не надо быть Сунь-Цзы, чтобы сказать, какую из крепостей легче защитить в случае атаки неприятеля.
Для построения периметра безопасности используются следующие подходы:
    В сети создают несколько подсетей с разными политиками выхода в интернет с помощью трансляции адресов (NAT). В Yandex Cloud есть два варианта: непосредственно включить NAT для подсети или создать NAT-инстанс — хост с сетевым интерфейсом, публичным IP-адресом и преднастроенными правилами маршрутизации и трансляции IP-адресов, через который ресурсы подсети получат доступ в интернет. NAT обеспечивает сетевую безопасность за счёт того, что сетевая топология и внутренние IP-адреса ресурсов не видны из-за пределов сети. Однако нужно понимать, что сам по себе NAT не может заменить межсетевые экраны.
    Если виртуальной машине не нужна связь с ресурсами в интернете, её следует размещать в подсети, для которой выход в интернет отключён.
    Для работы со входящим трафиком следует использовать лишь минимально необходимое число открытых портов и внешних IP-адресов для подключения. Целесообразно использовать сетевой балансировщик трафика для группы виртуальных машин во внутренней сети.
    Безопасно администрировать виртуальные машины можно через консоль управления Yandex Cloud. Также поможет VPN-шлюз на виртуальной машине с внешним IP-адресом. Это позволит сократить количество внешних адресов и открытых портов для обслуживания облачной инфраструктуры.
    Для ресурсов с внешними IP-адресами следует включить сервис защиты от DDoS-атак. Этот сервис помогает бороться с атаками на уровень L7 сетевой модели OSI, которые направлены на исчерпание ёмкости канала и вычислительных ресурсов виртуальных машин. Для защиты веб-ресурсов в этом случае можно использовать один из сервисов WAF (Web Application Firewall), доступных на Marketplace.
Принцип наименьших привилегий
Третий элемент обеспечения сетевой безопасности в облаке — реализация принципа наименьших привилегий. Этот принцип требует, чтобы каждый ресурс в вашем облаке был связан только с теми ресурсами в облаке или интернете, которые необходимы для его корректной работы.
Чтобы реализовать этот принцип на практике, используют группы безопасности. Такая группа назначается сетевому интерфейсу при создании или изменении виртуальной машины. Она содержит правила, которые определяют протоколы и IP-адреса для получения и отправки трафика. Группы безопасности действуют по принципу «запрещено всё, что не разрешено».
В группе безопасности может быть определено несколько правил, а для одной виртуальной машины можно назначить несколько групп. То, что может делать виртуальная машина в сети, определяется совокупностью всех правил назначенных для нее групп безопасности. Подробнее о том, как использовать группы безопасности, вы узнаете на одном из следующих уроков.
Task:
Что такое DDoS-атака?
Decision:
-Атака типа «Отказ в обслуживании», которая выполняется за счёт распределения нагрузки между серверами
+Атака типа «Отказ в обслуживании», которая основана на отправке большого числа пакетов TCP SYN или трафика прикладных протоколов
-Атака типа «Человек посередине» за счёт перехвата ключей шифрования в распределённой инфраструктуре
Task:
В чём заключается основная идея принципа наименьших привилегий, используемого для обеспечения информационной безопасности инфраструктуры?
Decision:
+Каждый элемент должен иметь доступ только к той информации и ресурсам, которые необходимы для их функционирования
-Каждая виртуальная машина должна запускаться с минимально необходимыми ресурсами
-Каждый сервис должен запускаться под учётной записью с минимальными правами 
Task:
Практическая работа. Организация защищённого канала
Decision:
Защита данных, передаваемых между вашей локальной инфраструктурой и облаком, — важный элемент информационной безопасности. А удалённая работа, которая получила распространение в период пандемии коронавируса и сейчас закрепилась в практиках многих компаний, сделала эту задачу ещё более актуальной.
Чтобы защитить передаваемую информацию, используют VPN (Virtual Private Network) — технологию, позволяющую развернуть защищённое сетевое соединение «поверх» незащищённой сети (чаще всего это интернет). VPN-соединение представляет собой канал передачи данных между двумя узлами. Этот канал обычно называют VPN-туннелем. Если за одним из узлов находится целая сеть, то его называют VPN-шлюзом.
Механизм работы VPN:
    Перед созданием туннеля узлы идентифицируют друг друга, чтобы удостовериться, что шифрованные данные будут отправлены на нужный узел.
    На обоих узлах нужно заранее определить, какие протоколы будут использоваться для шифрования и обеспечения целостности данных.
    Узлы сверяют настройки, чтобы договориться об используемых алгоритмах. Если настройки разные, туннель не создаётся.
    Если сверка прошла успешно, то создаётся ключ, который используется для симметричного шифрования.
Этот механизм регламентируют несколько стандартов. Один из самых популярных — IPSec (Internet Protocol Security).
На этом уроке вы научитесь настраивать IPSec VPN-туннель между двумя VPN-шлюзами с помощью демона strongSwan. Один шлюз вы настроите на виртуальной машине в Yandex Cloud, второй — на своей локальной машине или виртуальной машине в другой облачной сети.
Шаг 1. Создание ресурсов
Для практической работы вам понадобится сеть и подсеть в Yandex Cloud, а также созданная в этой подсети тестовая ВМ без публичного IP-адреса. Создайте эти ресурсы, если у вас их нет.
Теперь создадим IPSec-инстанс — ВМ, которая будет служить шлюзом для IPSec-туннеля. Чтобы это сделать:
    Откройте ваш каталог, нажмите кнопку Создать ресурс и выберите пункт Виртуальная машина.
    В поле Имя задайте имя ВМ, например ipsec-instance.
    Выберите зону доступности, где находится подсеть, к которой будет подключён IPSec-инстанс, и тестовая ВМ.
    В разделе Выбор образа/загрузочного диска перейдите в блок Cloud Marketplace и выберите образ IPSec-инстанс.
    В блоке Сетевые настройки выберите нужную сеть, подсеть и назначьте ВМ публичный IP-адрес из списка или автоматически.
Важно использовать только статические публичные IP-адреса из списка или сделать IP-адрес ВМ статическим после её создания. Динамический IP-адрес может измениться после перезагрузки ВМ, и туннель перестанет работать.
    В блоке Доступ укажите логин и SSH-ключ для доступа к ВМ.
    Нажмите кнопку Создать ВМ.
Виртуальная машина готова.
Шаг 2. Настраиваем IPSec
Теперь настроим шлюз с публичным IP-адресом, который будет устанавливать IPSec-соединение с удалённым шлюзом (вашей локальной машиной или ВМ в другой облачной сети).
Вы можете создать в своём каталоге ещё одну облачную сеть с подсетью, создать в ней IPSec-инстанс из образа и использовать его в качестве удалённого шлюза. Либо можно использовать в качестве шлюза машину в вашей локальной сети. Вам понадобится публичный IP-адрес удалённого шлюза и CIDR подсети.
Допустим, публичный IP-адрес вашего шлюза — 130.193.32.25, а за ним находится подсеть c префиксом подсети CIDR 10.128.0.0/24. Шлюз будет устанавливать IPSec-соединение с удалённым шлюзом с IP-адресом, например, 1.1.1.1, за которым находится подсеть с префиксом подсети CIDR 192.168.0.0/24.
    Подключитесь к ВМ IPSec-инстанс по SSH:
ssh <имя пользователя>@130.193.32.25 
    Откройте конфигурацию IPSec:
sudo nano /etc/ipsec.conf 
    В разделе config setup файла конфигурации задайте следующие параметры:
config setup
        charondebug="all"
        uniqueids=yes
        strictcrlpolicy=no 
    Добавьте новый раздел с описанием тестового соединения, например conn cloud-to-hq.
    Задайте параметры тестового соединения:
leftid — публичный IP-адрес IPSec-инстанса.
leftsubnet — CIDR подсети, к которой подключён IPSec-инстанс.
right — публичный IP-адрес шлюза на другом конце VPN-туннеля.
rightsubnet — CIDR подсети, к которой подключён VPN-шлюз на другом конце VPN-туннеля.
Параметры ike и esp — это алгоритмы шифрования, которые поддерживаются на удалённом шлюзе. Перечень поддерживаемых алгоритмов можно посмотреть на сайте strongSwan: IKEv1 и IKEv2.
    Укажите остальные настройки, консультируясь с документацией strongSwan и учитывая настройки удалённого шлюза.
    У вас должна получиться примерно такая конфигурация:
conn cloud-to-hq
        authby=secret
        left=%defaultroute
        leftid=130.193.32.25
        leftsubnet=10.128.0.0/24
        right=1.1.1.1
        rightsubnet=192.168.0.0/24
        ike=aes256-sha2_256-modp1024!
        esp=aes256-sha2_256!
        keyingtries=0
        ikelifetime=1h
        lifetime=8h
        dpddelay=30
        dpdtimeout=120
        dpdaction=restart
        auto=start 
    Сохраните изменения и закройте файл.
    Откройте файл /etc/ipsec.secrets и укажите в нём пароль для установки соединения:
130.193.32.25 1.1.1.1 : PSK "<пароль>" 
    Перезапустите strongSwan:
sudo systemctl restart strongswan-starter 
Шаг 3. Настраиваем статическую маршрутизацию
Теперь нужно настроить маршрутизацию между IPSec-инстансом и тестовой ВМ без публичного IP-адреса. Для этого создадим таблицу маршрутизации и добавим в неё статические маршруты.
    Откройте сервис Virtual Private Cloud в каталоге, где требуется создать статический маршрут.
    Выберите раздел Таблицы маршрутизации в панели слева и нажмите кнопку Создать таблицу маршрутизации.
    Задайте имя таблицы маршрутизации, выберите сеть, в которой требуется её создать, и нажмите кнопку Добавить маршрут.
    В открывшемся окне введите префикс подсети назначения на удалённой стороне, в примере это 192.168.0.0/24.
    В поле Next hop укажите внутренний IP-адрес IPSec-шлюза и нажмите кнопку Добавить.
    Нажмите кнопку Создать таблицу маршрутизации.
    Чтобы использовать статические маршруты, нужно привязать таблицу маршрутизации к подсети. Для этого в разделе Подсети, в строке нужной подсети, нажмите кнопку … и в открывшемся меню выберите пункт Привязать таблицу маршрутизации.
    В открывшемся окне выберите созданную таблицу и нажмите кнопку Привязать. Созданный маршрут можно применять и к другим подсетям этой сети.
Шаг 4. Настраиваем IPSec на другом шлюзе
Для работы VPN-туннеля нужно настроить второй шлюз.
    Настройте strongSwan аналогично первому IPSec-шлюзу, но с зеркальными настройками IP-адресов и подсетей в файле /etc/ipsec.conf. Должна получиться такая конфигурация:
conn hq-to-cloud
        authby=secret
        left=%defaultroute
        leftid=1.1.1.1
        leftsubnet=192.168.0.0/24
        right=130.193.32.25
        rightsubnet=10.128.0.0/24
        ike=aes256-sha2_256-modp1024!
        esp=aes256-sha2_256!
        keyingtries=0
        ikelifetime=1h
        lifetime=8h
        dpddelay=30
        dpdtimeout=120
        dpdaction=restart
        auto=start 
    Укажите пароль для соединения в файле /etc/ipsec.secrets, указав IP-адреса шлюзов в обратном порядке:
1.1.1.1 130.193.32.25 : PSK "<пароль>" 
    Перезапустите strongSwan:
sudo systemctl restart strongswan-starter 
Шаг 5. Проверяем, что всё работает
Чтобы убедиться, что туннель между шлюзами установлен, выполните на любом из шлюзов команду:
sudo ipsec status 
Если всё в порядке, то у вас должно появиться примерно такое сообщение:
Security Associations (1 up, 0 connecting):
hq-to-cloud[3]: ESTABLISHED 29 minutes ago, 10.128.0.26[130.193.33.12]...192.168.0.23[1.1.1.1]
hq-to-cloud{3}:  INSTALLED, TUNNEL, reqid 3, ESP in UDP SPIs: c7fa371d_i ce8b91ad_o
hq-to-cloud{3}:   10.128.0.0/24 === 192.168.0.0/24 
Статус ESTABLISHED означает, что туннель между шлюзами создан.
Сведения об установке и работе соединения находятся в логах strongSwan. Просмотреть логи можно с помощью команды:
sudo journalctl -u strongswan-starter 
Проверить статус демона strongSwan можно командой:
systemctl status strongswan-starter 
Осталось проверить связность соединения. Для этого создайте ещё одну тестовую виртуальную машину за вторым шлюзом, а затем пропингуйте одну тестовую машину с другой.
Поздравляем, ваше соединение с облаком безопасно!
Если вы не планируете использовать созданный VPN-туннель, удалите ненужные ресурсы.
Decision:

Task:
Группы безопасности
Decision:
О том, что такое группы безопасности, вы узнали в курсе «Виртуальные машины». В начале этого урока мы кратко напомним их суть, но возможно вам будет полезно освежить в памяти детали.
Группы безопасности используются для контроля входящего и исходящего трафика, то есть выполняют функцию межсетевого экрана. Фактически они представляют собой набор правил, которые назначаются сетевому интерфейсу. Эти правила устанавливают протоколы и диапазоны адресов и портов для получения и отправки трафика. Правила для входящего и исходящего трафика определяют отдельно.
Основной принцип, по которому работают группы безопасности: запрещено всё, что не разрешено в явном виде. Поэтому используемые в группах безопасности правила только разрешают что-либо. Если на ресурс распространяется действие нескольких групп безопасности, то возможность отправлять и получать трафик для него будет определяться совокупностью всех правил. Порядок их применения не имеет значения, поскольку сами по себе правила ничего не запрещают.
На этом и следующем уроках мы рассмотрим, как применять группы безопасности для защиты своих ресурсов в облаке. Начнем с того, в каких сервисах их можно использовать.
Compute Cloud
При создании или изменении ВМ вы можете назначить одну или несколько групп безопасности на каждый сетевой интерфейс. Если передать пустой список, то ВМ будет назначена группа безопасности по умолчанию для этой сети. Для назначения групп безопасности можно использовать консоль управления, интерфейс командной строки или Terraform.
Instance Groups
При создании групп виртуальных машин список групп безопасности указывается в шаблоне ВМ. Они будут переданы создаваемым ВМ точно так же, как и в сервисе Compute Cloud. Важный момент здесь заключается в том, что у каждой ВМ в рамках группы должен быть одинаковый список групп безопасности, по-другому нельзя.
Сетевой балансировщик нагрузки
Часто бывает необходимо защитить свой веб-сервер от несанкционированного доступа извне. Для этого можно применить сочетание сетевого балансировщика и групп безопасности.
Важно: для такого сценария группы безопасности применяются к ВМ за балансировщиком, а не к самому балансировщику.
В группе безопасности (на рисунке ниже она называется SG-LB) вы можете привести перечень IP-адресов, которые могут подключаться к вашему веб-сервису, и указать порты подключения (например 443 и 80).
Прикладной балансировщик нагрузки (L7-балансировщик)
В отличие от сетевого, прикладной балансировщик нагрузки (Application Load Balancer) поддерживает использование групп безопасности, они назначаются на него непосредственно. В этом случае группы безопасности позволяют контролировать потоки трафика, проходящие через прикладной балансировщик до групп бэкендов и целевых групп.
Следует также иметь в виду, что для такой схемы понадобится разрешить проверки состояния балансировщика, которые могут приходить по 80, 443 или какому-либо другому порту. Диапазон IP-адресов, с которых балансировщик отправляет проверки состояния, приведен в документации. Чтобы не запоминать эти адреса, в настройках группы безопасности добавьте правило для входящего трафика: в поле Диапазон портов укажите 1-32767, Протокол — TCP, Источник — Проверки состояния балансировщика.
Если использование этого диапазона не будет разрешено, то балансировщик отметит ваш сервис как неработоспособный (unhealthy).
Управляемые базы данных
Как и в группах виртуальных машин, в сервисах управляемых баз данных группа безопасности назначается на кластер целиком. Назначить на отдельные хосты разные группы безопасности нельзя.
При назначении групп безопасности важно учитывать, что IP-адреса хостов в кластере могут меняться, то есть использовать в правилах IP-адрес какого-либо хоста не стоит.
Сценарий использования групп безопасности
Рассмотрим теперь, как с помощью групп безопасности и некоторых других инструментов (сетевого балансировщика, сервиса DDoS Protection и VPN) можно защитить, например, веб-сервис.
Допустим наша система состоит из базы данных, сервера, на котором развёрнуто веб-приложение, и балансировщика нагрузки.
ШАГ 1
Сначала создадим для неё базовую сетевую инфраструктуру. Для этого можно воспользоваться сетью по умолчанию с тремя подсетями в каждой из зон доступности. Далее понадобится зарезервировать два публичных IP-адреса: один для сетевого балансировщика и один для VPN. При резервировании IP-адреса для балансировщика включаем сервис DDoS Protection.
Нам, естественно, будут нужны виртуальные машины, на которых будет работать веб-сервис. Чтобы обеспечить отказоустойчивость сервиса, развернём их в трёх зонах доступности.
Ещё понадобится кластер управляемой базы данных (например MySQL). Разместим хосты кластера также в трёх зонах доступности для отказоустойчивости.
Последний компонент — это IPSec-шлюз для удалённого доступа. С тем, как его создавать, вы познакомились на прошлом уроке.
ШАГ 2
Чтобы организовать удалённый доступ к ресурсам в облаке, следует настроить маршрутизацию на уровне облака, а не на уровне ВМ. Для этого нужно создать таблицу маршрутизации для удалённой сети (например 192.168.0.0/24), а в качестве параметра Next hop указать адрес виртуального шлюза.
Эту таблицу маршрутизации нужно привязать к каждой из трёх подсетей, чтобы можно было обеспечить удалённый доступ ко всем ресурсам.
Теперь, когда в нашей сетевой инфраструктуре создано все, что необходимо, — кластер для размещения базы данных, ВМ для веб-сервиса и VPN-шлюз для удалённого администрирования — займёмся ограничением доступа к ней извне.
ШАГ 3
Прежде всего нужно защитить VPN-шлюз. Воспользуемся для этого группами безопасности. Создадим группу безопасности (например Security Group-VPN), в которой зададим правила для входящего и исходящего трафика.
Правило для входящего трафика должно содержать разрешение на доступ по протоколу UDP через порты 500 и 4500 с опредёленного IP-адреса, на котором располагается шлюз с другой стороны VPN-туннеля. Также важно обеспечить сетевую связность между подсетями на стороне облака и в нашей локальной инфраструктуре. Поэтому добавим в группу безопасности еще одно правило, разрешающее трафик между ними, указав адреса этих подсетей (например 10.0.0.0/8 и 192.168.0.0/24).
Инфраструктура, которая у нас на данный момент получилась, представлена на схеме ниже.
ШАГ 4
Следующим шагом защитим ВМ с помощью группы безопасности (назовём её, например, sg-web). Для этого настроим для них правила для входящего и исходящего трафика.
Так как наш веб-сервис публичный, мы разрешаем входящий трафик по протоколу TCP через 80 и 443 порты с любого IP-адреса. Чтобы администрировать веб-серверы можно было только по защищённому соединению, разрешим доступ из подсети 192.168.0.0/24 по протоколу SSH.
Также создадим правило, разрешающее проверки состояния балансировщиком. В данном примере это правило избыточное, поскольку у нас публичный веб-сервис. Но если вы хотите ограничить список IP-адресов, с которых можно подключиться к веб-сервису, о нем важно не забывать.
Ещё одно правило с назначением Self разрешает трафик внутри самой группы безопасности, чтобы ВМ могли общаться друг с другом, например для синхронизации или передачи файлов контента.
Для исходящего трафика одно правило будет разрешать трафик между ВМ, а второе — трафик в сторону базы данных по протоколу TCP через порт 3306. Во втором случае назначением будет являться группа безопасности для кластера базы данных.
ШАГ 5
Перейдем к настройке правил для базы данных. Нам понадобится разрешить входящий трафик через порт 3306. Источником этого трафика будут ВМ, входящие в группу безопасности sg-web, которую мы создали ранее. Это защитит базу данных от несанкционированных подключений извне.
Созданные группы безопасности надо привязать к ВМ и кластеру базы данных. Это делается в настройках ВМ и кластера через консоль управления.
ШАГ 6
Последний шаг — это создание балансировщика нагрузки. Используем для его защиты сервис DDoS Protection, а в качестве целевой группы балансировщика укажем наши ВМ. Включим опцию проверки состояния ВМ, находящихся за балансировщиком.
Давайте посмотрим, что же получилось в итоге.
Мы создали систему, в которой:
    есть балансировщик нагрузки, защищённый от DDoS-атак;
    входящий трафик веб-приложения балансируется по трём ВМ;
    доступ к этому веб-приложению разрешён только для удалённых администраторов через VPN;
    открыты только необходимые порты (80 и 443);
    база данных получила дополнительную защиту, поскольку доступ к ней разрешён только с наших ВМ.
Наша система защищена: в ней обеспечена сетевая сегментация, нежелательный трафик фильтруется, разрешены только необходимые для работы порты и протоколы, ограничен удалённый доступ для администрирования.
Task:
Применение групп безопасности для сегментации и изоляции ресурсов
Decision:
На этом уроке вы узнаете, как группы безопасности могут использоваться для сегментации и изоляции ресурсов.
Изоляция ресурсов может быть важной во многих сценариях, например, когда требуется:
    создать инфраструктуру, в которой работает несколько команд или подразделений компании;
    разграничить инфраструктуры пользователей какого-либо SaaS-сервиса, чтобы они пользовались общими ресурсами, но не видели работу друг друга;
    обеспечить общий контролируемый доступ в интернет.
Рассмотрим следующую ситуацию. Предположим, наша облачная инфраструктура должна включать три сетевых сегмента: сегмент с общими сетевыми ресурсами для доступа в интернет, сегмент команды разработки и сегмент базы данных. Перед нами стоит задача повысить безопасность инфраструктуры путем разграничения этих сегментов и их изоляции друг от друга.
Эту задачу можно решить двумя путями: с использованием групп безопасности и без использования. Давайте сравним.
Вариант 1. Без использования групп безопасности
Если группы безопасности не использовать, то примерное решение будет выглядеть так.
Нам потребуется создать три виртуальных сети (на схеме они обозначены как VPC – public-net, VPC – dev-net и VPC – db-net). В каждой из них надо настроить корректную маршрутизацию, чтобы трафик передавался только в те подсети, куда он должен идти. Далее в каждой сети надо развернуть и настроить несколько виртуальных межсетевых экранов с несколькими интерфейсами каждый, которые будут обеспечивать маршрутизацию пакетов. В межсетевых экранах понадобится настроить правила фильтрации, правила маршрутизации и правила доступа.
Такая схема обеспечивает безопасность, но она сложна в эксплуатации. К её минусам относятся:
    обязательное использование межсетевых экранов для сегментации;
    настройка безопасности облака через интерфейсы сторонних приложений. Администратору придется постоянно переключаться между консолью управления Yandex Cloud и интерфейсом межсетевого экрана;
    необходимость настраивать одновременно параметры и облачных сетей, и межсетевого экрана заметно усложняет процесс и повышает вероятность ошибок.
Вариант 2. С использованием групп безопасности
Использование групп безопасности заметно упрощает решение задачи.
Во-первых, понадобится только одна сеть на всю организацию или проект. В этой сети создаются подсети: по одной для каждого из нужных нам трёх сегментов и подсеть для администраторов. В подсетях создаются виртуальные машины и другие необходимые ресурсы, на которые назначаются группы безопасности.
Каждая группа безопасности будет определять, во-первых, кто имеет доступ к ресурсам сегмента и, во-вторых, куда имеют доступ сами эти ресурсы. Когда используется такая схема, нет необходимости разворачивать отдельные сетевые экраны и настраивать сложную маршрутизацию. Требуется проанализировать структуру организации или проекта, понять кому какой доступ нужен и написать соответствующие правила для групп безопасности.
Еще одно преимущество использования групп безопасности для сегментации и изоляции ресурсов заключается в том, что при появлении новых проектов и новых ресурсов не нужно задумываться о новой маршрутизации и дополнительных настройках межсетевых экранов. Если, например, в инфраструктуре появилась новая база данных, достаточно назначить ей группу безопасности и сослаться на неё в других группах.
Таким образом, использование групп безопасности не только обеспечивает защиту облачной инфраструктуры, но и помогает значительно упростить её администрирование.
Task:
Лучшие практики обеспечения сетевой безопасности
Decision:
На этом уроке мы обобщим, какими проверенными методами нужно пользоваться, чтобы обеспечить безопасность ваших облачных сетей.
Доступ в инфраструктуру для администраторов
Одним из первых шагов, используемых для построения безопасной инфраструктуры, является организация доступа администраторов к ней по защищённому каналу. Для доступа по SSH создают бастионную виртуальную машину или VPN-шлюз. Доступ к такой машине или шлюзу из интернета должен быть ограничен при помощи групп безопасности.
Для дополнительного контроля действий администраторов рекомендуется использовать решения PAM (Privileged Access Management) с записью сессии администратора (например Teleport).
При организации доступа по SSH и VPN рекомендуется отказаться от использования паролей и использовать ключи доступа и X.509-сертификаты.
Доставка трафика в приложение и сетевая сегментация
Для защиты виртуальных машин на уровне облачной сети отдельно выделяют DMZ (так называемую демилитаризованную зону, то есть подсети с ресурсами, к которым открыт доступ из интернета) и другие сегменты. Для этого рекомендуется использовать механизм групп безопасности.
Чтобы доставлять трафик в приложение, находящееся в облачной инфраструктуре, рекомендуется использовать сетевой балансировщик нагрузки, который пропускает трафик только по заданным портам. Балансировщик следует использовать совместно с группами безопасности для ограничения списка IP-адресов, имеющих доступ к приложению.
Исходящий доступ в интернет
Для организации исходящего доступа в интернет следует использовать статические публичные IP-адреса. Принимающая сторона сможет внести их в список исключений своего файрвола. И независимо от того, статические или динамические публичные IP-адреса вы используете, убедитесь, что для ресурсов применяются группы безопасности.
Для исходящего трафика NAT-шлюз лучше не задействовать: через его IP-адрес могут отправлять трафик сразу несколько пользователей. Эту особенность нужно учитывать при моделировании угроз для инфраструктуры на базе Yandex Cloud, соответствующей стандартам PCI DSS.
Шифрование данных при передаче
При работе с чувствительными данными нужно шифровать трафик на уровне приложения, например, с использованием протокола TLS 1.2 и выше.
При использовании API Yandex Cloud следует убедиться, что в TLS-клиенте отключена возможность соединения с использованием небезопасных протоколов TLS (версии ниже 1.2) или что небезопасные протоколы не будут использованы при установлении соединения. Например, использование gRPC-интерфейсов Yandex Cloud гарантирует работу по TLS 1.2 и выше, так как протокол HTTP/2, на основе которого работает gRPC, устанавливает TLS 1.2 в качестве минимальной поддерживаемой версии протокола TLS.
Запись информации о входящем и исходящем трафике в облачной сети (flow logs) и обнаружение вторжений относится к ответственности пользователя. Используйте для решения этой задачи популярные системы обнаружения вторжений (например Suricata или Snort). Для управления потоками трафика и отправки их в такую систему можно использовать статические маршруты.
Task:
Сервис Certificate Manager
Decision:
На сегодняшний день большинство сайтов защищает передаваемые данные с помощью протокола HTTPS. Этот протокол основан на использовании текстового протокола HTTP поверх протокола безопасности транспортного уровня TLS. Когда вы используете TLS-протокол, информация передаётся внутри зашифрованной сессии. Для работы этого протокола нужно, чтобы на вашем сервере был установлен TLS-сертификат, а ваше устройство должно доверять этому сертификату.
TLS-сертификаты выпускаются доверенными центрами сертификации (Certification Authority). Список доверенных центров можно найти в настройках любого браузера. Сайты с сертификатами из этого списка считаются безопасными.
Сертификат можно купить, а можно получить бесплатно. В первом случае центр сертификации проводит расширенную проверку домена и даёт гарантии надёжности сертификата, то есть в случае утечки данных центр выплатит компенсацию. В случае бесплатного сертификата центр сертификации отвечает только за шифрование данных, но ответственность за их утечку лежит на пользователе.
С помощью сертификатов в HTTPS осуществляется аутентификация. По крайней мере на стороне сервера должен находиться доверенный сертификат, который выдан на соответствующее имя домена. При подключении к серверу клиентское приложение проверяет этот сертификат, в том числе:
    доменное имя сайта на совпадение с именем в сертификате;
    политики применения сертификата;
    информацию об издателе сертификата;
    срок действия сертификата.
Жизненным циклом сертификатов необходимо управлять: отслеживать окончание срока действия, формировать новые запросы на сертификаты с генерацией новой пары ключей, устанавливать сертификат и связанный с ним секретный ключ на серверы.
В Yandex Cloud для управления жизненным циклом сертификатов и их использованием предназначен сервис Certificate Manager.
Для корректной работы в этом сервисе сертификаты должны удовлетворять ряду требований:
    соответствовать стандарту X.509 v3;
    содержать публичный ключ, доменное имя сайта и информацию об издателе;
    быть актуальными на момент импорта (импортировать сертификат до начала и после окончания срока его действия нельзя);
    приватный ключ сертификата не должен быть зашифрован, то есть импортировать защищённый паролем приватный ключ нельзя;
    сертификат, цепочка промежуточных сертификатов и приватный ключ должны импортироваться в формате PEM-Encoded.
Сервис поддерживает два типа сертификатов:
    Пользовательские, которые импортирует сам пользователь. За обновлением таких сертификатов нужно следить самостоятельно.
    Сертификаты, которые выпускаются с помощью сервиса Let's Encrypt. Такие сертификаты управляются непосредственно сервисом Certificate Manager. Обновление сертификата запускается автоматически, однако в определенных случаях необходимо участие пользователя.
Чтобы добавить пользовательский сертификат, необходимо перейти к сервису Certificate Manager, выбрать на панели слева раздел Сертификаты, нажать кнопку Добавить сертификат и выбрать Добавить пользовательский сертификат.
Далее вам потребуется присвоить добавляемому сертификату имя, загрузить сам сертификат (или цепочку сертификатов), а также секретный ключ.
Важным достоинством сервиса Certificate Manager является возможность автоматизировать выпуск TLS-сертификатов Let's Encrypt. Для этого вам необходимо запросить сертификат в сервисе Let's Encrypt и пройти процедуру проверки прав на домены. После этого Certificate Manager будет управлять этими сертификатами, взаимодействуя с Let's Encrypt самостоятельно.
Let's Encrypt предоставляет TLS-сертификаты со статусом Domain Validation и сроком действия 90 дней. Если вам нужны сертификаты с большим сроком действия или другим статусом (например, Organization Validation или Extended Validation), воспользуйтесь сторонним центром сертификации и используйте пользовательский тип сертификата.
После загрузки или получения сертификата Certificate Manager отображает текущий статус сертификата исходя из его жизненного цикла. Жизненный цикл и набор статусов сертификата зависит от его типа.
Импортированные пользовательские сертификаты всегда находятся в статусе Issued. Это значит, что сертификат получен и может быть использован в сервисах, интегрированных с Certificate Manager.
Сертификаты от Let's Encrypt могут находиться в следующих статусах:
    Validating — запрос на сертификат был создан, сертификат запрошен у Let's Encrypt и ожидает проверки прав на домен.
    Issued — сертификат выпущен и получен.
    Invalid — сертификат не прошёл проверку. Такое сообщение может возникнуть, если процедура проверки прав на домен со стороны Let’s Encrypt не прошла в течение одной недели или завершилась с ошибкой.
    Renewing — сертификат в процессе обновления.
    Renewal_failed — не удалось обновить сертификат.
Сертификаты, которые загружены или выпущены с помощью Certificate Manager, можно использовать для организации защищённого доступа к статическим веб-сайтам, файлы которых размещены в объектном хранилище.
Напомним: чтобы привязать внешний домен к статическому сайту в объектном хранилище и использовать сертификат, нужно, чтобы имя домена совпадало с именем бакета, в котором хранятся файлы. Например, для сайта aibolit.ru имя бакета должно быть aibolit.ru. После того, как сертификат появился в Certificate Manager, достаточно перейти в раздел HTTPS настроек бакета и выбрать нужный сертификат в пункте Certificate Manager.
Task:
Какие типы сертификатов поддерживает Certificate Manager?
Decision:
+Пользовательские
-Внешние
+Let’s Encrypt
Task:
Происходит ли автоматическое обновление пользовательских сертификатов с помощью сервиса Certificate Manager?
Decision:
-Да
+Нет
Task:
Практическая работа. Выпуск сертификата для сайта
Decision:
В этой практической работе мы зарегистрируем домен, привяжем его к бакету в объектном хранилище и настроим для этого домена автоматический выпуск сертификата с помощью Certificate Manager.
Шаг 1
Если у вас нет своего домена, зарегистрируйте временный домен, например, на сайте freenom.com:
    Проверьте на сайте доступность имени, которое вы придумали для своего домена.
    Введите имя вместе с доменом верхнего уровня, например testpracticum2022.ml, иначе при попытке зарезервировать домен сервис будет сообщать, что домен занят.
    Если это имя доступно, добавьте домен в корзину и укажите свой email для подтверждения.
    Проверьте почту и подтвердите регистрацию домена.
    Обновите страницу с заказом.
    После подтверждения регистрации домена зайдите в объектное хранилище (Object Storage) и создайте новый публичный бакет. Его название должно совпадать с полным названием домена.
    Переключите доступ на чтение объектов в Публичный. Загрузите в бакет файлы статического сайта (вы можете воспользоваться файлами из практической работы курса «Хранение и анализ данных».
    Выберите на панели управления раздел Веб-сайт, переключите бакет в режим Хостинг и нажмите Сохранить.
Шаг 2
Настроить защищённый доступ к бакету можно двумя способами: загрузить сертификат прямо в бакет или с помощью Certificate Manager. Воспользуемся вторым способом.
    В консоли управления перейдите в сервис Certificate Manager. Для выпуска сертификата с помощью этого сервиса подтвердите владение доменом: в разделе Сертификаты нажмите кнопку Добавить сертификат и выберите Сертификат Let’s Encrypt.
    В открывшемся окне задайте имя создаваемого сертификата и заполните поле с именем вашего домена. Нажмите кнопку Создать.
Сервис автоматически направит запрос на создание сертификата, а домен перейдёт в статус проверки.
    Для выпуска сертификата необходимо подтвердить статус владения доменом. Откройте страницу с деталями запроса на сертификат:
На этой странице для нас важны два поля: имя DNS-записи и её значение. Если вы создавали домен на freenom.com, то перейдите в личный кабинет на этом сайте, выберите раздел Services → My Domains и нажмите кнопку Manage Domains:
Выберите Manage Freenom DNS:
В открывшемся редакторе добавьте TXT-запись для подтверждения владения доменом. В качестве названия записи задайте _acme-challenge без полного названия домена. В качестве значения TXT-записи — значение со страницы проверки прав на домен в консоли управления Yandex Cloud.
Аналогично внесите значение CNAME-записи со страницы проверки прав на домен в консоли управления Yandex Cloud.
Добавьте также запись CNAME для привязки поддомена WWW к вашему бакету:
В поле Target укажите полное имя бакета, включая .website.yandexcloud.net. Сохраните сделанные изменения.
Если вы используете собственный домен, задайте параметры DNS в настройках вашего DNS-сервера. Для применения настроек DNS потребуется некоторое время — обычно до 15 минут.
После окончания проверки домена Certificate Manager автоматически выпустит сертификат.
Шаг 3
Теперь настроим доступ к сайту, то есть к созданному бакету, по протоколу HTTPS с помощью сертификата. Для этого перейдите в раздел HTTPS и нажмите кнопку Настроить.
В поле Источник выберите Certificate Manager, в поле Сертификат — ранее выпущенный сертификат. Нажмите кнопку Сохранить.
Теперь ваш сайт доступен по протоколу HTTPS. Чтобы проверить это, откройте его в браузере. В адресной строке браузера должен отображаться значок защищённого соединения.
Decision:

Task:
Ключи шифрования. Сервис KMS
Decision:
Чтобы сохранить чувствительную информацию закрытой от тех, для кого она не предназначена, нужно прежде всего грамотно настроить к ней доступ. Следующим эшелоном защиты является шифрование хранящихся и передаваемых данных. Это поможет сохранить их конфиденциальность и целостность, даже если в вашу систему кто-то несанкционированно проник.
Современные криптографические алгоритмы позволяют надёжно защитить информацию. Чтобы расшифровать похищенные данные, злоумышленнику потребуются мощные суперкомпьютеры и годы работы. Для шифрования в этих алгоритмах используется ключ — последовательность сгенерированных случайным образом символов. Чем длиннее ключ, тем сильнее защита.
Существуют симметричные и асимметричные алгоритмы шифрования. В симметричных данные шифруются и расшифровываются одним и тем же ключом. Ассиметричные методы построены на использовании двух ключей: открытого и закрытого (секретного). Открытый ключ нужен для шифрования, а закрытый — для расшифрования. Данные устойчивы к атакам методом перебора, если они зашифрованы с использованием ключа длиной не менее 128 бит для симметричных алгоритмов и не менее 1024 бит для асимметричных. Сами алгоритмы при этом, естественно, должны быть современными и актуальными.
На надёжность защиты влияет не только длина ключа. Важно и то, где и как хранятся ключи. Это считается одним из наиболее уязвимых мест систем защиты, основанных на шифровании. Ведь если скомпрометирован ключ, то скомпрометированными оказываются и зашифрованные им данные. Злоумышленник сможет не только прочитать их, но и незаметно изменить нужным ему образом. Это приводит к необходимости решения практической задачи: как хранить ключи безопасно?
Предположим, что у нас есть единое хранилище данных, к которому обращаются несколько сервисов. Для защиты данных мы используем симметричную криптографию (она намного быстрее и требует меньше вычислительных ресурсов). Это значит, что у каждого сервиса должен быть доступ к ключу шифрования.
Хранить ключ в открытом виде на всех серверах, где запущены сервисы, рискованно. Если злоумышленник получит доступ хотя бы к одному из них, он сможет отыскать ключ в настройках, что скомпрометирует всю систему. Более надёжный вариант — зашифровать ключ шифрования данных DEK (Data Encryption Key) c помощью ещё одного ключа KEK (Key Encryption Key). Теперь даже если злоумышленник и найдёт на взломанном сервере зашифрованный ключ DEK, то расшифровать данные в хранилище он сможет далеко не сразу. Подобная схема защиты называется envelope encryption.
Чтобы использовать такую схему, нам придётся решить две проблемы: где хранить ключ KEK, чтобы минимизировать вероятность его компрометации, и как безопасно пользоваться открытой версией ключа DEK.
Одно из возможных решений — вынести криптографический модуль на отдельную защищённую машину, которая будет отвечать за шифрование и расшифрование всего трафика. Но это далеко не всегда оптимально. Причина в том, что так мы обеспечиваем конфиденциальность и целостность данных за счёт их доступности. Система с этим модулем не является отказоустойчивой — если сервер, на котором развёрнут криптографический модуль, выйдет из строя, то и система перестанет работать. Более того, такая система может масштабироваться только вертикально, то есть путем увеличения мощностей единственного сервера. Следовательно, мы получим проблемы с производительностью, если понадобится шифровать большие потоки данных.
Второй вариант — использовать специальный сервис KMS, который:
    надёжно хранит ключ KEK и никуда его не передаёт;
    после получения запроса раскодирует зашифрованный ключ DEK и отправляет его пользователю.
В общем виде схему работы этого сервиса можно изобразить следующим образом.
Зашифрованный ключ DEK хранится на диске рядом с зашифрованными данными (шифртекстом). Открытый ключ DEK не сохраняется на жёсткий диск на сервере пользователя, а находится в оперативной памяти. Для его надёжной защиты можно применять дополнительные ухищрения, например замаскировать с другим случайным блоком памяти. Всё это усложняет получение ключа DEK для злоумышленника и значительно затрудняет атаку на ключ KEK.
Важно! Открытый ключ DEK должен использоваться только во время выполнения операций шифрования и расшифрования данных, а после завершения этих операций его нужно сразу уничтожить.
На этом принципе работы построен сервис управления ключами Yandex KMS (Key Management Service). Давайте рассмотрим, для чего он предназначен, подробнее.
Прежде всего KMS нужен для того, чтобы генерировать ключи шифрования. Сделать это можно через консоль управления, с помощью CLI, а также REST или gRPC API. Сервис поддерживает симметричное шифрование с алгоритмами AES-128, AES-192 и AES-256) (число в названии алгоритма обозначает длину ключа в битах).
У созданного в сервисе ключа есть следующие параметры:
    идентификатор — уникален в рамках всего Yandex Cloud и используется для работы с ключами с помощью SDK, API и CLI;
    название — может быть не уникально и используется для работы с ключами с помощью CLI (только если в каталоге есть лишь один ключ с таким названием);
    используемый алгоритм шифрования;
    период ротации — промежуток времени между автоматической сменой ключа;
    статус — состояние ключа (Creating, Active или Inactive).
Чтобы сделать защиту более надёжной, срок действия ключа ограничивают. После истечения этого срока сервис автоматически создаст новую версию ключа, то есть новый ключ с такими же параметрами (отличаться будут лишь ID версий). Этот процесс называется ротацией ключа. Ротировать ключи можно и вручную.
Версия ключа, созданная при ротации, становится основной (Primary) и по умолчанию используется для операций шифрования. Шифровать данные можно и с помощью любой другой активной версии, указав её ID в запросе. Старые версии ключа используются для расшифровки данных, которые были зашифрованы с их помощью.
Для всех версий ключа, кроме основной, можно настроить время удаления. Когда версия ключа удалена, расшифровать данные, зашифрованные с её помощью, невозможно. Поэтому сервис удаляет версию ключа не сразу, а через определённое время (по умолчанию это два дня). Когда ключ находится в статусе Scheduled for Destruction (запланирован к удалению), воспользоваться им для расшифровки данных уже нельзя. Процесс удаления можно отменить. Кроме того, чтобы избежать потери данных, ключ можно защитить от удаления, выбрав параметр Защита от удаления.
С помощью KMS можно шифровать и расшифровывать данные. Если эти операции выполняются на стороне сервиса, то объём данных не может превышать 32 килобайта. Это обусловлено необходимостью ограничить нагрузку на сервис, чтобы обеспечить его высокую производительность. В качестве шифруемых данных могут выступать, например, секреты или сессионные ключи шифрования, что может быть использовано в пользовательских приложениях. Шифрование и расшифрование выполняются с помощью методов encrypt и decrypt REST API.
Если нужно шифровать большие объёмы данных, то сервис можно использовать по схеме envelope encryption. Ограничений на объём шифруемых данных в этом случае нет, поскольку операции шифрования и расшифрования выполняются в основном на стороне пользователя.
Шифрование происходит следующим образом.
    Пользователь самостоятельно генерирует ключ DEK и шифрует им данные на своей машине.
    Пользователь направляет в KMS запрос encrypt на шифрование DEK.
    Ключ KMS, которым выполняется шифрование DEK, выступает в роли ключа KEK.
    Сервис возвращает зашифрованный DEK.
    Пользователь сохраняет зашифрованный DEK рядом с шифртекстом и уничтожает незашифрованный DEK.
Для расшифрования пользователь считывает зашифрованный DEK, выполняет запрос к KMS на его расшифровку, получает от сервиса расшифрованный DEK и расшифровывает с его помощью данные. После использования расшифрованный DEK нужно уничтожить.
Сервис KMS используют в следующих сценариях:
    для шифрования данных с помощью утилиты yc (CLI);
    в пользовательских приложениях через REST или gRPC API;
    для шифрования данных в объектном хранилище;
    для защиты секретов при использовании сервиса управления кластерами Kubernetes.
На следующем уроке вы потренируетесь использовать сервис KMS, чтобы управлять ключами и шифровать данные.
Task:
Каких целей информационной безопасности можно достичь с помощью сервиса KMS?
Decision:
+Целостность
-Доступность
+Конфиденциальность 
Task:
Как называется подход, при котором ключ шифрования данных шифруется дополнительным ключом?
Decision:
-secondary encryption
-asymmetric encryption
+envelope encryption
Task:
Можно ли расшифровать данные с использованием помеченной на удаление версии ключа?
Decision:
-Да
+Нет
Task:
Практическая работа. Создание и ротация ключей шифрования
Decision:
На прошлом уроке вы познакомились с возможностями сервиса управления ключами шифрования KMS. В этой практической работе вы научитесь создавать ключи шифрования и управлять ими, а также использовать эти ключи для шифрования и расшифрования данных.
Шаг 1
Перейдите в панель управления Yandex Cloud, нажмите кнопку Создать ресурс и выберите из выпадающего списка пункт Ключ шифрования.
Задайте для создаваемого ключа имя (например yc-lab-key1), заполните поле Описание (это необязательно) и выберите алгоритм шифрования. Предположим, что ключ нужно ротировать каждый день. Для этого в поле Период ротации, дни выберите вариант Своё значение и введите число 1 в поле справа.
Нажмите кнопку Создать. Когда операция создания ключа завершится, новый ключ появится в списке.
Нажав на строку с ключом, вы перейдёте на страницу детальной информации. На ней приведены все параметры ключа, а также список его версий. Обратите внимание, что ID (идентификатор) ключа и ID конкретной версии ключа отличаются. Важно их не путать.
Шаг 2
Давайте используем созданный ключ для шифрования и расшифрования данных. Создайте у себя на диске файл (например, текстовый файл с именем plain.txt). Добавьте в него любой текст и сохраните содержимое. Напомним, что размер файла не должен превышать 32 килобайта.
Запустите утилиту командной строки (bash или cmd) и перейдите в каталог с файлом plain.txt. Зашифруйте этот файл с помощью утилиты yc, а результат операции шифрования выведите в файл encrypted.txt. Для этого выполните команду:
yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted.txt 
После выполнения команды будет создан файл encrypted.txt, который содержит зашифрованный текст. Утилита yc также выведет информацию о том, каким ключом и какой его версией файл был зашифрован.
Шаг 3
Теперь расшифруйте этот файл, а результат операции выведите в файл decrypted.txt. Для этого выполните команду:
yc kms symmetric-crypto decrypt --id <ID ключа> --ciphertext-file encrypted.txt --plaintext-file decrypted.txt 
В результате выполнения команды будет создан файл decrypted.txt с идентичным исходному файлу (plain.txt) содержимым.
Если расшифровать файл не удалось, утилита выдаст сообщение об ошибке.
Шаг 4
Создайте новую версию ключа. Для этого перейдите на страницу детальной информации о ключе и нажмите кнопку Ротировать. Новая версия ключа появится в списке версий и станет основной (Primary). Обратите внимание, что идентификаторы версий отличаются друг от друга.
Шаг 5
Запланируйте удаление первой версии ключа. Для этого в списке версий нажмите на значок … в строке с этой версией, а затем выберите пункт Запланировать удаление.
В появившемся окне установите время, по истечении которого ключ будет удалён, и нажмите Запланировать. Версия ключа не может быть удалена моментально, минимальный период времени для её удаления составляет один день.
После этого в списке версий удаляемый ключ будет помечен как запланированный на удаление (Scheduled For Destruction). Теперь этой версией ключа невозможно расшифровать файлы, которые были зашифрованы с её помощью.
Провести ротацию ключа можно и из командной строки. Для этого используется команда:
yc kms symmetric-key rotate <ID ключа> 
Шаг 6
Зашифруйте исходный файл plain.txt с помощью новой версии ключа. Результат запишите в файл encrypted_with_new_key.txt.
yc kms symmetric-crypto encrypt --id <ID ключа> --plaintext-file plain.txt --ciphertext-file encrypted_with_new_key.txt 
Теперь у вас есть два файла:
    encrypted.txt, зашифрованный версией ключа, которая помечена на удаление;
    encrypted_with_new_version.txt, зашифрованный новой версией ключа.
Попробуйте расшифровать данные из обоих файлов. Вы увидите, что расшифровать первый файл не получилось, а файл, который зашифрован второй версией ключа, расшифрован.
Запланированное удаление первой версии ключа можно отменить. Это позволит расшифровать данные из первого файла.
В строке версии ключа, которая запланирована на удаление, нажмите значок …, а затем кнопку кнопку Отменить удаление. Эта версия снова получит статус активной. Проверьте, что она работает, расшифровав файл encrypted.txt.
Decision:

Task:
Лучшие практики по работе с ключами
Decision:
На этом уроке вы познакомитесь с лучшими практиками использования ключей шифрования и работы с сервисом KMS.
Аутентификация и авторизация в сервисе KMS
Для доступа к самому сервису используйте IAM-токены. Рекомендуем получать IAM-токен сервисного аккаунта через механизм назначения сервисного аккаунта виртуальной машине. Подробнее см. в документации.
Общий подход к использованию KMS должен заключаться в том, чтобы выдавать пользователям и сервисным аккаунтам гранулярные доступы на конкретные ключи шифрования.
Рекомендуемые криптографические библиотеки
Для шифрования данных на уровне приложений используйте следующие библиотеки:
    AWS Encryption SDK и его интеграцию с KMS — для шифрования большого объема данных сложной структуры;
    SDK Yandex Cloud — в более простых случаях.
Криптозащита секретов с помощью KMS
С помощью KMS можно шифровать и расшифровывать нужные для работы приложения секреты (токены доступа, API-ключи и т. п.). Для этого воспользуйтесь такой схемой:
    Подготовьте файл с секретами и зашифруйте его с помощью ключа KMS от имени какого-либо пользователя.
    Отзовите у пользователя разрешение на проведение операций с этим ключом.
    Создайте для приложения сервисный аккаунт и выдайте ему роль kms.keys.encrypterDecrypter, то есть разрешение на использование ключа шифрования.
    При запуске приложения получите IAM-токен из сервиса метаданных и расшифруйте секреты в KMS.
    Используйте секреты в памяти приложения и сохраните на RAM-диск (tmpfs) или используйте механизм безопасности операционной системы.
Работа с секретами и ключами в Terraform
Когда вы используете KMS вместе с Terraform, то учтите, что секреты хранятся в открытом виде в файлах конфигураций. Кроме того, секреты в открытом виде попадут и в файлы state (.tfstate), в которых Terraform хранит информацию об управляемых ресурсах. Поэтому для обеспечения безопасности может потребоваться защищать как сами файлы конфигурации, так и state-файлы.
При работе с KMS в Terraform рекомендуется использовать блок lifecycle в файлах конфигурации для предотвращения удаления ключа и возможной потери данных. Подробнее об управлении ключами KMS с Terraform.
Ротация ключей KMS
Ротация ключей шифрования - одна из основных практик обеспечения информационной безопасности. Некоторые стандарты безопасности (например PCI DSS) напрямую требуют, чтобы ключи ротировались на регулярной основе.
Ротация ключей важна в силу необходимости ограничивать объём информации, зашифрованной одной и той же версией ключа. Чем больше такой объём, тем выше риск, что криптоанализ зашифрованных данных позволит провести успешную атаку на них. Поэтому чем чаще используется ключ, тем чаще нужно ротировать его версии.
Также для повышения уровня безопасности рекомендуется делить ключи на две группы:
    Ключи для сервисов (например Yandex Message Queue или Yandex Functions), которые обрабатывают, но не хранят чувствительные данные.
    Ключи для сервисов, которые хранят такие данные (например сервисы платформы данных).
Для первой группы ключей нужно настроить автоматическую ротацию ключей с периодом ротации чуть больше, чем срок обработки данных в используемом сервисе. По истечении периода ротации старые версии ключей должны быть удалены.
Для сервисов хранения данных способ ротации ключей зависит от внутренних процедур обработки чувствительных данных. Здесь ключи шифрования можно ротировать и вручную.
Важно! Помните, что при удалении старых версий ключа обработанные ими данные не смогут быть восстановлены и расшифрованы.
Task:
Чтобы назначить роли, необходимые для управления ключами шифрования, используется сервис:
Decision:
-KMS
+IAM
-RBAC
Task:
Может ли пользователь работать с ресурсами вашего облака, не имея в нем роли resource-manager.clouds.member?
Decision:
-Да, если у него есть роль admin
+Да, если он имеет доступ только к публичным ресурсам
-Нет
Task:
К привилегированным относятся пользователи, имеющие следующие роли:
Decision:
-resource-manager.clouds.member
+billing.accounts.owner
-container-registry.admin
+resource-manager.clouds.owner
Task:
Какие правовые документы определяют требования к защите персональных данных?
Decision:
-ГОСТ Р 57580.1‑2017
+GDPR
+Федеральный закон №152-ФЗ Российской Федерации
-PCI DSS
+приказ ФСТЭК России от 18.02.2013 № 21
Task:
Федерация удостоверений позволяет:
Decision:
-Провести аутентификацию пользователя с учетной записью Яндекс ID
+Предоставить доступ к облаку пользователю, который прошел аутентификацию через поставщика удостоверений
-Выполнять операции с облачными ресурсами от имени приложений
Task:
Если ключ шифрования помечен, как запланированный к удалению, то:
Decision:
-воспользоваться им для расшифрования данных можно, пока он не удален
-воспользоваться им для расшифрования можно, но только для тех данных, которые были им зашифрованы
+воспользоваться им для расшифрования нельзя
Task:
Модель Pay as you Go, понятие SKU
Decision:
Вы уже умеете создавать безопасные и отказоустойчивые системы в Yandex.Cloud (см. курсы «DevOps и автоматизация» и «Безопасность»). Но по-настоящему эффективная система ещё и экономически выгодна.
Чтобы создать такую систему, нужно понимать, как рассчитывается цена ресурсов, уметь мониторить затраты и сокращать их так, чтобы не терять качество.
Всё это вы научитесь делать на курсе о биллинге.
Модель оплаты. Pay as you go
Сначала разберёмся с тем, что влияет на цену.
В Yandex Cloud цена рассчитывается по принципу Pay as you go (PAYG) — «плати, пока пользуешься». Как в каршеринге: пока едешь на арендованной машине — платишь, поездка закончилась — закончили платить. Справедливо!
Например, у вас развёрнуты две виртуальные машины. Одна из них запущена, другая остановлена. Одна потребляет vCPU и RAM, другая — нет. Вы будете платить за vCPU и RAM только одной машины и только пока она работает. Но обе машины, в том числе остановленная, используют диски. Значит, за этот ресурс нужно платить по каждой машине.
Затраты в Yandex Cloud зависят от количества потреблённых ресурсов и времени их использования.
За что списывается оплата. SKU
С принципом оплаты за ресурсы мы разобрались. Но что такое сам ресурс именно с точки зрения биллинга?
Раньше мы называли ресурсом все сущности, которые можно создать в облаке: виртуальные машины, диски, виртуальные сети. Две машины из примера выше — это два ресурса.
Но платёжный аккаунт не воспринимает их как единую сущность, а раскладывает на составляющие: vCPU и RAM. С точки зрения биллинга каждый такой элемент и есть потребляемый ресурс, или единица учёта — SKU (stock keeping unit).
У каждой SKU свои правила тарификации. Их важно знать, чтобы оптимизировать затраты. Мы ещё вернёмся к SKU на уроках о мониторинге.
А дальше давайте вспомним, что такое платёжный аккаунт и как им эффективно пользоваться.
Task:
Какие основные факторы влияют на стоимость ресурсов в Yandex.Cloud?
Decision:
+Количество ресурсов
-Стоимость ресурсов Yandex.Cloud фиксированная и зависит только от установленных тарифов
+Время использования ресурсов
Task:
Платёжный аккаунт, кредитный лимит, гранты и промокоды
Decision:
Вопрос на засыпку: помните, что такое платёжный аккаунт?
Подсказка: это аккаунт не для управления облаком, а для… Точно: для оплаты ресурсов. Вы создали его в начале курса о виртуальных машинах и больше не обращались к этой теме. Теперь пришло время разобраться в ней подробнее.
Как выбрать тип аккаунта
В Yandex Cloud есть два типа платёжных аккаунтов: личный и бизнес-аккаунт. Менять тип аккаунта нельзя, поэтому сразу выбирайте тот, что подходит именно вам.
Личный аккаунт — для физических лиц. Нужно лишь привязать банковскую карту — и готово. Но его могут завести только резиденты России и Казахстана.
Бизнес-аккаунт — для организаций из России, Казахстана и ещё более чем 40 стран. Здесь будут доступны подробные финансовые документы и оплата переводом с расчётного счёта, а не только картой. Способ оплаты можно поменять в любой момент.
Тип аккаунта    Для кого    Способ оплаты   Документы об оплате
Личный аккаунт  Физические лица: резиденты РФ и РК  Банковская карта    Чек
Бизнес-аккаунт  Юридические лица: резиденты и нерезиденты РФ и РК   Банковская карта или перевод с расчётного счёта Акт и счёт-фактура
Как управлять аккаунтом
Когда у вас одно облако — справиться с оплатой его ресурсов несложно. Но что делать, если под вашим управлением много облачных систем?
Во-первых, к платёжному аккаунту можно привязать несколько облаков. Так проще платить за ресурсы, а статистика о затратах будет собираться в одном месте.
Во-вторых, почему вы должны делать всё сами? Предоставьте доступ к аккаунту пользователям и назначьте им роли. Например, предоставьте бухгалтеру доступ к платёжному аккаунту облака с ролью admin, а в самом облаке оставьте доступ с минимальными привилегиями — resource-manager.clouds.member. Тогда он сможет смотреть детализацию расходов, а вот сломать что-нибудь в облаке ему не удастся. Бухгалтер будет изучать затраты, а вы займётесь техническими вопросами. Все довольны!
Ещё одна подсказка. Если вы работаете с несколькими компаниями — заведите отдельный платёжный аккаунт для каждого клиента. Тот же совет годится на случай, если вам нужно оплачивать ресурсы с разных банковских карточек. При желании вы сможете привязать облако к другому платёжному аккаунту.
Как оплачивать ресурсы
У каждого платёжного аккаунта есть лицевой счёт. Вы зачисляете туда деньги, а Yandex Cloud списывает их по мере потребления ресурсов.
Но ситуации бывают разные. Представьте, что у вас инстанс-группа с двумя ВМ. На них развёрнут сайт чулочно-носочного комбината. Обычно это облако обходится в 1000 ₽. Но незадолго до 23 февраля нагрузка на сайт резко возросла. Как вы знаете, в такой ситуации Instance Groups автоматически создаст ещё одну ВМ в помощь. Потребление ресурсов увеличится, и 1000 ₽ на лицевом счёте явно не хватит, чтобы всё оплатить.
Неужели виртуальные машины отключатся, а сайт упадёт на пике продаж?
Нет. Вас спасет порог оплаты — допустимый отрицательный баланс лицевого счета. Он действует только если в качестве метода оплаты выбрана оплата картой. Размер порога оплаты рассчитывается индивидуально, а посмотреть его можно в консоли управления.
Когда наступит конец месяца или кредит будет исчерпан, Yandex Cloud попробует списать деньги с привязанной карты. И только если это сделать не получится, последует блокировка облака на 60 дней с последующим удалением ресурсов.
Этот алгоритм помогает в экстренных случаях. Но постоянно полагаться на него не стоит. Если вам не хватит кредита или списание с карты не пройдёт из-за проблем с банком — вы останетесь без ресурсов.
Лучший способ обезопаситься от блокировки — постоянно поддерживать положительный баланс лицевого счёта.
С полной схемой цикла оплаты для физических лиц и для юридических лиц вы можете ознакомиться в документации.
Гранты
Грант — это скидка на оплату сервисов Yandex Cloud. При оплате ресурсов сначала тратится грант, а потом те деньги, которые вы зачислили на лицевой счёт.
Грант можно получить в виде промокода во время рекламной акции или как специальное предложение от отдела продаж для тестирования пилотного проекта в Yandex Cloud. Если в работе платформы случаются сбои (надеемся, вы с ними никогда не столкнётесь), техподдержка начисляет грант в качестве компенсации.
Все гранты отображаются в консоли отдельно от основной суммы:
Грант сгорает, когда истекает срок действия. Если у вас несколько грантов, первым будет применён тот, который заканчивается раньше всех.
Фух... Вам не кажется, что теории уже достаточно? Давайте разомнём пальцы и на следующем уроке потренируемся прогнозировать расходы в калькуляторе тарифов!
Task:
Сколько облаков можно привязать к одному платёжному аккаунту?
Decision:
-Одно облако
+Любое количество облаков
-Бесплатно — три облака, потом любое количество за деньги
Task:
Вы физическое лицо из Киргизии. Какой тип платёжного аккаунта вы можете открыть?
Decision:
-Личный аккаунт. Он подходит для физических лиц.
-Бизнес-аккаунт. Нерезидентам РФ и РК можно иметь только такой тип аккаунта.
+Никакой. Yandex Cloud не работает с физическими лицами — нерезидентами РФ и РК.
Task:
Что произойдёт, если баланс лицевого счёта станет нулевым?
Decision:
+Начнётся использование кредитного лимита. Когда он будет исчерпан, Yandex Cloud попробует списать деньги с привязанной карты.
-Ничего не произойдёт. С вами просто свяжется техподдержка.
-Ресурсы будут заблокированы до пополнения лицевого счёта.
Task:
Практическая работа. Калькулятор расходов
Decision:
Любая стройка начинается со сметы. Облачная архитектура не исключение — прежде чем разворачивать систему, нужно просчитать её стоимость.
Чтобы сделать это в Yandex Cloud, откройте калькулятор тарифов.
Задание 1. Работа с калькулятором
Давайте для начала посчитаем, во сколько рублей в месяц обойдётся система со следующими параметрами:
Compute Cloud: Ubuntu 20.04 LTS, Intel Broadwell, 4 vCPU 100%, 16 ГБ RAM, SSD 100 ГБ, без публичного IP-адреса.
Managed Service for PostgreSQL: Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
Техподдержка: тариф «Бизнес» при потреблении 50 тыс. ₽ в месяц.
Сервисы Yandex.Cloud, стоимость которых можно рассчитать, указаны на верхней панели калькулятора:
Выберите Compute Cloud и задайте параметры виртуальной машины. Должно получиться вот так:
Как видите, справа указана стоимость сервиса с детализацией. Если выбрать ещё один сервис на панели вверху — его стоимость добавится в общий лист, а внизу отобразится общая сумма за все сервисы.
Давайте это проверим. Выберите PostgreSQL и укажите нужную конфигурацию: один кластер Intel Cascade Lake, standard, s2.micro, network-ssd, 200 ГБ, два хоста, публичный IP-адрес.
Остался последний сервис — техническая поддержка.
Плата за тарифы «Стандарт» и «Бизнес» зависит от количества потребляемых ресурсов. Тариф «Премиум» может быть дополнен различными услугами и рассчитывается индивидуально.
Узнаем стоимость тарифа «Бизнес» при потреблении ресурсов на 50 тыс. ₽ в месяц:
Это было легко!
Теперь вы знаете, сколько стоит система из трёх сервисов, и можете запустить её — или поэкспериментировать с конфигурациями, чтобы добиться приемлемой стоимости.
Задание 2. Расчёт вручную
А теперь задача со звёздочкой. В калькулятор тарифов пока добавлены не все сервисы Yandex Cloud. Стоимость некоторых придётся рассчитывать вручную. Потренируемся это делать.
Узнаем, например, сколько стоит сервис Monitoring. Прокрутите страницу калькулятора вниз и в блоке Инфраструктура и сеть выберите Monitoring.
Да, вы попадёте в документацию, но такова жизнь. Здесь указаны цены ресурсов. Со временем они могут меняться, поэтому для решения задачи ниже берите цены за август 2022 года:
Ваша задача: рассчитать стоимость месячного использования сервиса при записи 35 метрик с частотой два значения в минуту.
Подсказка: используйте раздел «Пример расчёта стоимости» как шпаргалку.
Укажите полученную сумму:
-65,03 ₽
-3 021 ₽
+21,168 ₽
-205,07 ₽
Decision:

Task:
Мониторинг затрат в консоли управления
Decision:
Теперь вы знаете, как и за что платят в Yandex Cloud. Но этого мало. Чтобы получить от облака максимум пользы за минимум средств, надо анализировать затраты.
Если вы всю жизнь писали код и никогда не работали с финансами — не пугайтесь. Для этого не нужно быть бухгалтером. Yandex Cloud даёт полную детализацию расходов и визуализирует её на удобных графиках.
Вот три способа контролировать затраты в Yandex Cloud:
Детализация затрат в консоли.
Использование DataLens.
Отгрузка детализации в в формате CSV.
Первый способ самый простой. С него и начнём.
Детализация затрат в консоли
Чтобы узнать, сколько ресурсов потребил сервис, зайдите в консоль управления и выберите Биллинг → Нужный платёжный аккаунт → Детализация.
Фильтр К оплате показывает стоимость ресурсов с учётом грантов (если они у вас есть), а фильтр Стоимость потребления — фактический расход ресурсов:
На графике можно посмотреть детализацию по одному или нескольким облакам и выбрать отрезок времени. Общая информация за выбранный период сводится в таблице под дашбордом.
Наведите указатель на график — вы увидите, сколько заплатили за каждый из ресурсов в определённый день.
Детализация может отображать не только стоимость сервиса, но и его процент от общего потребления. Эти данные будут в соседней колонке, и их удобно сравнивать:
Чтобы сделать так — нажмите третью кнопку тулбара с изображениями диаграмм:
Но и такое представление не всегда удобно. Что, если у вас несколько ВМ с разным объёмом RAM и vCPU? Стоят они по-разному, но вы видите только общую сумму за всё. Тут хочется видеть информацию не просто о сервисе (например Compute Cloud), а о каждой SKU.
Для этого выберите другой тип группировки — по продуктам:
Так вы получите подробный отчёт о каждой SKU:
Теперь вы легко проконтролируете расходы в Yandex Cloud и оцените, насколько они оправданны. Возможно, вы захотите выбрать другую конфигурацию сервиса или вообще отказаться от него. Главное — ваше решение будет взвешенным, ведь оно основано на анализе данных.
Мониторинг в консоли подходит, когда не нужно углубляться в частности: кто и сколько ресурсов потребил. Если же вы управляете облаками с развитой инфраструктурой, которой пользуются несколько отделов компании, то уследить за всем в консоли будет сложно.
В таких случаях лучше выбрать DataLens или настроить выгрузку детализации в CSV-файл. На следующем уроке вы научитесь пользоваться этими инструментами.
Task:
Какие бывают способы мониторинга затрат в Yandex.Cloud?
Decision:
+Просмотр детализации затрат в консоли
-Просмотр истории платежей
+DataLens
+Отгрузка детализации в формате CSV
-Калькулятор тарифов
Task:
Группировку по продуктам стоит использовать, если...
Decision:
+В облаке несколько продуктов одного сервиса, но с разными конфигурациями
-В платёжном аккаунте несколько облаков, но с разными сервисами
+Вы хотите узнать подробную детализацию по каждой SKU
-Вы хотите посмотреть не только стоимость сервиса, но и его процент от общего потребления
Task:
DataLens для контроля затрат. Экспорт детализации
Decision:
DataLens — это знакомый вам сервис визуализации данных. Для биллинга он удобен тем, что позволяет следить за каждым ресурсом. Например, за конкретной виртуальной машиной.
Чтобы начать мониторить затраты в DataLens, на странице подключений нажмите кнопку Создать подключение → Yandex Cloud Billing.  Сервис предложит автоматически создать дашборд, чарты и датасет над подключением — согласитесь с этим и нажмите кнопку Создать подключение. Вам останется дать подключению имя и ещё раз нажать Создать. Дашборд будет называться Yandex Cloud Billing Dashboard:
Возможности DataLens для мониторинга затрат
По сравнению с мониторингом в консоли управления у DataLens есть несколько преимуществ:
Метки (labels). Они позволяют следить за потреблением конкретных виртуальных машин, а не всего сервиса Compute. Если проводите тест на ВМ — повесьте на неё метку, чтобы знать, во сколько обойдутся испытания. Таким же образом легко найти самую затратную машину в облаке.
Вся информация о метках отображается в разделе Labels:
Детализация на уровне каталогов и ресурсов. В DataLens можно следить за потреблением каждого каталога или ресурса.
Например, ваша компания работает с несколькими проектами. Для каждого в отдельном каталоге созданы ВМ. На вкладке Folders вы увидите, как распределяются затраты и потребление по всем проектам. Таблица под графиками покажет детальное потребление SKU в каждом каталоге:
Сравнение данных аккаунтов. Это пригодится, если вы работаете с несколькими компаниями и ведёте много платёжных аккаунтов. DataLens покажет потребление по всем аккаунтам сразу на одном дашборде (вкладка Cross-account reports):
Вы можете выгрузить данные любого чарта в форматах CSV или XLSX или настроить регулярный экспорт детализации в этих форматах в Object Storage.
Чем полезен экспорт детализации?
CSV-файл даёт полную свободу действий: вы можете обрабатывать его автоматически исходя из своих задач.
Представьте, что ваша компания работает с 3D-графикой. У неё десятки ВМ для рендеринга и столько же заказчиков. Для каждого требуется создавать счета. Как разобраться, сколько ресурсов ушло на задачу клиента? Очень просто: маркируете машины метками по принадлежности к клиенту, выгружаете данные потребления в CSV-файл и на основе этих данных рассылаете счета за услуги.
Настроить экспорт нужно всего лишь один раз. Каждый день в бакете Object Storage будет автоматически создаваться файл с детализацией.
Теперь вы знаете о трёх способах контролировать затраты в Yandex Cloud — мониторинг в консоли управления, с помощью DataLens и путем выгрузки данных в формате CSV для последующего анализа. На следующем уроке вы научитесь анализировать полученные данные.
Task:
В DataLens можно следить за потреблением:
Decision:
+По облакам
+По каталогам
+По сервисам
+По ресурсам
+Конкретной виртуальной машины
Task:
В чём преимущество экспорта детализации в CSV-файл?
Decision:
+Обработку CSV-файлов можно автоматизировать
-Выгрузка детализации в Object Storage показывает потребление по продуктам
-Данные о потреблении обновляются чаще, чем в консоли управления и DataLens
Task:
Анализ потребления
Decision:
Детализация расходов нужна не только для того, чтобы узнать, сколько предстоит заплатить за услуги Yandex Cloud. Это ещё и инструмент сокращения затрат.
Пользователи часто совершают одну и ту же ошибку: разворачивают систему в облаке и не обращают внимания на то, как потребляются ресурсы. В то же время грамотный анализ потребления поможет найти ошибки, приводящие к переплатам, и устранить их.
На что уходит больше всего денег?
Допустим, у вас приложение для заказа такси. Вы решаете протестировать новую функциональность. Это вызывает всплеск потребления Managed Service for ClickHouse, тогда как потребление других сервисов остаётся примерно на прежнем уровне.
Теперь вы можете понять, насколько ваша идея экономически оправданна, и решить, стоит ли её запускать. А может, лучше выбрать другую базу данных и платить меньше?
Какие ресурсы работают вхолостую?
Бывает полезно сравнивать данные биллинга с данными Yandex Monitoring. Посмотрите, как неэффективно тратятся деньги:
Здесь расходы (вверху) соотнесены с загрузкой CPU (внизу) по дням недели. Каждый день пользователь оплачивает мощные ВМ, готовые в любой момент обеспечить пиковую нагрузку. Но пик был только в среду. Если такое повторяется регулярно — возможно, для других дней стоит изменить конфигурацию ВМ. Как это сделать с помощью Instance Groups, вы уже знаете.
А о том, как найти самую дорогую ВМ в системе с помощью меток, вы узнаете из следующего урока.
Примечание
Перед началом следующего урока создайте четыре ВМ — с 5, 20, 50 и 100% vCPU и сделайте небольшой перерыв. Нужно как минимум 15 минут, чтобы вы увидели их потребление на дашбордах.
Task:
Практическая работа. Поиск самой затратной виртуальной машины
Decision:
Примечание
Перед началом этого урока создайте четыре виртуальные машины — с 5, 20, 50 и 100% vCPU — и сделайте небольшой перерыв. Нужно как минимум 15 минут, чтобы вы увидели их потребление на дашбордах.
В боевых проектах вам часто придётся искать самую большую статью расходов, а потом придумывать, что с ней делать. Сейчас вы научитесь маркировать ВМ метками, чтобы понять, какая из них обходится дороже всего. Такими же метками можно размечать ресурсы и других сервисов Yandex Cloud.
Если у вас ещё нет интерфейса командной строки Yandex Cloud
Воспользуйтесь инструкцией, чтобы установить и инициализировать интерфейс командной строки (CLI) Yandex Cloud.
Универсальный шаблон команды добавления метки выглядит следующим образом:
 yc <имя сервиса> <тип ресурса> add-labels <имя или идентификатор ресурса> --labels <имя метки>=<значение метки>
Давайте уточним его для нашей задачи. Имя сервиса — compute, тип ресурса — instance. Идентификатор ресурса вы найдёте в консоли на странице обзора ВМ:
Имя метки и значение метки задайте на свой вкус: можно использовать латиницу, цифры, дефисы и подчёркивания (не больше 63 символов).
Если вы сделали всё правильно, то увидите результат выполнения команды:
Повторите те же действия для трёх оставшихся ВМ.
Когда закончите с метками, сделайте паузу. Системе понадобится несколько минут, чтобы отобразить данные размеченных машин в Yandex DataLens.
В DataLens откройте дашборд Yandex Cloud Billing Dashboard и перейдите на вкладку Labels. Найти самую дорогую ВМ будет несложно:
Устанавливайте метки на те ресурсы, которые требуют пристального внимания, и тогда вам будет легче за ними следить.
Однако самостоятельно следить за потреблением в режиме нон-стоп просто невозможно. Чтобы всегда оставаться в курсе потребления и не уйти в минус при его резком всплеске, нужен автоматический помощник. О нём мы расскажем на следующем уроке.
После практической работы не забудьте удалить ненужные виртуальные машины!
Decision:

Task:
Уведомления о расходах. Бюджеты
Decision:
Мониторинг ресурсов помогает выбрать стратегию потребления. Однако иногда решения нужно принимать на уровне тактики. Например, если происходит неожиданный всплеск потребления.
Представьте, что у вас в облаке развёрнуто приложение для заказа пиццы в Воронеже. На гастроли в город приехал Иван Ургант, сделал у вас заказ и рассказал об этом в инстаграме. Шквал нагрузки обеспечен. Но захотите ли вы за неё платить? Вряд ли, если все обращения окажутся из Москвы: никто ничего не купит, и ресурсы облака будут потрачены зря. Но если случится наплыв воронежских посетителей, вы наверняка решите быстрее пополнить лицевой счёт, чтобы система работала бесперебойно.
Как бы там ни было, хочется быстро узнавать о таких всплесках, чтобы принимать правильные решения.
Функция Бюджеты поможет контролировать затраты в облаке. Вы задаёте сумму ежемесячных расходов, а если эта цифра будет превышена, получаете уведомление.
Обратите внимание: если бюджет превышен — после уведомления облако продолжит работу, средства с лицевого счёта будут списываться и дальше.
Чтобы настроить бюджет, перейдите в Биллинг → Платёжный аккаунт → Бюджеты и нажмите кнопку Создать бюджет.
Чтобы уведомления получал ещё кто-то (бухгалтерия, например), такому пользователю нужна роль viewer. Выберите его в поле Уведомить:
Сумма бюджета указывается в валюте платёжного аккаунта. Чтобы учитывались грант или скидка, выберите тип бюджета К оплате. Чтобы считать только фактические расходы, выбирайте Стоимость потребления.
Бюджеты удобны тем, что их можно настроить для облаков, каталогов или даже сервисов. Например, вы тестируете в одном из каталогов новый продукт. Никто точно не знает, сколько он может съесть ресурсов, поэтому лучше подстраховаться и установить бюджет для этой части облака:
Ещё одна полезная настройка — Пороги. Она позволяет получить уведомления, когда достигнут порог, т. е. часть бюджета.
Скажем, вы запустили бета-тест приложения. По вашим расчётам, за две недели оно потребит ресурсов на 5 тыс. ₽. Но что-то пошло не так — эта сумма сгорела за четыре дня. Обидно! Если бы вы настроили бюджет с порогом 50%, то уже на второй день получили бы уведомление, остановили тест, сохранили 2,5 тыс. ₽ и доработали приложение.
Поздравляем, вы завершили тему «Мониторинг затрат»
Теперь вы знаете, как отслеживать затраты, и умеете контролировать их с помощью бюджетов. Дальше поговорим о том, как экономить на сервисах Yandex Cloud, не теряя при этом в качестве.
Task:
Что произойдёт, если бюджет будет превышен?
Decision:
-Потребление ресурсов остановится автоматически.
-Средства с лицевого счёта перестанут списываться.
-Облако будет заблокировано.
+Вы получите уведомление. Больше ничего не случится.
Task:
Способы экономии
Decision:
Чтобы выгодно работать с Yandex Cloud, изучите потребление ресурсов и найдите варианты оптимизации. Первое вы уже умеете, а о способах экономии узнаете на этом и двух следующих уроках.
Виртуальные машины
Кратковременные задачи. Для тестов или рендеринга видео используйте прерываемые ВМ. Для примера: стоимость 12 часов работы 100 обычных машин (48 vCPU, 96 RAM) — около 65 тыс. ₽. Аналогичные прерываемые обойдутся примерно в 17 тыс. ₽. Важно помнить, что прерываемые машины дешевле, так как могут остановиться в любой момент. Мы подробно говорили об этом на уроке о прерываемых ВМ.
Совет: Чтобы пользоваться прерываемыми ВМ больше 24 часов и экономить дольше, автоматизируйте их перезапуск через Instance Groups. Запускайте ВМ не все сразу, а блоками через каждые 15 минут. Это почти до нуля снизит вероятность того, что все машины остановятся сразу.
Небольшая нагрузка. Покупать 100% vCPU для пилотного проекта или демонстрационного стенда слишком расточительно. Если приложения не требуют высокой производительности — выбирайте 5, 20 или 50% ядра.
Волатильная нагрузка. Бывает, что в один день машины работают вполсилы, а в другой — загружены полностью. При таких явных пиках потребления не стоит постоянно платить за мощные ВМ, которые выдержат максимальную нагрузку. Рациональнее с помощью Instance Groups автоматически масштабировать систему только тогда, когда это действительно нужно.
«Мне только посчитать». Перед тем как создать ВМ, задумайтесь: а нужны ли они вообще? Для чат-бота или микросервиса лучше воспользоваться Cloud Functions и платить только за вычисления, а не за постоянно работающую ВМ.
Хранение данных
Большие объёмы данных, например записи бэкапов или видео с камер наблюдения, лучше не хранить на дисках ВМ. Вам нужно будет обеспечивать отказоустойчивость ВМ и дублировать их, а значит, платить больше. Скажем, чтобы безопасно хранить 1 ТБ данных, придётся платить ещё и за 2 ТБ на дисках машин-дублёров.
Выбирайте для больших данных Object Storage. Сервис изначально отказоустойчив, поэтому при тех же объёмах файлов обойдётся намного дешевле.
Базы данных
Чтобы экономить на БД, воспользуйтесь Managed Services. Наши специалисты администрируют их: отвечают за отказоустойчивость, обновления, техподдержку и мониторинг того, что происходит в БД. Это обходится значительно дешевле, даже если у вас есть свои сотрудники для администрирования БД:
Самостоятельное администрирование   Управляемая БД
Аренда ВМ: ≈ 27 500 ₽ в месяц   Управляемая БД (24 vCPU, 96 RAM, 240 ГБ SSD):\\
≈42 000 ₽ в месяц
Зарплата специалиста: ≈ 100 000 ₽, социальные взносы: 30 000 ₽.\\
(Потребуется два специалиста с 50%-й занятостью для бесперебойной работы на случай болезни, отпуска или увольнения одного из них.)  
≈ 157 500 ₽ в месяц ≈ 42 000 ₽ в месяц
И самое главное, сервисы управляемых БД освобождают от рутинных задач ваших специалистов. Вместо администрирования БД они смогут работать над продуктами компании и приносить намного больше пользы бизнесу.
Как видите, если грамотно использовать ресурсы облака, можно избежать переплат. На следующем уроке мы расскажем, как получить гарантированную скидку, если спрогнозировать потребление на длительный срок.
Task:
Представьте, что у вас интернет-магазин купальников. Количество покупателей растёт летом и падает зимой. Что лучше всего поможет сэкономить?
Decision:
-Управляемые БД
+Instance Groups
-Прерываемые виртуальные машины
Task:
В каких случаях стоит использовать ВМ с гарантированной долей vCPU ниже 100%?
Decision:
+Для пилотного проекта.
-Когда нагрузка на ВМ распределяется неравномерно по времени.
-Если используются средства гранта.
Task:
Почему управляемые БД выгодны, даже если у вас есть сотрудники для их администрирования?
Decision:
-Так можно сэкономить на аренде ВМ.
+Так можно освободить сотрудников для работы над продуктами компании, что принесёт больше прибыли.
+Стоимость управляемых БД значительно меньше зарплат собственных специалистов.
Task:
Гарантированная скидка на резервируемые ресурсы
Decision:
На этом уроке мы расскажем, как можно получить скидку на ресурсы независимо от типа задач.
Некоторые управляемые сервисы Yandex.Cloud стоят значительно дешевле, если резервировать их сразу на длительный срок (один или три года). Это называется резервируемое потребление (committed volume of services, CVoS).
Сравните, например, стоимость Managed Service for PostgreSQL с CVoS и без него:
Работает CVoS так. В разделе биллинга Резервы вы указываете параметры, а консоль рассчитывает размер оплаты и скидки сразу в двух вариантах — для резервирования на один год и для резервирования на три года.
Выбранная цена будет зафиксирована для вас на весь срок. Оплачивать полную стоимость сразу не нужно: средства будут списываться равномерно в течение всего срока.
В итоге вы получаете сразу несколько преимуществ:
цена на ресурсы ниже обычной;
цена останется прежней, даже если изменятся тарифы Yandex.Cloud;
цена не зависит от колебаний курса рубля.
Резервируемое потребление гарантирует скидку на ресурсы, но не гарантирует наличие мощностей в дата-центрах.
Чтобы выяснить, какой объём ресурсов зарезервировать, изучите потребление. Как это сделать, вы уже знаете из уроков о мониторинге. Самое главное: потребление должно быть стабильным и прогнозируемым на долгое время.
Например, у вас интернет-магазин сантехники. Ассортимент редко меняется, и спрос на такой товар не бывает сезонным. Значит, нагрузку на БД легко спрогнозировать.
А что, если через полгода вам понадобится больше мощностей, чем вы зарезервировали? Скажем, было 8 CPU, а теперь нужно 12, потому что вы решили расширить бизнес и продавать не только сантехнику, но и керамическую плитку. Как быть?
Есть два способа оплатить дополнительные ресурсы:
Добавить ресурсы через консоль. Тогда 8 CPU и дальше будут оплачиваться со скидкой, а дополнительные 4 — по стандартным тарифам.
Зарезервировать дополнительно 4 CPU и платить со скидкой за все ресурсы.
Теперь вы знаете всё о резервируемом потреблении. Советы следующего урока — для тех, кто работает с ПО Microsoft. Мы расскажем, как экономить на стоимости лицензирования.
Примечание
Сейчас самостоятельное оформление резервируемых ресурсов через консоль управления приостановлено. Но если ваша компания является клиентом Yandex Cloud, вы можете обсудить возможность резервирования с вашим аккаунт-менеджером.
Task:
От чего зависят затраты в Yandex.Cloud?
Decision:
+От количества ресурсов и продолжительности их потребления. Такая модель оплаты называется Pay as you go.
-От тарифного плана. Это модель оплаты по подписке.
Вы создали две группы виртуальных машин: в первой группе 20 ВМ с 5% vCPU, а во второй — две ВМ с 50% vCPU. Будут ли отличаться затраты на эти группы?
Task:
-Нет: в модели Pay as you go на затраты влияет количество потреблённых ресурсов, а каждая группа ВМ в сумме потребляет по 100% vCPU.
-Нет: стоимость сервиса Compute оплачивается сразу на месяц и не зависит от количества потреблённых ресурсов.
+Да: итоговая стоимость складывается из стоимости всех SKU, каждая из которых тарифицируется по своим правилам.
Task:
В последнее время вы не успеваете следить за затратами в облаке, так как загружены техническими вопросами. Бухгалтер может отслеживать и оптимизировать расходы. Но вы боитесь давать ему доступ к облаку: вдруг он что-нибудь там сломает. Как лучше поступить?
Decision:
-Каждый день делать бухгалтеру скрины с детализацией и отправлять на email.
+Предоставить бухгалтеру доступ к платёжному аккаунту облака с ролью admin, а в самом облаке оставить доступ с минимальными привилегиями — resource-manager.clouds.member.
-Завести для бухгалтера отдельный платёжный аккаунт.
Task:
Как узнать потребление одного кластера Managed Service for MySQL?
Decision:
-Никак. Можно узнать только потребление всего сервиса.
-Настроить группировку детализации в консоли по продуктам.
+Подключить коннектор биллинга в DataLens, перейти на вкладку Resources и следить за потреблением кластера по его идентификатору.
Task:
У вашей компании десять ВМ. Пять из них вы используете для рендеринга видео одного клиента, пять других — для другого. Как автоматически генерировать счета на оплату услуг?
Decision:
-Завести для каждого клиента облако.
-Завести для каждого клиента платёжный аккаунт.
+Обозначить ВМ метками по принадлежности к клиенту и настроить генерацию счетов на основе CSV-файла.
Task:
В каких случаях пригодится функция Бюджеты?
Decision:
-Если у вас небольшой проект и вам необходим бюджетный тариф на услуги Yandex Cloud.
+Если вы тестируете продукт и не знаете, сколько он может потребить ресурсов. Когда сумма бюджета будет достигнута, вы получите уведомление и сможете решить, продолжать ли тест.
-Если вы хотите подстраховаться от переплаты при всплеске потребления ресурсов. Как только сумма бюджета будет достигнута, потребление ресурсов автоматически остановится.
Task:
Как экономить на виртуальных машинах?
Decision:
+Покупать долю vCPU меньше 100%.
+Запускать прерываемые машины.
+Использовать Instance Groups.
-Ставить на ВМ свою ОС.
Task:
В чём преимущество резервирования ресурсов на длительный срок?
Decision:
+Цена на ресурсы формируется со скидкой.
+Цена останется прежней, даже если изменятся тарифы Yandex Cloud.
+Цена не зависит от колебаний курса рубля.
